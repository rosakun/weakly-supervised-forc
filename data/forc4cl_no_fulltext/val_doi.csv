acl_id,Level1,Level2,Level3,abstract,url,publisher,year,month,booktitle,author,title,doi,venue,data_index
L16-1238,"['Text Preprocessing', 'Domain-specific NLP', 'Low-resource Languages', 'Data Management and Generation']","['Part-of-Speech (POS) Tagging', 'Data Preparation', 'NLP for News and Media']",['NLP for Social Media'],"Part-of-Speech POS tagging is a key step in many NLP algorithms. However, tweets are difficult to POS tag because they are short, are not always written maintaining formal grammar and proper spelling, and abbreviations are often used to overcome their restricted lengths. Arabic tweets also show a further range of linguistic phenomena such as usage of different dialects, romanised Arabic and borrowing foreign words. In this paper, we present an evaluation and a detailed error analysis of state-of-the-art POS taggers for Arabic when applied to Arabic tweets. On the basis of this analysis, we combine normalisation and external knowledge to handle the domain noisiness and exploit bootstrapping to construct extra training data in order to improve POS tagging for Arabic tweets. Our results show significant improvements over the performance of a number of well-known taggers for Arabic.",https://aclanthology.org/L16-1238,European Language Resources Association (ELRA),2016,May,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),"Albogamy, Fahad  and
Ramsay, Allan",Fast and Robust POS tagger for Arabic Tweets Using Agreement-based Bootstrapping,,L16,1320
2020.trac-1.16,"['Learning Paradigms', 'Domain-specific NLP', 'Classification Applications', 'Model Architectures']","['Transformer Models', 'Transfer Learning', 'Hate and Offensive Speech Detection', 'NLP for News and Media']",['NLP for Social Media'],This paper describes the participation of the SAJA team to the TRAC 2020 shared task on aggressive identification in the English text. we have developed a system based on transfer learning technique depending on universal sentence encoder USE embedding that will be trained in our developed model using xgboost classifier to identify the aggressive text data from English content. A reference dataset has been provided from TRAC 2020 to evaluate the developed approach. The developed approach achieved in sub-task EN-A 60.75% F1 weighted which ranked fourteenth out of sixteen teams and achieved 85.66% F1 weighted in sub-task EN-B which ranked six out of fifteen teams.,https://aclanthology.org/2020.trac-1.16,European Language Resources Association (ELRA),2020,May,"Proceedings of the Second Workshop on Trolling, Aggression and Cyberbullying","Tawalbeh, Saja  and
Hammad, Mahmoud  and
AL-Smadi, Mohammad",SAJA at TRAC 2020 Shared Task: Transfer Learning for Aggressive Identification with XGBoost,,trac,373
2020.parlaclarin-1.7,"['Domain-specific NLP', 'Data Management and Generation', 'Information Extraction', 'Information Retrieval', 'Low-resource Languages']","['Data Preparation', 'Search Engines', 'Named Entity Recognition (NER)']",,"We show that it is straightforward to train a state of the art named entity tagger spaCy to recognize political actors in Dutch parliamentary proceedings with high accuracy. The tagger was trained on 3.4K manually labeled examples, which were created in a modest 2.5 days work. This resource is made available on github. Besides proper nouns of persons and political parties, the tagger can recognize quite complex definite descriptions referring to cabinet ministers, ministries, and parliamentary committees. We also provide a demo search engine which employs the tagged entities in its SERP and result summaries.",https://aclanthology.org/2020.parlaclarin-1.7,European Language Resources Association,2020,May,Proceedings of the Second ParlaCLARIN Workshop,"Kerkvliet, Lennart  and
Kamps, Jaap  and
Marx, Maarten",Who mentions whom? Recognizing political actors in proceedings,,parlaclarin,160
2020.paclic-1.39,"['Data Management and Generation', 'Low-resource Languages']",['Data Analysis'],,"Episodic memory and semantic memory are two subsystems of declarative memory which is considered to be related to language ability. We investigated the interdependence of the episodic and semantic memory and their association with syntactic complexity in two groups of Chinese older adults with higher and lower education levels. The results indicate that episodic memory and semantic memory are significantly correlated with each other, but only episodic memory shows a significant relationship with syntactic complexity. Educational attainment has a substantial influence on the performance of memory tasks, but no influence on the syntactic complexity. The study provides new evidence from Chinese older adults for clarifying the association between declarative memory and syntactic processing and the benefits from education on memory preservation in later life.",https://aclanthology.org/2020.paclic-1.39,Association for Computational Linguistics,2020,October,"Proceedings of the 34th Pacific Asia Conference on Language, Information and Computation","Xie, Chenwei  and
Feng, Yun  and
Wang, William Shi-Yuan",Association between declarative memory and language ability in older Chinese by education level,,paclic,824
D16-1143,"['Data Management and Generation', 'Classification Applications', 'Domain-specific NLP']","['Data Preparation', 'Multilabel Text Classification', 'NLP for News and Media']",,"In this paper, we improve microblog users' demographic prediction by fully utilizing their video related behaviors. First, we collect the describing words of currently popular videos, including video names, actor names and video keywords, from video websites. Secondly, we search these describing words in users' microblogs, and build the direct relationships between users and the appeared words. After that, to make the sparse relationship denser, we propose a Bayesian method to calculate the probability of connections between users and other video describing words. Lastly, we build two models to predict users' demographics with the obtained direct and indirect relationships. Based on a large realworld dataset, experiment results show that our method can significantly improve these words' demographic predictive ability.",https://aclanthology.org/D16-1143,Association for Computational Linguistics,2016,November,Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing,"Wang, Yuan  and
Xiao, Yang  and
Ma, Chao  and
Xiao, Zhen",Improving Users' Demographic Prediction via the Videos They Talk about,10.18653/v1/D16-1143,D16,208
S17-1027,"['Low-resource Languages', 'Classification Applications', 'Model Architectures']",['Recurrent Neural Networks (RNNs)'],,"Detecting aspectual properties of clauses in the form of situation entity types has been shown to depend on a combination of syntactic-semantic and contextual features. We explore this task in a deeplearning framework, where tuned word representations capture lexical, syntactic and semantic features. We introduce an attention mechanism that pinpoints relevant context not only for the current instance, but also for the larger context. Apart from implicitly capturing task relevant features, the advantage of our neural model is that it avoids the need to reproduce linguistic features for other languages and is thus more easily transferable. We present experiments for English and German that achieve competitive performance. We present a novel take on modeling and exploiting genre information and showcase the adaptation of our system from one language to another.",https://aclanthology.org/S17-1027,Association for Computational Linguistics,2017,August,Proceedings of the 6th Joint Conference on Lexical and Computational Semantics (*{SEM} 2017),"Becker, Maria  and
Staniek, Michael  and
Nastase, Vivi  and
Palmer, Alexis  and
Frank, Anette",Classifying Semantic Clause Types: Modeling Context and Genre Characteristics with Recurrent Neural Networks and Attention,10.18653/v1/S17-1027,S17,951
2020.fnp-1.18,"['Domain-specific NLP', 'Model Architectures', 'Automatic Text Summarization']","['NLP for Finance', 'Document Summarization']",,"Companies provide annual reports to their shareholders at the end of the financial year that describes their operations and financial conditions. The average length of these reports is 80, and it may extend up to 250 pages long. In this paper, we propose our methodology PoinT-5 the combination of Pointer Network and T-5 Test-to-text transfer Transformer algorithms that we used in the Financial Narrative Summarisation FNS 2020 task. The proposed method uses Pointer networks to extract important narrative sentences from the report, and then T-5 is used to paraphrase extracted sentences into a concise yet informative sentence. We evaluate our method using ROUGE-N 1,2, L,and SU4. The proposed method achieves the highest precision scores in all the metrics and highest F1 scores in ROUGE 1,and LCS and only solution to cross MUSE solution baseline in ROUGE-LCS metrics.",https://aclanthology.org/2020.fnp-1.18,COLING,2020,December,Proceedings of the 1st Joint Workshop on Financial Narrative Processing and MultiLing Financial Summarisation,"Singh, Abhishek",PoinT-5: Pointer Network and T-5 based Financial Narrative Summarisation,,fnp,1124
2020.splu-1.2,"['Information Extraction', 'Model Architectures']","['Relation Extraction', 'Transformer Models']",,"Spatial information extraction is essential to understand geographical information in text. This task is largely divided to two subtasks: spatial element extraction and spatial relation extraction. In this paper, we utilize BERT Devlin et al., 2018, which is very effective for many natural language processing applications. We propose a BERT-based spatial information extraction model, which uses BERT for spatial element extraction and R-BERT Wu and He, 2019 for spatial relation extraction. The model was evaluated with the SemEval 2015 dataset. The result showed a 15.4% point increase in spatial element extraction and an 8.2% point increase in spatial relation extraction in comparison to the baseline model Nichols and Botros, 2015.",https://aclanthology.org/2020.splu-1.2,Association for Computational Linguistics,2020,November,Proceedings of the Third International Workshop on Spatial Language Understanding,"Shin, Hyeong Jin  and
Park, Jeong Yeon  and
Yuk, Dae Bum  and
Lee, Jae Sung",BERT-based Spatial Information Extraction,10.18653/v1/2020.splu-1.2,splu,269
O16-2001,"['Text Preprocessing', 'Low-resource Languages', 'Model Architectures']",['Text Segmentation'],['Word Segmentation'],"Chinese Segmentation Ambiguity CSA is a fundamental problem confronted when processing Chinese language, where a sentence can generate more than one segmentation paths. Two techniques are commonly used to identify CSA: Omni-segmentation and Bi-directional Maximum Matching BiMM. Due to the high computational complexity, Omni-segmentation is difficult to be applied for big data. BiMM is easier to be implemented and has a higher speed. However, recall of BiMM is much lower. In this paper, a Segmentation Matrix SM method is presented, which encodes each sentence as a matrix, then maps string operation into set operations. To identify CSA, instead of scanning a whole sentence, only specific areas of the matrix are checked. SM has a computational complexity close to BiMM with recall the same as Omni-segmentation. In addition to CSA identification, SM also supports lexicon-based Chinese word segmentation. In our experiments, based on SM, several issues about CSA are explored. The result shows that SM is useful for CSA analysis.",https://aclanthology.org/O16-2001,,2016,June,"International Journal of Computational Linguistics {\&} {C}hinese Language Processing, Volume 21, Number 1, June 2016","Chen, Yanping  and
Zheng, Qinghua  and
Tian, Feng  and
Zheng, Deli",A Segmentation Matrix Method for Chinese Segmentation Ambiguity Analysis,,O16,220
2020.emnlp-main.708,"['Text Generation', 'Image and Video Processing']","['Image Captioning', 'Video Captioning']",,"A major challenge in visually grounded language generation is to build robust benchmark datasets and models that can generalize well in real-world settings. To do this, it is critical to ensure that our evaluation protocols are correct, and benchmarks are reliable. In this work, we set forth to design a set of experiments to understand an important but often ignored problem in visually grounded language generation: given that humans have different utilities and visual attention, how will the sample variance in multi-reference datasets affect the models' performance? Empirically, we study several multi-reference datasets and corresponding vision-and-language tasks. We show that it is of paramount importance to report variance in experiments; that humangenerated references could vary drastically in different datasets/tasks, revealing the nature of each task; that metric-wise, CIDEr has shown systematically larger variances than others. Our evaluations on reference-per-instance shed light on the design of reliable datasets in the future.",https://aclanthology.org/2020.emnlp-main.708,Association for Computational Linguistics,2020,November,Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),"Zhu, Wanrong  and
Wang, Xin  and
Narayana, Pradyumna  and
Sone, Kazoo  and
Basu, Sugato  and
Wang, William Yang",Towards Understanding Sample Variance in Visually Grounded Language Generation: Evaluations and Observations,10.18653/v1/2020.emnlp-main.708,emnlp,1289
2020.vardial-1.24,"['Domain-specific NLP', 'Multilingual NLP', 'Classification Applications', 'Low-resource Languages']",['NLP for News and Media'],['NLP for Social Media'],"We describe our approaches for the Social Media Geolocation SMG task at the VarDial Evaluation Campaign 2020. The goal was to predict geographical location latitudes and longitudes given an input text. There were three subtasks corresponding to German-speaking Switzerland CH, Germany and Austria DE-AT, and Croatia, Bosnia and Herzegovina, Montenegro and Serbia BCMS. We submitted solutions to all subtasks but focused our development efforts on the CH subtask, where we achieved third place out of 16 submissions with a median distance of 15.93 km and had the best result of 14 unconstrained systems. In the DE-AT subtask, we ranked sixth out of ten submissions fourth of 8 unconstrained systems and for BCMS we achieved fourth place out of 13 submissions second of 11 unconstrained systems.",https://aclanthology.org/2020.vardial-1.24,International Committee on Computational Linguistics (ICCL),2020,December,"Proceedings of the 7th Workshop on NLP for Similar Languages, Varieties and Dialects","Benites, Fernando  and
H{\""u}rlimann, Manuela  and
von D{\""a}niken, Pius  and
Cieliebak, Mark",ZHAW-InIT - Social Media Geolocation at VarDial 2020,10.21256/zhaw-21551,vardial,323
2016.amta-users.14,"['Text Preprocessing', 'Multilingual NLP', 'Low-resource Languages', 'Machine Translation (MT)']",['Text Segmentation'],,"In this paper we discuss the motivation and process for planning, building, and monitoring a translation memory TM that serves both human-and machine-translated text segments in a production e-commerce environment. We consider the quality improvements associated with serving human translations for commonly used and mis-translated strings, and the cost benefits of avoiding multiple re-translations of the same source text segments. We cover the technical considerations and architecture for each stage of the TM pipeline, and review the results of using and monitoring the TM in a production setting.",https://aclanthology.org/2016.amta-users.14,The Association for Machine Translation in the Americas,2016,October 28 - November 1,Conferences of the Association for Machine Translation in the Americas: MT Users' Track,"Gillespie, Duncan  and
Russell, Benjamin",Building a Translation Memory to Improve Machine Translation Coverage and Quality,,amta,1011
2020.rail-1.5,"['Parsing', 'Low-resource Languages']",['Syntactic Parsing'],['Dependency Parsing'],"In this paper, we compare four state-of-the-art neural network dependency parsers for the Semitic language Amharic. As Amharic is a morphologically-rich and less-resourced language, the out-of-vocabulary OOV problem will be higher when we develop data-driven models. This fact limits researchers to develop neural network parsers because the neural network requires large quantities of data to train a model. We empirically evaluate neural network parsers when a small Amharic treebank is used for training. Based on our experiment, we obtain an 83.79 LAS score using the UDPipe system. Better accuracy is achieved when the neural parsing system uses external resources like word embedding. Using such resources, the LAS score for UDPipe improves to 85.26. Our experiment shows that the neural networks can learn dependency relations better from limited data while segmentation and POS tagging require much data.",https://aclanthology.org/2020.rail-1.5,European Language Resources Association (ELRA),2020,May,Proceedings of the first workshop on Resources for African Indigenous Languages,"Seyoum, Binyam Ephrem  and
Miyao, Yusuke  and
Mekonnen, Baye Yimam",Comparing Neural Network Parsers for a Less-resourced and Morphologically-rich Language: Amharic Dependency Parser,,rail,24
U19-1004,"['Text Preprocessing', 'Parsing', 'Learning Paradigms', 'Low-resource Languages', 'Model Architectures']","['Text Segmentation', 'Syntactic Parsing', 'Part-of-Speech (POS) Tagging']","['Word Segmentation', 'Dependency Parsing']","We propose the first multi-task learning model for joint Vietnamese word segmentation, partof-speech POS tagging and dependency parsing. In particular, our model extends the BIST graph-based dependency parser Kiperwasser and Goldberg, 2016 with BiLSTM-CRF-based neural layers Huang et al., 2015 for word segmentation and POS tagging. On Vietnamese benchmark datasets, experimental results show that our joint model obtains stateof-the-art or competitive performances.",https://aclanthology.org/U19-1004,Australasian Language Technology Association,2019,4--6 December,Proceedings of the The 17th Annual Workshop of the Australasian Language Technology Association,"Nguyen, Dat Quoc","A neural joint model for Vietnamese word segmentation, POS tagging and dependency parsing",10.48550/arxiv.1812.11459,U19,1490
S17-2168,"['Learning Paradigms', 'Domain-specific NLP', 'Information Extraction', 'Classification Applications']","['NLP for Bibliometrics and Scientometrics', 'Hypernymy Extraction', 'Supervised Learning', 'Relation Extraction']",,"This paper presents our relation extraction system for subtask C of SemEval-2017 Task 10: ScienceIE. Assuming that the keyphrases are already annotated in the input data, our work explores a wide range of linguistic features, applies various feature selection techniques, optimizes the hyper parameters and class weights and experiments with different problem formulations single classification model vs individual classifiers for each keyphrase type, single-step classifier vs pipeline classifier for hyponym relations. Performance of five popular classification algorithms are evaluated for each problem formulation along with feature selection. The best setting achieved an F 1 score of 71.0% for synonym and 30.0% for hyponym relation on the test data. Problem Description Task C of ScienceIE at SemEval-2017 Augenstein et al., 2017 concerns identifying sentence level 'SYNONYM-OF' or 'same-as' and 'HYPONYM-OF' 'is-a' relations among three types of keyphrases: PROCESS PR, TASK TA and MATERIAL MA in scientific documents. The 'SYNONYM-OF' relation is symmetric, whereas the 'HYPONYM-OF' relation is directed. Hyponym relation prediction is thus associated with two ordered subtasks: 1 predicting relations between pairs of keyphrases; 2 predicting the direction of the relation. It is assumed that there are no relations between keyphrase of different types. Automatic identification of synonym/hyponym relations is useful for many NLP applications, e.g. knowledge base completion and ontology construction.",https://aclanthology.org/S17-2168,Association for Computational Linguistics,2017,August,Proceedings of the 11th International Workshop on Semantic Evaluation ({S}em{E}val-2017),"Barik, Biswanath  and
Marsi, Erwin",NTNU-2 at SemEval-2017 Task 10: Identifying Synonym and Hyponym Relations among Keyphrases in Scientific Documents,10.18653/v1/S17-2168,S17,1012
O17-1013,"['Audio Generation and Processing', 'Image and Video Processing', 'Learning Paradigms', 'Low-resource Languages', 'Classification Applications', 'Model Architectures']","['Multimodal Learning', 'Emotion Detection', 'Recurrent Neural Networks (RNNs)']",['Long Short-Term Memory (LSTM) Models'],"This paper tried to amplify a sense of emotion toward drama. Using Long Short-Term Memory Recurrent Neural Network to model and predict dynamic emotionArousal and Valence recognition. After building model, we transplant whole framework and take results from it on visualizing. We have two demo version: RGB version and Vignette version. RGB version is to modulate the RGB value of frame in video. The Vignette one is to add the vignette effect. Both version all are to amplify a sense of emotion toward drama. Let people have more fun during watching videos. The database we used is NNIME The NTHU-NTUA Chinese Interactive Multimodal Emotion Corpus 1. NNIME is a newlycollected multimodal corpus. This database includes recordings of 44 subjects engaged in spontaneous dyadic spoken interaction. The length of data is about 11 hours containing audio, video and electrocardiogram. The database is also completed with a rich set of emotion annotations on continuous-in-time annotation by four annotators. This carefullyengineered data collection and annotation processes provide us to create amplify framework.",https://aclanthology.org/O17-1013,The Association for Computational Linguistics and Chinese Language Processing (ACLCLP),2017,November,Proceedings of the 29th Conference on Computational Linguistics and Speech Processing ({ROCLING} 2017),"Chou, Huang-Cheng  and
Chang, Chun-Min  and
Liu, Yu-Shuo  and
Kao, Shiuan-Kai  and
Lee, Chi-Chun",Amplifying a Sense of Emotion toward Drama-Long Short-Term Memory Recurrent Neural Network for dynamic emotion recognition,,O17,858
2021.ranlp-1.99,"['Data Management and Generation', 'Classification Applications']","['Data Preparation', 'Hate and Offensive Speech Detection', 'Data Analysis']",['Annotation Processes'],"Abusive language detection has become an important tool for the cultivation of safe online platforms. We investigate the interaction of annotation quality and classifier performance. We use a new, fine-grained annotation scheme that allows us to distinguish between abusive language and colloquial uses of profanity that are not meant to harm. Our results show a tendency of crowd workers to overuse the abusive class, which creates an unrealistic class balance and affects classification accuracy. We also investigate different methods of distinguishing between explicit and implicit abuse and show lexicon-based approaches either over-or under-estimate the proportion of explicit abuse in data sets.",https://aclanthology.org/2021.ranlp-1.99,INCOMA Ltd.,2021,September,Proceedings of the International Conference on Recent Advances in Natural Language Processing (RANLP 2021),"Lopez Long, Holly  and
O{'}Neil, Alexandra  and
K{\""u}bler, Sandra",On the Interaction between Annotation Quality and Classifier Performance in Abusive Language Detection,10.26615/978-954-452-072-4_099,ranlp,987
2020.nlpbt-1.2,"['Audio Generation and Processing', 'Learning Paradigms']","['Multimodal Learning', 'Automatic Speech Recognition (ASR)']",,"Visual context has been shown to be useful for automatic speech recognition ASR systems when the speech signal is noisy or corrupted. Previous work, however, has only demonstrated the utility of visual context in an unrealistic setting, where a fixed set of words are systematically masked in the audio. In this paper, we simulate a more realistic masking scenario during model training, called Rand-WordMask, where the masking can occur for any word segment. Our experiments on the Flickr 8K Audio Captions Corpus show that multimodal ASR can generalize to recover different types of masked words in this unstructured masking setting. Moreover, our analysis shows that our models are capable of attending to the visual signal when the audio signal is corrupted. These results show that multimodal ASR systems can leverage the visual signal in more generalized noisy scenarios.",https://aclanthology.org/2020.nlpbt-1.2,Association for Computational Linguistics,2020,November,Proceedings of the First International Workshop on Natural Language Processing Beyond Text,"Srinivasan, Tejas  and
Sanabria, Ramon  and
Metze, Florian  and
Elliott, Desmond",Multimodal Speech Recognition with Unstructured Audio Masking,10.18653/v1/2020.nlpbt-1.2,nlpbt,715
D17-1090,"['Question Answering (QA)', 'Text Generation']",['Community QA'],,"This paper presents how to generate questions from given passages using neural networks, where large scale QA pairs are automatically crawled and processed from Community-QA website, and used as training data. The contribution of the paper is 2-fold: First, two types of question generation approaches are proposed, one is a retrieval-based method using convolution neural network CNN, the other is a generation-based method using recurrent neural network RNN; Second, we show how to leverage the generated questions to improve existing question answering systems. We evaluate our question generation method for the answer sentence selection task on three benchmark datasets, including SQuAD, MS MARCO, and WikiQA. Experimental results show that, by using generated questions as an extra signal, significant QA improvement can be achieved.",https://aclanthology.org/D17-1090,Association for Computational Linguistics,2017,September,Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing,"Duan, Nan  and
Tang, Duyu  and
Chen, Peng  and
Zhou, Ming",Question Generation for Question Answering,10.18653/v1/D17-1090,D17,415
2021.mmsr-1.4,"['Question Answering (QA)', 'Learning Paradigms', 'Evaluation Techniques', 'Model Architectures']","['Multimodal Learning', 'Visual QA (VQA)']",,"We investigate the reasoning ability of pretrained vision and language V&L models in two tasks that require multimodal integration: 1 discriminating a correct image-sentence pair from an incorrect one, and 2 counting entities in an image. We evaluate three pretrained V&L models on these tasks: ViLBERT, ViLBERT 12-in-1 and LXMERT, in zero-shot and finetuned settings. Our results show that models solve task 1 very well, as expected, since all models are pretrained on task 1. However, none of the pretrained V&L models is able to adequately solve task 2, our counting probe, and they cannot generalise to out-ofdistribution quantities. We propose a number of explanations for these findings: LXMERT and to some extent ViLBERT 12-in-1 show some evidence of catastrophic forgetting on task 1. Concerning our results on the counting probe, we find evidence that all models are impacted by dataset bias, and also fail to individuate entities in the visual input. While a selling point of pretrained V&L models is their ability to solve complex tasks, our findings suggest that understanding their reasoning and grounding capabilities requires more targeted investigations on specific phenomena.",https://aclanthology.org/2021.mmsr-1.4,Association for Computational Linguistics,2021,June,Proceedings of the 1st Workshop on Multimodal Semantic Representations (MMSR),"Parcalabescu, Letitia  and
Gatt, Albert  and
Frank, Anette  and
Calixto, Iacer",Seeing past words: Testing the cross-modal capabilities of pretrained V\&L models on counting tasks,,mmsr,956
2021.motra-1.3,"['Machine Translation (MT)', 'Evaluation Techniques']",,,"To facilitate effective translation modeling and translation studies, one of the crucial questions to address is how to assess translation quality. From the perspectives of accuracy, reliability, repeatability and cost, translation quality assessment TQA itself is a rich and challenging task. In this work, we present a high-level and concise survey of TQA methods, including both manual judgement criteria and automated evaluation metrics, which we classify into further detailed sub-categories. We hope that this work will be an asset for both translation model researchers and quality assessment researchers. In addition, we hope that it will enable practitioners to quickly develop a better understanding of the conventional TQA field, and to find corresponding closely relevant evaluation solutions for their own needs. This work may also serve inspire further development of quality assessment and evaluation methodologies for other natural language processing NLP tasks in addition to machine translation MT, such as automatic text summarization ATS, natural language understanding NLU and natural language generation NLG. 1",https://aclanthology.org/2021.motra-1.3,Association for Computational Linguistics,2021,May,Proceedings for the First Workshop on Modelling Translation: Translatology in the Digital Age,"Han, Lifeng  and
Smeaton, Alan  and
Jones, Gareth",Translation Quality Assessment: A Brief Survey on Manual and Automatic Methods,10.48550/arxiv.2105.03311,motra,723
2022.constraint-1.10,"['Learning Paradigms', 'Domain-specific NLP', 'Classification Applications', 'Data Management and Generation']","['Multimodal Learning', 'Misinformation Detection', 'Data Preparation', 'NLP for News and Media']",['Fake News Detection'],"This study investigates how fake news uses a thumbnail for a news article with a focus on whether a news article's thumbnail represents the news content correctly. A news article shared with an irrelevant thumbnail can mislead readers into having a wrong impression of the issue, especially in social media environments where users are less likely to click the link and consume the entire content. We propose to capture the degree of semantic incongruity in the multimodal relation by using the pretrained CLIP representation. From a source-level analysis, we found that fake news employs a more incongruous image to the main content than general news. Going further, we attempted to detect news articles with imagetext incongruity. Evaluation experiments suggest that CLIP-based methods can successfully detect news articles in which the thumbnail is semantically irrelevant to news text. This study contributes to the research by providing a novel view on tackling online fake news and misinformation. Code and datasets are available at https://github.com/ssu-humane/ fake-news-thumbnail.",https://aclanthology.org/2022.constraint-1.10,Association for Computational Linguistics,2022,May,Proceedings of the Workshop on Combating Online Hostile Posts in Regional Languages during Emergency Situations,"Choi, Hyewon  and
Yoon, Yejun  and
Yoon, Seunghyun  and
Park, Kunwoo",How does fake news use a thumbnail? CLIP-based Multimodal Detection on the Unrepresentative News Image,10.18653/v1/2022.constraint-1.10,constraint,1206
2022.deeplo-1.12,"['Question Answering (QA)', 'Learning Paradigms']",['Transfer Learning'],,"Pretrained language models have shown success in various areas of natural language processing, including reading comprehension tasks. However, when applying machine learning methods to new domains, labeled data may not always be available. To address this, we use supervised pretraining on source-domain data to reduce sample complexity on domainspecific downstream tasks. We evaluate zeroshot performance on domain-specific reading comprehension tasks by combining task transfer with domain adaptation to fine-tune a pretrained model with no labelled data from the target task. Our approach outperforms Domain-Adaptive Pretraining on downstream domainspecific reading comprehension tasks in 3 out of 4 domains.",https://aclanthology.org/2022.deeplo-1.12,Association for Computational Linguistics,2022,July,Proceedings of the Third Workshop on Deep Learning for Low-Resource Natural Language Processing,"Pan, Xiang  and
Sheng, Alex  and
Shimshoni, David  and
Singhal, Aditya  and
Rosenthal, Sara  and
Sil, Avirup",Task Transfer and Domain Adaptation for Zero-Shot Question Answering,10.18653/v1/2022.deeplo-1.12,deeplo,464
K17-3007,"['Parsing', 'Multilingual NLP', 'Learning Paradigms', 'Low-resource Languages', 'Model Architectures']","['Syntactic Parsing', 'Recurrent Neural Networks (RNNs)', 'Adversarial Learning']","['Dependency Parsing', 'Long Short-Term Memory (LSTM) Models']","We describe our submission to the CoNLL 2017 shared task, which exploits the shared common knowledge of a language across different domains via a domain adaptation technique. Our approach is an extension to the recently proposed adversarial training technique for domain adaptation, which we apply on top of a graph-based neural dependency parsing model on bidirectional LSTMs. In our experiments, we find our baseline graphbased parser already outperforms the official baseline model UDPipe by a large margin. Further, by applying our technique to the treebanks of the same language with different domains, we observe an additional gain in the performance, in particular for the domains with less training data.",https://aclanthology.org/K17-3007,Association for Computational Linguistics,2017,August,Proceedings of the {C}o{NLL} 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies,"Sato, Motoki  and
Manabe, Hitoshi  and
Noji, Hiroshi  and
Matsumoto, Yuji",Adversarial Training for Cross-Domain Universal Dependency Parsing,10.18653/v1/K17-3007,K17,429
2021.findings-acl.296,"['Knowledge Representation and Reasoning', 'Model Architectures']",['Knowledge Graphs'],,"The exploitation of syntactic graphs SyGs as a word's context has been shown to be beneficial for distributional semantic models DSMs, both at the level of individual word representations and in deriving phrasal representations via composition. However, notwithstanding the potential performance benefit, the syntactically-aware DSMs proposed to date have huge numbers of parameters compared to conventional DSMs and suffer from data sparsity. Furthermore, the encoding of the SyG links i.e., the syntactic relations has been largely limited to linear maps. The knowledge graphs' literature, on the other hand, has proposed light-weight models employing different geometric transformations GTs to encode edges in a knowledge graph KG. Our work explores the possibility of adopting this family of models to encode SyGs. Furthermore, we investigate which GT better encodes syntactic relations, so that these representations can be used to enhance phrase-level composition via syntactic contextualisation.",https://aclanthology.org/2021.findings-acl.296,Association for Computational Linguistics,2021,August,Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021,"Bertolini, Lorenzo  and
Weeds, Julie  and
Weir, David  and
Peng, Qiwei",Representing Syntax and Composition with Geometric Transformations,10.18653/v1/2021.findings-acl.296,findings,518
2021.alta-1.11,"['Topic Modeling', 'Information Retrieval', 'Data Management and Generation', 'Domain-specific NLP']","['Data Preparation', 'Information Filtering']",,"With the increasing impact of Natural Language Processing tools like topic models in social science research, the experimental rigor and comparability of models and datasets has come under scrutiny. Especially when contributing to research on topics with worldwide impacts like energy policy, objective analyses and reliable datasets are necessary. We contribute toward this goal in two ways: first, we release two diachronic corpora covering 23 years of energy discussions in the U.S. Energy Information Administration. Secondly, we propose a simple method for automatic topic labelling drawing on domain knowledge via political thesauri. We empirically evaluate the quality of our labels, and apply our labelling to topics induced by diachronic topic models on our energy corpora, and present a detailed analysis.",https://aclanthology.org/2021.alta-1.11,Australasian Language Technology Association,2021,December,Proceedings of the The 19th Annual Workshop of the Australasian Language Technology Association,"Scelsi, Thomas  and
Arranz, Alfonso Martinez  and
Frermann, Lea",Principled Analysis of Energy Discourse across Domains with Thesaurus-based Automatic Topic Labeling,,alta,272
2022.gebnlp-1.11,"['Biases in NLP', 'Domain-specific NLP', 'Data Management and Generation']","['Gender Bias', 'Data Analysis', 'Medical and Clinical NLP']",,"Although approximately 50% of medical school graduates today are women, female physicians tend to be underrepresented in senior positions, make less money than their male counterparts and receive fewer promotions. There is a growing body of literature demonstrating gender bias in various forms of evaluation in medicine, but this work was mainly conducted by looking for specific words using fixed dictionaries such as LIWC and focused on recommendation letters. We use a dataset of written and quantitative assessments of medical student performance on individual shifts of work, collected across multiple institutions, to investigate the extent to which gender bias exists in a day-to-day context for medical students. We investigate differences in the narrative comments given to male and female students by both male or female faculty assessors, using a fine-tuned BERT model. This allows us to examine whether groups are written about in systematically different ways, without relying on hand-crafted wordlists or topic models. We compare these results to results from the traditional LIWC method and find that, although we find no evidence of group-level gender bias in this dataset, terms related to family and children are used more in feedback given to women.",https://aclanthology.org/2022.gebnlp-1.11,Association for Computational Linguistics,2022,July,Proceedings of the 4th Workshop on Gender Bias in Natural Language Processing (GeBNLP),"Liu, Emmy  and
Tessler, Michael Henry  and
Dubosh, Nicole  and
Hiller, Katherine  and
Levy, Roger",Assessing Group-level Gender Bias in Professional Evaluations: The Case of Medical Student End-of-Shift Feedback,10.18653/v1/2022.gebnlp-1.11,gebnlp,935
D17-1186,"['Data Management and Generation', 'Information Extraction', 'Model Architectures']","['Data Preparation', 'Relation Extraction']",,"Distantly supervised relation extraction has been widely used to find novel relational facts from plain text. To predict the relation between a pair of two target entities, existing methods solely rely on those direct sentences containing both entities. In fact, there are also many sentences containing only one of the target entities, which also provide rich useful information but not yet employed by relation extraction. To address this issue, we build inference chains between two target entities via intermediate entities, and propose a path-based neural relation extraction model to encode the relational semantics from both direct sentences and inference chains. Experimental results on realworld datasets show that, our model can make full use of those sentences containing only one target entity, and achieves significant and consistent improvements on relation extraction as compared with strong baselines. The source code of this paper can be obtained from https:// github.com/thunlp/PathNRE.",https://aclanthology.org/D17-1186,Association for Computational Linguistics,2017,September,Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing,"Zeng, Wenyuan  and
Lin, Yankai  and
Liu, Zhiyuan  and
Sun, Maosong",Incorporating Relation Paths in Neural Relation Extraction,10.18653/v1/D17-1186,D17,299
2020.sustainlp-1.14,"['Information Retrieval', 'Learning Paradigms']",,,"Researchers have proposed simple yet effective techniques for the retrieval problem based on using BERT as a relevance classifier to rerank initial candidates from keyword search. In this work, we tackle the challenge of finetuning these models for specific domains in a data and computationally efficient manner. Typically, researchers fine-tune models using corpus-specific labeled data from sources such as TREC. We first answer the question: How much data of this type do we need? Recognizing that the most computationally efficient training is no training, we explore zero-shot ranking using BERT models that have already been fine-tuned with the large MS MARCO passage retrieval dataset. We arrive at the surprising and novel finding that ""some"" labeled in-domain data can be worse than none at all.",https://aclanthology.org/2020.sustainlp-1.14,Association for Computational Linguistics,2020,November,Proceedings of SustaiNLP: Workshop on Simple and Efficient Natural Language Processing,"Zhang, Xinyu  and
Yates, Andrew  and
Lin, Jimmy",A Little Bit Is Worse Than None: Ranking with Limited Training Data,10.18653/v1/2020.sustainlp-1.14,sustainlp,834
2020.coling-main.485,"['Model Architectures', 'Classification Applications', 'Learning Paradigms']","['Graph Neural Networks (GNNs)', 'Few-shot Learning', 'Transformer Models']",,"In this paper, we propose a new few-shot text classification method. Compared with supervised learning methods which require a large corpus of labeled documents, our method aims to make it possible to classify unlabeled text with few labeled data. To achieve this goal, we take advantage of advanced pre-trained language model to extract the semantic features of each document. Furthermore, we utilize an edge-labeling graph neural network to implicitly models the intra-cluster similarity and the inter-cluster dissimilarity of the documents. Finally, we take the results of the graph neural network as the input of a prototypical network to classify the unlabeled texts. We verify the effectiveness of our method on a sentiment analysis dataset and a relation classification dataset and achieve the state-of-the-art performance on both tasks.",https://aclanthology.org/2020.coling-main.485,International Committee on Computational Linguistics,2020,December,Proceedings of the 28th International Conference on Computational Linguistics,"Lyu, Chen  and
Liu, Weijie  and
Wang, Ping",Few-Shot Text Classification with Edge-Labeling Graph Neural Network-Based Prototypical Network,10.18653/v1/2020.coling-main.485,coling,709
2021.wmt-1.95,"['Machine Translation (MT)', 'Multilingual NLP', 'Learning Paradigms', 'Low-resource Languages', 'Model Architectures']",['Transformer Models'],,"This paper presents our submissions to the WMT2021 Shared Task on Quality Estimation, Task 1 Sentence-Level Direct Assessment. While top-performing approaches utilize massively multilingual Transformer-based language models which have been pre-trained on all target languages of the task, the resulting insights are limited, as it is unclear how well the approach performs on languages unseen during pre-training; more problematically, these approaches do not provide any solutions for extending the model to new languages or unseen scripts-arguably one of the objectives of this shared task. In this work, we thus focus on utilizing massively multilingual language models which only partly cover the target languages during their pre-training phase. We extend the model to new languages and unseen scripts using recent adapter-based methods and achieve on par performance or even surpass models pre-trained on the respective languages.",https://aclanthology.org/2021.wmt-1.95,Association for Computational Linguistics,2021,November,Proceedings of the Sixth Conference on Machine Translation,"Geigle, Gregor  and
Stadtm{\""u}ller, Jonas  and
Zhao, Wei  and
Pfeiffer, Jonas  and
Eger, Steffen",TUDa at WMT21: Sentence-Level Direct Assessment with Adapters,,wmt,511
2020.acl-main.713,"['Data Management and Generation', 'Information Extraction', 'Low-resource Languages', 'Multilingual NLP', 'Model Architectures']","['Relation Extraction', 'Data Preparation', 'Named Entity Recognition (NER)', 'Event Extraction']",,"Most existing joint neural models for Information Extraction IE use local task-specific classifiers to predict labels for individual instances e.g., trigger, relation regardless of their interactions. For example, a VICTIM of a DIE event is likely to be a VICTIM of an AT-TACK event in the same sentence. In order to capture such cross-subtask and cross-instance inter-dependencies, we propose a joint neural framework, ONEIE, that aims to extract the globally optimal IE result as a graph from an input sentence. ONEIE performs end-to-end IE in four stages: 1 Encoding a given sentence as contextualized word representations; 2 Identifying entity mentions and event triggers as nodes; 3 Computing label scores for all nodes and their pairwise links using local classifiers; 4 Searching for the globally optimal graph with a beam decoder. At the decoding stage, we incorporate global features to capture the cross-subtask and cross-instance interactions. Experiments show that adding global features improves the performance of our model and achieves new state-of-the-art on all subtasks. As ONEIE does not use any language-specific feature, we prove it can be easily applied to new languages or trained in a multilingual manner. Our code and models for English, Spanish and Chinese are publicly available for research purpose 1 . PER Erdogan PER Abdullah Gul End-Position resigned Elect won person person Example: Prime Minister Abdullah Gul resigned earlier Tuesday to make way for Erdogan, who won a parliamentary seat in by-elections Sunday.",https://aclanthology.org/2020.acl-main.713,Association for Computational Linguistics,2020,July,Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,"Lin, Ying  and
Ji, Heng  and
Huang, Fei  and
Wu, Lingfei",A Joint Neural Model for Information Extraction with Global Features,10.18653/v1/2020.acl-main.713,acl,1107
2020.iwltp-1.13,"['Data Management and Generation', 'Low-resource Languages']",['Data Preparation'],,"This paper presents RELATE http://relate.racai.ro, a high-performance natural language platform designed for Romanian language. It is meant both for demonstration of available services, from text-span annotations to syntactic dependency trees as well as playing or automatically synthesizing Romanian words, and for the development of new, annotated corpora. It also incorporates the search engines for the large CoRoLa reference corpus of contemporary Romanian and the Romanian wordnet. It integrates multiple text and speech processing modules and exposes their functionality through a web interface designed for the linguist researcher. It makes use of a scheduler-runner architecture, allowing processing to be distributed across multiple computing nodes. A series of input/output converters allows large corpora to be loaded, processed and exported according to user preferences.",https://aclanthology.org/2020.iwltp-1.13,European Language Resources Association,2020,May,Proceedings of the 1st International Workshop on Language Technology Platforms,"P{\u{a}}i{\textcommabelow{s}}, Vasile  and
Ion, Radu  and
Tufi{\textcommabelow{s}}, Dan",A Processing Platform Relating Data and Tools for Romanian Language,,iwltp,486
2022.dialdoc-1.20,"['Data Management and Generation', 'Classification Applications', 'Knowledge Representation and Reasoning']","['Taxonomy Construction', 'Intent Detection', 'Data Preparation']",,"Comments are widely used by users in collaborative documents every day. The documents' comments enable collaborative editing and review dynamics, transforming each document into a context-sensitive communication channel. Understanding the role of comments in communication dynamics within documents is the first step towards automating their management. In this paper we propose the first ever taxonomy for different types of in-document comments based on analysis of a large scale dataset of public documents from the web. We envision that the next generation of intelligent collaborative document experiences allow interactive creation and consumption of content, there We also introduce the components necessary for developing novel tools that automate the handling of comments through natural language interaction with the documents. We identify the commands that users would use to respond to various types of comments. We train machine learning algorithms to recognize the different types of comments and assess their feasibility. We conclude by discussing some of the implications for the design of automatic document management tools. 1 Introduction Comments on collaborative documents serve as a communication channel. This type of contextspecific communication allows dynamics to review and edit content within the document. Collaborative text editors have visual components that allow users to associate a comment with a specific part of the content. This provides additional context in situations where the conversation focuses on a specific part of the document Churchill et al., 2000. As we can see, the amount of contextualization in communication that document comments permit is too complex and costly to recreate in other communications means outside of a document. For example, a request for changing a certain part of a document's content e.g. a paragraph's sentence through email would require much 044 additional information to be provided about all 045 of the context before requesting the change. 046 In this paper, we present a novel taxonomy of 047 the types of comments detected in a collection 048 of public documents. We detect three main cat-049 egories of intents for comments that are Modifi-050 cation, Information Exchange, and Social Com-051 munication. We show that supervised models 052 can successfully be trained to identify the type 053 of comments. We conducted additional studies 054 where users provided commands for resolve 055 each type of comment. Users were asked to 056 provide commands the way they would when 057 interacting with a voice assistant through natu-058 ral language. We find the most common com-059 mands as well as their structure. The following 060 summarizes our contributions: 061 1. Using a large-scale public document 062 dataset that we have curated and release 063 with this paper, we analyze the role of doc-064 ument comments and propose a taxonomy 065 of comments' intents and sub-intents. 066 2. We propose methods for determining the 067 intent of comments and discuss their po-068 tential for automation.",https://aclanthology.org/2022.dialdoc-1.20,Association for Computational Linguistics,2022,May,Proceedings of the Second DialDoc Workshop on Document-grounded Dialogue and Conversational Question Answering,"Nouri, Elnaz  and
Toxtli, Carlos",Handling Comments in Documents through Interactions,10.18653/v1/2022.dialdoc-1.20,dialdoc,52
2020.trac-1.5,"['Domain-specific NLP', 'Classification Applications']","['NLP for News and Media', 'Hate and Offensive Speech Detection']",['NLP for Social Media'],"The way people communicate have changed in many ways with the outbreak of social media. One of the aspects of social media is the ability for their information producers to hide, fully or partially, their identity during a discussion; leading to cyber-aggression and interpersonal aggression. Automatically monitoring user-generated content in order to help moderating it is thus a very hot topic. In this paper, we propose to use the transformer based language model BERT Bidirectional Encoder Representation from Transformer Devlin et al., 2019 to identify aggressive content. Our model is also used to predict the level of aggressiveness. The evaluation part of this paper is based on the dataset provided by the TRAC shared task Kumar et al., 2018a . When compared to the other participants of this shared task, our model achieved the third best performance according to the weighted F1 measure on both Facebook and Twitter collections.",https://aclanthology.org/2020.trac-1.5,European Language Resources Association (ELRA),2020,May,"Proceedings of the Second Workshop on Trolling, Aggression and Cyberbullying","Ramiandrisoa, Faneva  and
Mothe, Josiane",Aggression Identification in Social Media: a Transfer Learning Based Approach,,trac,469
L18-1715,"['Parsing', 'Data Management and Generation', 'Low-resource Languages']","['Data Preparation', 'Syntactic Parsing']",['Dependency Parsing'],"Chinese is a language rich in nonlocal dependencies. Correctly resolving these dependencies is crucial in understanding the predicateargument structure of a sentence. Making full use of the trace annotations in the Penn Chinese Treebank Xue et al., 2005 , this research contributes several test sets of Chinese nonlocal dependencies which occur in different grammatical constructions. These datasets can be used by an automatic dependency parser to evaluate its performance on nonlocal dependency resolution in various syntactic constructions in Chinese.",https://aclanthology.org/L18-1715,European Language Resources Association (ELRA),2018,May,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),"Duan, Manjuan  and
Schuler, William",Test Sets for Chinese Nonlocal Dependency Parsing,,L18,792
P16-1096,"['Model Architectures', 'Domain-specific NLP', 'Classification Applications', 'Data Management and Generation']","['Data Preparation', 'NLP for News and Media', 'Medical and Clinical NLP']",['NLP for Social Media'],"Automatically recognising medical concepts mentioned in social media messages e.g. tweets enables several applications for enhancing health quality of people in a community, e.g. real-time monitoring of infectious diseases in population. However, the discrepancy between the type of language used in social media and medical ontologies poses a major challenge. Existing studies deal with this challenge by employing techniques, such as lexical term matching and statistical machine translation. In this work, we handle the medical concept normalisation at the semantic level. We investigate the use of neural networks to learn the transition between layman's language used in social media messages and formal medical language used in the descriptions of medical concepts in a standard ontology. We evaluate our approaches using three different datasets, where social media texts are extracted from Twitter messages and blog posts. Our experimental results show that our proposed approaches significantly and consistently outperform existing effective baselines, which achieved state-of-the-art performance on several medical concept normalisation tasks, by up to 44%.",https://aclanthology.org/P16-1096,Association for Computational Linguistics,2016,August,Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),"Limsopatham, Nut  and
Collier, Nigel",Normalising Medical Concepts in Social Media Texts by Learning Semantic Representation,10.18653/v1/P16-1096,P16,438
N19-1209,"['Machine Translation (MT)', 'Learning Paradigms', 'Low-resource Languages', 'Domain-specific NLP']",['Neural MT (NMT)'],,"Continued training is an effective method for domain adaptation in neural machine translation. However, in-domain gains from adaptation come at the expense of general-domain performance. In this work, we interpret the drop in general-domain performance as catastrophic forgetting of general-domain knowledge. To mitigate it, we adapt Elastic Weight Consolidation EWC-a machine learning method for learning a new task without forgetting previous tasks. Our method retains the majority of general-domain performance lost in continued training without degrading indomain performance, outperforming the previous state-of-the-art. We also explore the full range of general-domain performance available when some in-domain degradation is acceptable. 1 See Cadwell et al. 2018 and Porro Rodriguez et al. 2017 for discussions about lack of trust in MT.",https://aclanthology.org/N19-1209,Association for Computational Linguistics,2019,June,"Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)","Thompson, Brian  and
Gwinnup, Jeremy  and
Khayrallah, Huda  and
Duh, Kevin  and
Koehn, Philipp",Overcoming Catastrophic Forgetting During Domain Adaptation of Neural Machine Translation,10.18653/v1/N19-1209,N19,735
2020.cogalex-1.10,"['Data Management and Generation', 'Information Extraction', 'Classification Applications']",['Data Analysis'],,"Textual definitions constitute a fundamental source of knowledge when seeking the meaning of words, and they are the cornerstone of lexical resources like glossaries, dictionaries, encyclopedia or thesauri. In this paper, we present an in-depth analytical study on the main features relevant to the task of definition extraction. Our main goal is to study whether linguistic structures from canonical the Aristotelian or genus et differentia model can be leveraged to retrieve definitions from corpora in different domains of knowledge and textual genres alike. To this end, we develop a simple linear classifier and analyze the contribution of several sets of linguistic features. Finally, as a result of our experiments, we also shed light on the particularities of existing benchmarks as well as the most challenging aspects of the task.",https://aclanthology.org/2020.cogalex-1.10,Association for Computational Linguistics,2020,December,Proceedings of the Workshop on the Cognitive Aspects of the Lexicon,"Roig Mirapeix, Mireia  and
Espinosa Anke, Luis  and
Camacho-Collados, Jose",Definition Extraction Feature Analysis: From Canonical to Naturally-Occurring Definitions,,cogalex,1346
K19-1074,"['Automatic Text Summarization', 'Model Architectures']","['Transformer Models', 'Abstractive Text Summarization', 'Document Summarization']",,"In this paper, we propose a novel pretrainingbased encoder-decoder framework, which can generate the output sequence based on the input sequence in a two-stage manner. For the encoder of our model, we encode the input sequence into context representations using BERT. For the decoder, there are two stages in our model, in the first stage, we use a Transformer-based decoder to generate a draft output sequence. In the second stage, we mask each word of the draft sequence and feed it to BERT, then by combining the input sequence and the draft representation generated by BERT, we use a Transformer-based decoder to predict the refined word for each masked position. To the best of our knowledge, our approach is the first method which applies the BERT into text generation tasks. As the first step in this direction, we evaluate our proposed method on the text summarization task. Experimental results show that our model achieves new state-of-the-art on both CNN/Daily Mail and New York Times datasets.",https://aclanthology.org/K19-1074,Association for Computational Linguistics,2019,November,Proceedings of the 23rd Conference on Computational Natural Language Learning (CoNLL),"Zhang, Haoyu  and
Cai, Jingjing  and
Xu, Jianjun  and
Wang, Ji",Pretraining-Based Natural Language Generation for Text Summarization,10.18653/v1/K19-1074,K19,699
2021.triton-1.24,['Domain-specific NLP'],,,The Covid pandemic upended translation teaching globally. The forced move to online teaching represented a gargantuan challenge for anyone only experienced in face-to-face teaching. Online translation teaching requires distinct approaches to guarantee that students can reach the targeted learning goals. This paper presents a literature review on the provision of effective feedback in the light of these drastic changes in translation teaching as well as a description as how existing research on online feedback for translation training has been applied to the design of online courses at the translation program at Rutgers University.,https://aclanthology.org/2021.triton-1.24,INCOMA Ltd.,2021,July,Proceedings of the Translation and Interpreting Technology Online Conference,"Jimenez-Crespo, Miguel A.",Feedback in Online Translation Courses and the Covid Era,10.26615/978-954-452-071-7_024,triton,206
2020.rail-1.9,"['Data Management and Generation', 'Low-resource Languages']",,,"In a context where open-source NLP resources and tools in African languages are scarce and dispersed, it is difficult for researchers to truly fit African languages into current algorithms of artificial intelligence. Created in 2017, with the aim of building communities of voluntary contributors around African native and/or national languages, cultures, NLP technologies and artificial intelligence, the NTeALan association has set up a series of web collaborative platforms intended to allow the aforementioned communities to create and administer their own lexicographic resources. In this article, we present on the one hand the first versions of the three platforms: the REST API for saving lexicographical resources, the dictionary management platform and the collaborative dictionary platform; on the other hand, we describe the data format chosen and used to encapsulate our resources. After experimenting with a few dictionaries and some users feedback, we are convinced that only collaboration-based approach and platforms can effectively respond to the production of good resources in African native and/or national languages.",https://aclanthology.org/2020.rail-1.9,European Language Resources Association (ELRA),2020,May,Proceedings of the first workshop on Resources for African Indigenous Languages,"Mboning Tchiaze, Elvis  and
Bassahak, Jean Marc  and
Baleba, Daniel  and
Wandji, Ornella  and
Assoumou, Jules",Building Collaboration-based Resources in Endowed African Languages: Case of NTeALan Dictionaries Platform,,rail,8
2020.figlang-1.25,['Text Generation'],['Text Style Transfer'],,"Understanding and identifying humor has been increasingly popular, as seen by the number of datasets created to study humor. However, one area of humor research, humor generation, has remained a difficult task, with machine generated jokes failing to match humancreated humor. As many humor prediction datasets claim to aid in generative tasks, we examine whether these claims are true. We focus our experiments on the most popular dataset, included in the 2020 SemEval's Task 7, and teach our model to take normal text and ""translate"" it into humorous text. We evaluate our model compared to humorous human generated headlines, finding that our model is preferred equally in A/B testing with the human edited versions, a strong success for humor generation, and is preferred over an intelligent random baseline 72% of the time. We also show that our model is assumed to be human written comparable with that of the human edited headlines and is significantly better than random, indicating that this dataset does indeed provide potential for future humor generation systems.",https://aclanthology.org/2020.figlang-1.25,Association for Computational Linguistics,2020,July,Proceedings of the Second Workshop on Figurative Language Processing,"Weller, Orion  and
Fulda, Nancy  and
Seppi, Kevin",Can Humor Prediction Datasets be used for Humor Generation? Humorous Headline Generation via Style Transfer,10.18653/v1/2020.figlang-1.25,figlang,1349
2020.globalex-1.1,"['Low-resource Languages', 'Knowledge Representation and Reasoning']","['Ontologies', 'Link Prediction']",,"The OntoLex vocabulary enjoys increasing popularity as a means of publishing lexical resources with RDF and as Linked Data. The recent publication of a new OntoLex module for lexicography, lexicog, reflects its increasing importance for digital lexicography. However, not all aspects of digital lexicography have been covered to the same extent. In particular, supplementary information drawn from corpora such as frequency information, links to attestations, and collocation data were considered to be beyond the scope of lexicog. Therefore, the OntoLex community has put forward the proposal for a novel module for frequency, attestation and corpus information FrAC, that not only covers the requirements of digital lexicography, but also accommodates essential data structures for lexical information in natural language processing. This paper introduces the current state of the OntoLex-FrAC vocabulary, describes its structure, some selected use cases, elementary concepts and fundamental definitions, with a focus on frequency and attestations.",https://aclanthology.org/2020.globalex-1.1,European Language Resources Association,2020,May,Proceedings of the 2020 Globalex Workshop on Linked Lexicography,"Chiarcos, Christian  and
Ionov, Maxim  and
de Does, Jesse  and
Depuydt, Katrien  and
Khan, Anas Fahad  and
Stolk, Sander  and
Declerck, Thierry  and
McCrae, John Philip",Modelling Frequency and Attestations for OntoLex-Lemon,,globalex,216
2022.nlppower-1.11,"['Biases in NLP', 'Domain-specific NLP', 'Evaluation Techniques', 'Low-resource Languages', 'Classification Applications', 'Model Architectures']","['Transformer Models', 'Gender Bias', 'Hate and Offensive Speech Detection', 'NLP for News and Media']",['NLP for Social Media'],"Warning: This paper contains examples of language that some people may find offensive. Transformer-based Natural Language Processing models have become the standard for hate speech detection. However, the unconscious use of these techniques for such a critical task comes with negative consequences. Various works have demonstrated that hate speech classifiers are biased. These findings have prompted efforts to explain classifiers, mainly using attribution methods. In this paper, we provide the first benchmark study of interpretability approaches for hate speech detection. We cover four post-hoc token attribution approaches to explain the predictions of Transformer-based misogyny classifiers in English and Italian. Further, we compare generated attributions to attention analysis. We find that only two algorithms provide faithful explanations aligned with human expectations. Gradient-based methods and attention, however, show inconsistent outputs, making their value for explanations questionable for hate speech detection tasks.",https://aclanthology.org/2022.nlppower-1.11,Association for Computational Linguistics,2022,May,Proceedings of NLP Power! The First Workshop on Efficient Benchmarking in NLP,"Attanasio, Giuseppe  and
Nozza, Debora  and
Pastor, Eliana  and
Hovy, Dirk",Benchmarking Post-Hoc Interpretability Approaches for Transformer-based Misogyny Detection,10.18653/v1/2022.nlppower-1.11,nlppower,298
W18-0621,"['Domain-specific NLP', 'Classification Applications']","['NLP for News and Media', 'Medical and Clinical NLP']","['NLP for Mental Health', 'NLP for Social Media']","In recent years, online communities have formed around suicide and self-harm prevention. While these communities offer support in moment of crisis, they can also normalize harmful behavior, discourage professional treatment, and instigate suicidal ideation. In this work, we focus on how interaction with others in such a community affects the mental state of users who are seeking support. We first build a dataset of conversation threads between users in a distressed state and community members offering support. We then show how to construct a classifier to predict whether distressed users are helped or harmed by the interactions in the thread, and we achieve a macro-F1 score of up to 0.69.",https://aclanthology.org/W18-0621,Association for Computational Linguistics,2018,June,Proceedings of the Fifth Workshop on Computational Linguistics and Clinical Psychology: From Keyboard to Clinic,"Soldaini, Luca  and
Walsh, Timothy  and
Cohan, Arman  and
Han, Julien  and
Goharian, Nazli",Helping or Hurting? Predicting Changes in Users' Risk of Self-Harm Through Online Community Interactions,10.18653/v1/W18-0621,W18,811
2016.tc-1.7,"['Multilingual NLP', 'Low-resource Languages', 'Machine Translation (MT)']",,,"This paper describes an experiment conducted by the author in November 2015 with 69 MSc Translation students at CenTraS @ UCL covering 14 target languages and in August 2016 with 30 professional translators in Saudi Arabia covering English to Arabic. The experiment was inspired by Lynne Bowker's pilot study Productivity vs Quality? A pilot study on the impact of translation memory systems published in Localisation Focus in March 2005. The author of this paper wanted to find out whether translators who are fairly new to translation technology would ""blindly"" trust the content of a TM or whether they would still check the content thoroughly and make any necessary changes to the translation. Students and professional translators were asked to translate a short text consisting of 14 sentences and a total of 217 words in Wordfast Anywhere/SDL Trados Studio 2015. They also received a translation memory TM for their respective language combination. All TMs contained mistakes, which the author did not mention to the students and the professional translators. Interestingly, while the professional translators fared better at editing fuzzy matches than the students, they did not pick up on incorrect 100% matches as well as the student translators, tended to lack attention to detail by, for example, introducing double spaces into sentences, and not all professional translators translated the new sentences given for translation.",https://aclanthology.org/2016.tc-1.7,AsLing,2016,November 17-18,Proceedings of Translating and the Computer 38,"Ford, Daniela",Can you trust a TM? Results of an experiment conducted in November 2015 and August 2016 with students and professional translators,,tc,760
2021.case-1.10,"['Domain-specific NLP', 'Model Architectures', 'Classification Applications']","['NLP for News and Media', 'Misinformation Detection', 'Transformer Models']",['NLP for Social Media'],"The dynamics and influence of fake news on Twitter during the 2020 US presidential election remains to be clarified. Here, we use a dataset related to 2020 U.S Election that consists of news articles and tweets on those articles. Therefore, it is extremely important to stop the spread of fake news before it reaches a mass level, which is a big challenge. We propose a novel fake news detection framework that can address this challenge. Our proposed framework exploits the information from news articles and social contexts to detect fake news. The proposed model is based on a Transformer architecture, which can learn useful representations from fake news data and predicts the probability of a news as being fake or real. Experimental results on realworld data show that our model can detect fake news with higher accuracy and much earlier, compared to the baselines.",https://aclanthology.org/2021.case-1.10,Association for Computational Linguistics,2021,August,Proceedings of the 4th Workshop on Challenges and Applications of Automated Extraction of Socio-political Events from Text (CASE 2021),"Raza, Shaina",Automatic Fake News Detection in Political Platforms - A Transformer-based Approach,10.18653/v1/2021.case-1.10,case,656
S19-2211,"['Domain-specific NLP', 'Information Extraction', 'Classification Applications']",['NLP for News and Media'],,"We present the INRIA approach to the suggestion mining task at SemEval 2019. The task consists of two subtasks: suggestion mining under single-domain Subtask A and crossdomain Subtask B settings. We used the Support Vector Machines algorithm trained on handcrafted features, function words, sentiment features, digits, and verbs for Subtask A, and handcrafted features for Subtask B. Our best run archived a F1-score of 51.18% on Subtask A, and ranked in the top ten of the submissions for Subtask B with 73.30% F1-score.",https://aclanthology.org/S19-2211,Association for Computational Linguistics,2019,June,Proceedings of the 13th International Workshop on Semantic Evaluation,"Markov, Ilia  and
Villemonte de la Clergerie, Eric",INRIA at SemEval-2019 Task 9: Suggestion Mining Using SVM with Handcrafted Features,10.18653/v1/S19-2211,S19,512
J16-4003,['Data Management and Generation'],"['Data Preparation', 'Data Analysis']",,"the other hand, the results suggest that negation, when addressed from a broader pragmatic perspective, far from being a nuisance, is an ideal application domain for distributional semantic methods.",https://aclanthology.org/J16-4003,MIT Press,2016,December,,"Kruszewski, Germ{\'a}n  and
Paperno, Denis  and
Bernardi, Raffaella  and
Baroni, Marco","There Is No Logical Negation Here, But There Are Alternatives: Modeling Conversational Negation with Distributional Semantics",10.1162/COLI_a_00262,J16,644
Y18-1048,"['Data Management and Generation', 'Low-resource Languages']",['Data Preparation'],,"Two literacies and three languages"" are used by Hong Kong people. In Hong Kong non-Chinese speaking students are trilingual speakers: they speak their mother language, English and Chinese Cantonese or Putonghua. Since Urdu speakers make up a large portion of the population among non-Chinese speaking students, the study chose Pakistani students from a local secondary school as subjects and examined the perceived accent of new and similar vowels in Hong Kong Cantonese produced by non-native speakers and rated by native listeners. The results show that language learners got much more accent in producing the new vowels than the similar vowels. It also demonstrated that the use of Cantonese L3 correlated closest to the accent scores, followed by Urdu L1 use and age-related factors.",https://aclanthology.org/Y18-1048,Association for Computational Linguistics,2018,1{--}3 December,"Proceedings of the 32nd Pacific Asia Conference on Language, Information and Computation","Liu, Yi  and
Ning, Jinghong",Factors Affecting Accent of New and Similar Vowels in Hong Kong Cantonese Pronounced by Urdu Speakers from Secondary School,,Y18,1413
P19-2059,"['Parsing', 'Argument Mining', 'Data Management and Generation']","['Semantic Parsing', 'Data Preparation', 'Data Analysis']","['Semantic Role Labeling', 'Annotation Processes']","For analyzing online persuasions, one of the important goals is to semantically understand how people construct comments to persuade others. However, analyzing the semantic role of arguments for online persuasion has been less emphasized. Therefore, in this study, we propose a novel annotation scheme that captures the semantic role of arguments in a popular online persuasion forum, so-called ChangeMyView. Through this study, we have made the following contributions: i proposing a scheme that includes five types of elementary units EUs and two types of relations; ii annotating ChangeMyView which results in 4612 EUs and 2713 relations in 345 posts; and iii analyzing the semantic role of persuasive arguments. Our analyses captured certain characteristic phenomena for online persuasion.",https://aclanthology.org/P19-2059,Association for Computational Linguistics,2019,July,Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: Student Research Workshop,"Egawa, Ryo  and
Morio, Gaku  and
Fujita, Katsuhide",Annotating and Analyzing Semantic Role of Elementary Units and Relations in Online Persuasive Arguments,10.18653/v1/P19-2059,P19,1363
W18-4401,"['Domain-specific NLP', 'Low-resource Languages', 'Classification Applications', 'Model Architectures']","['Hate and Offensive Speech Detection', 'NLP for News and Media']",['NLP for Social Media'],"In this paper, we present the report and findings of the Shared Task on Aggression Identification organised as part of the First Workshop on Trolling, Aggression and Cyberbullying TRAC -1 at COLING 2018. The task was to develop a classifier that could discriminate between Overtly Aggressive, Covertly Aggressive, and Non-aggressive texts. For this task, the participants were provided with a dataset of 15,000 aggression-annotated Facebook Posts and Comments each in Hindi in both Roman and Devanagari script and English for training and validation. For testing, two different sets -one from Facebook and another from a different social media -were provided. A total of 130 teams registered to participate in the task, 30 teams submitted their test runs, and finally 20 teams also sent their system description paper which are included in the TRAC workshop proceedings. The best system obtained a weighted F-score of 0.64 for both Hindi and English on the Facebook test sets, while the best scores on the surprise set were 0.60 and 0.50 for English and Hindi respectively. The results presented in this report depict how challenging the task is. The positive response from the community and the great levels of participation in the first edition of this shared task also highlights the interest in this topic.",https://aclanthology.org/W18-4401,Association for Computational Linguistics,2018,August,"Proceedings of the First Workshop on Trolling, Aggression and Cyberbullying ({TRAC}-2018)","Kumar, Ritesh  and
Ojha, Atul Kr.  and
Malmasi, Shervin  and
Zampieri, Marcos",Benchmarking Aggression Identification in Social Media,,W18,953
2020.nlposs-1.3,"['Parsing', 'Data Management and Generation', 'Embeddings']",,,"The CLEVR dataset has been used extensively in language grounded visual reasoning in Machine Learning ML and Natural Language Processing NLP domains. We present a graph parser library for CLEVR, that provides functionalities for object-centric attributes and relationships extraction, and construction of structural graph representations for dual modalities. Structural order-invariant representations enable geometric learning and can aid in downstream tasks like language grounding to vision, robotics, compositionality, interpretability, and computational grammar construction. We provide three extensible main components -parser, embedder, and visualizer that can be tailored to suit specific learning setups. We also provide out-of-thebox functionality for seamless integration with popular deep graph neural network GNN libraries. Additionally, we discuss downstream usage and applications of the library, and how it accelerates research for the NLP research community 1 .",https://aclanthology.org/2020.nlposs-1.3,Association for Computational Linguistics,2020,November,Proceedings of Second Workshop for NLP Open Source Software (NLP-OSS),"Saqur, Raeid  and
Deshpande, Ameet",CLEVR Parser: A Graph Parser Library for Geometric Learning on Language Grounded Image Scenes,10.18653/v1/2020.nlposs-1.3,nlposs,1005
2020.calcs-1.6,"['Domain-specific NLP', 'Classification Applications', 'Embeddings', 'Low-resource Languages', 'Cross-lingual Application']","['Word Embeddings', 'NLP for News and Media', 'Sentiment Analysis (SA)']",['NLP for Social Media'],"This paper investigates the use of unsupervised cross-lingual embeddings for solving the problem of code-mixed social media text understanding. We specifically investigate the use of these embeddings for a sentiment analysis task for Hinglish Tweets, viz. English combined with transliterated Hindi. In a first step, baseline models, initialized with monolingual embeddings obtained from large collections of tweets in English and code-mixed Hinglish, were trained. In a second step, two systems using cross-lingual embeddings were researched, being 1 a supervised classifier and 2 a transfer learning approach trained on English sentiment data and evaluated on code-mixed data. We demonstrate that incorporating cross-lingual embeddings improves the results F1-score of 0.635 versus a monolingual baseline of 0.616, without any parallel data required to train the cross-lingual embeddings. In addition, the results show that the cross-lingual embeddings not only improve the results in a fully supervised setting, but they can also be used as a base for distant supervision, by training a sentiment model in one of the source languages and evaluating on the other language projected in the same space. The transfer learning experiments result in an F1-score of 0.556 which is almost on par with the supervised settings and speak to the robustness of the cross-lingual embeddings approach.",https://aclanthology.org/2020.calcs-1.6,European Language Resources Association,2020,May,Proceedings of the The 4th Workshop on Computational Approaches to Code Switching,"Singh, Pranaydeep  and
Lefever, Els",Sentiment Analysis for Hinglish Code-mixed Tweets by means of Cross-lingual Word Embeddings,,calcs,883
W18-0616,"['Domain-specific NLP', 'Image and Video Processing']",['Medical and Clinical NLP'],,"Autism spectrum disorder ASD is a neurodevelopmental condition characterized by impaired social communication and the presence of restricted, repetitive patterns of behaviors and interests. Prior research suggests that restricted patterns of behavior in ASD may be cross-domain phenomena that are evident in a variety of modalities. Computational studies of language in ASD provide support for the existence of an underlying dimension of restriction that emerges during a conversation. Similar evidence exists for restricted patterns of facial movement. Using tools from computational linguistics, computer vision, and information theory, this study tests whether cognitive-motor restriction can be detected across multiple behavioral domains in adults with ASD during a naturalistic conversation. Our methods identify restricted behavioral patterns, as measured by entropy in word use and mouth movement. Results suggest that adults with ASD produce significantly less diverse mouth movements and words than neurotypical adults, with an increased reliance on repeated patterns in both domains. The diversity values of the two domains are not significantly correlated, suggesting that they provide complementary information.",https://aclanthology.org/W18-0616,Association for Computational Linguistics,2018,June,Proceedings of the Fifth Workshop on Computational Linguistics and Clinical Psychology: From Keyboard to Clinic,"Parish-Morris, Julia  and
Sariyanidi, Evangelos  and
Zampella, Casey  and
Bartley, G. Keith  and
Ferguson, Emily  and
Pallathra, Ashley A.  and
Bateman, Leila  and
Plate, Samantha  and
Cola, Meredith  and
Pandey, Juhi  and
Brodkin, Edward S.  and
Schultz, Robert T.  and
Tun{\c{c}}, Birkan",Oral-Motor and Lexical Diversity During Naturalistic Conversations in Adults with Autism Spectrum Disorder,10.18653/v1/W18-0616,W18,556
2021.acl-long.239,"['Question Answering (QA)', 'Learning Paradigms', 'Model Architectures']","['Transformer Models', 'Few-shot Learning']",,"In several question answering benchmarks, pretrained models have reached human parity through fine-tuning on an order of 100,000 annotated questions and answers. We explore the more realistic few-shot setting, where only a few hundred training examples are available, and observe that standard models perform poorly, highlighting the discrepancy between current pretraining objectives and question answering. We propose a new pretraining scheme tailored for question answering: recurring span selection. Given a passage with multiple sets of recurring spans, we mask in each set all recurring spans but one, and ask the model to select the correct span in the passage for each masked span. Masked spans are replaced with a special token, viewed as a question representation, that is later used during fine-tuning to select the answer span. The resulting model obtains surprisingly good results on multiple benchmarks e.g., 72.7 F1 on SQuAD with only 128 training examples, while maintaining competitive performance in the high-resource setting. 1 * Equal contribution. 1 Our code, models, and datasets are publicly available: https://github.com/oriram/splinter.",https://aclanthology.org/2021.acl-long.239,Association for Computational Linguistics,2021,August,Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),"Ram, Ori  and
Kirstain, Yuval  and
Berant, Jonathan  and
Globerson, Amir  and
Levy, Omer",Few-Shot Question Answering by Pretraining Span Selection,10.18653/v1/2021.acl-long.239,acl,529
2020.alta-1.10,"['Data Management and Generation', 'Model Architectures', 'Classification Applications']","['Data Analysis', 'Transformer Models']",,"Free text data from social media is now widely used in natural language processing research, and one of the most common machine learning tasks performed on this data is classification. Generally speaking, performances of supervised classification algorithms on social media datasets are lower than those on texts from other sources, but recentlyproposed transformer-based models have considerably improved upon legacy state-of-theart systems. Currently, there is no study that compares the performances of different variants of transformer-based models on a wide range of social media text classification datasets. In this paper, we benchmark the performances of transformer-based pre-trained models on 25 social media text classification datasets, 6 of which are health-related. We compare three pre-trained language models, RoBERTa-base, BERTweet and Clinical-BioBERT in terms of classification accuracy. Our experiments show that RoBERTa-base and BERTweet perform comparably on most datasets, and considerably better than Clinical-BioBERT, even on health-related datasets.",https://aclanthology.org/2020.alta-1.10,Australasian Language Technology Association,2020,December,Proceedings of the The 18th Annual Workshop of the Australasian Language Technology Association,"Guo, Yuting  and
Dong, Xiangjue  and
Al-Garadi, Mohammed Ali  and
Sarker, Abeed  and
Paris, Cecile  and
Aliod, Diego Moll{\'a}",Benchmarking of Transformer-Based Pre-Trained Models on Social Media Text Classification Datasets,,alta,4
2022.nlppower-1.7,"['Multilingual NLP', 'Evaluation Techniques', 'Low-resource Languages', 'Model Architectures']",['Large Language Models (LLMs)'],,"Although recent Massively Multilingual Language Models MMLMs like mBERT and XLMR support around 100 languages, most existing multilingual NLP benchmarks provide evaluation data in only a handful of these languages with little linguistic diversity. We argue that this makes the existing practices in multilingual evaluation unreliable and does not provide a full picture of the performance of MMLMs across the linguistic landscape. We propose that the recent work done in Performance Prediction for NLP tasks can serve as a potential solution in fixing benchmarking in Multilingual NLP by utilizing features related to data and language typology to estimate the performance of an MMLM on different languages. We compare performance prediction with translating test data with a case study on four different multilingual datasets, and observe that these methods can provide reliable estimates of the performance that are often onpar with the translation based approaches, without the need for any additional translation as well as evaluation costs.",https://aclanthology.org/2022.nlppower-1.7,Association for Computational Linguistics,2022,May,Proceedings of NLP Power! The First Workshop on Efficient Benchmarking in NLP,"Ahuja, Kabir  and
Dandapat, Sandipan  and
Sitaram, Sunayana  and
Choudhury, Monojit",Beyond Static models and test sets: Benchmarking the potential of pre-trained models across tasks and languages,10.18653/v1/2022.nlppower-1.7,nlppower,1343
2021.wat-1.21,"['Machine Translation (MT)', 'Low-resource Languages', 'Text Preprocessing']","['Text Segmentation', 'Neural MT (NMT)']",['Word Segmentation'],"Dravidian languages, such as Kannada and Tamil, are notoriously difficult to translate by state-of-the-art neural models. This stems from the fact that these languages are morphologically very rich as well as being lowresourced. In this paper, we focus on subword segmentation and evaluate Linguistically Motivated Vocabulary Reduction LMVR against the more commonly used SentencePiece SP for the task of translating from English into four different Dravidian languages. Additionally we investigate the optimal subword vocabulary size for each language. We find that SP is the overall best choice for segmentation, and that larger subword vocabulary sizes lead to higher translation quality.",https://aclanthology.org/2021.wat-1.21,Association for Computational Linguistics,2021,August,Proceedings of the 8th Workshop on Asian Translation (WAT2021),"Dhar, Prajit  and
Bisazza, Arianna  and
van Noord, Gertjan",Optimal Word Segmentation for Neural Machine Translation into Dravidian Languages,10.18653/v1/2021.wat-1.21,wat,1262
2020.ccl-1.99,"['Data Management and Generation', 'Model Architectures', 'Low-resource Languages', 'Discourse Analysis']",['Data Preparation'],,"Elementary Discourse Unit EDU recognition is the basic task of discourse analysis, and the Chinese and English discourse alignment corpus is helpful to the studies of EDU recognition. This paper first builds Chinese-English parallel discourse corpus, in which EDUs are annotated and aligned. Then, we present the framework of Bi-LSTM-CRF EDUs recognition model using word embedding, POS and syntactic features, which can combine the advantage of CRF and Bi-LSTM. The results show that F1 is about 2% higher than the traditional method. Compared with CRF and Bi-LSTM, the Bi-LSTM-CRF model can combine the advantages of them and obtains satisfactory results for Chinese and English EDUs recognition. The experiment of feature contribution shows that using all features together can get best result, the syntactic feature outperforms than other features.",https://aclanthology.org/2020.ccl-1.99,Chinese Information Processing Society of China,2020,October,Proceedings of the 19th Chinese National Conference on Computational Linguistics,"Yancui, Li  and
Chunxiao, Lai  and
Jike, Feng  and
Hongyu, Feng",Chinese and English Elementary Discourse Units Segmentation based on Bi-LSTM-CRF Model,,ccl,249
P16-1152,"['Machine Translation (MT)', 'Learning Paradigms', 'Classification Applications', 'Image and Video Processing']","['Optical Character Recognition (OCR)', 'Statistical MT (SMT)']",,"Structured prediction from bandit feedback describes a learning scenario where instead of having access to a gold standard structure, a learner only receives partial feedback in form of the loss value of a predicted structure. We present new learning objectives and algorithms for this interactive scenario, focusing on convergence speed and ease of elicitability of feedback. We present supervised-to-bandit simulation experiments for several NLP tasks machine translation, sequence labeling, text classification, showing that bandit learning from relative preferences eases feedback strength and yields improved empirical convergence.",https://aclanthology.org/P16-1152,Association for Computational Linguistics,2016,August,Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),"Sokolov, Artem  and
Kreutzer, Julia  and
Lo, Christopher  and
Riezler, Stefan",Learning Structured Predictors from Bandit Feedback for Interactive NLP,10.18653/v1/P16-1152,P16,273
2021.motra-1.10,"['Machine Translation (MT)', 'Evaluation Techniques', 'Low-resource Languages', 'Classification Applications']",,,"By using a trigram model and fine-tuning a pretrained BERT model for sequence classification, we show that machine translation and human translation can be classified with an accuracy above chance level, which suggests that machine translation and human translation are different in a systematic way. The classification accuracy of machine translation is much higher than of human translation. We show that this may be explained by the difference in lexical diversity between machine translation and human translation. If machine translation has independent patterns from human translation, automatic metrics which measure the deviation of machine translation from human translation may conflate difference with quality. Our experiment with two different types of automatic metrics shows correlation with the result of the classification task. Therefore, we suggest the difference in lexical diversity between machine translation and human translation be given more attention in machine translation evaluation.",https://aclanthology.org/2021.motra-1.10,Association for Computational Linguistics,2021,May,Proceedings for the First Workshop on Modelling Translation: Translatology in the Digital Age,"Fu, Yingxue  and
Nederhof, Mark-Jan",Automatic Classification of Human Translation and Machine Translation: A Study from the Perspective of Lexical Diversity,10.48550/arxiv.2105.04616,motra,694
2021.socialnlp-1.9,"['Ethics', 'Domain-specific NLP', 'Classification Applications', 'Model Architectures']","['Hate and Offensive Speech Detection', 'NLP for News and Media']",['NLP for Social Media'],"The use of attention mechanisms in deep learning approaches has become popular in natural language processing due to its outstanding performance. The use of these mechanisms allows one managing the importance of the elements of a sequence in accordance to their context, however, this importance has been observed independently between the pairs of elements of a sequence self-attention and between the application domain of a sequence contextual attention, leading to the loss of relevant information and limiting the representation of the sequences. To tackle these particular issues we propose the self-contextualized attention mechanism, which trades off the previous limitations, by considering the internal and contextual relationships between the elements of a sequence. The proposed mechanism was evaluated in four standard collections for the abusive language identification task achieving encouraging results. It outperformed the current attention mechanisms and showed a competitive performance with respect to state-of-the-art approaches.",https://aclanthology.org/2021.socialnlp-1.9,Association for Computational Linguistics,2021,June,Proceedings of the Ninth International Workshop on Natural Language Processing for Social Media,"Jarqu{\'\i}n-V{\'a}squez, Horacio  and
Escalante, Hugo Jair  and
Montes, Manuel",Self-Contextualized Attention for Abusive Language Identification,10.18653/v1/2021.socialnlp-1.9,socialnlp,544
2020.onion-1.5,"['Domain-specific NLP', 'Data Management and Generation', 'Classification Applications', 'Low-resource Languages']","['Data Preparation', 'Medical and Clinical NLP', 'Sentiment Analysis (SA)']",,"Humans frequently are able to read and interpret emotions of others by directly taking verbal and non-verbal signals in human-to-human communication into account or to infer or even experience emotions from mediated stories. For computers, however, emotion recognition is a complex problem: Thoughts and feelings are the roots of many behavioural responses and they are deeply entangled with neurophysiological changes within humans. As such, emotions are very subjective, often are expressed in a subtle manner, and are highly depending on context. For example, machine learning approaches for text-based sentiment analysis often rely on incorporating sentiment lexicons or language models to capture the contextual meaning. This paper explores if and how we further can enhance sentiment analysis using biofeedback of humans which are experiencing emotions while reading texts. Specifically, we record the heart rate and brain waves of readers that are presented with short texts which have been annotated with the emotions they induce. We use these physiological signals to improve the performance of a lexicon-based sentiment classifier. We find that the combination of several biosignals can improve the ability of a text-based classifier to detect the presence of a sentiment in a text on a per-sentence level.",https://aclanthology.org/2020.onion-1.5,European Language Resources Association (ELRA),2020,May,"Proceedings of LREC2020 Workshop ``People in language, vision and the mind'' (ONION2020)","Schl{\""o}r, Daniel  and
Zehe, Albin  and
Kobs, Konstantin  and
Veseli, Blerta  and
Westermeier, Franziska  and
Br{\""u}bach, Larissa  and
Roth, Daniel  and
Latoschik, Marc Erich  and
Hotho, Andreas",Improving Sentiment Analysis with Biofeedback Data,,onion,1382
2021.smm4h-1.22,"['Learning Paradigms', 'Domain-specific NLP', 'Classification Applications', 'Model Architectures']","['Transformer Models', 'Adversarial Learning', 'NLP for News and Media', 'Medical and Clinical NLP']",['NLP for Social Media'],"In this paper, we describe our system entry for Shared Task 8 at SMM4H-2021 , which is on automatic classification of self-reported breast cancer posts on Twitter. In our system, we use a transformer-based language model fine-tuning approach to automatically identify tweets in the self-reports category. Furthermore, we involve a Gradient-based Adversarial fine-tuning to improve the overall model's robustness. Our system achieved an F1-score of 0.8625 on the development set and 0.8501 on the test set in Shared Task-8 of SMM4H-2021.",https://aclanthology.org/2021.smm4h-1.22,Association for Computational Linguistics,2021,June,Proceedings of the Sixth Social Media Mining for Health ({\#}SMM4H) Workshop and Shared Task,"Kumar, Adarsh  and
Kamal, Ojasv  and
Mazumdar, Susmita",Adversities are all you need: Classification of self-reported breast cancer posts on Twitter using Adversarial Fine-tuning,10.18653/v1/2021.smm4h-1.22,smm4h,261
2021.bionlp-1.5,"['Domain-specific NLP', 'Information Retrieval', 'Knowledge Representation and Reasoning']",['Medical and Clinical NLP'],,"We explore whether state-of-the-art BERT models encode sufficient domain knowledge to correctly perform domain-specific inference. Although BERT implementations such as BioBERT are better at domain-based reasoning than those trained on general-domain corpora, there is still a wide margin compared to human performance on these tasks. To bridge this gap, we explore whether supplementing textual domain knowledge in the medical NLI task: a by further language model pretraining on the medical domain corpora, b by means of lexical match algorithms such as the BM25 algorithm, c by supplementing lexical retrieval with dependency relations, or d by using a trained retriever module, can push this performance closer to that of humans. We do not find any significant difference between knowledge supplemented classification as opposed to the baseline BERT models, however. This is contrary to the results for evidence retrieval on other tasks such as open domain question answering QA. By examining the retrieval output, we show that the methods fail due to unreliable knowledge retrieval for complex domain-specific reasoning. We conclude that the task of unsupervised text retrieval to bridge the gap in existing information to facilitate inference is more complex than what the state-of-the-art methods can solve, and warrants extensive research in the future.",https://aclanthology.org/2021.bionlp-1.5,Association for Computational Linguistics,2021,June,Proceedings of the 20th Workshop on Biomedical Language Processing,"Sushil, Madhumita  and
Suster, Simon  and
Daelemans, Walter",Are we there yet? Exploring clinical domain knowledge of BERT models,10.18653/v1/2021.bionlp-1.5,bionlp,820
2020.fnp-1.32,"['Learning Paradigms', 'Information Extraction', 'Classification Applications', 'Domain-specific NLP']","['Unsupervised Learning', 'Sentiment Analysis (SA)', 'NLP for Finance']",,"We present a novel approach to unsupervised information extraction by identifying and extracting relevant concept-value pairs from textual data. The system's building blocks are domain agnostic, making it universally applicable. In this paper, we describe each component of the system and how it extracts relevant economic information from U.S. Federal Open Market Committee 1 FOMC statements. Our methodology achieves an impressive 96% accuracy for identifying relevant information for a set of seven economic indicators: household spending, inflation, unemployment, economic activity, fixed investment, federal funds rate, and labor market.",https://aclanthology.org/2020.fnp-1.32,COLING,2020,December,Proceedings of the 1st Joint Workshop on Financial Narrative Processing and MultiLing Financial Summarisation,"Frunza, Oana",Information Extraction from Federal Open Market Committee Statements,,fnp,604
2021.eacl-main.24,"['Dialogue Systems', 'Model Architectures']","['Transformer Models', 'Open Domain Dialogue Systems', 'Chatbots']",,"Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we highlight other ingredients. Good conversation requires blended skills: providing engaging talking points, and displaying knowledge, empathy and personality appropriately, while maintaining a consistent persona. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter models, and make our models and code publicly available. Human evaluations show our best models outperform existing approaches in multi-turn dialogue on engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.",https://aclanthology.org/2021.eacl-main.24,Association for Computational Linguistics,2021,April,Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume,"Roller, Stephen  and
Dinan, Emily  and
Goyal, Naman  and
Ju, Da  and
Williamson, Mary  and
Liu, Yinhan  and
Xu, Jing  and
Ott, Myle  and
Smith, Eric Michael  and
Boureau, Y-Lan  and
Weston, Jason",Recipes for Building an Open-Domain Chatbot,10.18653/v1/2021.eacl-main.24,eacl,186
2022.naacl-main.32,"['Machine Translation (MT)', 'Data Management and Generation', 'Low-resource Languages']","['Data Augmentation', 'Neural MT (NMT)']",,"Back translation BT is one of the most significant technologies in NMT research fields. Existing attempts on BT share a common characteristic: they employ either beam search or random sampling to generate synthetic data with a backward model but seldom work studies the role of synthetic data in the performance of BT. This motivates us to ask a fundamental question: what kind of synthetic data contributes to BT performance? Through both theoretical and empirical studies, we identify two key factors on synthetic data controlling the back-translation NMT performance, which are quality and importance. Furthermore, based on our findings, we propose a simple yet effective method to generate synthetic data to better trade off both factors so as to yield a better performance for BT. We run extensive experiments on WMT14 DE-EN, EN-DE, and RU-EN benchmark tasks. By employing our proposed method to generate synthetic data, our BT model significantly outperforms the standard BT baselines i.e., beam and sampling based methods for data generation, which proves the effectiveness of our proposed methods.",https://aclanthology.org/2022.naacl-main.32,Association for Computational Linguistics,2022,July,Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,"Xu, Jiahao  and
Ruan, Yubin  and
Bi, Wei  and
Huang, Guoping  and
Shi, Shuming  and
Chen, Lihui  and
Liu, Lemao",On Synthetic Data for Back Translation,10.18653/v1/2022.naacl-main.32,naacl,50
2021.germeval-1.1,"['Data Management and Generation', 'Low-resource Languages', 'Classification Applications', 'Domain-specific NLP']","['Claim Verification', 'Data Preparation', 'Hate and Offensive Speech Detection', 'NLP for News and Media']",['NLP for Social Media'],"We present the GermEval 2021 shared task on the identification of toxic, engaging, and factclaiming comments. This shared task comprises three binary classification subtasks with the goal to identify: toxic comments, engaging comments, and comments that include indications of a need for fact-checking, here referred to as fact-claiming comments. Building on the two previous GermEval shared tasks on the identification of offensive language in 2018 and 2019, we extend this year's task definition to meet the demand of moderators and community managers to also highlight comments that foster respectful communication, encourage in-depth discussions, and check facts that lines of arguments rely on. The dataset comprises 4,188 posts extracted from the Facebook page of a German political talk show of a national public television broadcaster. A theoretical framework and additional reliability tests during the data annotation process ensure particularly high data quality. The shared task had 15 participating teams submitting 31 runs for the subtask on toxic comments, 25 runs for the subtask on engaging comments, and 31 for the subtask on fact-claiming comments. The shared task website can be found at https://germeval2021toxic.github. io/SharedTask/.",https://aclanthology.org/2021.germeval-1.1,Association for Computational Linguistics,2021,September,"Proceedings of the GermEval 2021 Shared Task on the Identification of Toxic, Engaging, and Fact-Claiming Comments","Risch, Julian  and
Stoll, Anke  and
Wilms, Lena  and
Wiegand, Michael","Overview of the GermEval 2021 Shared Task on the Identification of Toxic, Engaging, and Fact-Claiming Comments",,germeval,487
2020.stoc-1.4,"['Ethics', 'Data Management and Generation', 'Domain-specific NLP']","['Data Preparation', 'NLP for News and Media']",['NLP for Social Media'],"Privacy is going to be an integral part of data science and analytics in the coming years. The next hype of data experimentation is going to be heavily dependent on privacy preserving techniques mainly as it's going to be a legal responsibility rather than a mere social responsibility. Privacy preservation becomes more challenging specially in the context of unstructured data. Social networks have become predominantly popular over the past couple of decades and they are creating a huge data lake at a high velocity. Social media profiles contain a wealth of personal and sensitive information, creating enormous opportunities for third parties to analyze them with different algorithms, draw conclusions and use in disinformation campaigns and micro targeting based dark advertising. This study provides a mitigation mechanism for disinformation campaigns that are done based on the insights extracted from personal/sensitive data analysis. Specifically, this research is aimed at building a privacy preserving data publishing middleware for unstructured social media data without compromising the true analytical value of those data. A novel way is proposed to apply traditional structured privacy preserving techniques on unstructured data. Creating a comprehensive twitter corpus annotated with privacy attributes is another objective of this research, especially because the research community is lacking one.",https://aclanthology.org/2020.stoc-1.4,European Language Resources Association,2020,May,Proceedings for the First International Workshop on Social Threats in Online Conversations: Understanding and Management,"Abeywardana, Prasadi  and
Thayasivam, Uthayasanker","A Privacy Preserving Data Publishing Middleware for Unstructured, Textual Social Media Data",,stoc,948
2020.cl-2.6,"['Multilingual NLP', 'Knowledge Representation and Reasoning', 'Low-resource Languages']",,,"syntax is an interlingual representation used in compilers. Grammatical Framework GF applies the abstract syntax idea to natural languages. The development of GF started in 1998, first as a tool for controlled language implementations, where it has gained an established position in both academic and commercial projects. GF provides grammar resources for over 40 languages, enabling accurate generation and translation, as well as grammar engineering Submission",https://aclanthology.org/2020.cl-2.6,,2020,June,,"Ranta, Aarne  and
Angelov, Krasimir  and
Gruzitis, Normunds  and
Kolachina, Prasanth",Abstract Syntax as Interlingua: Scaling Up the Grammatical Framework from Controlled Languages to Robust Pipelines,10.1162/coli_a_00378,cl,1398
2020.mwe-1.18,"['Model Architectures', 'Multilingual NLP', 'Classification Applications', 'Low-resource Languages']",['Transformer Models'],,"This paper describes the TRAVIS system built for the PARSEME Shared Task 2020 on semisupervised identification of verbal multiword expressions. TRAVIS is a fully feature-independent model, relying only on the contextual embeddings. We have participated with two variants of TRAVIS, TRAVIS multi and TRAVIS mono , where the former employs multilingual contextual embeddings and the latter uses monolingual ones. Our systems are ranked second and third among seven submissions in the open track, respectively. Thorough comparison of both systems on eight languages reveals that despite the strong performance of multilingual contextual embeddings across all languages, language-specific contextual embeddings exhibit much better generalization capabilities.",https://aclanthology.org/2020.mwe-1.18,Association for Computational Linguistics,2020,December,Proceedings of the Joint Workshop on Multiword Expressions and Electronic Lexicons,"Kurfal{\i}, Murathan",TRAVIS at PARSEME Shared Task 2020: How good is mBERT at seeing the unseen?,,mwe,366
2021.eacl-main.311,"['Question Answering (QA)', 'Discourse Analysis', 'Data Management and Generation']",['Data Analysis'],,"An in-depth analysis of the level of language understanding required by existing Machine Reading Comprehension MRC benchmarks can provide insight into the reading capabilities of machines. In this paper, we propose an ablation-based methodology to assess the extent to which MRC datasets evaluate the understanding of explicit discourse relations. We define seven MRC skills which require the understanding of different discourse relations. We then introduce ablation methods that verify whether these skills are required to succeed on a dataset. By observing the drop in performance of neural MRC models evaluated on the original and the modified dataset, we can measure to what degree the dataset requires these skills, in order to be understood correctly. Experiments on three large-scale datasets with the BERT-base and ALBERT-xxlarge model show that the relative changes for all skills are small less than 6%. These results imply that most of the answered questions in the examined datasets do not require understanding the discourse structure of the text. To specifically probe for natural language understanding, there is a need to design more challenging benchmarks that can correctly evaluate the intended skills 1 .",https://aclanthology.org/2021.eacl-main.311,Association for Computational Linguistics,2021,April,Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume,"Wu, Yulong  and
Schlegel, Viktor  and
Batista-Navarro, Riza",Is the Understanding of Explicit Discourse Relations Required in Machine Reading Comprehension?,10.18653/v1/2021.eacl-main.311,eacl,1160
2022.deelio-1.9,"['Knowledge Representation and Reasoning', 'Model Architectures']","['Transformer Models', 'Knowledge Graphs', 'Link Prediction']",,"In the real world, many relational facts require context; for instance, a politician holds a given elected position only for a particular timespan. This context the timespan is typically ignored in knowledge graph link prediction tasks, or is leveraged by models designed specifically to make use of it i.e. n-ary link prediction models. Here, we show that the task of n-ary link prediction is easily performed using language models, applied with a basic method for constructing cloze-style query sentences. We introduce a pre-training methodology based around an auxiliary entity-linked corpus that outperforms other popular pre-trained models like BERT, even with a smaller model. This methodology also enables n-ary link prediction without access to any n-ary training set, which can be invaluable in circumstances where expensive and time-consuming curation of n-ary knowledge graphs is not feasible. We achieve state-ofthe-art performance on the primary n-ary link prediction dataset WD50K and on WikiPeople facts that include literals -typically ignored by knowledge graph embedding methods.",https://aclanthology.org/2022.deelio-1.9,Association for Computational Linguistics,2022,May,Proceedings of Deep Learning Inside Out (DeeLIO 2022): The 3rd Workshop on Knowledge Extraction and Integration for Deep Learning Architectures,"Brayne, Angus  and
Wiatrak, Maciej  and
Corneil, Dane",On Masked Language Models for Contextual Link Prediction,10.18653/v1/2022.deelio-1.9,deelio,161
2021.mmsr-1.9,"['Model Architectures', 'Audio Generation and Processing', 'Image and Video Processing']",,,"We describe work in progress for training a humanoid robot to produce iconic arm and head gestures as part of task-oriented dialogic interaction. This involves the development of a multimodal dialogue manager and corresponding system architecture for non-experts to 'program' the robot through speech and vision. Using this system, videos of gesture demonstrations are collected. Motor positions are extracted from the videos to specify motor trajectories, where collections of motor trajectories are used to produce robot gestures following a Gaussian mixtures approach. Concluding discussion considers how learned representations may be used for gesture recognition by the robot, and how the core system may mature into a robust system to address language grounding and semantic representation.",https://aclanthology.org/2021.mmsr-1.9,Association for Computational Linguistics,2021,June,Proceedings of the 1st Workshop on Multimodal Semantic Representations (MMSR),"Brady, Michael  and
Du, Han",Teaching Arm and Head Gestures to a Humanoid Robot through Interactive Demonstration and Spoken Instruction,,mmsr,92
2021.naacl-main.64,"['Commonsense Reasoning', 'Domain-specific NLP', 'Multi-agent Communication Systems', 'Data Management and Generation', 'Knowledge Representation and Reasoning', 'Learning Paradigms']","['Reinforcement Learning', 'Data Preparation', 'Intelligent Agents', 'Knowledge Graphs']",,"We seek to create agents that both act and communicate with other agents in pursuit of a goal. Towards this end, we extend LIGHT Urbanek et al., 2019 -a large-scale crowd-sourced fantasy text-game-with a dataset of ""quests"". 1 . These contain natural language motivations paired with in-game goals and human demonstrations; completing a quest might require dialogue or actions or both. We introduce a reinforcement learning system that 1 incorporates large-scale language modeling-based and commonsense reasoning-based pre-training to imbue the agent with relevant priors; and 2 leverages a factorized action space of action commands and dialogue, balancing between the two. We conduct zero-shot evaluations using held-out human expert demonstrations, showing that our agents are able to act consistently and talk naturally with respect to their motivations. Insssssolent pessst! I should immolate you for this tresssspasss. And why is that, dragon? Ssstealing my preccciousss golden egg! I'll tell you what, I'll give you 10 sssseconds to amussse me with your sssstory and THEN I'll burn you alive! You said you wanted to attack me, dragon, did you not? Go ahead, I'm lisssssstening. get golden dragon egg Now now! I would have given you that had you asked! Assssssk for my own property back? What a riduculousss notion Look here, I told you to watch your mouth and you didn't, so leave or I'll make you leave. And now threatsss! Thisss is proving to be a mossst engaging conversssation. hit knight Give my regardsss to the valley floor below!",https://aclanthology.org/2021.naacl-main.64,Association for Computational Linguistics,2021,June,Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,"Ammanabrolu, Prithviraj  and
Urbanek, Jack  and
Li, Margaret  and
Szlam, Arthur  and
Rockt{\""a}schel, Tim  and
Weston, Jason",How to Motivate Your Dragon: Teaching Goal-Driven Agents to Speak and Act in Fantasy Worlds,10.18653/v1/2021.naacl-main.64,naacl,1047
2022.repl4nlp-1.1,"['Learning Paradigms', 'Model Architectures']",['Recurrent Neural Networks (RNNs)'],,"Neural machine learning models can successfully model language that is similar to their training distribution, but they are highly susceptible to degradation under distribution shift, which occurs in many practical applications when processing out-of-domain OOD text. This has been attributed to ""shortcut learning"": relying on weak correlations over arbitrary large contexts. We propose a method based on OOD detection with Random Network Distillation to allow an autoregressive language model to automatically disregard OOD context during inference, smoothly transitioning towards a less expressive but more robust model as the data becomes more OOD, while retaining its full context capability when operating in-distribution. We apply our method to a GRU architecture, demonstrating improvements on multiple language modeling LM datasets.",https://aclanthology.org/2022.repl4nlp-1.1,Association for Computational Linguistics,2022,May,Proceedings of the 7th Workshop on Representation Learning for NLP,"Valerio Miceli Barone, Antonio  and
Birch, Alexandra  and
Sennrich, Rico",Distributionally Robust Recurrent Decoders with Random Network Distillation,10.18653/v1/2022.repl4nlp-1.1,repl4nlp,1453
2022.findings-acl.88,"['Ethics', 'Biases in NLP', 'Multilingual NLP', 'Learning Paradigms', 'Low-resource Languages', 'Classification Applications', 'Model Architectures']","['Transformer Models', 'Hate and Offensive Speech Detection']",,"Warning: This paper contains examples of language that some people may find offensive. Natural Language Processing NLP models risk overfitting to specific terms in the training data, thereby reducing their performance, fairness, and generalizability. E.g., neural hate speech detection models are strongly influenced by identity terms like gay, or women, resulting in false positives, severe unintended bias, and lower performance. Most mitigation techniques use lists of identity terms or samples from the target domain during training. However, this approach requires a-priori knowledge and introduces further bias if important terms are neglected. Instead, we propose a knowledge-free Entropy-based Attention Regularization EAR to discourage overfitting to training-specific terms. An additional objective function penalizes tokens with low selfattention entropy. We fine-tune BERT via EAR: the resulting model matches or exceeds stateof-the-art performance for hate speech classification and bias metrics on three benchmark corpora in English and Italian. EAR also reveals overfitting terms, i.e., terms most likely to induce bias, to help identify their effect on the model, task, and predictions.",https://aclanthology.org/2022.findings-acl.88,Association for Computational Linguistics,2022,May,Findings of the Association for Computational Linguistics: ACL 2022,"Attanasio, Giuseppe  and
Nozza, Debora  and
Hovy, Dirk  and
Baralis, Elena",Entropy-based Attention Regularization Frees Unintended Bias Mitigation from Lists,10.18653/v1/2022.findings-acl.88,findings,738
O16-3002,"['Embeddings', 'Parsing', 'Low-resource Languages']",['Syntactic Parsing'],,"Rescoring approaches for parsing aims to re-rank and change the order of parse trees produced by a general parser for a given sentence. The re-ranking performance depends on whether or not the rescoring function is able to precisely estimate the quality of parse trees by using more complex features from the whole parse tree. However it is a challenge to design an appropriate rescoring function since complex features usually face the severe problem of data sparseness. And it is also difficult to obtain sufficient information requisite in re-estimatation of tree structures because existing annotated Treebanks are generally small-sized. To address the issue, in this paper, we utilize a large amount of auto-parsed trees to learn the syntactic and sememtic information. And we propose a simple but effective score function in order to integrate the scores provided by the baseline parser and dependency association scores based on dependency-based word embeddings, learned from auto-parsed trees. The dependency association scores can relieve the problem of data sparseness, since they can be still calculated by word embeddings even without occurrence of a dependency word pair in a corpus. Moreover, semantic role labels are also considered to distinct semantic relation of word pairs. Experimental results show that our proposed model improves the base Chinese parser significantly.",https://aclanthology.org/O16-3002,,2016,December,"International Journal of Computational Linguistics {\&} {C}hinese Language Processing, Volume 21, Number 2, {D}ecember 2016","Hsieh, Yu-Ming  and
Ma, Wei-Yun",N-best Rescoring for Parsing Based on Dependency-Based Word Embeddings,,O16,758
2020.sustainlp-1.17,['Model Architectures'],['Transformer Models'],,"Humans read and write hundreds of billions of messages every day. Further, due to the availability of large datasets, large computing systems, and better neural network models, natural language processing NLP technology has made significant strides in understanding, proofreading, and organizing these messages. Thus, there is a significant opportunity to deploy NLP in myriad applications to help web users, social networks, and businesses. Toward this end, we consider smartphones and other mobile devices as crucial platforms for deploying NLP models at scale. However, today's highly-accurate NLP neural network models such as BERT and RoBERTa are extremely computationally expensive, with BERT-base taking 1.7 seconds to classify a text snippet on a Pixel 3 smartphone. To begin to address this problem, we draw inspiration from the computer vision community, where work such as MobileNet has demonstrated that grouped convolutions e.g., depthwise convolutions can enable speedups without sacrificing accuracy. We demonstrate how to replace several operations in self-attention layers with grouped convolutions and use this technique in a novel network architecture called Squeeze-BERT, which runs 4.3x faster than BERT-base on the Pixel 3 while achieving competitive accuracy on the GLUE test set. A PyTorch-based implementation of Squeeze-BERT is available as part of the Hugging Face Transformers library: https:// huggingface.co/squeezebert",https://aclanthology.org/2020.sustainlp-1.17,Association for Computational Linguistics,2020,November,Proceedings of SustaiNLP: Workshop on Simple and Efficient Natural Language Processing,"Iandola, Forrest  and
Shaw, Albert  and
Krishna, Ravi  and
Keutzer, Kurt",SqueezeBERT: What can computer vision teach NLP about efficient neural networks?,10.18653/v1/2020.sustainlp-1.17,sustainlp,25
2020.sustainlp-1.2,['Model Architectures'],,,"Sequence model based NLP applications can be large. Yet, many applications that benefit from them run on small devices with very limited compute and storage capabilities, while still having run-time constraints. As a result, there is a need for a compression technique that can achieve significant compression without negatively impacting inference run-time and task accuracy. This paper proposes a new compression technique called Hybrid Matrix Factorization that achieves this dual objective. HLF improves low-rank matrix factorization LMF techniques by doubling the rank of the matrix using an intelligent hybrid-structure leading to better accuracy than LMF. Further, by preserving dense matrices, it leads to faster inference run-time than pruning or structure matrix based compression technique. We evaluate the impact of this technique on 5 NLP benchmarks across multiple tasks Translation, Intent Detection, Language Modeling and show that for similar accuracy values and compression factors, HLF can achieve more than 2.32× faster inference run-time than pruning and 16.77% better accuracy than LMF.",https://aclanthology.org/2020.sustainlp-1.2,Association for Computational Linguistics,2020,November,Proceedings of SustaiNLP: Workshop on Simple and Efficient Natural Language Processing,"Thakker, Urmish  and
Beu, Jesse  and
Gope, Dibakar  and
Dasika, Ganesh  and
Mattina, Matthew",Rank and run-time aware compression of NLP Applications,10.18653/v1/2020.sustainlp-1.2,sustainlp,991
2020.paclic-1.40,"['Data Management and Generation', 'Low-resource Languages']",['Data Analysis'],,"Concerning the acquisition of relative clauses RCs, studies on head-initial languages consistently reported a preference for subjectgapped RCs, but the issue of subject-object asymmetry is still a controversial one in research on the acquisition of RCs in head-final languages. Using written corpus data, this study investigated the second language production of RCs in Mandarin Chinese Chinese by Japanese-speaking and Thai-speaking learners with various proficiency levels. We first extracted the RCs produced by Japanese and Thai learners from the HKS Dynamic Composition Corpus, and coded head type and gap type for further analyses. The learners from the intermediate-level groups produced a significant number of error-free RCs, which suggests that the intermediate learners have already mastered Chinese RCs. Both Japanese and Thai learners exhibited a strong preference for the subject RCs, which is consistent with predictions that follow from the Noun Phrase Accessibility Hierarchy NPAH and the results of studies on head-initial languages. Our data also provided partial support for the Subject-Object Hierarchy SOH. However, the size of the corpus was insufficient to exhaustively investigate the tested theories. More data are needed to examine the applicability of the NPAH and SOH hypotheses in L2 Chinese and in general.",https://aclanthology.org/2020.paclic-1.40,Association for Computational Linguistics,2020,October,"Proceedings of the 34th Pacific Asia Conference on Language, Information and Computation","Yang, Yike",A corpus-based analysis of Chinese relative clauses produced by Japanese and Thai learners,,paclic,586
2022.wnu-1.7,"['Model Architectures', 'Domain-specific NLP', 'Classification Applications', 'Data Management and Generation']","['Transformer Models', 'Medical and Clinical NLP', 'Data Preparation', 'NLP for News and Media']",['NLP for Social Media'],"Narratives have been shown to be an effective way to communicate health risks and promote health behavior change, and given the growing amount of health information being shared on social media, it is crucial to study healthrelated narratives in social media. However, expert identification of a large number of narrative texts is a time consuming process, and larger scale studies on the use of narratives may be enabled through automatic text classification approaches. Prior work has demonstrated that automatic narrative detection is possible, but modern deep learning approaches have not been used for this task in the domain of online health communities. Therefore, in this paper, we explore the use of deep learning methods to automatically classify the presence of narratives in social media posts, finding that they outperform previously proposed approaches. We also find that in many cases, these models generalize well across posts from different health organizations. Finally, in order to better understand the increase in performance achieved by deep learning models, we use feature analysis techniques to explore the features that most contribute to narrative detection for posts in online health communities.",https://aclanthology.org/2022.wnu-1.7,Association for Computational Linguistics,2022,July,Proceedings of the 4th Workshop of Narrative Understanding (WNU2022),"Ganti, Achyutarama  and
Wilson, Steven  and
Ma, Zexin  and
Zhao, Xinyan  and
Ma, Rong",Narrative Detection and Feature Analysis in Online Health Communities,10.18653/v1/2022.wnu-1.7,wnu,846
2021.clpsych-1.20,"['Domain-specific NLP', 'Data Management and Generation']","['Data Preparation', 'Data Analysis', 'Medical and Clinical NLP']",['NLP for Mental Health'],"A growing amount of psychiatric research incorporates machine learning and natural language processing methods, however findings have yet to be translated into actual clinical decision support systems. Many of these studies are based on relatively small datasets in homogeneous populations, which has the associated risk that the models may not perform adequately on new data in real clinical practice. The nature of serious mental illness is that it is hard to define, hard to capture, and requires frequent monitoring, which leads to imperfect data where attribute and class noise are common. With the goal of an effective AI-mediated clinical decision support system, there must be computational safeguards placed on the models used in order to avoid spurious predictions and thus allow humans to review data in the settings where models are unstable or bound not to generalize. This paper describes two approaches to implementing safeguards: 1 the determination of cases in which models are unstable by means of attribute and class based outlier detection and 2 finding the extent to which models show inductive bias. These safeguards are illustrated in the automated scoring of a story recall task via natural language processing methods. With the integration of human-in-the-loop machine learning in the clinical implementation process, incorporating safeguards such as these into the models will offer patients increased protection from spurious predictions.",https://aclanthology.org/2021.clpsych-1.20,Association for Computational Linguistics,2021,June,Proceedings of the Seventh Workshop on Computational Linguistics and Clinical Psychology: Improving Access,"Chandler, Chelsea  and
Foltz, Peter  and
Cohen, Alex  and
Holmlund, Terje  and
Elvev{\aa}g, Brita",Safeguarding against spurious AI-based predictions: The case of automated verbal memory assessment,10.18653/v1/2021.clpsych-1.20,clpsych,572
W19-8709,"['Machine Translation (MT)', 'Low-resource Languages']","['Neural MT (NMT)', 'Statistical MT (SMT)']",,"In this study, we compare the output quality of two MT systems, a statistical SMT and a neural NMT engine, customised for Swiss Post's Language Service using the same training data. We focus on the point of view of professional translators and investigate how they perceive the differences between the MT output and a human reference namely deletions, substitutions, insertions and word order. Our findings show that translators more frequently consider these differences to be errors in SMT than NMT, and that deletions are the most serious errors in both architectures. We also observe there to be less agreement on differences to be corrected in NMT than SMT, suggesting that errors are easier to identify in SMT. These findings confirm the ability of NMT to produce correct paraphrases, which could also explain why BLEU is often considered to be an inadequate metric to evaluate the performance of NMT systems.",https://aclanthology.org/W19-8709,"Incoma Ltd., Shoumen, Bulgaria",2019,September,Proceedings of the Human-Informed Translation and Interpreting Technology Workshop (HiT-IT 2019),"Mutal, Jonathan  and
Volkart, Lise  and
Bouillon, Pierrette  and
Girletti, Sabrina  and
Estrella, Paula",Differences between SMT and NMT Output - a Translators' Point of View,10.26615/issn.2683-0078.2019_009,W19,185
2020.scai-1.2,"['Text Generation', 'Adversarial Attacks and Robustness', 'Question Answering (QA)']",,,"The dependency between an adequate question formulation and correct answer selection is a very intriguing but still underexplored area. In this paper, we show that question rewriting QR of the conversational context allows to shed more light on this phenomenon and also use it to evaluate robustness of different answer selection approaches. We introduce a simple framework that enables an automated analysis of the conversational question answering QA performance using question rewrites, and present the results of this analysis on the TREC CAsT and QuAC CANARD datasets. Our experiments uncover sensitivity to question formulation of the popular state-of-the-art models for reading comprehension and passage ranking. Our results demonstrate that the reading comprehension model is insensitive to question formulation, while the passage ranking changes dramatically with a little variation in the input question. The benefit of QR is that it allows us to pinpoint and group such cases automatically. We show how to use this methodology to verify whether QA models are really learning the task or just finding shortcuts in the dataset, and better understand the frequent types of error they make.",https://aclanthology.org/2020.scai-1.2,Association for Computational Linguistics,2020,November,Proceedings of the 5th International Workshop on Search-Oriented Conversational AI (SCAI),"Vakulenko, Svitlana  and
Longpre, Shayne  and
Tu, Zhucheng  and
Anantha, Raviteja",A Wrong Answer or a Wrong Question? An Intricate Relationship between Question Reformulation and Answer Selection in Conversational Question Answering,10.18653/v1/2020.scai-1.2,scai,384
2021.wassa-1.13,"['Learning Paradigms', 'Classification Applications']",['Sentiment Analysis (SA)'],,"While Curriculum Learning CL has recently gained traction in Natural language Processing Tasks, it is still not adequately analyzed. Previous works only show their effectiveness but fail short to explain and interpret the internal workings fully. In this paper, we analyze curriculum learning in sentiment analysis along multiple axes. Some of these axes have been proposed by earlier works that need more in-depth study. Such analysis requires understanding where curriculum learning works and where it does not. Our axes of analysis include Task difficulty on CL, comparing CL pacing techniques, and qualitative analysis by visualizing the movement of attention scores in the model as curriculum phases progress. We find that curriculum learning works best for difficult tasks and may even lead to a decrement in performance for tasks with higher performance without curriculum learning. We see that One-Pass curriculum strategies suffer from catastrophic forgetting and attention movement visualization within curriculum pacing. This shows that curriculum learning breaks down the challenging main task into easier sub-tasks solved sequentially.",https://aclanthology.org/2021.wassa-1.13,Association for Computational Linguistics,2021,April,"Proceedings of the Eleventh Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis","Rao Vijjini, Anvesh  and
Anuranjana, Kaveri  and
Mamidi, Radhika","Analyzing Curriculum Learning for Sentiment Analysis along Task Difficulty, Pacing and Visualization Axes",10.48550/arxiv.2102.09990,wassa,1076
2020.latechclfl-1.11,['Data Management and Generation'],['Data Analysis'],,"An increasing amount of historic data is now available in digital text formats. This gives quantitative researchers an opportunity to use distant reading techniques, as opposed to traditional close reading, in order to analyse larger quantities of historic data. Distant reading allows researchers to view overall patterns within the data and reduce researcher bias. One such data set that has recently been transcribed is a collection of over 500 Australian World War I WW1 diaries held by the State Library of New South Wales. Here we apply distant reading techniques to this corpus to understand what soldiers wrote about and how they felt over the course of the war. Extracting dates accurately is important as it allows us to perform our analysis over time, however, it is very challenging due to the variety of date formats and abbreviations diarists use. But with that data, topic modelling and sentiment analysis can then be applied to show trends, for instance, that despite the horrors of war, Australians in WW1 primarily wrote about their everyday routines and experiences. Our results detail some of the challenges likely to be encountered by quantitative researchers intending to analyse historical texts, and provide some approaches to these issues.",https://aclanthology.org/2020.latechclfl-1.11,International Committee on Computational Linguistics,2020,December,"Proceedings of the The 4th Joint SIGHUM Workshop on Computational Linguistics for Cultural Heritage, Social Sciences, Humanities and Literature","Dennis-Henderson, Ashley  and
Roughan, Matthew  and
Mitchell, Lewis  and
Tuke, Jonathan",Life still goes on: Analysing Australian WW1 Diaries through Distant Reading,,latechclfl,787
2021.emnlp-main.653,"['Data Management and Generation', 'Low-resource Languages']",['Data Analysis'],,"Homophony's widespread presence in natural languages is a controversial topic. Recent theories of language optimality have tried to justify its prevalence, despite its negative effects on cognitive processing time; e.g., Piantadosi et al. 2012 argued homophony enables the reuse of efficient wordforms and is thus beneficial for languages. This hypothesis has recently been challenged by Trott and Bergen 2020, who posit that good wordforms are more often homophonous simply because they are more phonotactically probable. In this paper, we join in on the debate. We first propose a new information-theoretic quantification of a language's homophony: the sample Rényi entropy. Then, we use this quantification to revisit Trott and Bergen's claims. While their point is theoretically sound, a specific methodological issue in their experiments raises doubts about their results. After addressing this issue, we find no clear pressure either towards or against homophony-a much more nuanced result than either Piantadosi et al.'s or Trott and Bergen's findings.",https://aclanthology.org/2021.emnlp-main.653,Association for Computational Linguistics,2021,November,Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,"Pimentel, Tiago  and
Meister, Clara  and
Teufel, Simone  and
Cotterell, Ryan",On Homophony and R\'enyi Entropy,10.18653/v1/2021.emnlp-main.653,emnlp,355
2020.isa-1.4,"['Data Management and Generation', 'Image and Video Processing']",['Data Preparation'],['Annotation Processes'],"People's visual perception is very pronounced and therefore it is usually no problem for them to describe the space around them in words. Conversely, people also have no problems imagining a concept of a described space. In recent years many efforts have been made to develop a linguistic scheme for spatial and spatial-temporal relations. However, the systems have not really caught on so far, which in our opinion is due to the complex models on which they are based and the lack of available training data and automated taggers. In this paper we describe a project to support spatial annotation, which could facilitate annotation by its many functions, but also enrich it with many more information. This is to be achieved by an extension by means of a VR environment, with which spatial relations can be better visualized and connected with real objects. And we want to use the available data to develop a new state-of-the-art tagger and thus lay the foundation for future systems such as improved text understanding for Text2Scene Generation.",https://aclanthology.org/2020.isa-1.4,European Language Resources Association,2020,May,16th Joint ACL - ISO Workshop on Interoperable Semantic Annotation PROCEEDINGS,"Henlein, Alexander  and
Abrami, Giuseppe  and
Kett, Attila  and
Mehler, Alexander",Transfer of ISOSpace into a 3D Environment for Annotations and Applications,,isa,475
2021.blackboxnlp-1.4,"['Evaluation Techniques', 'Prompt Engineering', 'Text Generation', 'Classification Applications']",,,"Temporary syntactic ambiguities arise when the beginning of a sentence is compatible with multiple syntactic analyses. We inspect to which extent neural language models LMs exhibit uncertainty over such analyses when processing temporarily ambiguous inputs, and how that uncertainty is modulated by disambiguating cues. We probe the LM's expectations by generating from it: we use stochastic decoding to derive a set of sentence completions, and estimate the probability that the LM assigns to each interpretation based on the distribution of parses across completions. Unlike scoring-based methods for targeted syntactic evaluation, this technique makes it possible to explore completions that are not hypothesized in advance by the researcher. We apply this method to study the behavior of two LMs GPT2 and an LSTM on three types of temporary ambiguity, using materials from human sentence processing experiments. We find that LMs can track multiple analyses simultaneously; the degree of uncertainty varies across constructions and contexts. As a response to disambiguating cues, the LMs often select the correct interpretation, but occasional errors point to potential areas of improvement.",https://aclanthology.org/2021.blackboxnlp-1.4,Association for Computational Linguistics,2021,November,Proceedings of the Fourth BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP,"Aina, Laura  and
Linzen, Tal",The Language Model Understood the Prompt was Ambiguous: Probing Syntactic Uncertainty Through Generation,10.18653/v1/2021.blackboxnlp-1.4,blackboxnlp,1259
2021.codi-main.13,"['Machine Translation (MT)', 'Embeddings', 'Discourse Analysis']","['Word Embeddings', 'Neural MT (NMT)']",,"In the present paper, we explore lexical contexts of discourse markers in translation and interpreting on the basis of word embeddings. Our special interest is on contextual variation of the same discourse markers in written translation vs. simultaneous interpreting. To explore this variation at the lexical level, we use a data-driven approach: we compare bilingual neural word embeddings trained on source-to-translation and source-tointerpreting aligned corpora. Our results show more variation of semantically related items in translation spaces vs. interpreting ones and a more consistent use of fewer connectives in interpreting. We also observe different trends with regard to the discourse relation types.",https://aclanthology.org/2021.codi-main.13,Association for Computational Linguistics,2021,November,Proceedings of the 2nd Workshop on Computational Approaches to Discourse,"Lapshinova-Koltunski, Ekaterina  and
Przybyl, Heike  and
Bizzoni, Yuri",Tracing variation in discourse connectives in translation and interpreting through neural semantic spaces,10.18653/v1/2021.codi-main.13,codi,669
2022.dlg4nlp-1.3,"['Information Extraction', 'Model Architectures']","['Graph Neural Networks (GNNs)', 'Temporal Event Understanding']",,"There has been an increasing interest in modeling continuous-time dynamics of temporal graph data. Previous methods encode timeevolving relational information into a lowdimensional representation by specifying discrete layers of neural networks, while realworld dynamic graphs often vary continuously over time. Hence, we propose Continuous Temporal Graph Networks CTGNs to capture continuous dynamics of temporal graph data. We use both the link starting timestamps and link duration as evolving information to model continuous dynamics of nodes. The key idea is to use neural ordinary differential equations ODE to characterize the continuous dynamics of node representations over dynamic graphs. We parameterize ordinary differential equations using a novel graph neural network. The existing dynamic graph networks can be considered as a specific discretization of CTGNs. Experiment results on both transductive and inductive tasks demonstrate the effectiveness of our proposed approach over competitive baselines.",https://aclanthology.org/2022.dlg4nlp-1.3,Association for Computational Linguistics,2022,July,Proceedings of the 2nd Workshop on Deep Learning on Graphs for Natural Language Processing (DLG4NLP 2022),"Guo, Jin  and
Han, Zhen  and
Zhou, Su  and
Li, Jiliang  and
Tresp, Volker  and
Wang, Yuyi",Continuous Temporal Graph Networks for Event-Based Graph Data,10.18653/v1/2022.dlg4nlp-1.3,dlg4nlp,1025
2021.acl-long.465,"['Data Management and Generation', 'Text Generation']",['Data Preparation'],,"This paper explores the task of Difficulty-Controllable Question Generation DCQG, which aims at generating questions with required difficulty levels. Previous research on this task mainly defines the difficulty of a question as whether it can be correctly answered by a Question Answering QA system, lacking interpretability and controllability. In our work, we redefine question difficulty as the number of inference steps required to answer it and argue that Question Generation QG systems should have stronger control over the logic of generated questions. To this end, we propose a novel framework that progressively increases question difficulty through step-bystep rewriting under the guidance of an extracted reasoning chain. A dataset is automatically constructed to facilitate the research, on which extensive experiments are conducted to test the performance of our method.",https://aclanthology.org/2021.acl-long.465,Association for Computational Linguistics,2021,August,Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),"Cheng, Yi  and
Li, Siyao  and
Liu, Bang  and
Zhao, Ruihui  and
Li, Sujian  and
Lin, Chenghua  and
Zheng, Yefeng",Guiding the Growth: Difficulty-Controllable Question Generation through Step-by-Step Rewriting,10.18653/v1/2021.acl-long.465,acl,852
2021.reinact-1.1,"['Dialogue Systems', 'Knowledge Representation and Reasoning']",['Knowledge Graphs'],,"The next generation of conversational AI systems need to: 1 process language incrementally, token-by-token to be more responsive and enable handling of conversational phenomena such as pauses, restarts and selfcorrections; 2 reason incrementally allowing meaning to be established beyond what is said; 3 be transparent and controllable, allowing designers as well as the system itself to easily establish reasons for particular behaviour and tailor to particular user groups, or domains. In this short paper we present ongoing preliminary work combining Dynamic Syntax DSan incremental, semantic grammar framework -with the Resource Description Framework RDF. This paves the way for the creation of incremental semantic parsers that progressively output semantic RDF graphs as an utterance unfolds in real-time. We also outline how the parser can be integrated with an incremental reasoning engine through RDF. We argue that this DS-RDF hybrid satisfies the desiderata listed above, yielding semantic infrastructure that can be used to build responsive, realtime, interpretable Conversational AI that can be rapidly customised for specific user groups such as people with dementia.",https://aclanthology.org/2021.reinact-1.1,Association for Computational Linguistics,2021,October,Proceedings of the Reasoning and Interaction Conference (ReInAct 2021),"Addlesee, Angus  and
Eshghi, Arash",Incremental Graph-Based Semantics and Reasoning for Conversational AI,,reinact,395
2021.konvens-1.10,"['Domain-specific NLP', 'Data Management and Generation', 'Information Extraction', 'Low-resource Languages', 'Classification Applications']","['Data Preparation', 'Temporal Event Understanding', 'NLP for News and Media']",['Annotation Processes'],"Existing datasets and methods that aim at the identification of time expressions in natural language text do not pay particular attention to expressions that are imprecise and that cannot be easily represented on a timeline. We call these vague time expressions VTEs. We present an analysis of existing time extraction approaches and steps towards a novel scheme for the annotation of VTEs, developed using a corpus of German news articles. To the best of our knowledge, this work is the first to suggest an extension of the ISO standard TimeML with the goal of enabling the annotation of VTEs. In addition, we present a collection of 339 German VTEs as well as classification experiments on the news corpus with results from 60 up to 77 macro-avg. F1 score.",https://aclanthology.org/2021.konvens-1.10,KONVENS 2021 Organizers,2021,6--9 September,Proceedings of the 17th Conference on Natural Language Processing (KONVENS 2021),"May, Ulrike  and
Zaczynska, Karolina  and
Moreno-Schneider, Juli{\'a}n  and
Rehm, Georg",Extraction and Normalization of Vague Time Expressions in German,,konvens,150
2021.konvens-1.24,"['Domain-specific NLP', 'Low-resource Languages']",,,"This paper presents WORDGUESS, a gamewith-a-purpose vocabulary training where -in order to guess a target word such as snowthe player is offered associations of that target word such as winter, white, cold. The game relies on existing association norms and co-occurrence information to establish an entertaining way of deepening the player's learning and understanding of vocabulary and of associative relatedness between words in the vocabulary. WORDGUESS comes with data in English and German and can be extended with data from further languages. From an application-oriented point of view the players' data enables us to induce conditions and weights for word association and to quantify contextual relationships, which is useful for many NLP purposes such as ontology induction and anaphora resolution.",https://aclanthology.org/2021.konvens-1.24,KONVENS 2021 Organizers,2021,6--9 September,Proceedings of the 17th Conference on Natural Language Processing (KONVENS 2021),"Oguz, Cennet  and
Blessing, Andr{\'e}  and
Kuhn, Jonas  and
Schulte Im Walde, Sabine","WordGuess: Using Associations for Guessing, Learning and Exploring Related Words",,konvens,308
2021.wnut-1.24,['Domain-specific NLP'],['NLP for News and Media'],['NLP for Social Media'],"Twitter data has become established as a valuable source of data for various application scenarios in the past years. For many such applications, it is necessary to know where Twitter posts tweets were sent from or what location they refer to. Researchers have frequently used exact coordinates provided in a small percentage of tweets, but Twitter removed the option to share these coordinates in mid-2019. Moreover, there is reason to suspect that a large share of the provided coordinates did not correspond to GPS coordinates of the user even before that. In this paper, we explain the situation and the 2019 policy change and shed light on the various options of still obtaining location information from tweets. We provide usage statistics including changes over time, and analyze what the removal of exact coordinates means for various common research tasks performed with Twitter data. Finally, we make suggestions for future research requiring geolocated tweets.",https://aclanthology.org/2021.wnut-1.24,Association for Computational Linguistics,2021,November,Proceedings of the Seventh Workshop on Noisy User-generated Text (W-NUT 2021),"Kruspe, Anna  and
H{\""a}berle, Matthias  and
Hoffmann, Eike J.  and
Rode-Hasinger, Samyo  and
Abdulahhad, Karam  and
Zhu, Xiao Xiang",Changes in Twitter geolocations: Insights and suggestions for future usage,10.18653/v1/2021.wnut-1.24,wnut,1456
2021.case-1.27,"['Domain-specific NLP', 'Information Extraction', 'Data Management and Generation']","['Event Extraction', 'Data Preparation', 'NLP for News and Media']",['NLP for Social Media'],"Evaluating the state-of-the-art event detection systems on determining spatio-temporal distribution of the events on the ground is performed unfrequently. But, the ability to both 1 extract events ""in the wild"" from text and 2 properly evaluate event detection systems has potential to support a wide variety of tasks such as monitoring the activity of sociopolitical movements, examining media coverage and public support of these movements, and informing policy decisions. Therefore, we study performance of the best event detection systems on detecting Black Lives Matter BLM events from tweets and news articles. The murder of George Floyd, an unarmed Black man, at the hands of police officers received global attention throughout the second half of 2020. Protests against police violence emerged worldwide and the BLM movement, which was once mostly regulated to the United States, was now seeing activity globally. This shared task asks participants to identify BLM related events from large unstructured data sources, using systems pretrained to extract socio-political events from text. We evaluate several metrics, assessing each system's ability to evolution of protest events both temporally and spatially. Results show that identifying daily protest counts is an easier task than classifying spatial and temporal protest trends simultaneously, with maximum performance of 0.745 Spearman and 0.210 Pearson r, respectively. Additionally, all baselines and participant systems suffered from low recall max.5.08, confirming the high impact of media sourcing in the modelling of protest movements.",https://aclanthology.org/2021.case-1.27,Association for Computational Linguistics,2021,August,Proceedings of the 4th Workshop on Challenges and Applications of Automated Extraction of Socio-political Events from Text (CASE 2021),"Giorgi, Salvatore  and
Zavarella, Vanni  and
Tanev, Hristo  and
Stefanovitch, Nicolas  and
Hwang, Sy  and
Hettiarachchi, Hansi  and
Ranasinghe, Tharindu  and
Kalyan, Vivek  and
Tan, Paul  and
Tan, Shaun  and
Andrews, Martin  and
Hu, Tiancheng  and
Stoehr, Niklas  and
Re, Francesco Ignazio  and
Vegh, Daniel  and
Atzenhofer, Dennis  and
Curtis, Brenda  and
H{\""u}rriyeto{\u{g}}lu, Ali","Discovering Black Lives Matter Events in the United States: Shared Task 3, CASE 2021",10.18653/v1/2021.case-1.27,case,410
2021.findings-acl.28,"['Model Architectures', 'Parsing', 'Information Extraction', 'Image and Video Processing']","['Transformer Models', 'Syntactic Parsing', 'Document Layout Analysis (DLA)']",['Dependency Parsing'],"Information Extraction IE for semistructured document images is often approached as a sequence tagging problem by classifying each recognized input token into one of the IOB Inside, Outside, and Beginning categories. However, such problem setup has two inherent limitations that 1 it cannot easily handle complex spatial relationships and 2 it is not suitable for highly structured information, which are nevertheless frequently observed in real-world document images. To tackle these issues, we first formulate the IE task as spatial dependency parsing problem that focuses on the relationship among text tokens in the documents. Under this setup, we then propose SPADE SPAtial DEpendency parser that models highly complex spatial relationships and an arbitrary number of information layers in the documents in an end-to-end manner. We evaluate it on various kinds of documents such as receipts, name cards, forms, and invoices, and show that it achieves a similar or better performance compared to strong baselines including BERT-based IOB taggger.",https://aclanthology.org/2021.findings-acl.28,Association for Computational Linguistics,2021,August,Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021,"Hwang, Wonseok  and
Yim, Jinyeong  and
Park, Seunghyun  and
Yang, Sohee  and
Seo, Minjoon",Spatial Dependency Parsing for Semi-Structured Document Information Extraction,10.18653/v1/2021.findings-acl.28,findings,976
K18-2008,"['Text Preprocessing', 'Parsing', 'Multilingual NLP', 'Low-resource Languages', 'Model Architectures']","['Syntactic Parsing', 'Part-of-Speech (POS) Tagging', 'Recurrent Neural Networks (RNNs)']","['Dependency Parsing', 'Long Short-Term Memory (LSTM) Models']","We propose a novel neural network model for joint part-of-speech POS tagging and dependency parsing. Our model extends the well-known BIST graph-based dependency parser Kiperwasser and Goldberg, 2016 by incorporating a BiLSTM-based tagging component to produce automatically predicted POS tags for the parser. On the benchmark English Penn treebank, our model obtains strong UAS and LAS scores at 94.51% and 92.87%, respectively, producing 1.5+% absolute improvements to the BIST graph-based parser, and also obtaining a state-of-the-art POS tagging accuracy at 97.97%. Furthermore, experimental results on parsing 61 ""big"" Universal Dependencies treebanks from raw texts show that our model outperforms the baseline UDPipe Straka and Straková, 2017 with 0.8% higher average POS tagging score and 3.6% higher average LAS score. In addition, with our model, we also obtain state-of-the-art downstream task scores for biomedical event extraction and opinion analysis applications. Our code is available together with all pretrained models at: https://github. com/datquocnguyen/jPTDP.",https://aclanthology.org/K18-2008,Association for Computational Linguistics,2018,October,Proceedings of the {C}o{NLL} 2018 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies,"Nguyen, Dat Quoc  and
Verspoor, Karin",An Improved Neural Network Model for Joint POS Tagging and Dependency Parsing,10.18653/v1/K18-2008,K18,890
2016.gwc-1.57,"['Data Management and Generation', 'Bilingual Lexicon Induction (BLI)', 'Low-resource Languages', 'Cross-lingual Application', 'Knowledge Representation and Reasoning']",['Data Preparation'],,"This paper reports the work of creating bilingual mappings in English for certain synsets of Hindi wordnet, the need for doing this, the methods adopted and the tools created for the task. Hindi wordnet, which forms the foundation for other Indian language wordnets, has been linked to the English WordNet. To maximize linkages, an important strategy of using direct and hypernymy linkages has been followed. However, the hypernymy linkages were found to be inadequate in certain cases and posed a challenge due to sense granularity of language. Thus, the idea of creating bilingual mappings was adopted as a solution. A bilingual mapping means a linkage between a concept in two different languages, with the help of translation and/or transliteration. Such mappings retain meaningful representations, while capturing semantic similarity at the same time. This has also proven to be a great enhancement of Hindi wordnet and can be a crucial resource for multilingual applications in natural language processing, including machine translation and cross language information retrieval.",https://aclanthology.org/2016.gwc-1.57,Global Wordnet Association,2016,27--30 January,Proceedings of the 8th Global WordNet Conference (GWC),"Singh, Meghna  and
Shukla, Rajita  and
Saraswati, Jaya  and
Kashyap, Laxmi  and
Kanojia, Diptesh  and
Bhattacharyya, Pushpak",Mapping it differently: A solution to the linking challenges,,gwc,813
2020.wnut-1.48,"['Domain-specific NLP', 'Classification Applications']","['Medical and Clinical NLP', 'NLP for News and Media']",['NLP for Social Media'],This paper presents our submission to Task 2 of the Workshop on Noisy User-generated Text. We explore improving the performance of a pre-trained transformer-based language model fine-tuned for text classification through an ensemble implementation that makes use of corpus level information and a handcrafted feature. We test the effectiveness of including the aforementioned features in accommodating the challenges of a noisy data set centred on a specific subject outside the remit of the pre-training data. We show that inclusion of additional features can improve classification results and achieve a score within 2 points of the top performing team.,https://aclanthology.org/2020.wnut-1.48,Association for Computational Linguistics,2020,November,Proceedings of the Sixth Workshop on Noisy User-generated Text (W-NUT 2020),"Perrio, Calum  and
Tayyar Madabushi, Harish",CXP949 at WNUT-2020 Task 2: Extracting Informative COVID-19 Tweets - RoBERTa Ensembles and The Continued Relevance of Handcrafted Features,10.18653/v1/2020.wnut-1.48,wnut,922
Q17-1035,"['Machine Translation (MT)', 'Low-resource Languages', 'Model Architectures']","['Neural MT (NMT)', 'Statistical MT (SMT)']",,"Current word alignment models do not distinguish between different types of alignment links. In this paper, we provide a new probabilistic model for word alignment where word alignments are associated with linguistically motivated alignment types. We propose a novel task of joint prediction of word alignment and alignment types and propose novel semi-supervised learning algorithms for this task. We also solve a sub-task of predicting the alignment type given an aligned word pair. In our experimental results, the generative models we introduce to model alignment types significantly outperform the models without alignment types.",https://aclanthology.org/Q17-1035,MIT Press,2017,,,"Mansouri Bigvand, Anahita  and
Bu, Te  and
Sarkar, Anoop",Joint Prediction of Word Alignment with Alignment Types,10.1162/tacl_a_00076,Q17,237
2020.parlaclarin-1.2,"['Domain-specific NLP', 'Data Management and Generation', 'Text Preprocessing', 'Low-resource Languages']",,,"The Swedish parliamentary debates have been available since 2010 through the parliament's open data web site Riksdagens öppna data. While fairly comprehensive, the structure of the data can be hard to understand and its content is somewhat noisy for use as a quality language resource. In order to make it easier to use and process -in particular for language technology research, but also for political science and other fields with an interest in parliamentary data -we have published a large selection of the debates in a cleaned and structured format, annotated with linguistic information and augmented with semantic links. Especially prevalent in the parliament's data were end-line hyphenations -something that tokenisers generally are not equipped for -and a lot of the effort went into resolving these. In this paper, we provide detailed descriptions of the structure and contents of the resource, and explain how it differs from the parliament's own version.",https://aclanthology.org/2020.parlaclarin-1.2,European Language Resources Association,2020,May,Proceedings of the Second ParlaCLARIN Workshop,"R{\o}dven Eide, Stian","Anf\""oranden: Annotated and Augmented Parliamentary Debates from Sweden",,parlaclarin,835
2022.repl4nlp-1.18,"['Embeddings', 'Multilingual NLP', 'Learning Paradigms', 'Low-resource Languages', 'Cross-lingual Application']","['Unsupervised Learning', 'Sentence Embeddings']",,"We propose novel structural-based approaches for the generation and comparison of cross lingual sentence representations. We do so by applying geometric and topological methods to analyze the structure of sentences, as captured by their word embeddings. The key properties of our methods are: a They are designed to be isometric invariant, in order to provide language-agnostic representations. b They are fully unsupervised, and use no cross-lingual signal. The quality of our representations, and their preservation across languages, are evaluated in similarity comparison tasks, achieving competitive results. Furthermore, we show that our structural-based representations can be combined with existing methods for improved results.",https://aclanthology.org/2022.repl4nlp-1.18,Association for Computational Linguistics,2022,May,Proceedings of the 7th Workshop on Representation Learning for NLP,"Haim Meirom, Shaked  and
Bobrowski, Omer",Unsupervised Geometric and Topological Approaches for Cross-Lingual Sentence Representation and Comparison,10.18653/v1/2022.repl4nlp-1.18,repl4nlp,196
2022.lnls-1.5,"['Learning Paradigms', 'Data Management and Generation']",['Data Analysis'],,"Training a model with access to human explanations can improve data efficiency and model performance on in-and out-of-domain data. Adding to these empirical findings, similarity with the process of human learning makes learning from explanations a promising way to establish a fruitful human-machine interaction. Several methods have been proposed for improving natural language processing NLP models with human explanations, that rely on different explanation types and mechanism for integrating these explanations into the learning process. These methods are rarely compared with each other, making it hard for practitioners to choose the best combination of explanation type and integration mechanism for a specific use-case. In this paper, we give an overview of different methods for learning from human explanations, and discuss different factors that can inform the decision of which method to choose for a specific use-case.",https://aclanthology.org/2022.lnls-1.5,Association for Computational Linguistics,2022,May,Proceedings of the First Workshop on Learning with Natural Language Supervision,"Hartmann, Mareike  and
Sonntag, Daniel",A survey on improving NLP models with human explanations,10.18653/v1/2022.lnls-1.5,lnls,831
2020.argmining-1.12,"['Data Management and Generation', 'Classification Applications']","['Data Preparation', 'Data Analysis', 'Multilabel Text Classification']",,"Using the appropriate style is key for writing a high-quality text. Reliable computational style analysis is hence essential for the automation of nearly all kinds of text synthesis tasks. Research on style analysis focuses on recognition problems such as authorship identification; the respective technology e.g., n-gram distribution divergence quantification showed to be effective for discrimination, but inappropriate for text synthesis since the ""essence of a style"" remains implicit. This paper contributes right here: it studies the automatic analysis of style at the knowledge-level based on rhetorical devices. To this end, we developed and evaluated a grammar-based approach for identifying 26 syntax-based devices. Then, we employed that approach to distinguish various patterns of style in selected sets of argumentative articles and presidential debates. The patterns reveal several insights into the style used there, while being adequate for integration in text synthesis systems.",https://aclanthology.org/2020.argmining-1.12,Association for Computational Linguistics,2020,December,Proceedings of the 7th Workshop on Argument Mining,"Al Khatib, Khalid  and
Morari, Viorel  and
Stein, Benno",Style Analysis of Argumentative Texts by Mining Rhetorical Devices,,argmining,1246
2021.bsnlp-1.8,"['Text Generation', 'Data Management and Generation', 'Low-resource Languages']","['Data Preparation', 'Text Simplification']",,"Parallel language corpora where regular texts are aligned with their simplified versions can be used in both natural language processing and theoretical linguistic studies. They are essential for the task of automatic text simplification, but can also provide valuable insights into the characteristics that make texts more accessible and reveal strategies that human experts use to simplify texts. Today, there exist a few parallel datasets for English and Simple English, but many other languages lack such data. In this paper we describe our work on creating an aligned Russian-Simple Russian dataset composed of Russian literature texts adapted for learners of Russian as a foreign language. This will be the first parallel dataset in this domain, and one of the first Simple Russian datasets in general.",https://aclanthology.org/2021.bsnlp-1.8,Association for Computational Linguistics,2021,April,Proceedings of the 8th Workshop on Balto-Slavic Natural Language Processing,"Dmitrieva, Anna  and
Tiedemann, J{\""o}rg",Creating an Aligned Russian Text Simplification Dataset from Language Learner Data,,bsnlp,933
2021.calcs-1.2,"['Evaluation Techniques', 'Multilingual NLP', 'Low-resource Languages']",,,"Code-mixing is a frequent communication style among multilingual speakers where they mix words and phrases from two different languages in the same utterance of text or speech. Identifying and filtering code-mixed text is a challenging task due to its co-existence with monolingual and noisy text. Over the years, several code-mixing metrics have been extensively used to identify and validate codemixed text quality. This paper demonstrates several inherent limitations of code-mixing metrics with examples from the already existing datasets that are popularly used across various experiments.",https://aclanthology.org/2021.calcs-1.2,Association for Computational Linguistics,2021,June,Proceedings of the Fifth Workshop on Computational Approaches to Linguistic Code-Switching,"Srivastava, Vivek  and
Singh, Mayank",Challenges and Limitations with the Metrics Measuring the Complexity of Code-Mixed Text,10.18653/v1/2021.calcs-1.2,calcs,71
2020.nlpcovid19-acl.4,"['Domain-specific NLP', 'Data Management and Generation', 'Information Extraction', 'Learning Paradigms', 'Classification Applications']","['Event Extraction', 'Medical and Clinical NLP', 'Data Preparation', 'NLP for News and Media', 'Transfer Learning']","['Annotation Processes', 'NLP for Social Media']","Social-science investigations can benefit from a direct comparison of heterogenous corpora: in this work, we compare U.S. state-level COVID-19 policy announcements with policy discussions on Twitter. To perform this task, we require classifiers with high transfer accuracy to both 1 classify policy announcements and 2 classify tweets. We find that cotraining using event-extraction views significantly improves the transfer accuracy of our RoBERTa classifier by 3% above a RoBERTa baseline and 11% above other baselines. The same improvements are not observed for baseline views. With a set of 576 COVID-19 policy announcements, hand-labeled into 1 of 6 categories, our classifier observes a maximum transfer accuracy of .77 f1-score on a handvalidated set of tweets. This work represents the first known application of these techniques to an NLP transfer learning task and facilitates cross-corpora comparisons necessary for studies of social science phenomena.",https://aclanthology.org/2020.nlpcovid19-acl.4,Association for Computational Linguistics,2020,July,Proceedings of the 1st Workshop on {NLP} for {COVID-19} at {ACL} 2020,"Spangher, Alexander  and
Peng, Nanyun  and
May, Jonathan  and
Ferrara, Emilio",Enabling Low-Resource Transfer Learning across COVID-19 Corpora by Combining Event-Extraction and Co-Training,,nlpcovid19,1256
2021.codi-main.1,"['Question Answering (QA)', 'Data Management and Generation', 'Classification Applications']",['Data Preparation'],,"Indirect answers are replies to polar questions without the direct use of word cues such as 'yes' and 'no'. Humans are very good at understanding indirect answers, such as 'I gotta go home sometime', when asked 'You wanna crash on the couch?'. Understanding indirect answers is a challenging problem for dialogue systems. In this paper, we introduce a new English corpus to study the problem of understanding indirect answers. Instead of crowdsourcing both polar questions and answers, we collect questions and indirect answers from transcripts of a prominent TV series and manually annotate them for answer type. The resulting dataset contains 5,930 question-answer pairs. We release both aggregated and raw human annotations. We present a set of experiments in which we evaluate Convolutional Neural Networks CNNs for this task, including a cross-dataset evaluation and experiments with learning from disagreements in annotation. Our results show that the task of interpreting indirect answers remains challenging, yet we obtain encouraging improvements when explicitly modeling human disagreement.",https://aclanthology.org/2021.codi-main.1,Association for Computational Linguistics,2021,November,Proceedings of the 2nd Workshop on Computational Approaches to Discourse,"Damgaard, Cathrine  and
Toborek, Paulina  and
Eriksen, Trine  and
Plank, Barbara",``I'll be there for you'': The One with Understanding Indirect Answers,10.18653/v1/2021.codi-main.1,codi,233
2020.lt4gov-1.1,"['Information Extraction', 'Domain-specific NLP', 'Data Management and Generation']",['Data Preparation'],,"The disability benefits programs administered by the US Social Security Administration SSA receive between 2 and 3 million new applications each year. Adjudicators manually review hundreds of evidence pages per case to determine eligibility based on financial, medical, and functional criteria. Natural Language Processing NLP technology is uniquely suited to support this adjudication work and is a critical component of an ongoing inter-agency collaboration between SSA and the National Institutes of Health. This NLP work provides resources and models for document ranking, named entity recognition, and terminology extraction in order to automatically identify documents and reports pertinent to a case, and to allow adjudicators to search for and locate desired information quickly. In this paper, we describe our vision for how NLP can impact SSA's adjudication process, present the resources and models that have been developed, and discuss some of the benefits and challenges in working with large-scale government data, and its specific properties in the functional domain.",https://aclanthology.org/2020.lt4gov-1.1,European Language Resources Association,2020,May,Proceedings of the 1st Workshop on Language Technologies for Government and Public Administration (LT4Gov),"Desmet, Bart  and
Porcino, Julia  and
Zirikly, Ayah  and
Newman-Griffis, Denis  and
Divita, Guy  and
Rasch, Elizabeth",Development of Natural Language Processing Tools to Support Determination of Federal Disability Benefits in the U.S.,,lt4gov,130
2022.clpsych-1.8,"['Learning Paradigms', 'Data Management and Generation', 'Classification Applications', 'Domain-specific NLP']","['Transfer Learning', 'Data Analysis', 'Medical and Clinical NLP']",['NLP for Mental Health'],"Evidence has demonstrated the presence of similarities in language use across people with various mental health conditions. In this work we investigate these relationships both as described in literature and as a data analysis problem. We also introduce a novel transfer learning based approach that learns from linguistic feature spaces of previous conditions and predicts unknown ones. Our model achieves strong performance, with F 1 scores of 0.75, 0.80, and 0.76 at detecting depression, stress, and suicidal ideation in a first-of-its-kind transfer task and offering promising evidence that language models can harness learned patterns from known mental health conditions to aid in their prediction of others that may lie latent.",https://aclanthology.org/2022.clpsych-1.8,Association for Computational Linguistics,2022,July,Proceedings of the Eighth Workshop on Computational Linguistics and Clinical Psychology,"Aich, Ankit  and
Parde, Natalie",Are You Really Okay? A Transfer Learning-based Approach for Identification of Underlying Mental Illnesses,10.18653/v1/2022.clpsych-1.8,clpsych,833
2020.clssts-1.7,"['Evaluation Techniques', 'Cross-lingual Application', 'Information Retrieval', 'Low-resource Languages']",,,"In the IARPA MATERIAL program, information retrieval IR is treated as a hard detection problem; the system has to output a single global ranking over all queries, and apply a hard threshold on this global list to come up with all the hypothesized relevant documents. This means that how queries are ranked relative to each other can have a dramatic impact on performance. In this paper, we study such a performance measure, the Average Query Weighted Value AQWV, which is a combination of miss and false alarm rates. AQWV requires that the same detection threshold is applied to all queries. Hence, detection scores of different queries should be comparable, and, to do that, a score normalization technique commonly used in keyword spotting from speech should be used. We describe unsupervised methods for score normalization, which are borrowed from the speech field and adapted accordingly for IR, and demonstrate that they greatly improve AQWV on the task of cross-language information retrieval CLIR, on three low-resource languages used in MATERIAL. We also present a novel supervised score normalization approach which gives additional gains. * While at Raytheon BBN Technologies. 1 We are using document-level granularity in this paper, although similar techniques can be used for different granularities as well.",https://aclanthology.org/2020.clssts-1.7,European Language Resources Association,2020,May,Proceedings of the workshop on Cross-Language Search and Summarization of Text and Speech (CLSSTS2020),"Karakos, Damianos  and
Zbib, Rabih  and
Hartmann, William  and
Schwartz, Richard  and
Makhoul, John",Reformulating Information Retrieval from Speech and Text as a Detection Problem,,clssts,351
Q17-1016,"['Domain-specific NLP', 'Model Architectures']",,,"Debate and deliberation play essential roles in politics and government, but most models presume that debates are won mainly via superior style or agenda control. Ideally, however, debates would be won on the merits, as a function of which side has the stronger arguments. We propose a predictive model of debate that estimates the effects of linguistic features and the latent persuasive strengths of different topics, as well as the interactions between the two. Using a dataset of 118 Oxford-style debates, our model's combination of content as latent topics and style as linguistic features allows us to predict audience-adjudicated winners with 74% accuracy, significantly outperforming linguistic features alone 66%. Our model finds that winning sides employ stronger arguments, and allows us to identify the linguistic features associated with strong or weak arguments.",https://aclanthology.org/Q17-1016,MIT Press,2017,,,"Wang, Lu  and
Beauchamp, Nick  and
Shugars, Sarah  and
Qin, Kechen",Winning on the Merits: The Joint Effects of Content and Style on Debate Outcomes,10.1162/tacl_a_00057,Q17,907
J17-4003,['Model Architectures'],['Recurrent Neural Networks (RNNs)'],,"We present novel methods for analyzing the activation patterns of recurrent neural networks from a linguistic point of view and explore the types of linguistic structure they learn. As a case study, we use a standard standalone language model, and a multi-task gated recurrent network architecture consisting of two parallel pathways with shared word embeddings: The VISUAL pathway is trained on predicting the representations of the visual scene corresponding to an input sentence, and the TEXTUAL pathway is trained to predict the next word in the same sentence. We propose a method for estimating the amount of contribution of individual tokens in the input to the final prediction of the networks. Using this method, we show that the VISUAL pathway pays selective attention to lexical categories and grammatical functions that carry semantic information, and learns to treat word types differently depending on their grammatical function and their position in the sequential structure of the sentence. In contrast, the language models are comparatively more sensitive to words with a syntactic function. Further analysis of the most informative n-gram contexts for each model shows that in comparison with the VISUAL pathway, the language models react more strongly to abstract contexts that represent syntactic constructions.",https://aclanthology.org/J17-4003,MIT Press,2017,December,,"K{\'a}d{\'a}r, {\'A}kos  and
Chrupa{\l}a, Grzegorz  and
Alishahi, Afra",Representation of Linguistic Form and Function in Recurrent Neural Networks,10.1162/COLI_a_00300,J17,1364
2022.naacl-main.261,"['Evaluation Techniques', 'Data Management and Generation', 'Image and Video Processing']",['Data Preparation'],,"Recent video-text models can retrieve relevant videos based on text with a high accuracy, but to what extent do they comprehend the semantics of the text? Can they discriminate between similar entities and actions? To answer this, we propose an evaluation framework that probes video-text models with hard negatives. We automatically build contrast sets, where true textual descriptions are manipulated in ways that change their semantics while maintaining plausibility. Specifically, we leverage a pre-trained language model and a set of heuristics to create verb and person entity focused contrast sets. We apply these in the multiple choice video-totext classification setting. We test the robustness of recent methods on the proposed automatic contrast sets, and compare them to additionally collected human-generated counterparts, to assess their effectiveness. We see that model performance suffers across all methods, erasing the gap between recent CLIP-based methods vs. the earlier methods. 1",https://aclanthology.org/2022.naacl-main.261,Association for Computational Linguistics,2022,July,Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,"Park, Jae Sung  and
Shen, Sheng  and
Farhadi, Ali  and
Darrell, Trevor  and
Choi, Yejin  and
Rohrbach, Anna",Exposing the Limits of Video-Text Models through Contrast Sets,10.18653/v1/2022.naacl-main.261,naacl,1430
W19-5413,"['Machine Translation (MT)', 'Data Management and Generation', 'Low-resource Languages', 'Model Architectures']","['Transformer Models', 'Data Augmentation', 'Neural MT (NMT)']",,"This paper describes Unbabel's submission to the WMT2019 APE Shared Task for the English-German language pair. Following the recent rise of large, powerful, pretrained models, we adapt the BERT pretrained model to perform Automatic Post-Editing in an encoder-decoder framework. Analogously to dual-encoder architectures we develop a BERT-based encoder-decoder BED model in which a single pretrained BERT encoder receives both the source src and machine translation mt strings. Furthermore, we explore a conservativeness factor to constrain the APE system to perform fewer edits. As the official results show, when trained on a weighted combination of in-domain and artificial training data, our BED system with the conservativeness penalty improves significantly the translations of a strong Neural Machine Translation NMT system by −0.78 and +1.23 in terms of TER and BLEU, respectively. Finally, our submission achieves a new state-of-the-art, exaequo, in English-German APE of NMT.",https://aclanthology.org/W19-5413,Association for Computational Linguistics,2019,August,"Proceedings of the Fourth Conference on Machine Translation (Volume 3: Shared Task Papers, Day 2)","Lopes, Ant{\'o}nio V.  and
Farajian, M. Amin  and
Correia, Gon{\c{c}}alo M.  and
Tr{\'e}nous, Jonay  and
Martins, Andr{\'e} F. T.",Unbabel's Submission to the WMT2019 APE Shared Task: BERT-Based Encoder-Decoder for Automatic Post-Editing,10.18653/v1/W19-5413,W19,1342
W17-3528,"['Text Generation', 'Domain-specific NLP', 'Low-resource Languages']","['Data-to-Text Generation', 'NLP for News and Media']",,"Despite increasing amounts of data and ever improving natural language generation techniques, work on automated journalism is still relatively scarce. In this paper, we explore the field and challenges associated with building a journalistic natural language generation system. We present a set of requirements that should guide system design, including transparency, accuracy, modifiability and transferability. Guided by the requirements, we present a data-driven architecture for automated journalism that is largely domain and language independent. We illustrate its practical application in the production of news articles upon a user request about the 2017 Finnish municipal elections in three languages, demonstrating the successfulness of the data-driven, modular approach of the design. We then draw some lessons for future automated journalism.",https://aclanthology.org/W17-3528,Association for Computational Linguistics,2017,September,Proceedings of the 10th International Conference on Natural Language Generation,"Lepp{\""a}nen, Leo  and
Munezero, Myriam  and
Granroth-Wilding, Mark  and
Toivonen, Hannu",Data-Driven News Generation for Automated Journalism,10.18653/v1/W17-3528,W17,1277
2019.icon-1.20,"['Data Management and Generation', 'Classification Applications', 'Learning Paradigms']","['Unsupervised Learning', 'Data Preparation', 'Emotion Detection']",,"Existing supervised solutions for emotion classification demand large amount of emotion annotated data. Such resources may not be available for many languages. However, it is common to have sentiment annotated data available in these languages. The sentiment information +1 or -1 is useful to segregate between positive emotions or negative emotions. In this paper, we propose an unsupervised approach for emotion recognition by taking advantage of the sentiment information. Given a sentence and its sentiment information, recognize the best possible emotion for it. For every sentence, the semantic relatedness between the words from sentence and a set of emotionspecific words is calculated using cosine similarity. An emotion vector representing the emotion score for each emotion category of Ekman's model, is created. It is further improved with the dependency relations and the best possible emotion is predicted. The results show the significant improvement in f-score values for text with sentiment information as input over our baseline as text without sentiment information. We report the weighted fscore on three different datasets with the Ekman's emotion model. This supports that by leveraging the sentiment value, better emotion annotated data can be created.",https://aclanthology.org/2019.icon-1.20,NLP Association of India,2019,December,Proceedings of the 16th International Conference on Natural Language Processing,"Kulkarni, Manasi  and
Bhattacharyya, Pushpak",Converting Sentiment Annotated Data to Emotion Annotated Data,,icon,399
2021.cl-3.22,['Embeddings'],,,"Word vector representations have a long tradition in several research fields, such as cognitive science or computational linguistics. They have been used to represent the meaning of various units of natural languages, including, among others, words, phrases, and sentences. Before the deep learning tsunami, count-based vector space models had been successfully used in computational linguistics to represent the semantics of natural languages. However, the rise of neural networks in NLP popularized the use of word embeddings, which are now applied as pre-trained vectors in most machine learning architectures. This book, written by Mohammad Taher Pilehvar and Jose Camacho-Collados, provides a comprehensive and easy-to-read review of the theory and advances in vector models for NLP, focusing specially on semantic representations and their applications. It is a great introduction to different types of embeddings and the background and motivations behind them. In this sense, the authors adequately present the most relevant concepts and approaches that have been used to build vector representations. They also keep track of the most recent advances of this vibrant and fast-evolving area of research, discussing cross-lingual representations and current language models based on the Transformer. Therefore, this is a useful book for researchers interested in computational methods for semantic representations and artificial intelligence. Although some basic knowledge of machine learning may be necessary to follow a few topics, the book includes clear illustrations and explanations, which make it accessible to a wide range of readers. Apart from the preface and the conclusions, the book is organized into eight chapters. In the first two, the authors introduce some of the core ideas of NLP and artificial neural networks, respectively, discussing several concepts that will be useful throughout the book. Then, Chapters 3 to 6 present different types of vector representations at the lexical level word embeddings, graph embeddings, sense embeddings, and contextualized embeddings, followed by a brief chapter 7 about sentence and document embeddings. For each specific topic, the book includes methods and data sets to assess the quality of the embeddings. Finally, Chapter 8 raises ethical issues involved",https://aclanthology.org/2021.cl-3.22,MIT Press,2021,November,,"Garcia, Marcos",Embeddings in Natural Language Processing: Theory and Advances in Vector Representations of Meaning,10.1162/coli_r_00410,cl,580
2020.emnlp-main.342,"['Model Architectures', 'Information Retrieval']",['Transformer Models'],,"Recent innovations in Transformer-based ranking models have advanced the state-ofthe-art in information retrieval. However, these Transformers are computationally expensive, and their opaque hidden states make it hard to understand the ranking process. In this work, we modularize the Transformer ranker into separate modules for text representation and interaction. We show how this design enables substantially faster ranking using offline pre-computed representations and light-weight online interactions. The modular design is also easier to interpret and sheds light on the ranking process in Transformer rankers. 1",https://aclanthology.org/2020.emnlp-main.342,Association for Computational Linguistics,2020,November,Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),"Gao, Luyu  and
Dai, Zhuyun  and
Callan, Jamie",Modularized Transfomer-based Ranking Framework,10.18653/v1/2020.emnlp-main.342,emnlp,611
2022.humeval-1.9,"['Text Generation', 'Evaluation Techniques']",['Text Style Transfer'],,"Although text style transfer has witnessed rapid development in recent years, there is as yet no established standard for evaluation, which is performed using several automatic metrics, lacking the possibility of always resorting to human judgement. We focus on the task of formality transfer, and on the three aspects that are usually evaluated: style strength, content preservation, and fluency. To cast light on how such aspects are assessed by common and new metrics, we run a human-based evaluation and perform a rich correlation analysis. We are then able to offer some recommendations on the use of such metrics in formality transfer, also with an eye to their generalisability or not to related tasks. 1",https://aclanthology.org/2022.humeval-1.9,Association for Computational Linguistics,2022,May,Proceedings of the 2nd Workshop on Human Evaluation of NLP Systems (HumEval),"Lai, Huiyuan  and
Mao, Jiali  and
Toral, Antonio  and
Nissim, Malvina",Human Judgement as a Compass to Navigate Automatic Metrics for Formality Transfer,10.18653/v1/2022.humeval-1.9,humeval,1482
Q19-1008,['Model Architectures'],['Recurrent Neural Networks (RNNs)'],,"Stacking long short-term memory LSTM cells or gated recurrent units GRUs as part of a recurrent neural network RNN has become a standard approach to solving a number of tasks ranging from language modeling to text summarization. Although LSTMs and GRUs were designed to model long-range dependencies more accurately than conventional RNNs, they nevertheless have problems copying or recalling information from the long distant past. Here, we derive a phase-coded representation of the memory state, Rotational Unit of Memory RUM, that unifies the concepts of unitary learning and associative memory. We show experimentally that RNNs based on RUMs can solve basic sequential tasks such as memory copying and memory recall much better than LSTMs/GRUs. We further demonstrate that by replacing LSTM/GRU with RUM units we can apply neural networks to real-world problems such as language modeling and text summarization, yielding results comparable to the state of the art.",https://aclanthology.org/Q19-1008,MIT Press,2019,,,"Dangovski, Rumen  and
Jing, Li  and
Nakov, Preslav  and
Tatalovi{\'c}, Mi{\'c}o  and
Solja{\v{c}}i{\'c}, Marin",Rotational Unit of Memory: A Novel Representation Unit for RNNs with Scalable Applications,10.1162/tacl_a_00258,Q19,702
R19-1155,"['Data Management and Generation', 'Domain-specific NLP']","['Data Analysis', 'Medical and Clinical NLP']",,"NLP approaches to automatic text adaptation often rely on user-need guidelines which are generic and do not account for the differences between various types of target groups. One such group are adults with high-functioning autism, who are usually able to read long sentences and comprehend difficult words but whose comprehension may be impeded by other linguistic constructions. This is especially challenging for real-world usergenerated texts such as product reviews, which cannot be controlled editorially and are thus in a stronger need of automatic adaptation. To address this problem, we present a mixedmethods survey conducted with 24 adult webusers diagnosed with autism and an agematched control group of 33 neurotypical participants. The aim of the survey is to identify whether the group with autism experiences any barriers when reading online reviews, what these potential barriers are, and what NLP methods would be best suited to improve the accessibility of online reviews for people with autism. The group with autism consistently reported significantly greater difficulties with understanding online product reviews compared to the control group and identified issues related to text length, poor topic organisation, identifying the intention of the author, trustworthiness, and the use of irony, sarcasm and exaggeration.",https://aclanthology.org/R19-1155,INCOMA Ltd.,2019,September,Proceedings of the International Conference on Recent Advances in Natural Language Processing (RANLP 2019),"Yaneva, Victoria  and
Orasan, Constantin  and
Ha, Le An  and
Ponomareva, Natalia",A Survey of the Perceived Text Adaptation Needs of Adults with Autism,10.26615/978-954-452-056-4_155,R19,864
2021.wassa-1.3,"['Domain-specific NLP', 'Classification Applications', 'Data Management and Generation']","['Data Preparation', 'NLP for News and Media', 'Sentiment Analysis (SA)', 'Medical and Clinical NLP']",,"Ideological differences have had a large impact on individual and community response to the COVID-19 pandemic in the United States. Early behavioral research during the pandemic showed that conservatives were less likely to adhere to health directives, which contradicts a body of work suggesting that conservative ideology emphasizes a rule abiding, loss aversion, and prevention focus. We reconcile this contradiction by analyzing semantic content of local press releases, federal press releases, and localized tweets during the first month of the government response to COVID-19 in the United States. Controlling for factors such as COVID-19 confirmed cases and deaths, local economic indicators, and more, we find that online expressions of fear in conservative areas lead to an increase in adherence to public health recommendations concerning COVID-19, and that expressions of fear in government press releases are a significant predictor of expressed fear on Twitter.",https://aclanthology.org/2021.wassa-1.3,Association for Computational Linguistics,2021,April,"Proceedings of the Eleventh Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis","Lindow, Mike  and
DeFranza, David  and
Mishra, Arul  and
Mishra, Himanshu",Partisanship and Fear are Associated with Resistance to COVID-19 Directives,,wassa,769
W17-4424,"['Domain-specific NLP', 'Information Extraction']","['Named Entity Recognition (NER)', 'NLP for News and Media']",['NLP for Social Media'],"Detecting previously unseen named entities in text is a challenging task. The paper describes how three initial classifier models were built using Conditional Random Fields CRFs, Support Vector Machines SVMs and a Long Short-Term Memory LSTM recurrent neural network. The outputs of these three classifiers were then used as features to train another CRF classifier working as an ensemble. 5-fold cross-validation based on training and development data for the emerging and rare named entity recognition shared task showed precision, recall and F 1score of 66.87%, 46.75% and 54.97%, respectively. For surface form evaluation, the CRF ensemble-based system achieved precision, recall and F 1 scores of 65.18%, 45.20% and 53.30%. When applied to unseen test data, the model reached 47.92% precision, 31.97% recall and 38.55% F 1score for entity level evaluation, with the corresponding surface form evaluation values of 44.91%, 30.47% and 36.31%.",https://aclanthology.org/W17-4424,Association for Computational Linguistics,2017,September,Proceedings of the 3rd Workshop on Noisy User-generated Text,"Sikdar, Utpal Kumar  and
Gamb{\""a}ck, Bj{\""o}rn",A Feature-based Ensemble Approach to Recognition of Emerging and Rare Named Entities,10.18653/v1/W17-4424,W17,853
K18-2004,"['Parsing', 'Learning Paradigms', 'Multilingual NLP', 'Text Preprocessing']","['Syntactic Parsing', 'Part-of-Speech (POS) Tagging']",['Dependency Parsing'],"This paper describes the ICS PAS system which took part in CoNLL 2018 shared task on Multilingual Parsing from Raw Text to Universal Dependencies. The system consists of jointly trained tagger, lemmatizer, and dependency parser which are based on features extracted by a biL-STM network. The system uses both fully connected and dilated convolutional neural architectures. The novelty of our approach is the use of an additional loss function, which reduces the number of cycles in the predicted dependency graphs, and the use of self-training to increase the system performance. The proposed system, i.e. ICS PAS Warszawa, ranked 3th/4th in the official evaluation 1 obtaining the following overall results: 73.02 LAS, 60.25 MLAS and 64.44 BLEX.",https://aclanthology.org/K18-2004,Association for Computational Linguistics,2018,October,Proceedings of the {C}o{NLL} 2018 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies,"Rybak, Piotr  and
Wr{\'o}blewska, Alina","Semi-Supervised Neural System for Tagging, Parsing and Lematization",10.18653/v1/K18-2004,K18,942
K17-2012,"['Learning Paradigms', 'Low-resource Languages', 'Model Architectures']","['Supervised Learning', 'Recurrent Neural Networks (RNNs)']",['Long Short-Term Memory (LSTM) Models'],"This paper describes the Stockholm University/University of Groningen SU-RUG system for the SIGMORPHON 2017 shared task on morphological inflection. Our system is based on an attentional sequence-to-sequence neural network model using Long Short-Term Memory LSTM cells, with joint training of morphological inflection and the inverse transformation, i.e. lemmatization and morphological analysis. Our system outperforms the baseline with a large margin, and our submission ranks as the 4 th best team for the track we participate in task 1, high-resource.",https://aclanthology.org/K17-2012,Association for Computational Linguistics,2017,August,Proceedings of the {C}o{NLL} {SIGMORPHON} 2017 Shared Task: Universal Morphological Reinflection,"{\""O}stling, Robert  and
Bjerva, Johannes",SU-RUG at the CoNLL-SIGMORPHON 2017 shared task: Morphological Inflection with Attentional Sequence-to-Sequence Models,10.18653/v1/K17-2012,K17,425
N18-3008,"['Automated Essay Scoring', 'Audio Generation and Processing', 'Domain-specific NLP', 'Data Management and Generation', 'Information Retrieval']","['Data Preparation', 'Information Filtering']",,"In large-scale educational assessments, the use of automated scoring has recently become quite common. While the majority of student responses can be processed and scored without difficulty, there are a small number of responses that have atypical characteristics that make it difficult for an automated scoring system to assign a correct score. We describe a pipeline that detects and processes these kinds of responses at run-time. We present the most frequent kinds of what are called non-scorable responses along with effective filtering models based on various NLP and speech processing technologies. We give an overview of two operational automated scoring systems -one for essay scoring and one for speech scoringand describe the filtering models they use. Finally, we present an evaluation and analysis of filtering models used for spoken responses in an assessment of language proficiency.",https://aclanthology.org/N18-3008,Association for Computational Linguistics,2018,June,"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 3 (Industry Papers)","Yoon, Su-Youn  and
Cahill, Aoife  and
Loukina, Anastassia  and
Zechner, Klaus  and
Riordan, Brian  and
Madnani, Nitin",Atypical Inputs in Educational Applications,10.18653/v1/N18-3008,N18,1434
2020.spnlp-1.10,"['Evaluation Techniques', 'Low-resource Languages', 'Machine Translation (MT)']",,,"Many sequence-to-sequence generation tasks, including machine translation and text-tospeech, can be posed as estimating the density of the output y given the input x: py|x. Given this interpretation, it is natural to evaluate sequence-to-sequence models using conditional log-likelihood on a test set. However, the goal of sequence-to-sequence generation or structured prediction is to find the best output ŷ given an input x, and each task has its own downstream metric R that scores a model output by comparing against a set of references y * : Rŷ, y * |x. While we hope that a model that excels in density estimation also performs well on the downstream metric, the exact correlation has not been studied for sequence generation tasks. In this paper, by comparing several density estimators on five machine translation tasks, we find that the correlation between rankings of models based on log-likelihood and BLEU varies significantly depending on the range of the model families being compared. First, log-likelihood is highly correlated with BLEU when we consider models within the same family e.g. autoregressive models, or latent variable models with the same parameterization of the prior. However, we observe no correlation between rankings of models across different families: 1 among non-autoregressive latent variable models, a flexible prior distribution is better at density estimation but gives worse generation quality than a simple prior, and 2 autoregressive models offer the best translation performance overall, while latent variable models with a normalizing flow prior give the highest held-out log-likelihood across all datasets.",https://aclanthology.org/2020.spnlp-1.10,Association for Computational Linguistics,2020,November,Proceedings of the Fourth Workshop on Structured Prediction for NLP,"Lee, Jason  and
Tran, Dustin  and
Firat, Orhan  and
Cho, Kyunghyun",On the Discrepancy between Density Estimation and Sequence Generation,10.18653/v1/2020.spnlp-1.10,spnlp,1092
2020.codi-1.5,"['Data Management and Generation', 'Information Extraction', 'Classification Applications', 'Learning Paradigms', 'Low-resource Languages', 'Model Architectures']","['Supervised Learning', 'Data Preparation', 'Humor Detection']",['Annotation Processes'],"Sketch comedy and crosstalk are two popular types of comedy. They can relieve people's stress and thus benefit their mental health, especially when performances and scripts are high-quality. However, writing a script is time-consuming and its quality is difficult to achieve. In order to minimise the time and effort needed for producing an excellent script, we explore ways of predicting the audience's response from the comedy scripts. For this task, we present a corpus of annotated scripts from popular television entertainment programmes in recent years. Annotations include a text classification labels, indicating which actor's lines made the studio audience laugh; b information extraction labels, i.e. the text spans that made the audience laughed immediately after the performers said them. The corpus will also be useful for dialogue systems and discourse analysis, since our annotations are based on entire scripts. In addition, we evaluate different baseline algorithms. Experimental results demonstrate that BERT models can achieve the best predictions among all the baseline methods. Furthermore, we conduct an error analysis and investigate predictions across scripts with different styles. 1 * The research was conducted during non-working time. The idea of this research was inspired by a discussion with my friend about an entertainment TV programme in which the comedians mentioned the difficulties of producing a highquality script.",https://aclanthology.org/2020.codi-1.5,Association for Computational Linguistics,2020,November,Proceedings of the First Workshop on Computational Approaches to Discourse,"Li, Maolin",Supporting Comedy Writers: Predicting Audience's Response from Sketch Comedy and Crosstalk Scripts,10.18653/v1/2020.codi-1.5,codi,31
2021.mmtlrl-1.1,['Machine Translation (MT)'],,,"In this talk, I will describe current research directions in my group that aim to make machine translation MT more human-centered. Instead of viewing MT solely as a task that aims to transduce a source sentence into a wellformed target language equivalent, we revisit all steps of the MT research and development lifecycle with the goal of designing MT systems that are able to help people communicate across language barriers. I will present methods to better characterize the parallel training data that powers MT systems, and how the degree of equivalence impacts translation quality. I will introduce models that enable flexible conditional language generation, and will discuss recent work on framing machine translation tasks and evaluation to center human factors.",https://aclanthology.org/2021.mmtlrl-1.1,INCOMA Ltd.,2021,September,Proceedings of the First Workshop on Multimodal Machine Translation for Low Resource Languages (MMTLRL 2021),"Carpuat, Marine",Models and Tasks for Human-Centered Machine Translation,10.26615/978-954-452-073-1_001,mmtlrl,1267
2016.gwc-1.32,"['Parsing', 'Data Management and Generation', 'Knowledge Representation and Reasoning', 'Low-resource Languages']","['Data Preparation', 'Semantic Parsing']",['Semantic Role Labeling'],"The aim of this paper is to show a language-independent process of creating a new semantic relation between adjectives and nouns in wordnets. The existence of such a relation is expected to improve the detection of figurative language and sentiment analysis SA. The proposed method uses an annotated corpus to explore the semantic knowledge contained in linguistic constructs performing as the rhetorical figure Simile. Based on the frequency of occurrence of similes in an annotated corpus, we propose a new relation, which connects the noun synset with the synset of an adjective representing that noun's specific attribute. We elaborate on adding this new relation in the case of the Serbian WordNet SWN. The proposed method is evaluated by human judgement in order to determine the relevance of automatically selected relation items. The evaluation has shown that 84% of the automatically selected and the most frequent linguistic constructs, whose frequency threshold was equal to 3, were also selected by humans.",https://aclanthology.org/2016.gwc-1.32,Global Wordnet Association,2016,27--30 January,Proceedings of the 8th Global WordNet Conference (GWC),"Mladenovi{\'c}, Miljana  and
Mitrovi{\'c}, Jelena  and
Krstev, Cvetana",A Language-independent Model for Introducing a New Semantic Relation Between Adjectives and Nouns in a WordNet,,gwc,167
2017.mtsummit-commercial.13,"['Audio Generation and Processing', 'Data Management and Generation', 'Low-resource Languages', 'Machine Translation (MT)']",['Data Analysis'],,"In this paper, we report on a pilot mixed-methods experiment investigating the effects on productivity and on the translator experience of integrating machine translation MT postediting PE with voice recognition VR and translation dictation TD. The experiment was performed with a sample of native Spanish participants. In the quantitative phase of the experiment, they performed four tasks under four different conditions, namely 1 conventional TD; 2 PE in dictation mode; 3 TD with VR; and 4 PE with VR PEVR. In the follow-on qualitative phase, the participants filled out an online survey, providing details of their perceptions of the task and of PEVR in general. Our results suggest that PEVR may be a usable way to add MT to a translation workflow, with some caveats. When asked about their experience with the tasks, our participants preferred translation without the 'constraint' of MT, though the quantitative results show that PE tasks were generally more efficient. This paper provides a brief overview of past work exploring VR for from-scratch translation and PE purposes, describes our pilot experiment in detail, presents an overview and analysis of the data collected, and outlines avenues for future work.",https://aclanthology.org/2017.mtsummit-commercial.13,,2017,September 18 {--} September 22,Proceedings of Machine Translation Summit XVI: Commercial MT Users and Translators Track,"Zapata, Juli{\'a}n  and
Castilho, Sheila  and
Moorkens, Joss",Translation Dictation vs. Post-editing with Cloud-based Voice Recognition: A Pilot Experiment,,mtsummit,1396
2021.mtsummit-research.7,"['Machine Translation (MT)', 'Data Management and Generation', 'Multilingual NLP', 'Learning Paradigms', 'Low-resource Languages']","['Unsupervised Learning', 'Neural MT (NMT)', 'Data Augmentation']",,"For most language combinations, parallel data is either scarce or simply unavailable. To address this, unsupervised machine translation UMT exploits large amounts of monolingual data by using synthetic data generation techniques such as back-translation and noising, while self-supervised NMT SSNMT identifies parallel sentences in smaller comparable data and trains on them. To date, the inclusion of UMT data generation techniques in SSNMT has not been investigated. We show that including UMT techniques into SSNMT significantly outperforms SSNMT and UMT on all tested language pairs, with improvements of up to +4.3 BLEU, +50.8 BLEU, +51.5 over SSNMT, statistical UMT and hybrid UMT, respectively, on Afrikaans to English. We further show that the combination of multilingual denoising autoencoding, SSNMT with backtranslation and bilingual finetuning enables us to learn machine translation even for distant language pairs for which only small amounts of monolingual data are available, e.g. yielding BLEU scores of 11.6 English to Swahili.",https://aclanthology.org/2021.mtsummit-research.7,Association for Machine Translation in the Americas,2021,August,Proceedings of Machine Translation Summit XVIII: Research Track,"Ruiter, Dana  and
Klakow, Dietrich  and
van Genabith, Josef  and
Espa{\~n}a-Bonet, Cristina",Integrating Unsupervised Data Generation into Self-Supervised Neural Machine Translation for Low-Resource Languages,10.48550/arxiv.2107.08772,mtsummit,1203
2021.ltedi-1.8,"['Domain-specific NLP', 'Data Management and Generation', 'Multilingual NLP', 'Low-resource Languages', 'Classification Applications', 'Model Architectures']","['Data Preparation', 'Hope Speech Detection', 'NLP for News and Media']","['NLP for Social Media', 'Annotation Processes']","Hope plays a crucial role in the well-being, recuperation and restoration of human life. Hope speech reflects the belief that one can discover pathways to their desired objectives and become motivated to utilise those pathways. We created a hope speech detection dataset to encourage research in Natural Language Processing NLP towards reinforcement of positivity rather than minimising negativity through control of hate speech etc.. In this paper, we report the findings of the shared task of hope speech detection for Tamil, English, and Malayalam languages conducted as a part of the EACL 2021 workshop on Language Technology for Equality, Diversity, and Inclusion LT-EDI-2021. We also present an overview of the methods and results of the competing systems. The datasets for this challenge are openly available 1 . To the best of our knowledge, this is the first shared task to conduct hope speech detection.",https://aclanthology.org/2021.ltedi-1.8,Association for Computational Linguistics,2021,April,"Proceedings of the First Workshop on Language Technology for Equality, Diversity and Inclusion","Chakravarthi, Bharathi Raja  and
Muralidaran, Vigneshwaran","Findings of the Shared Task on Hope Speech Detection for Equality, Diversity, and Inclusion",,ltedi,1038
2020.cl-2.7,"['Biases in NLP', 'Embeddings', 'Ethics']",['Word Embeddings'],,"Analogies such as man is to king as woman is to X are often used to illustrate the amazing power of word embeddings. Concurrently, they have also been used to expose how strongly human biases are encoded in vector spaces trained on natural language, with examples like man is to computer programmer as woman is to homemaker. Recent work has shown that analogies are in fact not an accurate diagnostic for bias, but this does not mean that they are not used anymore, or that their legacy is fading. Instead of focusing on the intrinsic problems of the analogy task as a bias detection tool, we discuss a series of issues involving implementation as well as subjective choices that might have yielded a distorted picture of bias in word embeddings. We stand by the truth that human biases are present in word embeddings, and, of course, the need to address them. But analogies are not an accurate tool to do so, and the way they have been most often used has exacerbated some possibly non-existing biases and perhaps hidden others. Because they are still widely popular, and some of them have become classics within and outside the NLP community, we deem it important to provide a series of clarifications that should put well-known, and potentially new analogies, into the right perspective.",https://aclanthology.org/2020.cl-2.7,,2020,June,,"Nissim, Malvina  and
van Noord, Rik  and
van der Goot, Rob",Fair Is Better than Sensational: Man Is to Doctor as Woman Is to Doctor,10.1162/coli_a_00379,cl,1187
2020.dt4tp-1.3,"['Text Generation', 'Data Management and Generation', 'Low-resource Languages', 'Domain-specific NLP']",['Data Preparation'],['Annotation Processes'],We describe our work on QUD-oriented annotation of driving reports for the generation of corresponding texts -texts that are a mix of technical details of the new vehicle that has been put on the market together with the impressions of the test driver on driving characteristics. Generating these texts pose a challenge since they express non-at-issue and expressive content that cannot be retrieved from a database. Instead these subjective meanings must be justified by comparisons with attributes of other vehicles. We describe our current annotation task for the extraction of the relevant information for generating these driving reports.,https://aclanthology.org/2020.dt4tp-1.3,Association for Computational Linguistics,2020,December,Proceedings of the Workshop on Discourse Theories for Text Planning,"Hesse, Christoph  and
Benz, Anton  and
Langner, Maurice  and
Theodor, Felix  and
Klabunde, Ralf",Annotating QUDs for generating pragmatically rich texts,,dt4tp,1125
2020.tlt-1.7,"['Data Management and Generation', 'Multilingual NLP', 'Low-resource Languages']",['Data Analysis'],,"Languages differ in the degree of semantic flexibility of their syntactic roles. For example, English and Indonesian are considered more flexible with regard to the semantics of subjects, whereas German and Japanese are less flexible. In Hawkins' classification, more flexible languages are said to have a loose fit, and less flexible ones are those that have a tight fit. This classification has been based on manual inspection of example sentences. The present paper proposes a new, quantitative approach to deriving the measures of looseness and tightness from corpora. We use corpora of online news from the Leipzig Corpora Collection in thirty typologically and genealogically diverse languages and parse them syntactically with the help of the Universal Dependencies annotation software. Next, we compute Mutual Information scores for each language using the matrices of lexical lemmas and four syntactic dependencies intransitive subjects, transitive subject, objects and obliques. The new approach allows us not only to reproduce the results of previous investigations, but also to extend the typology to new languages. We also demonstrate that verb-final languages tend to have a tighter relationship between lexemes and syntactic roles, which helps language users to recognize thematic roles early during comprehension. Theoretical background and aims of the paper This paper proposes a quantitative bottom-up corpus-based approach to cross-linguistic comparison, determining how tightly or loosely different lexemes can be mapped on basic syntactic roles. The idea goes back to Hawkins 1986 Hawkins  : 121-127, 1995; see also Müller-Gotama 1994, who coined the terms 'tight-fit' and 'loose-fit' languages. The former have unique surface forms that map onto more constrained meanings, whereas the latter have more vague forms with less constrained meanings. For instance, Present-Day English has fewer semantic restrictions on the subject and object than Old English, German or Russian. Consider several examples below. 1 a. Locative: This tent sleeps four. b. Temporal: 2020 witnessed a spread of the highly infectious coronavirus disease. c. Instrument: 10 Euros will buy you a meal. d. Source: The roof leaks water. While these sentences are perfectly acceptable in English, their German or Russian equivalents would be unacceptable or strange. This means that subjects in English are less semantically restricted than subjects in German and Russian see also Plank 1984 . Tightness and looseness have several components. Semantic flexibility of arguments is only one of them. Other features of tight languages include formal case marking, avoidance of raisings and long WH-movements and lower reliance on context in interpretation. Languages can change their degree of tightness. English is a well-known example of shifting from tight to loose Hawkins 1986. As the case was lost, the zero-marked NPs in Middle English became more dependent on the verb for theta-role assignment. This is why the rigid SVO order emerged, which",https://aclanthology.org/2020.tlt-1.7,Association for Computational Linguistics,2020,October,Proceedings of the 19th International Workshop on Treebanks and Linguistic Theories,"Levshina, Natalia",How tight is your language? A semantic typology based on Mutual Information,10.18653/v1/2020.tlt-1.7,tlt,157
W16-3717,"['Machine Translation (MT)', 'Low-resource Languages']",['Neural MT (NMT)'],,"Neural machine translation NMT models have recently been shown to be very successful in machine translation MT. The use of LSTMs in machine translation has significantly improved the translation performance for longer sentences by being able to capture the context and long range correlations of the sentences in their hidden layers. The attention model based NMT system has become state-of-the-art, performing equal or better than other statistical MT approaches. In this paper, we studied the performance of the attention-model based NMT system on the Indian language pair, Hindi and Bengali. We analysed the types of errors that occur in morphologically rich languages when there is a scarcity of large parallel training corpus. We then carried out certain post-processing heuristic steps to improve the quality of the translated statements and suggest further measures.",https://aclanthology.org/W16-3717,The COLING 2016 Organizing Committee,2016,December,Proceedings of the 6th Workshop on South and Southeast {A}sian Natural Language Processing ({WSSANLP}2016),"Das, Ayan  and
Yerra, Pranay  and
Kumar, Ken  and
Sarkar, Sudeshna",A study of attention-based neural machine translation model on Indian languages,,W16,1045
2021.lchange-1.8,"['Language Change Analysis', 'Model Architectures', 'Knowledge Representation and Reasoning']",['Semantic Change Analysis'],,"Languages evolve over time and the meaning of words can shift. Furthermore, individual words can have multiple senses. However, existing language models often only reflect one word sense per word and do not reflect semantic changes over time. While there are language models that can either model semantic change of words or multiple word senses, none of them cover both aspects simultaneously. We propose a novel force-directed graph layout algorithm to draw a network of frequently cooccurring words. In this way, we are able to use the drawn graph to visualize the evolution of word senses. In addition, we hope that jointly modeling semantic change and multiple senses of words results in improvements for the individual tasks.",https://aclanthology.org/2021.lchange-1.8,Association for Computational Linguistics,2021,August,Proceedings of the 2nd International Workshop on Computational Approaches to Historical Language Change 2021,"Reke, Tim  and
Schwanhold, Robert  and
Krestel, Ralf",Modeling the Evolution of Word Senses with Force-Directed Layouts of Co-occurrence Networks,10.18653/v1/2021.lchange-1.8,lchange,1272
2020.pam-1.10,['Discourse Analysis'],,,"Henderson and McCready 2017 , 2018 , 2019 build a novel theory of so-called 'dogwhistle' communication by extending the social meaning games of Burnett 2017. This work reports on an ongoing project to build systems to model the evolution of dogwhistle communication in a population based on probability monads Erwig and Kollmansberger, 2006; Kidd, 2007 . The ultimate results will be useful not just for dogwhistles, but modeling the diffusion and evolution of social meaning in populations in general. The initial results presented here is a computational implementation of Henderson and McCready 2018, which will serve as the basis for models with multiple speakers and repeated interactions.",https://aclanthology.org/2020.pam-1.10,Association for Computational Linguistics,2020,June,Proceedings of the Probability and Meaning Conference (PaM 2020),"Henderson, Robert  and
McCready, Elin","Towards functional, agent-based models of dogwhistle communication",,pam,1419
2020.stoc-1.2,"['Dialogue Systems', 'Classification Applications']","['Email Spam and Phishing Detection', 'Response Generation']",,We present a paradigm for extensible lexicon development based on Lexical Conceptual Structure to support social engineering detection and response generation. We leverage the central notions of ask elicitation of behaviors such as providing access to money and framing risk/reward implied by the ask. We demonstrate improvements in ask/framing detection through refinements to our lexical organization and show that response generation qualitatively improves as ask/framing detection performance improves. The paradigm presents a systematic and efficient approach to resource adaptation for improved task-specific performance.,https://aclanthology.org/2020.stoc-1.2,European Language Resources Association,2020,May,Proceedings for the First International Workshop on Social Threats in Online Conversations: Understanding and Management,"Bhatia, Archna  and
Dalton, Adam  and
Mather, Brodie  and
Santhanam, Sashank  and
Shaikh, Samira  and
Zemel, Alan  and
Strzalkowski, Tomek  and
Dorr, Bonnie J.",Adaptation of a Lexical Organization for Social Engineering Detection and Response Generation,,stoc,207
2021.cmcl-1.21,['Data Management and Generation'],['Data Analysis'],,"This paper compares two influential theories of processing difficulty: Gibson 2000's Dependency Locality Theory DLT and Hale 2001's Surprisal Theory. While prior work has aimed to compare DLT and Surprisal Theory see Demberg and Keller, 2008 , they have not yet been compared using more modern and powerful methods for estimating surprisal and DLT integration cost. I compare estimated surprisal values from two models, an RNN and a Transformer neural network, as well as DLT integration cost from a hand-parsed treebank, to reading times from the Dundee Corpus. The results for integration cost corroborate those of Demberg and Keller 2008 , finding that it is a negative predictor of reading times overall and a strong positive predictor for nouns, but contrast with their observations for surprisal, finding strong evidence for lexicalized surprisal as a predictor of reading times. Ultimately, I conclude that a broad-coverage model must integrate both theories in order to most accurately predict processing difficulty.",https://aclanthology.org/2021.cmcl-1.21,Association for Computational Linguistics,2021,June,Proceedings of the Workshop on Cognitive Modeling and Computational Linguistics,"Rathi, Neil",Dependency Locality and Neural Surprisal as Predictors of Processing Difficulty: Evidence from Reading Times,10.18653/v1/2021.cmcl-1.21,cmcl,880
2020.cogalex-1.3,"['Data Management and Generation', 'Low-resource Languages']",['Data Analysis'],,"Cross-linguistic studies of concepts provide valuable insights for the investigation of the mental lexicon. Recent developments of cross-linguistic databases facilitate an exploration of a diverse set of languages on the basis of comparative concepts. These databases make use of a wellestablished reference catalog, the Concepticon, which is built from concept lists published in linguistics. A recently released feature of the Concepticon includes data on norms, ratings, and relations for words and concepts. The present study used data on word frequencies to test two hypotheses. First, I examined the assumption that related languages i.e., English and German share concepts with more similar frequencies than non-related languages i.e., English and Chinese. Second, the variation of frequencies across both language pairs was explored to answer the question of whether the related languages share fewer concepts with a large difference between the frequency than the non-related languages. The findings indicate that related languages experience less variation in their frequencies. If there is variation, it seems to be due to cultural and structural differences. The implications of this study are far-reaching in that it exemplifies the use of cross-linguistic data for the study of the mental lexicon.",https://aclanthology.org/2020.cogalex-1.3,Association for Computational Linguistics,2020,December,Proceedings of the Workshop on the Cognitive Aspects of the Lexicon,"Tjuka, Annika","General patterns and language variation: Word frequencies across English, German, and Chinese",,cogalex,143
2021.sdp-1.12,"['Text Preprocessing', 'Domain-specific NLP', 'Automatic Text Summarization', 'Model Architectures']","['NLP for Bibliometrics and Scientometrics', 'Document Summarization']",['Scientific Document Summarization'],"Most summarization task focuses on generating relatively short summaries. Such a length constraint might not be appropriate when summarizing scientific work. The LongSumm task needs participants generate long summary for scientific document. This task usual can be solved by language model. But an important problem is that model like BERT is limit to memory, and can not deal with a long input like a document. Also generate a long output is hard. In this paper, we propose a session based automatic summarization model SBAS which using a session and ensemble mechanism to generate long summary. And our model achieves the best performance in the LongSumm task.",https://aclanthology.org/2021.sdp-1.12,Association for Computational Linguistics,2021,June,Proceedings of the Second Workshop on Scholarly Document Processing,"Ying, Senci  and
Yan Zhao, Zheng  and
Zou, Wuhe",LongSumm 2021: Session based automatic summarization model for scientific document,10.18653/v1/2021.sdp-1.12,sdp,944
2016.tc-1.6,['Data Management and Generation'],,,"This is to inform the business and decision making communities among the ASLING audience about the high level benefits of bitext and XLIFF 2. Translator and Engineering communities will also benefit, as they need the high level arguments to make the call for XLIFF 2 adoption in their organizations. We start with a conceptual outline what bitext is, what different sorts of bitext exist and how they are useful at various stages in various industry processes, such as translation, localisation, terminology management, quality and sanity assurance projects etc. Examples of projects NOT based on bitext are given, benefits and drawbacks compared on a practical level of tasks performed. The following is demonstrated: That bitext management is a core process for efficient multilingual content value chains; That usage of an open standard bitext creates a greater sum of good than usage of proprietary bitext formats; and finally: That XLIFF 2 is the core format and data model to base bitext management on.",https://aclanthology.org/2016.tc-1.6,AsLing,2016,November 17-18,Proceedings of Translating and the Computer 38,"Filip, David",Why XLIFF and why XLIFF 2?,,tc,777
2020.ldl-1.11,"['Data Management and Generation', 'Information Extraction', 'Learning Paradigms', 'Low-resource Languages', 'Embeddings']","['Supervised Learning', 'Data Preparation', 'Hypernymy Extraction']",,"This paper addresses the task of supervised hypernymy detection in Spanish through an order embedding and using pretrained word vectors as input. Although the task has been widely addressed in English, there is not much work in Spanish, and according to our knowledge there is not any available dataset for supervised hypernymy detection in Spanish. We built a supervised hypernymy dataset for Spanish using WordNet and corpus statistics, with different versions according to the lexical intersection between its partitions: random and lexical split. We show the results of using the resulting dataset within an order embedding consuming pretrained word vectors as input. We show the ability of pretrained word vectors to transfer learning to unseen lexical units according to the results in the lexical split dataset. To finish, we study the results of giving additional information in training time, such as, co-hyponymy links and instances extracted through lexico-syntactic patterns.",https://aclanthology.org/2020.ldl-1.11,European Language Resources Association,2020,May,Proceedings of the 7th Workshop on Linked Data in Linguistics (LDL-2020),"Lee, Gun Woo  and
Etcheverry, Mathias  and
Fernandez Sanchez, Daniel  and
Wonsever, Dina",Supervised Hypernymy Detection in Spanish through Order Embeddings,,ldl,1498
D17-1217,"['Classification Applications', 'Model Architectures']",['Sentiment Analysis (SA)'],['Aspect-Based SA (ABSA)'],"Document-level multi-aspect sentiment classification is an important task for customer relation management. In this paper, we model the task as a machine comprehension problem where pseudo questionanswer pairs are constructed by a small number of aspect-related keywords and aspect ratings. A hierarchical iterative attention model is introduced to build aspectspecific representations by frequent and repeated interactions between documents and aspect questions. We adopt a hierarchical architecture to represent both word level and sentence level information, and use the attention operations for aspect questions and documents alternatively with the multiple hop mechanism. Experimental results on the TripAdvisor and BeerAdvocate datasets show that our model outperforms classical baselines.",https://aclanthology.org/D17-1217,Association for Computational Linguistics,2017,September,Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing,"Yin, Yichun  and
Song, Yangqiu  and
Zhang, Ming",Document-Level Multi-Aspect Sentiment Classification as Machine Comprehension,10.18653/v1/D17-1217,D17,843
2020.msr-1.3,"['Data Management and Generation', 'Text Preprocessing']",['Data Augmentation'],,"In this paper, we describe the ADAPT submission to the Surface Realization Shared Task 2020. We present a neural-based system trained on the English Web Treebank and an augmented dataset, automatically created from existing text corpora.",https://aclanthology.org/2020.msr-1.3,Association for Computational Linguistics,2020,December,Proceedings of the Third Workshop on Multilingual Surface Realisation,"Elder, Henry",ADAPT at SR'20: How Preprocessing and Data Augmentation Help to Improve Surface Realization,,msr,284
U16-1011,"['Learning Paradigms', 'Information Extraction', 'Classification Applications', 'Model Architectures']",,,"Supervised domain-specific term extraction often suffers from two common problems, namely labourious manual feature selection, and the lack of labelled data. In this paper, we introduce a weakly supervised bootstrapping approach using two deep learning classifiers. Each classifier learns the representations of terms separately by taking word embedding vectors as inputs, thus no manually selected feature is required. The two classifiers are firstly trained on a small set of labelled data, then independently make predictions on a subset of the unlabeled data. The most confident predictions are subsequently added to the training set to retrain the classifiers. This co-training process minimises the reliance on labelled data. Evaluations on two datasets demonstrate that the proposed co-training approach achieves a competitive performance with limited training data as compared to standard supervised learning baseline.",https://aclanthology.org/U16-1011,,2016,December,Proceedings of the Australasian Language Technology Association Workshop 2016,"Wang, Rui  and
Liu, Wei  and
McDonald, Chris",Featureless Domain-Specific Term Extraction with Minimal Labelled Data,,U16,680
2021.louhi-1.5,"['Domain-specific NLP', 'Classification Applications', 'Model Architectures']","['Transformer Models', 'Medical and Clinical NLP', 'Graph Neural Networks (GNNs)', 'NLP for News and Media']",['NLP for Social Media'],This paper investigates incorporating quality knowledge sources developed by experts for the medical domain as well as syntactic information for classification of tweets into four different health oriented categories. We claim that resources such as the MeSH hierarchy and currently available parse information are effective extensions of moderately sized training datasets for various fine-grained tweet classification tasks of self-reported health issues.,https://aclanthology.org/2021.louhi-1.5,Association for Computational Linguistics,2021,April,Proceedings of the 12th International Workshop on Health Text Mining and Information Analysis,"Bagherzadeh, Parsa  and
Bergler, Sabine",Leveraging knowledge sources for detecting self-reports of particular health issues on social media,,louhi,193
2020.aacl-srw.23,['Text Generation'],,,"Several recent state-of-the-art transfer learning methods model classification tasks as text generation, where labels are represented as strings for the model to generate. We investigate the effect that the choice of strings used to represent labels has on how effectively the model learns the task. For four standard text classification tasks, we design a diverse set of possible string representations for labels, ranging from canonical label definitions to random strings. We experiment with T5 Raffel et al., 2019 on these tasks, varying the label representations as well as the amount of training data. We find that, in the low data setting, label representation impacts task performance on some tasks, with task-related labels being most effective, but fails to have an impact on others. In the full data setting, our results are largely negative: Different label representations do not affect overall task performance.",https://aclanthology.org/2020.aacl-srw.23,Association for Computational Linguistics,2020,December,Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing: Student Research Workshop,"Chen, Xinyi  and
Xu, Jingxian  and
Wang, Alex",Label Representations in Modeling Classification as Text Generation,10.18653/v1/2020.aacl-srw.23,aacl,1371
2021.gebnlp-1.11,"['Biases in NLP', 'Text Generation', 'Data Management and Generation', 'Low-resource Languages', 'Classification Applications']","['Data Augmentation', 'Gender Bias', 'Data Preparation', 'Paraphrase and Rephrase Generation']",,"Gender bias is a frequent occurrence in NLPbased applications, especially pronounced in gender-inflected languages. Bias can appear through associations of certain adjectives and animate nouns with the natural gender of referents, but also due to unbalanced grammatical gender frequencies of inflected words. This type of bias becomes more evident in generating conversational utterances where gender is not specified within the sentence, because most current NLP applications still work on a sentence-level context. As a step towards more inclusive NLP, this paper proposes an automatic and generalisable re-writing approach for short conversational sentences. The rewriting method can be applied to sentences that, without extra-sentential context, have multiple equivalent alternatives in terms of gender. The method can be applied both for creating gender balanced outputs as well as for creating gender balanced training data. The proposed approach is based on a neural machine translation NMT system trained to 'translate' from one gender alternative to another. Both the automatic and manual analysis of the approach show promising results for automatic generation of gender alternatives for conversational sentences in Spanish.",https://aclanthology.org/2021.gebnlp-1.11,Association for Computational Linguistics,2021,August,Proceedings of the 3rd Workshop on Gender Bias in Natural Language Processing,"Jain, Nishtha  and
Popovi{\'c}, Maja  and
Groves, Declan  and
Vanmassenhove, Eva",Generating Gender Augmented Data for NLP,10.18653/v1/2021.gebnlp-1.11,gebnlp,1031
2022.naacl-industry.22,"['Domain-specific NLP', 'Machine Translation (MT)', 'Information Extraction', 'Learning Paradigms', 'Model Architectures']",['Named Entity Recognition (NER)'],,"During their pre-flight briefings, aircraft pilots must analyze a long list of NOTAMs NOtice To AirMen indicating potential hazards along the flight route, sometimes up to 100 pages for long-haul flights. NOTAM free-text fields typically have a very special phrasing, with lots of acronyms and domain-specific vocabulary, which makes it differ significantly from standard English. In this paper, we pretrain language models derived from BERT on circa 1 million unlabeled NOTAMs and reuse the learnt representations on three downstream tasks valuable for pilots: criticality prediction, named entity recognition and translation into a structured language called Airlang. This selfsupervised approach, where smaller amounts of labeled data are enough for task-specific finetuning, is well suited in the aeronautical context since expert annotations are expensive and time-consuming. We present evaluation scores across the tasks showing a high potential for an operational usability of such models by pilots, airlines or service providers, which is a first to the best of our knowledge.",https://aclanthology.org/2022.naacl-industry.22,Association for Computational Linguistics,2022,July,Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Industry Track,"Arnold, Alexandre  and
Ernez, Fares  and
Kobus, Catherine  and
Martin, Marion-C{\'e}cile",Knowledge extraction from aeronautical messages NOTAMs with self-supervised language models for aircraft pilots,10.18653/v1/2022.naacl-industry.22,naacl,803
P18-1119,"['Parsing', 'Data Management and Generation']",['Data Preparation'],,"The purpose of text geolocation is to associate geographic information contained in a document with a set or sets of coordinates, either implicitly by using linguistic features and/or explicitly by using geographic metadata combined with heuristics. We introduce a geocoder location mention disambiguator that achieves state-of-the-art SOTA results on three diverse datasets by exploiting the implicit lexical clues. Moreover, we propose a new method for systematic encoding of geographic metadata to generate two distinct views of the same text. To that end, we introduce the Map Vector MapVec, a sparse representation obtained by plotting prior geographic probabilities, derived from population figures, on a World Map. We then integrate the implicit language and explicit map features to significantly improve a range of metrics. We also introduce an open-source dataset for geoparsing of news events covering global disease outbreaks and epidemics to help future evaluation in geoparsing.",https://aclanthology.org/P18-1119,Association for Computational Linguistics,2018,July,Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),"Gritta, Milan  and
Pilehvar, Mohammad Taher  and
Collier, Nigel",Which Melbourne? Augmenting Geocoding with Maps,10.18653/v1/P18-1119,P18,885
L16-1706,"['Ethics', 'Machine Translation (MT)']",,,"In order to develop its full potential, global communication needs linguistic support systems such as Machine Translation MT. In the past decade, free online MT tools have become available to the general public, and the quality of their output is increasing. However, the use of such tools may entail various legal implications, especially as far as processing of personal data is concerned. This is even more evident if we take into account that their business model is largely based on providing translation in exchange for data, which can subsequently be used to improve the translation model, but also for commercial purposes. The purpose of this paper is to examine how free online MT tools fit in the European data protection framework, harmonised by the EU Data Protection Directive. The perspectives of both the user and the MT service provider are taken into account.",https://aclanthology.org/L16-1706,European Language Resources Association (ELRA),2016,May,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),"Kamocki, Pawel  and
O{'}Regan, Jim",Privacy Issues in Online Machine Translation Services - European Perspective,,L16,327
2020.aespen-1.1,"['Domain-specific NLP', 'Information Extraction']","['NLP for News and Media', 'Event Extraction']",,"We describe our effort on automated extraction of socio-political events from news in the scope of a workshop and a shared task we organized at Language Resources and Evaluation Conference LREC 2020. We believe the event extraction studies in computational linguistics and social and political sciences should further support each other in order to enable large scale socio-political event information collection across sources, countries, and languages. The event consists of regular research papers and a shared task, which is about event sentence coreference identification ESCI, tracks. All submissions were reviewed by five members of the program committee. The workshop attracted research papers related to evaluation of machine learning methodologies, language resources, material conflict forecasting, and a shared task participation report in the scope of socio-political event information collection. It has shown us the volume and variety of both the data sources and event information collection approaches related to socio-political events and the need to fill the gap between automated text processing techniques and requirements of social and political sciences.",https://aclanthology.org/2020.aespen-1.1,European Language Resources Association (ELRA),2020,May,Proceedings of the Workshop on Automated Extraction of Socio-political Events from News 2020,"H{\""u}rriyeto{\u{g}}lu, Ali  and
Zavarella, Vanni  and
Tanev, Hristo  and
Y{\""o}r{\""u}k, Erdem  and
Safaya, Ali  and
Mutlu, Osman",Automated Extraction of Socio-political Events from News AESPEN: Workshop and Shared Task Report,,aespen,623
2020.iwdp-1.2,"['Machine Translation (MT)', 'Discourse Analysis']","['Neural MT (NMT)', 'Statistical MT (SMT)']",,"Machine translation MT models usually translate a text at sentence level by considering isolated sentences, which is based on a strict assumption that the sentences in a text are independent of one another. However, the fact is that the texts at discourse level have properties going beyond individual sentences. These properties reveal texts in the frequency and distribution of words, word senses, referential forms and syntactic structures. Disregarding dependencies across sentences will harm translation quality especially in terms of coherence, cohesion, and consistency. To solve these problems, several approaches have previously been investigated for conventional statistical machine translation SMT. With the fast growth of neural machine translation NMT, discourse-level NMT has drawn increasing attention from researchers. In this work, we review major works on addressing discourse related problems for both SMT and NMT models with a survey of recent trends in the fields.",https://aclanthology.org/2020.iwdp-1.2,Association for Computational Linguistics,2020,December,Proceedings of the Second International Workshop of Discourse Processing,"Zhang, Xiaojun",A Review of Discourse-level Machine Translation,10.18653/v1/2020.iwdp-1.2,iwdp,135
2020.conll-1.19,"['Data Management and Generation', 'Error Detection and Correction', 'Low-resource Languages', 'Machine Translation (MT)']",['Data Preparation'],,"This work presents a detailed analysis of translation errors perceived by readers as comprehensibility and/or adequacy issues. The main finding is that good comprehensibility, similarly to good fluency, can mask a number of adequacy errors. Of all major adequacy errors, 30% were fully comprehensible, thus fully misleading the reader to accept the incorrect information. Another 25% of major adequacy errors were perceived as almost comprehensible, thus being potentially misleading. Also, a vast majority of omissions about 70% is hidden by comprehensibility. Further analysis of misleading translations revealed that the most frequent error types are ambiguity, mistranslation, noun phrase error, word-by-word translation, untranslated word, subject-verb agreement, and spelling error in the source text. However, none of these error types appears exclusively in misleading translations, but are also frequent in fully incorrect incomprehensible inadequate and discarded correct incomprehensible adequate translations. Deeper analysis is needed to potentially detect underlying phenomena specifically related to misleading translations.",https://aclanthology.org/2020.conll-1.19,Association for Computational Linguistics,2020,November,Proceedings of the 24th Conference on Computational Natural Language Learning,"Popovi{\'c}, Maja",Relations between comprehensibility and adequacy errors in machine translation output,10.18653/v1/2020.conll-1.19,conll,924
2022.eamt-1.30,"['Machine Translation (MT)', 'Data Management and Generation']","['Data Preparation', 'Neural MT (NMT)', 'Data Analysis']",,"With the arrival of neural machine translation, the boundaries between revision and post-editing PE have started to blur Koponen et al., 2020. To shed light on current professional practices and provide new pedagogical perspectives, we set up a survey-based study to investigate how PE and revision are carried out in professional settings. We received 86 responses from corporate translators working at 23 different corporate in-house language services in Switzerland. Although the differences between the two activities seem to be clear for in-house linguists, our findings show that they tend to use the same reading strategies when working with humantranslated and machine-translated texts.",https://aclanthology.org/2022.eamt-1.30,European Association for Machine Translation,2022,June,Proceedings of the 23rd Annual Conference of the European Association for Machine Translation,"Girletti, Sabrina",Working with Pre-translated Texts: Preliminary Findings from a Survey on Post-editing and Revision Practices in Swiss Corporate In-house Language Services,,eamt,400
S18-2008,['Data Management and Generation'],['Data Analysis'],,"Across disciplines, researchers are eager to gain insight into empirical features of abstract vs. concrete concepts. In this work, we provide a detailed characterisation of the distributional nature of abstract and concrete words across 16,620 English nouns, verbs and adjectives. Specifically, we investigate the following questions: 1 What is the distribution of concreteness in the contexts of concrete and abstract target words? 2 What are the differences between concrete and abstract words in terms of contextual semantic diversity? 3 How does the entropy of concrete and abstract word contexts differ? Overall, our studies show consistent differences in the distributional representation of concrete and abstract words, thus challenging existing theories of cognition and providing a more fine-grained description of their nature.",https://aclanthology.org/S18-2008,Association for Computational Linguistics,2018,June,Proceedings of the Seventh Joint Conference on Lexical and Computational Semantics,"Naumann, Daniela  and
Frassinelli, Diego  and
Schulte im Walde, Sabine",Quantitative Semantic Variation in the Contexts of Concrete and Abstract Words,10.18653/v1/S18-2008,S18,1044
2021.sdp-1.2,"['Information Retrieval', 'Learning Paradigms', 'Text Generation', 'Data Management and Generation']","['Unsupervised Learning', 'Paraphrase and Rephrase Generation', 'Data Augmentation']",,"One of the challenges in information retrieval IR is the vocabulary mismatch problem, which happens when the terms between queries and documents are lexically different but semantically similar. While recent work has proposed to expand the queries or documents by enriching their representations with additional relevant terms to address this challenge, they usually require a large volume of query-document pairs to train an expansion model. In this paper, we propose an Unsupervised Document Expansion with Generation UDEG framework with a pretrained language model, which generates diverse supplementary sentences for the original document without using labels on querydocument pairs for training. For generating sentences, we further stochastically perturb their embeddings to generate more diverse sentences for document expansion. We validate our framework on two standard IR benchmark datasets. The results show that our framework significantly outperforms relevant expansion baselines for IR.",https://aclanthology.org/2021.sdp-1.2,Association for Computational Linguistics,2021,June,Proceedings of the Second Workshop on Scholarly Document Processing,"Jeong, Soyeong  and
Baek, Jinheon  and
Park, ChaeHun  and
Park, Jong",Unsupervised Document Expansion for Information Retrieval with Stochastic Text Generation,10.18653/v1/2021.sdp-1.2,sdp,306
2021.quasy-1.2,"['Data Management and Generation', 'Low-resource Languages']",['Data Analysis'],,"Menzerath's law is a quantitative generalization which predicts a negative correlation between the mean size of parts of a unit and the number of parts in the unit. In this paper, I use Universal Dependencies to perform a cross-linguistic test of Menzerath's law at two syntactic levels: whether the number of clauses in a sentence negatively correlates with mean clause length in this sentence and whether the number of words in a clause negatively correlates with mean word length in this clause. Menzerath's largely holds at the former level and largely does not at the latter. I discuss other interesting patterns observed in the data and propose some tentative partial explanations.",https://aclanthology.org/2021.quasy-1.2,Association for Computational Linguistics,2021,December,"Proceedings of the Second Workshop on Quantitative Syntax (Quasy, SyntaxFest 2021)","Berdicevskis, Aleksandrs",Successes and failures of Menzerath's law at the syntactic level,,quasy,665
2021.metanlp-1.4,"['Text Generation', 'Learning Paradigms', 'Model Architectures']",['Text Style Transfer'],,"Text-style transfer aims to convert text given in one domain into another by paraphrasing the sentence or substituting the keywords without altering the content. By necessity, state-ofthe-art methods have evolved to accommodate nonparallel training data, as it is frequently the case there are multiple data sources of unequal size, with a mixture of labeled and unlabeled sentences. Moreover, the inherent style defined within each source might be distinct. A generic bidirectional e.g., formal ⇔ informal style transfer regardless of different groups may not generalize well to different applications. In this work, we developed a task adaptive meta-learning framework that can simultaneously perform a multi-pair text-style transfer using a single model. The proposed method can adaptively balance the difference of meta-knowledge across multiple tasks. Results show that our method leads to better quantitative performance as well as coherent style variations. Common challenges of unbalanced data and mismatched domains are handled well by this method.",https://aclanthology.org/2021.metanlp-1.4,Association for Computational Linguistics,2021,August,Proceedings of the 1st Workshop on Meta Learning and Its Applications to Natural Language Processing,"Han, Xing  and
Lundin, Jessica",Multi-Pair Text Style Transfer for Unbalanced Data via Task-Adaptive Meta-Learning,10.18653/v1/2021.metanlp-1.4,metanlp,337
2022.unimplicit-1.4,"['Figurative Language', 'Text Generation', 'Information Extraction', 'Information Retrieval', 'Classification Applications']","['Information Filtering', 'Sentiment Analysis (SA)', 'Paraphrase and Rephrase Generation']",,"This paper presents a linguistically driven proof of concept for finding potentially euphemistic terms, or PETs. Acknowledging that PETs tend to be commonly used expressions for a certain range of sensitive topics, we make use of distributional similarities to select and filter phrase candidates from a sentence and rank them using a set of simple sentiment-based metrics. We present the results of our approach tested on a corpus of sentences containing euphemisms, demonstrating its efficacy for detecting single and multi-word PETs from a broad range of topics. We also discuss future potential for sentiment-based methods on this task.",https://aclanthology.org/2022.unimplicit-1.4,Association for Computational Linguistics,2022,July,Proceedings of the Second Workshop on Understanding Implicit and Underspecified Language,"Lee, Patrick  and
Gavidia, Martha  and
Feldman, Anna  and
Peng, Jing",Searching for PETs: Using Distributional and Sentiment-Based Methods to Find Potentially Euphemistic Terms,10.18653/v1/2022.unimplicit-1.4,unimplicit,860
2020.framenet-1.7,"['Parsing', 'Data Management and Generation', 'Low-resource Languages', 'Cross-lingual Application', 'Knowledge Representation and Reasoning', 'Multilingual NLP']","['Data Preparation', 'Semantic Parsing']","['Semantic Role Labeling', 'Annotation Processes']","Large coverage lexical resources that bear deep linguistic information have always been considered useful for many natural language processing NLP applications including Machine Translation MT. In this respect, Frame-based resources have been developed for many languages following Frame Semantics and the Berkeley FrameNet project. However, to a great extent, all those efforts have been kept fragmented. Consequentially, the Global FrameNet initiative has been conceived of as a joint effort to bring together FrameNets in different languages. The proposed paper is aimed at describing ongoing work towards developing the Greek EL counterpart of the Global FrameNet and our efforts to contribute to the Shared Annotation Task. In the paper, we will elaborate on the annotation methodology employed, the current status and progress made so far, as well as the problems raised during annotation.",https://aclanthology.org/2020.framenet-1.7,European Language Resources Association,2020,May,"Proceedings of the International FrameNet Workshop 2020: Towards a Global, Multilingual FrameNet","Giouli, Voula  and
Pilitsidou, Vera  and
Christopoulos, Hephaestion",Greek within the Global FrameNet Initiative: Challenges and Conclusions so far,,framenet,1154
2021.nlpmc-1.1,"['Text Generation', 'Data Management and Generation', 'Dialogue Systems', 'Domain-specific NLP']","['Data Preparation', 'Medical and Clinical NLP']","['NLP for Mental Health', 'Annotation Processes']","The acquisition of a dialogue corpus is a key step in the process of training a dialogue model. In this context, corpora acquisitions have been designed either for open-domain information retrieval or slot-filling e.g. restaurant booking tasks. However, there has been scarce research in the problem of collecting personal conversations with users over a long period of time. In this paper we focus on the types of dialogues that are required for mental health applications. One of these types is the follow-up dialogue that a psychotherapist would initiate in reviewing the progress of a Cognitive Behavioral Therapy CBT intervention. The elicitation of the dialogues is achieved through textual stimuli presented to dialogue writers. We propose an automatic algorithm that generates textual stimuli from personal narratives collected during psychotherapy interventions. The automatically generated stimuli are presented as a seed to dialogue writers following principled guidelines. We analyze the linguistic quality of the collected corpus and compare the performances of psychotherapists and non-expert dialogue writers. Moreover, we report the human evaluation of a corpus-based response-selection model.",https://aclanthology.org/2021.nlpmc-1.1,Association for Computational Linguistics,2021,June,Proceedings of the Second Workshop on Natural Language Processing for Medical Conversations,"Mousavi, Seyed Mahed  and
Cervone, Alessandra  and
Danieli, Morena  and
Riccardi, Giuseppe",Would you like to tell me more? Generating a corpus of psychotherapy dialogues,10.18653/v1/2021.nlpmc-1.1,nlpmc,1101
2021.nlp4musa-1.7,"['Domain-specific NLP', 'Classification Applications']","['Sentiment Analysis (SA)', 'Emotion Detection', 'NLP for News and Media']",['NLP for Social Media'],"Music forms a big part of our identity and as such, people with a shared preference for certain kinds of music may also share similar traits. In this study, we explore differences in the emotional language of fan communities of different music genres. In focusing on Reddit, we analyze the utterances on online community forums of different music genres using lexicon-based sentiment emotion analysis. Upon clustering Subreddit forums, we obtained two clusters: forums discussing genres like Rock, RnB, Country, and Jazz were found to have a higher abundance of positively valenced emotions and a lower amount of negatively valenced emotions. Likewise, Subreddits discussing genres like Metal, Punk, and Rap had a lower amount of positively valenced emotions and a higher abundance of negatively valenced emotion. We observed a high correlation between counts in lyrics of a genre and counts in a fan community for the emotions of anger, disgust, fear, and joy. In sum, we found differences in the emotional language of fan utterances by genre, and these could be partially attributed to the emotions contained in the lyrics.",https://aclanthology.org/2021.nlp4musa-1.7,Association for Computational Linguistics,2021,November,Proceedings of the 2nd Workshop on NLP for Music and Spoken Audio (NLP4MusA),"Mishra, Vipul  and
Liew, Kongmeng  and
Epure, Elena V.  and
Hennequin, Romain  and
Aramaki, Eiji",Are Metal Fans Angrier than Jazz Fans? A Genre-Wise Exploration of the Emotional Language of Music Listeners on Reddit,,nlp4musa,35
2021.sigdial-1.37,"['Question Answering (QA)', 'Data Management and Generation', 'Dialogue Systems', 'Multi-agent Communication Systems']","['Data Preparation', 'Intelligent Agents', 'Data Analysis']",,"Intelligent agents that are confronted with novel concepts in situated environments will need to ask their human teammates questions to learn about the physical world. To better understand this problem, we need data about asking questions in situated task-based interactions. To this end, we present the Human-Robot Dialogue Learning HuRDL Corpus -a novel dialogue corpus collected in an online interactive virtual environment in which human participants play the role of a robot performing a collaborative tool-organization task. We describe the corpus data and a corresponding annotation scheme to offer insight into the form and content of questions that humans ask to facilitate learning in a situated environment. We provide the corpus as an empirically-grounded resource for improving question generation in situated intelligent agents.",https://aclanthology.org/2021.sigdial-1.37,Association for Computational Linguistics,2021,July,Proceedings of the 22nd Annual Meeting of the Special Interest Group on Discourse and Dialogue,"Gervits, Felix  and
Roque, Antonio  and
Briggs, Gordon  and
Scheutz, Matthias  and
Marge, Matthew",How Should Agents Ask Questions For Situated Learning? An Annotated Dialogue Corpus,10.18653/v1/2021.sigdial-1.37,sigdial,1274
2021.maiworkshop-1.5,"['Learning Paradigms', 'Classification Applications', 'Model Architectures']",['Multimodal Learning'],,"Large-scale multi-modal classification aim to distinguish between different multi-modal data, and it has drawn dramatically attentions since last decade. In this paper, we propose a multi-task learning-based framework for the multimodal classification task, which consists of two branches: multi-modal autoencoder branch and attention-based multi-modal modeling branch. Multi-modal autoencoder can receive multi-modal features and obtain the interactive information which called multi-modal encoder feature, and use this feature to reconstitute all the input data. Besides, multimodal encoder feature can be used to enrich the raw dataset, and improve the performance of downstream tasks such as classification task. As for attention-based multimodal modeling branch, we first employ attention mechanism to make the model focused on important features, then we use the multi-modal encoder feature to enrich the input information, achieve a better performance. We conduct extensive experiments on different dataset, the results demonstrate the effectiveness of proposed framework.",https://aclanthology.org/2021.maiworkshop-1.5,Association for Computational Linguistics,2021,June,Proceedings of the Third Workshop on Multimodal Artificial Intelligence,"Zeng, Danting",Multi Task Learning based Framework for Multimodal Classification,10.18653/v1/2021.maiworkshop-1.5,maiworkshop,313
2021.germeval-1.13,"['Domain-specific NLP', 'Low-resource Languages', 'Classification Applications', 'Model Architectures']","['Misinformation Detection', 'Hate and Offensive Speech Detection', 'NLP for News and Media']",['NLP for Social Media'],"This paper presents the contribution of the Data Science Kitchen at GermEval 2021 shared task on the identification of toxic, engaging, and fact-claiming comments. The task aims at extending the identification of offensive language, by including additional subtasks that identify comments which should be prioritized for fact-checking by moderators and community managers. Our contribution focuses on a feature-engineering approach with a conventional classification backend. We combine semantic and writing style embeddings derived from pre-trained deep neural networks with additional numerical features, specifically designed for this task. Ensembles of Logistic Regression classifiers and Support Vector Machines are used to derive predictions for each subtask via a majority voting scheme. Our best submission achieved macro-averaged F1-scores of 66.8%, 69.9% and 72.5% for the identification of toxic, engaging, and factclaiming comments.",https://aclanthology.org/2021.germeval-1.13,Association for Computational Linguistics,2021,September,"Proceedings of the GermEval 2021 Shared Task on the Identification of Toxic, Engaging, and Fact-Claiming Comments","Hildebrandt, Niclas  and
Boenninghoff, Benedikt  and
Orth, Dennis  and
Schymura, Christopher","Data Science Kitchen at GermEval 2021: A Fine Selection of Hand-Picked Features, Delivered Fresh from the Oven",,germeval,963
2020.trac-1.15,"['Domain-specific NLP', 'Multilingual NLP', 'Low-resource Languages', 'Classification Applications', 'Model Architectures']","['Transformer Models', 'Hate and Offensive Speech Detection', 'NLP for News and Media']",['NLP for Social Media'],"This paper describes our results for TRAC 2020 competition held together with the conference LREC 2020. Our team name was Ms8qQxMbnjJMgYcw. The competition consisted of 2 subtasks in 3 languages Bengali, English and Hindi where the participants' task was to classify aggression in short texts from social media and decide whether it is gendered or not. We used a single BERT-based system with two outputs for all tasks simultaneously. Our model placed first in English and second in Bengali gendered text classification competition tasks with 0.87 and 0.93 in F1-score respectively.",https://aclanthology.org/2020.trac-1.15,European Language Resources Association (ELRA),2020,May,"Proceedings of the Second Workshop on Trolling, Aggression and Cyberbullying","Gordeev, Denis  and
Lykova, Olga","BERT of all trades, master of some",,trac,752
2022.dadc-1.5,"['Question Answering (QA)', 'Data Management and Generation', 'Adversarial Attacks and Robustness']",['Data Preparation'],,"Developing methods to adversarially challenge NLP systems is a promising avenue for improving both model performance and interpretability. Here, we describe the approach of the team ""longhorns"" on Task 1 of the The First Workshop on Dynamic Adversarial Data Collection DADC, which asked teams to manually fool a model on an Extractive Question Answering task. Our team finished first, with a model error rate of 62%. 1 We advocate for a systematic, linguistically informed approach to formulating adversarial questions, and we describe the results of our pilot experiments, as well as our official submission.",https://aclanthology.org/2022.dadc-1.5,Association for Computational Linguistics,2022,July,Proceedings of the First Workshop on Dynamic Adversarial Data Collection,"Kovatchev, Venelin  and
Chatterjee, Trina  and
Govindarajan, Venkata S  and
Chen, Jifan  and
Choi, Eunsol  and
Chronis, Gabriella  and
Das, Anubrata  and
Erk, Katrin  and
Lease, Matthew  and
Li, Junyi Jessy  and
Wu, Yating  and
Mahowald, Kyle",longhorns at DADC 2022: How many linguists does it take to fool a Question Answering model? A systematic approach to adversarial attacks.,10.18653/v1/2022.dadc-1.5,dadc,691
C16-1300,"['Embeddings', 'Bilingual Lexicon Induction (BLI)', 'Low-resource Languages', 'Cross-lingual Application', 'Model Architectures']",['Word Embeddings'],,"Being able to induce word translations from non-parallel data is often a prerequisite for crosslingual processing in resource-scarce languages and domains. Previous endeavors typically simplify this task by imposing the one-to-one translation assumption, which is too strong to hold for natural languages. We remove this constraint by introducing the Earth Mover's Distance into the training of bilingual word embeddings. In this way, we take advantage of its capability to handle multiple alternative word translations in a natural form of regularization. Our approach shows significant and consistent improvements across four language pairs. We also demonstrate that our approach is particularly preferable in resource-scarce settings as it only requires a minimal seed lexicon.",https://aclanthology.org/C16-1300,The COLING 2016 Organizing Committee,2016,December,"Proceedings of {COLING} 2016, the 26th International Conference on Computational Linguistics: Technical Papers","Zhang, Meng  and
Liu, Yang  and
Luan, Huanbo  and
Liu, Yiqun  and
Sun, Maosong",Inducing Bilingual Lexica From Non-Parallel Data With Earth Mover's Distance Regularization,,C16,1362
Q19-1011,"['Text Preprocessing', 'Topic Modeling', 'Data Management and Generation', 'Low-resource Languages', 'Classification Applications', 'Model Architectures']","['Text Segmentation', 'Data Preparation', 'Recurrent Neural Networks (RNNs)']",['Long Short-Term Memory (LSTM) Models'],"When searching for information, a human reader first glances over a document, spots relevant sections, and then focuses on a few sentences for resolving her intention. However, the high variance of document structure complicates the identification of the salient topic of a given section at a glance. To tackle this challenge, we present SECTOR, a model to support machine reading systems by segmenting documents into coherent sections and assigning topic labels to each section. Our deep neural network architecture learns a latent topic embedding over the course of a document. This can be leveraged to classify local topics from plain text and segment a document at topic shifts. In addition, we contribute WikiSection, a publicly available data set with 242k labeled sections in English and German from two distinct domains: diseases and cities. From our extensive evaluation of 20 architectures, we report a highest score of 71.6% F1 for the segmentation and classification of 30 topics from the English city domain, scored by our SECTOR long short-term memory model with Bloom filter embeddings and bidirectional segmentation. This is a significant improvement of 29.5 points F1 over state-of-the-art CNN classifiers with baseline segmentation.",https://aclanthology.org/Q19-1011,MIT Press,2019,,,"Arnold, Sebastian  and
Schneider, Rudolf  and
Cudr{\'e}-Mauroux, Philippe  and
Gers, Felix A.  and
L{\""o}ser, Alexander",SECTOR: A Neural Model for Coherent Topic Segmentation and Classification,10.1162/tacl_a_00261,Q19,812
2020.gamnlp-1.12,"['Data Management and Generation', 'Low-resource Languages', 'Classification Applications', 'Domain-specific NLP']",['Data Preparation'],['Annotation Processes'],"While playing the communication game ""Are You a Werewolf"", a player always guesses other players' roles through discussions, based on his own role and other players' crucial utterances. The underlying goal of this paper is to construct an agent that can analyze the participating players' utterances and play the werewolf game as if it is a human. For a step of this underlying goal, this paper studies how to accumulate werewolf game log data annotated with identification of players revealing oneselves as seer/medium, the acts of the divination and the medium and declaring the results of the divination and the medium. In this paper, we divide the whole task into four sub tasks and apply CNN/SVM classifiers to each sub task and evaluate their performance.",https://aclanthology.org/2020.gamnlp-1.12,European Language Resources Association,2020,May,Workshop on Games and Natural Language Processing,"Lin, Youchao  and
Kasamatsu, Miho  and
Chen, Tengyang  and
Fujita, Takuya  and
Deng, Huanjin  and
Utsuro, Takehito",Automatic Annotation of Werewolf Game Corpus with Players Revealing Oneselves as Seer/Medium and Divination/Medium Results,,gamnlp,140
2022.nlppower-1.6,"['Evaluation Techniques', 'Data Management and Generation', 'Knowledge Representation and Reasoning']","['Data Preparation', 'Data Analysis', 'Ontologies']",['Ontology Extension'],"Measuring the performance of natural language processing models is challenging. Traditionally used metrics, such as BLEU and ROUGE, originally devised for machine translation and summarization, have been shown to suffer from low correlation with human judgment and a lack of transferability to other tasks and languages. In the past 15 years, a wide range of alternative metrics have been proposed. However, it is unclear to what extent this has had an impact on NLP benchmarking efforts. Here we provide the first large-scale cross-sectional analysis of metrics used for measuring performance in natural language processing. We curated, mapped and systematized more than 3500 machine learning model performance results from the open repository 'Papers with Code' to enable a global and comprehensive analysis. Our results suggest that the large majority of natural language processing metrics currently used have properties that may result in an inadequate reflection of a models' performance. Furthermore, we found that ambiguities and inconsistencies in the reporting of metrics may lead to difficulties in interpreting and comparing model performances, impairing transparency and reproducibility in NLP research.",https://aclanthology.org/2022.nlppower-1.6,Association for Computational Linguistics,2022,May,Proceedings of NLP Power! The First Workshop on Efficient Benchmarking in NLP,"Blagec, Kathrin  and
Dorffner, Georg  and
Moradi, Milad  and
Ott, Simon  and
Samwald, Matthias",A global analysis of metrics used for measuring performance in natural language processing,10.18653/v1/2022.nlppower-1.6,nlppower,539
2022.slpat-1.8,"['Biases in NLP', 'Domain-specific NLP', 'Data Management and Generation']","['Data Preparation', 'Data Analysis', 'Medical and Clinical NLP']",,"Stereotypes are a positive or negative, generalized, and often widely shared belief about the attributes of certain groups of people, such as people with sensory disabilities. If stereotypes manifest in assistive technologies used by deaf or blind people, they can harm the user in a number of ways-especially considering the vulnerable nature of the target population. AI models underlying assistive technologies have been shown to contain biased stereotypes, including racial, gender, and disability biases. We build on this work to present a psychologybased stereotype assessment of the representation of disability, deafness, and blindness in BERT using the Stereotype Content Model. We show that BERT contains disability bias, and that this bias differs along established stereotype dimensions.",https://aclanthology.org/2022.slpat-1.8,Association for Computational Linguistics,2022,May,Ninth Workshop on Speech and Language Processing for Assistive Technologies (SLPAT-2022),"Herold, Brienna  and
Waller, James  and
Kushalnagar, Raja",Applying the Stereotype Content Model to assess disability bias in popular pre-trained NLP models underlying AI-based assistive technologies,10.18653/v1/2022.slpat-1.8,slpat,78
Y17-1051,"['Machine Translation (MT)', 'Data Management and Generation', 'Low-resource Languages']","['Data Preparation', 'Neural MT (NMT)', 'Statistical MT (SMT)']",,"Neural-based and phrase-based methods have shown the effectiveness and promising results in the development of current machine translation. The two methods are compared on some European languages, which show the advantages of the neural machine translation. Nevertheless, there are few work of comparing the two methods on low-resource languages, which there are only small bilingual corpora. The problem of unavailable large bilingual corpora causes a bottleneck for machine translation for such language pairs. In this paper, we present a comparison of the phrase-based and neural-based machine translation methods on several Asian language pairs: Japanese-English, Indonesian-Vietnamese, and English-Vietnamese. Additionally, we extracted a bilingual corpus from Wikipedia to enhance machine translation performance. Experimental results showed that when using the extracted corpus to enlarge the training data, neural machine translation models achieved the higher improvement and outperformed the phrase-based models. This work can be useful as a basis for further development of machine translation on the low-resource languages.",https://aclanthology.org/Y17-1051,The National University (Phillippines),2017,November,"Proceedings of the 31st Pacific Asia Conference on Language, Information and Computation","Trieu, Hai Long  and
Tran, Duc-Vu  and
Nguyen, Le Minh",Investigating Phrase-Based and Neural-Based Machine Translation on Low-Resource Settings,,Y17,325
N16-1154,"['Question Answering (QA)', 'Learning Paradigms', 'Information Retrieval', 'Model Architectures']","['Unsupervised Learning', 'Open-Domain QA']",,"We present a simple yet powerful approach to non-factoid answer reranking whereby question-answer pairs are represented by concatenated distributed representation vectors and a multilayer perceptron is used to compute the score for an answer. Despite its simplicity, our approach achieves state-of-the-art performance on a public dataset of How questions, outperforming systems which employ sophisticated feature sets. We attribute this good performance to the use of paragraph instead of word vector representations and to the use of suitable data for training these representations.",https://aclanthology.org/N16-1154,Association for Computational Linguistics,2016,June,Proceedings of the 2016 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies,"Bogdanova, Dasha  and
Foster, Jennifer",This is how we do it: Answer Reranking for Open-domain How Questions with Paragraph Vectors and Minimal Feature Engineering,10.18653/v1/N16-1154,N16,558
N16-1132,"['Machine Translation (MT)', 'Data Management and Generation', 'Low-resource Languages', 'Topic Modeling']",['Statistical MT (SMT)'],,"Most work on extracting parallel text from comparable corpora depends on linguistic resources such as seed parallel documents or translation dictionaries. This paper presents a simple baseline approach for bootstrapping a parallel collection. It starts by observing documents published on similar dates and the cooccurrence of a small number of identical tokens across languages. It then uses fast, online inference for a latent variable model to represent multilingual documents in a shared topic space where it can do efficient nearestneighbor search. Starting from the Gigaword collections in English and Spanish, we train a translation system that outperforms one trained on the WMT'11 parallel training set.",https://aclanthology.org/N16-1132,Association for Computational Linguistics,2016,June,Proceedings of the 2016 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies,"Krstovski, Kriste  and
Smith, David",Bootstrapping Translation Detection and Sentence Extraction from Comparable Corpora,10.18653/v1/N16-1132,N16,411
2021.bppf-1.3,"['Evaluation Techniques', 'Data Management and Generation']",['Data Preparation'],['Annotation Processes'],"Evaluation is of paramount importance in datadriven research fields such as Natural Language Processing NLP and Computer Vision CV. But current evaluation practice in NLP, except for end-to-end tasks such as machine translation, spoken dialogue systems, or NLG, largely hinges on the existence of a single ""ground truth"" against which we can meaningfully compare the prediction of a model. However, this assumption is flawed for two reasons. 1 In many cases, more than one answer is correct. 2 Even where there is a single answer, disagreement among annotators is ubiquitous, making it difficult to decide on a gold standard. We discuss three sources of disagreement: from the annotator, the data, and the context, and show how this affects even seemingly objective tasks. Current methods of adjudication, agreement, and evaluation ought to be reconsidered at the light of this evidence. Some researchers now propose to address this issue by minimizing disagreement, creating cleaner datasets. We argue that such a simplification is likely to result in oversimplified models just as much as it would do for end-to-end tasks such as machine translation. Instead, we suggest that we need to improve today's evaluation practice to better capture such disagreement. Datasets with multiple annotations are becoming more common, as are methods to integrate disagreement into modeling. The logical next step is to extend this to evaluation.",https://aclanthology.org/2021.bppf-1.3,Association for Computational Linguistics,2021,August,"Proceedings of the 1st Workshop on Benchmarking: Past, Present and Future","Basile, Valerio  and
Fell, Michael  and
Fornaciari, Tommaso  and
Hovy, Dirk  and
Paun, Silviu  and
Plank, Barbara  and
Poesio, Massimo  and
Uma, Alexandra",We Need to Consider Disagreement in Evaluation,10.18653/v1/2021.bppf-1.3,bppf,270
W18-6021,"['Learning Paradigms', 'Parsing', 'Low-resource Languages']","['Syntactic Parsing', 'Reinforcement Learning']",['Dependency Parsing'],"We present a general approach with reinforcement learning RL to approximate dynamic oracles for transition systems where exact dynamic oracles are difficult to derive. We treat oracle parsing as a reinforcement learning problem, design the reward function inspired by the classical dynamic oracle, and use Deep Q-Learning DQN techniques to train the oracle with gold trees as features. The combination of a priori knowledge and data-driven methods enables an efficient dynamic oracle, which improves the parser performance over static oracles in several transition systems.",https://aclanthology.org/W18-6021,Association for Computational Linguistics,2018,November,Proceedings of the Second Workshop on Universal Dependencies ({UDW} 2018),"Yu, Xiang  and
Vu, Ngoc Thang  and
Kuhn, Jonas",Approximate Dynamic Oracle for Dependency Parsing with Reinforcement Learning,10.18653/v1/W18-6021,W18,34
2020.pam-1.15,"['Data Management and Generation', 'Model Architectures']",['Data Preparation'],,"Natural Language Inference models have reached almost human-level performance but their generalisation capabilities have not been yet fully characterized. In particular, sensitivity to small changes in the data is a current area of investigation. In this paper, we focus on the effect of punctuation on such models. Our findings can be broadly summarized as follows: 1 irrelevant changes in punctuation are correctly ignored by the recent transformer models BERT while older RNN-based models were sensitive to them. 2 All models, both transformers and RNN-based models, are incapable of taking into account small relevant changes in the punctuation.",https://aclanthology.org/2020.pam-1.15,Association for Computational Linguistics,2020,June,Proceedings of the Probability and Meaning Conference (PaM 2020),"Ek, Adam  and
Bernardy, Jean-Philippe  and
Chatzikyriakidis, Stergios",How does Punctuation Affect Neural Models in Natural Language Inference,,pam,513
2022.semeval-1.230,"['Domain-specific NLP', 'Data Management and Generation', 'Information Extraction', 'Knowledge Representation and Reasoning', 'Model Architectures']","['Data Preparation', 'Named Entity Recognition (NER)', 'NLP for Bibliometrics and Scientometrics', 'Taxonomy Construction', 'Relation Extraction']",['Annotation Processes'],"We describe Symlink, a SemEval shared task of extracting mathematical symbols and their descriptions from LaTeX source of scientific documents. This is a new task in SemEval 2022, which attracted 180 individual registrations and 59 final submissions from 7 participant teams. We expect the data developed for this task and the findings reported to be valuable for the scientific knowledge extraction and automated knowledge base construction communities. The data used in this task is publicly accessible at https://github. com/nlp-uoregon/symlink.",https://aclanthology.org/2022.semeval-1.230,Association for Computational Linguistics,2022,July,Proceedings of the 16th International Workshop on Semantic Evaluation (SemEval-2022),"Lai, Viet  and
Pouran Ben Veyseh, Amir  and
Dernoncourt, Franck  and
Nguyen, Thien",SemEval 2022 Task 12: Symlink - Linking Mathematical Symbols to their Descriptions,10.18653/v1/2022.semeval-1.230,semeval,353
2022.sigmorphon-1.21,"['Text Preprocessing', 'Text Generation', 'Low-resource Languages', 'Model Architectures']","['Text Segmentation', 'Recurrent Neural Networks (RNNs)']","['Word Segmentation', 'Long Short-Term Memory (LSTM) Models']","This paper describes the submissions of the team of the Department of Computational Linguistics, University of Zurich, to the SIGMOR-PHON 2022 Shared Tasks on Morpheme Segmentation and Inflection Generation. Our submissions use a character-level neural transducer that operates over traditional edit actions. While this model has been found particularly well-suited for low-resource settings, using it with large data quantities has been difficult. Existing implementations could not fully profit from GPU acceleration and did not efficiently implement mini-batch training, which could be tricky for a transition-based system. For this year's submission, we have ported the neural transducer to PyTorch and implemented true mini-batch training. This has allowed us to successfully scale the approach to large data quantities and conduct extensive experimentation. We report competitive results for morpheme segmentation including sharing first place in part 2 of the challenge. We also demonstrate that reducing sentence-level morpheme segmentation to a word-level problem is a simple yet effective strategy. Additionally, we report strong results in inflection generation the overall best result for large training sets in part 1, the best results in low-resource learning trajectories in part 2. Our code is publicly available.",https://aclanthology.org/2022.sigmorphon-1.21,Association for Computational Linguistics,2022,July,"Proceedings of the 19th SIGMORPHON Workshop on Computational Research in Phonetics, Phonology, and Morphology","Wehrli, Silvan  and
Clematide, Simon  and
Makarov, Peter",CLUZH at SIGMORPHON 2022 Shared Tasks on Morpheme Segmentation and Inflection Generation,10.18653/v1/2022.sigmorphon-1.21,sigmorphon,452
2021.bucc-1.5,"['Information Extraction', 'Data Management and Generation', 'Low-resource Languages']","['Data Preparation', 'Named Entity Recognition (NER)']",,"We propose a novel approach for rapid prototyping of named entity recognisers through the development of semi-automatically annotated data sets. We demonstrate the proposed pipeline on two under-resourced agglutinating languages: the Dravidian language Malayalam and the Bantu language isiZulu. Our approach is weakly supervised and bootstraps training data from Wikipedia and Google Knowledge Graph. Moreover, our approach is relatively language independent and can consequently be ported quickly and hence cost-effectively from one language to another, requiring only minor language-specific tailoring.",https://aclanthology.org/2021.bucc-1.5,INCOMA Ltd.,2021,September,Proceedings of the 14th Workshop on Building and Using Comparable Corpora (BUCC 2021),"Krishnan, Aravind  and
Ziehe, Stefan  and
Pannach, Franziska  and
Sporleder, Caroline",Employing Wikipedia as a resource for Named Entity Recognition in Morphologically complex under-resourced languages,,bucc,1219
2020.findings-emnlp.92,"['Low-resource Languages', 'Model Architectures']",['Transformer Models'],,"We present PhoBERT with two versions-PhoBERT base and PhoBERT large -the first public large-scale monolingual language models pre-trained for Vietnamese. Experimental results show that PhoBERT consistently outperforms the recent best pre-trained multilingual model XLM-R Conneau et al., 2020 and improves the state-of-the-art in multiple Vietnamese-specific NLP tasks including Part-of-speech tagging, Dependency parsing, Named-entity recognition and Natural language inference. We release PhoBERT to facilitate future research and downstream applications for Vietnamese NLP. Our PhoBERT models are available at: https://github. com/VinAIResearch/PhoBERT.",https://aclanthology.org/2020.findings-emnlp.92,Association for Computational Linguistics,2020,November,Findings of the Association for Computational Linguistics: EMNLP 2020,"Nguyen, Dat Quoc  and
Tuan Nguyen, Anh",PhoBERT: Pre-trained language models for Vietnamese,10.18653/v1/2020.findings-emnlp.92,findings,215
W16-4110,"['Text Generation', 'Data Management and Generation', 'Low-resource Languages']","['Data Analysis', 'Text Simplification']",,"In this paper, we present a comparative analysis of statistically predictive syntactic features of complexity and the treatment of these features by humans when simplifying texts. To that end, we have used a list of the most five statistically predictive features obtained automatically and the Corpus of Basque Simplified Texts CBST to analyse how the syntactic phenomena in these features have been manually simplified. Our aim is to go beyond the descriptions of operations found in the corpus and relate the multidisciplinary findings to understand text complexity from different points of view. We also present some issues that can be important when analysing linguistic complexity.",https://aclanthology.org/W16-4110,The COLING 2016 Organizing Committee,2016,December,Proceedings of the Workshop on Computational Linguistics for Linguistic Complexity ({CL}4{LC}),"Gonzalez-Dios, Itziar  and
Aranzabe, Mar{\'\i}a Jes{\'u}s  and
D{\'\i}az de Ilarraza, Arantza",A Preliminary Study of Statistically Predictive Syntactic Complexity Features and Manual Simplifications in Basque,,W16,296
2021.findings-acl.75,"['Machine Translation (MT)', 'Multilingual NLP', 'Data Management and Generation', 'Low-resource Languages']",['Data Augmentation'],,"Cognate prediction is the task of generating, in a given language, the likely cognates of words in a related language, where cognates are words in related languages that have evolved from a common ancestor word. It is a task for which little data exists and which can aid linguists in the discovery of previously undiscovered relations. Previous work has applied machine translation MT techniques to this task, based on the tasks' similarities, without, however, studying their numerous differences or optimising architectural choices and hyper-parameters. In this paper, we investigate whether cognate prediction can benefit from insights from low-resource MT. We first compare statistical MT SMT and neural MT NMT architectures in a bilingual setup. We then study the impact of employing data augmentation techniques commonly seen to give gains in low-resource MT: monolingual pretraining, backtranslation and multilinguality. Our experiments on several Romance languages show that cognate prediction behaves only to a certain extent like a standard lowresource MT task. In particular, MT architectures, both statistical and neural, can be successfully used for the task, but using supplementary monolingual data is not always as beneficial as using additional language data, contrarily to what is observed for MT.",https://aclanthology.org/2021.findings-acl.75,Association for Computational Linguistics,2021,August,Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021,"Fourrier, Cl{\'e}mentine  and
Bawden, Rachel  and
Sagot, Beno{\^\i}t",Can Cognate Prediction Be Modelled as a Low-Resource Machine Translation Task?,10.18653/v1/2021.findings-acl.75,findings,465
2020.cogalex-1.5,"['Data Management and Generation', 'Low-resource Languages', 'Cross-lingual Application', 'Multilingual NLP', 'Model Architectures']",['Data Preparation'],,"The shared task of the CogALex-VI workshop focuses on the monolingual and multilingual identification of semantic relations. We provided training and validation data for the following languages: English, German and Chinese. Given a word pair, systems had to be trained to identify which relation holds between them, with possible choices being synonymy, antonymy, hypernymy and no relation at all. Two test sets were released for evaluating the participating systems. One containing pairs for each of the training languages systems were evaluated in a monolingual fashion and the other one proposing a surprise language to test the crosslingual transfer capabilities of the systems. Among the submitted systems, top performance was achieved by a transformer-based model in both the monolingual and in the multilingual setting, for all the tested languages, proving the potentials of this recently-introduced neural architecture. The shared task description and the results are available at https://sites.google.com/site/ cogalexvisharedtask/.",https://aclanthology.org/2020.cogalex-1.5,Association for Computational Linguistics,2020,December,Proceedings of the Workshop on the Cognitive Aspects of the Lexicon,"Xiang, Rong  and
Chersoni, Emmanuele  and
Iacoponi, Luca  and
Santus, Enrico",The CogALex Shared Task on Monolingual and Multilingual Identification of Semantic Relations,,cogalex,552
Q16-1032,"['Parsing', 'Model Architectures']","['Syntactic Parsing', 'Recurrent Neural Networks (RNNs)']","['Dependency Parsing', 'Long Short-Term Memory (LSTM) Models']","We suggest a compositional vector representation of parse trees that relies on a recursive combination of recurrent-neural network encoders. To demonstrate its effectiveness, we use the representation as the backbone of a greedy, bottom-up dependency parser, achieving very strong accuracies for English and Chinese, without relying on external word embeddings. The parser's implementation is available for download at the first author's webpage.",https://aclanthology.org/Q16-1032,MIT Press,2016,,,"Kiperwasser, Eliyahu  and
Goldberg, Yoav",Easy-First Dependency Parsing with Hierarchical Tree LSTMs,10.1162/tacl_a_00110,Q16,1186
2020.sigdial-1.5,"['Model Architectures', 'Dialogue Systems', 'Knowledge Representation and Reasoning']",,,"We will demonstrate a deployed conversational AI system that acts as a host of a smartbuilding on a university campus. The system combines open-domain social conversation with task-based conversation regarding navigation in the building, live resource updates e.g. available computers and events in the building. We are able to demonstrate the system on several platforms: Google Home devices, Android phones, and a Furhat robot.",https://aclanthology.org/2020.sigdial-1.5,Association for Computational Linguistics,2020,July,Proceedings of the 21th Annual Meeting of the Special Interest Group on Discourse and Dialogue,"Siei{\'n}ska, Weronika  and
Dondrup, Christian  and
Gunson, Nancie  and
Lemon, Oliver",Conversational Agents for Intelligent Buildings,10.18653/v1/2020.sigdial-1.5,sigdial,16
K18-1016,"['Cross-lingual Application', 'Domain-specific NLP', 'Dialogue Systems', 'Data Management and Generation', 'Multilingual NLP', 'Low-resource Languages', 'Classification Applications', 'Model Architectures']","['Intent Detection', 'Data Preparation', 'Chatbots', 'NLP for News and Media']",['NLP for Social Media'],"We propose a new method to detect when users express the intent to leave a service, also known as churn. While previous work focuses solely on social media, we show that this intent can be detected in chatbot conversations. As companies increasingly rely on chatbots, they need an overview of potentially churny users. To this end, we crowdsource and publish a dataset of churn intent expressions in chatbot interactions in German and English. We show that classifiers trained on social media data can detect the same intent in the context of chatbots. We introduce a classification architecture that outperforms existing work on churn intent detection in social media. Moreover, we show that, using bilingual word embeddings, a system trained on combined English and German data outperforms monolingual approaches. As the only existing dataset is in English, we crowdsource and publish a novel dataset of German tweets. We thus underline the universal aspect of the problem, as examples of churn intent in English help us identify churn in German tweets and chatbot conversations.",https://aclanthology.org/K18-1016,Association for Computational Linguistics,2018,October,Proceedings of the 22nd Conference on Computational Natural Language Learning,"Abbet, Christian  and
M{'}hamdi, Meryem  and
Giannakopoulos, Athanasios  and
West, Robert  and
Hossmann, Andreea  and
Baeriswyl, Michael  and
Musat, Claudiu",Churn Intent Detection in Multilingual Chatbot Conversations and Social Media,10.18653/v1/K18-1016,K18,982
J19-1003,"['Parsing', 'Low-resource Languages', 'Model Architectures']","['Syntactic Parsing', 'Recurrent Neural Networks (RNNs)']","['Dependency Parsing', 'Long Short-Term Memory (LSTM) Models']","We report our work on building linguistic resources and data-driven parsers in the grammatical relation GR analysis for Mandarin Chinese. Chinese, as an analytic language, encodes grammatical information in a highly configurational rather than morphological way. Accordingly, it is possible and reasonable to represent almost all grammatical relations as bilexical dependencies. In this work, we propose to represent grammatical information using general directed dependency graphs. Both only-local and rich long-distance dependencies are explicitly represented. To create high-quality annotations, we take advantage of an existing TreeBank, namely, Chinese TreeBank CTB, which is grounded on the Government and Binding theory. We define a set of linguistic rules to explore CTB's implicit phrase structural information and build deep dependency graphs. The reliability of this linguistically motivated GR extraction procedure is highlighted by manual Submission",https://aclanthology.org/J19-1003,MIT Press,2019,March,,"Sun, Weiwei  and
Chen, Yufei  and
Wan, Xiaojun  and
Liu, Meichun",Parsing Chinese Sentences with Grammatical Relations,10.1162/coli_a_00343,J19,1119
W19-0606,"['Embeddings', 'Domain-specific NLP', 'Data Management and Generation']",['Data Preparation'],,"The early phases of requirements engineering RE deal with a vast amount of software requirements i.e.,requirements that define characteristics of software systems, which are typically expressed in natural language. Analysing such unstructured requirements, usually obtained from stakeholders' inputs, is considered a challenging task due to the inherent ambiguity and inconsistency of natural language. To support such a task, methods based on natural language processing NLP can be employed. One of the more recent advances in NLP is the use of word embeddings for capturing contextual information, which can then be applied in word analogy tasks. In this paper, we describe a new resource, i.e., embedding-based representations of semantic frames in FrameNet, which was developed to support the detection of relations between software requirements. Our embeddings, which encapsulate contextual information at the semantic frame level, were trained on a large corpus of requirements i.e., a collection of more than three million mobile application reviews. The similarity between these frame embeddings is then used as a basis for detecting semantic relatedness between software requirements. Compared with existing resources underpinned by frame embeddings built upon pre-trained vectors, our proposed frame embeddings obtained better performance against judgments of an RE expert. These encouraging results demonstrate the potential of the resource in supporting RE analysis tasks e.g., traceability, which we plan to investigate as part of our immediate future work.",https://aclanthology.org/W19-0606,Association for Computational Linguistics,2019,May,Proceedings of the 13th International Conference on Computational Semantics - Student Papers,"Alhoshan, Waad  and
Batista-Navarro, Riza  and
Zhao, Liping",Semantic Frame Embeddings for Detecting Relations between Software Requirements,10.18653/v1/W19-0606,W19,1087
2021.motra-1.5,"['Machine Translation (MT)', 'Discourse Analysis', 'Low-resource Languages', 'Data Management and Generation']","['Data Preparation', 'Data Analysis']",,"Multiword expressions are of key importance in language generation and processing. Certain multiword expressions also could operate as discourse markers. In this research, we combined the alignment model of the phrase-based statistical machine translation and manual treatment of the data in order to examine English multiword discourse markers and their equivalents in Lithuanian and Hebrew, by researching their changes in translation. After establishing a full list of multiword discourse markers in our generated parallel corpus, we focused on the two most frequent ones functioning as stance attitudinal discourse markers: I think and you know aiming to research if they demonstrate their functional stability as stance attitudinal discourse markers in translation and what changes they undergo in Lithuanian and Hebrew translation. Our research proves that the examined multiword discourse markers preserve their function as stance attitudinal discourse markers and tend to remain multiword discourse markers in the Hebrew translation but turn into one-word discourse markers in Lithuanian due to the translation tendency relying on inflections.",https://aclanthology.org/2021.motra-1.5,Association for Computational Linguistics,2021,May,Proceedings for the First Workshop on Modelling Translation: Translatology in the Digital Age,"Oleskeviciene, Giedre Valunaite  and
Liebeskind, Chaya",Multiword expressions as discourse markers in Hebrew and Lithuanian,,motra,620
2020.readi-1.12,"['Data Management and Generation', 'Multilingual NLP', 'Low-resource Languages', 'Text Generation']","['Text Simplification', 'Data Analysis']",,"In text simplification and readability research, several features have been proposed to estimate or simplify a complex text, e.g., readability scores, sentence length, or proportion of POS tags. These features are however mainly developed for English. In this paper, we investigate their relevance for Czech, German, English, Spanish, and Italian text simplification corpora. Our multi-lingual and multi-domain corpus analysis shows that the relevance of different features for text simplification is different per corpora, language, and domain. For example, the relevance of the lexical complexity is different across all languages, the BLEU score across all domains, and 14 features within the web domain corpora. Overall, the negative statistical tests regarding the other features across and within domains and languages lead to the assumption that text simplification models may be transferable between different domains or different languages.",https://aclanthology.org/2020.readi-1.12,European Language Resources Association,2020,May,Proceedings of the 1st Workshop on Tools and Resources to Empower People with REAding DIfficulties (READI),"Stodden, Regina  and
Kallmeyer, Laura",A multi-lingual and cross-domain analysis of features for text simplification,,readi,798
2020.bea-1.18,"['Domain-specific NLP', 'Data Management and Generation', 'Learning Paradigms', 'Embeddings', 'Evaluation Techniques']","['Transfer Learning', 'Data Preparation', 'Medical and Clinical NLP', 'Word Embeddings']",,"Many clinical assessment instruments used to diagnose language impairments in children include a task in which the subject must formulate a sentence to describe an image using a specific target word. Because producing sentences in this way requires the speaker to integrate syntactic and semantic knowledge in a complex manner, responses are typically evaluated on several different dimensions of appropriateness yielding a single composite score for each response. In this paper, we present a dataset consisting of non-clinically elicited responses for three related sentence formulation tasks, and we propose an approach for automatically evaluating their appropriateness. Using neural machine translation, we generate correct-incorrect sentence pairs to serve as synthetic data in order to increase the amount and diversity of training data for our scoring model. Our scoring model uses transfer learning to facilitate automatic sentence appropriateness evaluation. We further compare custom word embeddings with pre-trained contextualized embeddings serving as features for our scoring model. We find that transfer learning improves scoring accuracy, particularly when using pre-trained contextualized embeddings.",https://aclanthology.org/2020.bea-1.18,Association for Computational Linguistics,2020,July,Proceedings of the Fifteenth Workshop on Innovative Use of NLP for Building Educational Applications,"Wang, Yiyi  and
Prud{'}hommeaux, Emily  and
Asgari, Meysam  and
Dolata, Jill",Automated Scoring of Clinical Expressive Language Evaluation Tasks,10.18653/v1/2020.bea-1.18,bea,281
2021.nlp4posimpact-1.11,"['Domain-specific NLP', 'Classification Applications', 'Model Architectures']","['Transformer Models', 'Recurrent Neural Networks (RNNs)', 'Multilabel Text Classification']",['Long Short-Term Memory (LSTM) Models'],"Understanding the gaps between job requirements and university curricula is crucial for improving student success and institutional effectiveness in higher education. In this context, natural language processing NLP can be leveraged to generate granular insights into where the gaps are and how they change. This paper proposes a three-dimensional research framework that combines NLP techniques with economic and educational research to quantify the alignment between course syllabi and job postings. We elaborate on key technical details of the framework and further discuss its potential positive impacts on practice, including unveiling the inequalities in and long-term consequences of educationoccupation alignment to inform policymakers, and fostering information systems to support students, institutions and employers in the school-to-work pipeline.",https://aclanthology.org/2021.nlp4posimpact-1.11,Association for Computational Linguistics,2021,August,Proceedings of the 1st Workshop on NLP for Positive Impact,"Yu, Renzhe  and
Das, Subhro  and
Gurajada, Sairam  and
Varshney, Kush  and
Raghavan, Hari  and
Lastra-Anadon, Carlos",A Research Framework for Understanding Education-Occupation Alignment with NLP Techniques,10.18653/v1/2021.nlp4posimpact-1.11,nlp4posimpact,1279
2022.constraint-1.1,"['Learning Paradigms', 'Information Extraction', 'Data Management and Generation', 'Domain-specific NLP']","['Multimodal Learning', 'Data Preparation', 'Named Entity Recognition (NER)', 'NLP for News and Media']","['NLP for Social Media', 'Annotation Processes']","We present the findings of the shared task at the CONSTRAINT 2022 workshop on ""Hero, Villain, and Victim: Dissecting Harmful Memes for Semantic Role Labeling of Entities."" The task aims to delve deeper into meme comprehension by deciphering the connotations behind the entities present in a meme. In more nuanced terms, the shared task focuses on determining the victimizing, glorifying, and vilifying intentions embedded in meme entities to explicate their connotations. To this end, we curate HVVMemes, a novel meme dataset of about 7,000 memes spanning the domains of COVID-19 and US Politics, each containing entities and their associated roles: hero, villain, victim, or other. The shared task attracted 105 registered participants, but eventually only nine of them made official submissions. The most successful systems used ensembles combining textual and multimodal models, with the best system achieving an F1-score of 58.67.",https://aclanthology.org/2022.constraint-1.1,Association for Computational Linguistics,2022,May,Proceedings of the Workshop on Combating Online Hostile Posts in Regional Languages during Emergency Situations,"Sharma, Shivam  and
Suresh, Tharun  and
Kulkarni, Atharva  and
Mathur, Himanshi  and
Nakov, Preslav  and
Akhtar, Md. Shad  and
Chakraborty, Tanmoy","Findings of the CONSTRAINT 2022 Shared Task on Detecting the Hero, the Villain, and the Victim in Memes",10.18653/v1/2022.constraint-1.1,constraint,533
2020.nlpmc-1.3,"['Data Management and Generation', 'Low-resource Languages', 'Discourse Analysis', 'Domain-specific NLP']","['Data Preparation', 'Data Analysis', 'Medical and Clinical NLP']",['Annotation Processes'],"Medical conversation is a central part of medical care. Yet, the current state and quality of medical conversation is far from perfect. Therefore, a substantial amount of research has been done to obtain a better understanding of medical conversation and to address its practical challenges and dilemmas. In line with this stream of research, we have developed a multi-layer structure annotation scheme to analyze medical conversation, and are using the scheme to construct a corpus of naturally occurring medical conversation in Chinese pediatric primary care setting. Some of the preliminary findings are reported regarding 1 how a medical conversation starts, 2 where communication problems tend to occur, and 3 how physicians close a conversation. Challenges and opportunities for research on medical conversation with NLP techniques will be discussed.",https://aclanthology.org/2020.nlpmc-1.3,Association for Computational Linguistics,2020,July,Proceedings of the First Workshop on Natural Language Processing for Medical Conversations,"Wang, Nan  and
Song, Yan  and
Xia, Fei",Studying Challenges in Medical Conversation with Structured Annotation,10.18653/v1/2020.nlpmc-1.3,nlpmc,264
J18-2003,['Parsing'],,,"Spurious ambiguity is the phenomenon whereby distinct derivations in grammar may assign the same structural reading, resulting in redundancy in the parse search space and inefficiency in parsing. Understanding the problem depends on identifying the essential mathematical structure of derivations. This is trivial in the case of context free grammar, where the parse structures are ordered trees; in the case of type logical categorial grammar, the parse structures are proof nets. However, with respect to multiplicatives, intrinsic proof nets have not yet been given for displacement calculus, and proof nets for additives, which have applications to polymorphism, are not easy to characterize. In this context we approach here multiplicative-additive spurious ambiguity by means of the proof-theoretic technique of focalization. 1 This paper is a revised and expanded version of Morrill and Valentín 2015 .",https://aclanthology.org/J18-2003,MIT Press,2018,June,,"Morrill, Glyn  and
Valent{\'\i}n, Oriol",Spurious Ambiguity and Focalization,10.1162/COLI_a_00316,J18,849
2020.dmr-1.7,"['Domain-specific NLP', 'Parsing', 'Question Answering (QA)', 'Information Retrieval', 'Knowledge Representation and Reasoning']","['Medical and Clinical NLP', 'Semantic Parsing', 'Abstract Meaning Representation (AMR)']",,"This paper examines how Abstract Meaning Representation AMR can be utilized for finding answers to research questions in medical scientific documents, in particular, to advance the study of UV ultraviolet inactivation of the novel coronavirus that causes the disease COVID-19. We describe the development of a proof-of-concept prototype tool, InfoForager, which uses AMR to conduct a semantic search, targeting the meaning of the user question, and matching this to sentences in medical documents that may contain information to answer that question. This work was conducted as a sprint over a period of six weeks, and reveals both promising results and challenges in reducing the user search time relating to COVID-19 research, and in general, domain adaption of AMR for this task. This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http:// creativecommons.org/licenses/by/4.0/. 1 SARS-CoV-2: Severe acute respiratory syndrome coronavirus 2, first identified in 2019. 2 Germicidal UV: Also known as UVC, relating to the UV spectrum between 200 -280 nm.",https://aclanthology.org/2020.dmr-1.7,Association for Computational Linguistics,2020,December,Proceedings of the Second International Workshop on Designing Meaning Representations,"Bonial, Claire  and
Lukin, Stephanie M.  and
Doughty, David  and
Hill, Steven  and
Voss, Clare",InfoForager: Leveraging Semantic Search with AMR for COVID-19 Research,,dmr,1178
2020.lrec-1.450,"['Learning Paradigms', 'Multilingual NLP', 'Low-resource Languages', 'Machine Translation (MT)']","['Neural MT (NMT)', 'Transfer Learning']",,"In the context of under-resourced neural machine translation NMT, transfer learning from an NMT model trained on a high resource language pair, or from a multilingual NMT M-NMT model, has been shown to boost performance to a large extent. In this paper, we focus on so-called cold start transfer learning from an M-NMT model, which means that the parent model is not trained on any of the child data. Such a set-up enables quick adaptation of M-NMT models to new languages. We investigate the effectiveness of cold start transfer learning from a many-to-many M-NMT model to an under-resourced child. We show that sufficiently large sub-word vocabularies should be used for transfer learning to be effective in such a scenario. When adopting relatively large sub-word vocabularies we observe increases in performance thanks to transfer learning from a parent M-NMT model, both when translating to and from the under-resourced language. Our proposed approach involving dynamic vocabularies is both practical and effective. We report results on two under-resourced language pairs, i.e. Icelandic-English and Irish-English.",https://aclanthology.org/2020.lrec-1.450,European Language Resources Association,2020,May,Proceedings of the 12th Language Resources and Evaluation Conference,"Defauw, Arne  and
Vanallemeersch, Tom  and
Van Winckel, Koen  and
Szoc, Sara  and
Van den Bogaert, Joachim",Being Generous with Sub-Words towards Small NMT Children,,lrec,1412
2022.nlp4convai-1.16,"['Evaluation Techniques', 'Data Management and Generation', 'Dialogue Systems', 'Model Architectures']","['Open Domain Dialogue Systems', 'Data Preparation', 'Response Generation']",['Annotation Processes'],"Personality traits influence human actions and thoughts, which is manifested in day to day conversations. Although glimpses of personality traits are observable in existing open domain conversation corpora, leveraging generic language modelling for response generation overlooks the interlocutor idiosyncrasies, resulting in non-customizable personality agnostic responses. With the motivation of enabling stylistically configurable response generators, in this paper we experiment with end-to-end mechanisms to ground neural response generators based on both i interlocutor Big-5 personality traits, and ii discourse intent as stylistic control codes. Since most of the existing large scale open domain chat corpora do not include Big-5 personality traits and discourse intent, we employ automatic annotation schemes to enrich the corpora with noisy estimates of personality and intent annotations, and further assess the impact of using such features as control codes for response generation using automatic evaluation metrics, ablation studies and human judgement. Our experiments illustrate the effectiveness of this strategy resulting in improvements to existing benchmarks. Additionally, we yield two silver standard annotated corpora with intents and personality traits annotated, which can be of use to the research community.",https://aclanthology.org/2022.nlp4convai-1.16,Association for Computational Linguistics,2022,May,Proceedings of the 4th Workshop on NLP for Conversational AI,"Saha, Sougata  and
Das, Souvik  and
Srihari, Rohini",Stylistic Response Generation by Controlling Personality Traits and Intent,10.18653/v1/2022.nlp4convai-1.16,nlp4convai,1317
Q17-1014,"['Topic Modeling', 'Domain-specific NLP', 'Model Architectures']",['NLP for Bibliometrics and Scientometrics'],['Citation Analysis'],"Much of scientific progress stems from previously published findings, but searching through the vast sea of scientific publications is difficult. We often rely on metrics of scholarly authority to find the prominent authors but these authority indices do not differentiate authority based on research topics. We present Latent Topical-Authority Indexing LTAI for jointly modeling the topics, citations, and topical authority in a corpus of academic papers. Compared to previous models, LTAI differs in two main aspects. First, it explicitly models the generative process of the citations, rather than treating the citations as given. Second, it models each author's influence on citations of a paper based on the topics of the cited papers, as well as the citing papers. We fit LTAI into four academic corpora: CORA, Arxiv Physics, PNAS, and Citeseer. We compare the performance of LTAI against various baselines, starting with the latent Dirichlet allocation, to the more advanced models including author-link topic model and dynamic author citation topic model. The results show that LTAI achieves improved accuracy over other similar models when predicting words, citations and authors of publications.",https://aclanthology.org/Q17-1014,MIT Press,2017,,,"Kim, Jooyeon  and
Kim, Dongwoo  and
Oh, Alice","Joint Modeling of Topics, Citations, and Topical Authority in Academic Corpora",10.1162/tacl_a_00055,Q17,393
W19-1006,['Knowledge Representation and Reasoning'],,,"We describe a representation for modeling meaning of natural language constructs that is closely related to fuzzy sets. Same as fuzzy sets, it allows to express quantitative relationships between different concepts, and is designed to support vagueness and imprecision common to natural language. We compare the representations using several examples, and argue that in some cases the proposed representation may be a good alternative to the fuzzy set based representation, and that it may also be easier to learn from data.",https://aclanthology.org/W19-1006,Association for Computational Linguistics,2019,June,"Proceedings of the {IWCS} 2019 Workshop on Computing Semantics with Types, Frames and Related Structures","Kapustin, Pavlo  and
Kapustin, Michael",Modeling language constructs with compatibility intervals,10.18653/v1/W19-1006,W19,349
C18-1275,"['Information Retrieval', 'Parsing', 'Data Management and Generation', 'Domain-specific NLP']","['Data Preparation', 'Semantic Parsing']",['Annotation Processes'],"Technical support problems are very complex. In contrast to regular web queries that contain few keywords or factoid questions which are a few sentences, these problems usually include attributes like a detailed description of what is failing symptom, steps taken in an effort to remediate the failure activity, and sometimes a specific request or ask intent. Automating support is the task of automatically providing answers to these problems given a corpus of solution documents. Traditional approaches to this task rely on information retrieval and are keyword based; looking for keyword overlap between the question and solution documents and ignoring these attributes. We present an approach for semantic parsing of technical questions that uses grammatical structure to extract these attributes as a baseline, and a CRF based model that can improve performance considerably in the presence of annotated data for training. We also demonstrate that combined with reasoning, these attributes help outperform retrieval baselines.",https://aclanthology.org/C18-1275,Association for Computational Linguistics,2018,August,Proceedings of the 27th International Conference on Computational Linguistics,"Gupta, Abhirut  and
Ray, Anupama  and
Dasgupta, Gargi  and
Singh, Gautam  and
Aggarwal, Pooja  and
Mohapatra, Prateeti",Semantic Parsing for Technical Support Questions,,C18,371
2020.ngt-1.28,"['Text Generation', 'Machine Translation (MT)', 'Data Management and Generation', 'Multilingual NLP', 'Low-resource Languages']","['Data Preparation', 'Paraphrase and Rephrase Generation']",,"We present the task of Simultaneous Translation and Paraphrasing for Language Education STAPLE. Given a prompt in one language, the goal is to generate a diverse set of correct translations that language learners are likely to produce. This is motivated by the need to create and maintain large, high-quality sets of acceptable translations for exercises in a language-learning application, and synthesizes work spanning machine translation, MT evaluation, automatic paraphrasing, and language education technology. We developed a novel corpus with unique properties for five languages Hungarian, Japanese, Korean, Portuguese, and Vietnamese, and report on the results of a shared task challenge which attracted 20 teams to solve the task. In our meta-analysis, we focus on three aspects of the resulting systems: external training corpus selection, model architecture and training decisions, and decoding and filtering strategies. We find that strong systems start with a large amount of generic training data, and then finetune with in-domain data, sampled according to our provided learner response frequencies.",https://aclanthology.org/2020.ngt-1.28,Association for Computational Linguistics,2020,July,Proceedings of the Fourth Workshop on Neural Generation and Translation,"Mayhew, Stephen  and
Bicknell, Klinton  and
Brust, Chris  and
McDowell, Bill  and
Monroe, Will  and
Settles, Burr",Simultaneous Translation and Paraphrase for Language Education,10.18653/v1/2020.ngt-1.28,ngt,345
2022.ltedi-1.3,"['Ethics', 'Domain-specific NLP', 'Data Management and Generation', 'Low-resource Languages', 'Classification Applications']",['Data Preparation'],['Annotation Processes'],"This paper makes the case for studying concreteness in language as a bridge that will allow language technology to support the understanding and improvement of ethnic inclusivity in job advertisements. We propose an annotation scheme that guides the assignment of sentences in job ads to classes that reflect concrete actions, i.e., what the employer needs people to do, and abstract dispositions, i.e., who the employer expects people to be. Using an annotated dataset of Dutch-language job ads, we demonstrate that machine learning technology is effectively able to distinguish these classes.",https://aclanthology.org/2022.ltedi-1.3,Association for Computational Linguistics,2022,May,"Proceedings of the Second Workshop on Language Technology for Equality, Diversity and Inclusion","Adams, Jetske  and
Poelmans, Kyrill  and
Hendrickx, Iris  and
Larson, Martha",Doing not Being: Concrete Language as a Bridge from Language Technology to Ethnically Inclusive Job Ads,10.18653/v1/2022.ltedi-1.3,ltedi,628
2022.insights-1.14,"['Evaluation Techniques', 'Text Clustering', 'Data Management and Generation']",,,"In natural language processing, multi-dataset benchmarks for common tasks e.g., Su-perGLUE for natural language inference and MRQA for question answering have risen in importance. Invariably, tasks and individual examples vary in difficulty. Recent analysis methods infer properties of examples such as difficulty. In particular, Item Response Theory IRT jointly infers example and model properties from the output of benchmark tasks i.e., scores for each model-example pair. Therefore, it seems sensible that methods like IRT should be able to detect differences between datasets in a task. This work shows that current IRT models are not as good at identifying differences as we would expect, explain why this is difficult, and outline future directions that incorporate more textual signal from examples.",https://aclanthology.org/2022.insights-1.14,Association for Computational Linguistics,2022,May,Proceedings of the Third Workshop on Insights from Negative Results in NLP,"Rodriguez, Pedro  and
Htut, Phu Mon  and
Lalor, John  and
Sedoc, Jo{\~a}o",Clustering Examples in Multi-Dataset Benchmarks with Item Response Theory,10.18653/v1/2022.insights-1.14,insights,1094
2022.tacl-1.6,"['Evaluation Techniques', 'Data Management and Generation', 'Classification Applications', 'Model Architectures']","['Transformer Models', 'Data Preparation']",['Annotation Processes'],"Majority voting and averaging are common approaches used to resolve annotator disagreements and derive single ground truth labels from multiple annotations. However, annotators may systematically disagree with one another, often reflecting their individual biases and values, especially in the case of subjective tasks such as detecting affect, aggression, and hate speech. Annotator disagreements may capture important nuances in such tasks that are often ignored while aggregating annotations to a single ground truth. In order to address this, we investigate the efficacy of multi-annotator models. In particular, our multi-task based approach treats predicting each annotators' judgements as separate subtasks, while sharing a common learned representation of the task. We show that this approach yields same or better performance than aggregating labels in the data prior to training across seven different binary classification tasks. Our approach also provides a way to estimate uncertainty in predictions, which we demonstrate better correlate with annotation disagreements than traditional methods. Being able to model uncertainty is especially useful in deployment scenarios where knowing when not to make a prediction is important.",https://aclanthology.org/2022.tacl-1.6,MIT Press,2022,,,"Davani, Aida Mostafazadeh  and
D{\'\i}az, Mark  and
Prabhakaran, Vinodkumar",Dealing with Disagreements: Looking Beyond the Majority Vote in Subjective Annotations,10.1162/tacl_a_00449,tacl,7
2020.crac-1.12,"['Learning Paradigms', 'Data Management and Generation', 'Information Extraction']","['Data Preparation', 'Coreference Resolution', 'Active Learning']",['Annotation Processes'],"In this paper we describe our attempt to increase the amount of information that can be retrieved through active learning sessions compared to previous approaches. We optimise the annotator's labelling process using active learning in the context of coreference resolution. Using simulated active learning experiments, we suggest three adjustments to ensure the labelling time is spent as efficiently as possible. All three adjustments provide more information to the machine learner than the baseline, though a large impact on the F1 score over time is not observed. Compared to previous models, we report a marginal F1 improvement on the final coreference models trained using for two out of the three approaches tested when applied to the English OntoNotes 2012 Coreference Resolution data. Our best-performing model achieves 58.01 F1, an increase of 0.93 F1 over the baseline model.",https://aclanthology.org/2020.crac-1.12,Association for Computational Linguistics,2020,December,"Proceedings of the Third Workshop on Computational Models of Reference, Anaphora and Coreference","Espeland, Vebj{\o}rn  and
Alex, Beatrice  and
Bach, Benjamin",Enhanced Labelling in Active Learning for Coreference Resolution,,crac,595
2020.bea-1.11,"['Domain-specific NLP', 'Data Management and Generation', 'Classification Applications', 'Learning Paradigms', 'Low-resource Languages']","['Supervised Learning', 'Data Preparation']",['Annotation Processes'],"In undergraduate theses, a good methodology section should describe the series of steps that were followed in performing the research. To assist students in this task, we develop machine-learning models and an app that uses them to provide feedback while students write. We construct an annotated corpus that identifies sentences representing methodological steps and labels when a methodology contains a logical sequence of such steps. We train machine-learning models based on language modeling and lexical features that can identify sentences representing methodological steps with 0.939 f-measure, and identify methodology sections containing a logical sequence of steps with an accuracy of 87%. We incorporate these models into a Microsoft Office Addin, and show that students who improved their methodologies according to the model feedback received better grades on their methodologies.",https://aclanthology.org/2020.bea-1.11,Association for Computational Linguistics,2020,July,Proceedings of the Fifteenth Workshop on Innovative Use of NLP for Building Educational Applications,"Gonz{\'a}lez-L{\'o}pez, Samuel  and
Bethard, Steven  and
Lopez-Lopez, Aurelio",Assisting Undergraduate Students in Writing Spanish Methodology Sections,10.18653/v1/2020.bea-1.11,bea,491
2020.codi-1.9,"['Domain-specific NLP', 'Data Management and Generation', 'Classification Applications', 'Discourse Analysis', 'Embeddings']","['NLP for Bibliometrics and Scientometrics', 'Sentence Embeddings', 'Data Preparation']",,"We introduce four tasks designed to determine which sentence encoders best capture discourse properties of sentences from scientific abstracts, namely coherence between clauses of a sentence, and discourse relations within sentences. We show that even if contextual encoders such as BERT or SciBERT encodes the coherence in discourse units, they do not help to predict three discourse relations commonly used in scientific abstracts. We discuss what these results underline, namely that these discourse relations are based on particular phrasing that allow non-contextual encoders to perform well.",https://aclanthology.org/2020.codi-1.9,Association for Computational Linguistics,2020,November,Proceedings of the First Workshop on Computational Approaches to Discourse,"Huber, Laurine  and
Memmadi, Chaker  and
Dargnat, Mathilde  and
Toussaint, Yannick",Do sentence embeddings capture discourse properties of sentences from Scientific Abstracts ?,10.18653/v1/2020.codi-1.9,codi,1163
2021.semeval-1.157,"['Learning Paradigms', 'Classification Applications', 'Model Architectures']","['Humor Detection', 'Hate and Offensive Speech Detection', 'Supervised Learning']",,"This paper describes the approach that was developed for SemEval 2021 Task 7 Hahackathon: Incorporating Demographic Factors into Shared Humor Tasks Meaney et al., 2021 by the DUTH Team. We used and compared a variety of preprocessing techniques, vectorization methods, and numerous conventional machine learning algorithms, in order to construct classification and regression models for the given tasks. We used majority voting to combine the models' outputs with small Neural Networks NN for classification tasks and their mean for regression for improving our system's performance. While these methods proved weaker than modern, deep learning models, they are still relevant in research tasks because of their low requirements on computational power and faster training.",https://aclanthology.org/2021.semeval-1.157,Association for Computational Linguistics,2021,August,Proceedings of the 15th International Workshop on Semantic Evaluation (SemEval-2021),"Karasakalidis, Alexandros  and
Effrosynidis, Dimitrios  and
Arampatzis, Avi",DUTH at SemEval-2021 Task 7: Is Conventional Machine Learning for Humorous and Offensive Tasks enough in 2021?,10.18653/v1/2021.semeval-1.157,semeval,1130
P18-2108,"['Domain-specific NLP', 'Information Extraction', 'Model Architectures']","['Graph Neural Networks (GNNs)', 'Medical and Clinical NLP']",['Biomedical NLP'],"We propose a novel neural method to extract drug-drug interactions DDIs from texts using external drug molecular structure information. We encode textual drug pairs with convolutional neural networks and their molecular pairs with graph convolutional networks GCNs, and then we concatenate the outputs of these two networks. In the experiments, we show that GCNs can predict DDIs from the molecular structures of drugs in high accuracy and the molecular information can enhance text-based DDI extraction by 2.39 percent points in the F-score on the DDIExtraction 2013 shared task data set.",https://aclanthology.org/P18-2108,Association for Computational Linguistics,2018,July,Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),"Asada, Masaki  and
Miwa, Makoto  and
Sasaki, Yutaka",Enhancing Drug-Drug Interaction Extraction from Texts by Molecular Structure Information,10.18653/v1/P18-2108,P18,292
2020.ldl-1.5,"['Multilingual NLP', 'Low-resource Languages', 'Knowledge Representation and Reasoning']",,,"In recent years, there has been increasing interest in publishing lexicographic and terminological resources as linked data. The benefit of using linked data technologies to publish terminologies is that terminologies can be linked to each other, thus creating a cloud of linked terminologies that cross domains, languages and that support advanced applications that do not work with single terminologies but can exploit multiple terminologies seamlessly. We present Terme-à-LLOD TAL, a new paradigm for transforming and publishing terminologies as linked data which relies on a virtualization approach. The approach rests on a preconfigured virtual image of a server that can be downloaded and installed. We describe our approach to simplifying the transformation and hosting of terminological resources in the remainder of this paper. We provide a proof-of-concept for this paradigm showing how to apply it to the conversion of the well-known IATE terminology as well as to various smaller terminologies. Further, we discuss how the implementation of our paradigm can be integrated into existing NLP service infrastructures that rely on virtualization technology. While we apply this paradigm to the transformation and hosting of terminologies as linked data, the paradigm can be applied to any other resource format as well.",https://aclanthology.org/2020.ldl-1.5,European Language Resources Association,2020,May,Proceedings of the 7th Workshop on Linked Data in Linguistics (LDL-2020),"di Buono, Maria Pia  and
Cimiano, Philipp  and
Elahi, Mohammad Fazleh  and
Grimm, Frank",Terme-\`a-LLOD: Simplifying the Conversion and Hosting of Terminological Resources as Linked Data,,ldl,47
D19-1264,"['Evaluation Techniques', 'Learning Paradigms', 'Knowledge Representation and Reasoning', 'Model Architectures']","['Reinforcement Learning', 'Knowledge Graphs', 'Recurrent Neural Networks (RNNs)']",['Long Short-Term Memory (LSTM) Models'],"Knowledge Graph KG reasoning aims at finding reasoning paths for relations, in order to solve the problem of incompleteness in KG. Many previous path-based methods like PRA and DeepPath suffer from lacking memory components, or stuck in training. Therefore, their performances always rely on wellpretraining. In this paper, we present a deep reinforcement learning based model named by AttnPath, which incorporates LSTM and Graph Attention Mechanism as the memory components. We define two metrics, Mean Selection Rate MSR and Mean Replacement Rate MRR, to quantitatively measure how difficult it is to learn the query relations, and take advantages of them to fine-tune the model under the framework of reinforcement learning. Meanwhile, a novel mechanism of reinforcement learning is proposed by forcing an agent to walk forward every step to avoid the agent stalling at the same entity node constantly. Based on this operation, the proposed model not only can get rid of the pretraining process, but also achieves state-of-the-art performance comparing with the other models. We test our model on FB15K-237 and NELL-995 datasets with different tasks. Extensive experiments show that our model is effective and competitive with many current state-ofthe-art methods, and also performs well in practice.",https://aclanthology.org/D19-1264,Association for Computational Linguistics,2019,November,Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"Wang, Heng  and
Li, Shuangyin  and
Pan, Rong  and
Mao, Mingzhi",Incorporating Graph Attention Mechanism into Knowledge Graph Reasoning Based on Deep Reinforcement Learning,10.18653/v1/D19-1264,D19,602

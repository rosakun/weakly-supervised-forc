acl_id,Level1,Level2,Level3,abstract,url,publisher,year,month,booktitle,author,title,doi,venue,data_index
C16-1138,"['Data Management and Generation', 'Classification Applications', 'Model Architectures']","['Data Augmentation', 'Recurrent Neural Networks (RNNs)']",,"Nowadays, neural networks play an important role in the task of relation classification. By designing different neural architectures, researchers have improved the performance to a large extent in comparison with traditional methods. However, existing neural networks for relation classification are usually of shallow architectures e.g., one-layer convolutional neural networks or recurrent networks. They may fail to explore the potential representation space in different abstraction levels. In this paper, we propose deep recurrent neural networks DRNNs for relation classification to tackle this challenge. Further, we propose a data augmentation method by leveraging the directionality of relations. We evaluated our DRNNs on the SemEval-2010 Task 8, and achieve an F 1 -score of 86.1%, outperforming previous state-of-the-art recorded results. 1",https://aclanthology.org/C16-1138,The COLING 2016 Organizing Committee,2016,December,"Proceedings of {COLING} 2016, the 26th International Conference on Computational Linguistics: Technical Papers","Xu, Yan  and
Jia, Ran  and
Mou, Lili  and
Li, Ge  and
Chen, Yunchuan  and
Lu, Yangyang  and
Jin, Zhi",Improved relation classification by deep recurrent neural networks with data augmentation,,C16,692
2021.vardial-1.16,"['Learning Paradigms', 'Domain-specific NLP', 'Low-resource Languages']","['Unsupervised Learning', 'NLP for News and Media']",['NLP for Social Media'],"This paper describes the Helsinki-Ljubljana contribution to the VarDial 2021 shared task on social media variety geolocation. Following our successful participation at VarDial 2020, we again propose constrained and unconstrained systems based on the BERT architecture. In this paper, we report experiments with different tokenization settings and different pre-trained models, and we contrast our parameter-free regression approach with various classification schemes proposed by other participants at VarDial 2020. Both the code and the best-performing pre-trained models are made freely available.",https://aclanthology.org/2021.vardial-1.16,Association for Computational Linguistics,2021,April,"Proceedings of the Eighth Workshop on NLP for Similar Languages, Varieties and Dialects","Scherrer, Yves  and
Ljube{\v{s}}i{\'c}, Nikola",Social Media Variety Geolocation with geoBERT,,vardial,1000
2020.rocling-1.18,"['Information Extraction', 'Low-resource Languages']",,,"This paper aims to investigate the variation between two Chinese causative auxiliaries shi '使' and rang '讓' from a corpus-based perspective. We conduct a logistic regression analysis to the Chinese data extracted from two corpora and propose a direct/indirect distinction (Verhagen and Kemmer 1997) between the two auxiliary verbs. The results retrieved by the regression model show that the theory of direct/indirect causation provides a reasonable account for the characteristics and lexical meanings of the verbs. We indicate that the verb shi is correlated with ""direct causation"" because it is typically used when inanimate participants are involved in the causing event, in which the force initiated by the cause inevitably and directly leads to the resulted stage of the causee. On the other hand, the verb rang should be classified as ""indirect causation"" because it is typically used in scenarios where animate participants are both involved, and some extra force besides the causer also plays a role in the effected event.",https://aclanthology.org/2020.rocling-1.18,The Association for Computational Linguistics and Chinese Language Processing (ACLCLP),2020,September,Proceedings of the 32nd Conference on Computational Linguistics and Speech Processing (ROCLING 2020),"Shih, Cing-Fang  and
Ku, Mao-Chang  and
Hsieh, Shu-Kai",Lectal Variation of the Two {C}hinese Causative Auxiliaries,,rocling,1284
2020.privatenlp-1.5,"['Ethics', 'Model Architectures', 'Learning Paradigms']",,,"Language modeling is a keystone task in natural language processing. When training a language model on sensitive information, differential privacy DP allows us to quantify the degree to which our private data is protected. However, training algorithms which enforce differential privacy often lead to degradation in model quality. We study the feasibility of learning a language model which is simultaneously high-quality and privacy preserving by tuning a public base model on a private corpus. We find that DP fine-tuning boosts the performance of language models in the private domain, making the training of such models possible. 1",https://aclanthology.org/2020.privatenlp-1.5,Association for Computational Linguistics,2020,November,Proceedings of the Second Workshop on Privacy in NLP,"Kerrigan, Gavin  and
Slack, Dylan  and
Tuyls, Jens",Differentially Private Language Models Benefit from Public Pre-training,10.18653/v1/2020.privatenlp-1.5,privatenlp,868
2021.mmsr-1.5,"['Model Architectures', 'Learning Paradigms', 'Knowledge Representation and Reasoning', 'Image and Video Processing']","['Transformer Models', 'Multimodal Learning', 'Image Captioning']",,"The problem of interpretation of knowledge learned by multi-head self-attention in transformers has been one of the central questions in NLP. However, a lot of work mainly focused on models trained for uni-modal tasks, e.g. machine translation. In this paper, we examine masked self-attention in a multi-modal transformer trained for the task of image captioning. In particular, we test whether the multi-modality of the task objective affects the learned attention patterns. Our visualisations of masked self-attention demonstrate that i it can learn general linguistic knowledge of the textual input, and ii its attention patterns incorporate artefacts from visual modality even though it has never accessed it directly. We compare our transformer's attention patterns with masked attention in distilgpt-2 tested for uni-modal text generation of image captions. Based on the maps of extracted attention weights, we argue that masked self-attention in image captioning transformer seems to be enhanced with semantic knowledge from images, exemplifying joint language-and-vision information in its attention patterns.",https://aclanthology.org/2021.mmsr-1.5,Association for Computational Linguistics,2021,June,Proceedings of the 1st Workshop on Multimodal Semantic Representations (MMSR),"Ilinykh, Nikolai  and
Dobnik, Simon",How Vision Affects Language: Comparing Masked Self-Attention in Uni-Modal and Multi-Modal Transformer,,mmsr,1328
2021.mrl-1.9,"['Embeddings', 'Bilingual Lexicon Induction (BLI)', 'Learning Paradigms', 'Low-resource Languages', 'Cross-lingual Application']",['Transfer Learning'],,"In this work, we analyze the performance and properties of cross-lingual word embedding models created by mapping-based alignment methods. We use several measures of corpus and embedding similarity to predict BLI scores of cross-lingual embedding mappings over three types of corpora, three embedding methods and 55 language pairs. Our experimental results corroborate that instead of mere size, the amount of common content in the training corpora is essential. This phenomenon manifests in that i despite of the smaller corpus sizes, using only the comparable parts of Wikipedia for training the monolingual embedding spaces to be mapped is often more efficient than relying on all the contents of Wikipedia, ii the smaller, in return less diversified Spanish Wikipedia works almost always much better as a training corpus for bilingual mappings than the ubiquitously used English Wikipedia.",https://aclanthology.org/2021.mrl-1.9,Association for Computational Linguistics,2021,November,Proceedings of the 1st Workshop on Multilingual Representation Learning,"Cserh{\'a}ti, R{\'e}ka  and
Berend, G{\'a}bor",Identifying the Importance of Content Overlap for Better Cross-lingual Embedding Mappings,10.18653/v1/2021.mrl-1.9,mrl,106
2021.maiworkshop-1.7,"['Learning Paradigms', 'Model Architectures']",['Multimodal Learning'],,"Large neural networks are impractical to deploy on mobile devices due to their heavy computational cost and slow inference. Knowledge distillation KD is a technique to reduce the model size while retaining performance by transferring knowledge from a large ""teacher"" model to a smaller ""student"" model. However, KD on multimodal datasets such as vision-language datasets is relatively unexplored and digesting such multimodal information is challenging since different modalities present different types of information. In this paper, we propose modality-specific distillation MSD to effectively transfer knowledge from a teacher on multimodal datasets. Existing KD approaches can be applied to multimodal setup, but a student doesn't have access to modality-specific predictions. Our idea aims at mimicking a teacher's modalityspecific predictions by introducing an auxiliary loss term for each modality. Because each modality has different importance for predictions, we also propose weighting approaches for the auxiliary losses; a meta-learning approach to learn the optimal weights on these loss terms. In our experiments, we demonstrate the effectiveness of our MSD and the weighting scheme and show that it achieves better performance than KD.",https://aclanthology.org/2021.maiworkshop-1.7,Association for Computational Linguistics,2021,June,Proceedings of the Third Workshop on Multimodal Artificial Intelligence,"Jin, Woojeong  and
Sanjabi, Maziar  and
Nie, Shaoliang  and
Tan, Liang  and
Ren, Xiang  and
Firooz, Hamed",Modality-specific Distillation,10.18653/v1/2021.maiworkshop-1.7,maiworkshop,591
2021.metanlp-1.7,"['Audio Generation and Processing', 'Learning Paradigms']",['Automatic Speech Recognition (ASR)'],,"Speech separation is a problem in the field of speech processing that has been studied in full swing recently. However, there has not been much work studying a multi-accent speech separation scenario. Unseen speakers with new accents and noise aroused the domain mismatch problem which cannot be easily solved by conventional joint training methods. Thus, we applied MAML and FOMAML to tackle this problem and obtained higher average Si-SNRi values than joint training on almost all the unseen accents. This proved that these two methods do have the ability to generate well-trained parameters for adapting to speech mixtures of new speakers and accents. Furthermore, we found out that FOMAML obtains similar performance compared to MAML while saving a lot of time.",https://aclanthology.org/2021.metanlp-1.7,Association for Computational Linguistics,2021,August,Proceedings of the 1st Workshop on Meta Learning and Its Applications to Natural Language Processing,"Huang, Kuan Po  and
Wu, Yuan-Kuei  and
Lee, Hung-yi",Multi-accent Speech Separation with One Shot Learning,10.18653/v1/2021.metanlp-1.7,metanlp,943
S16-1172,"['Question Answering (QA)', 'Classification Applications', 'Model Architectures']",['Community QA'],,"This paper describes the KeLP system participating in the SemEval-2016 Community Question Answering cQA task. The challenge tasks are modeled as binary classification problems: kernel-based classifiers are trained on the SemEval datasets and their scores are used to sort the instances and produce the final ranking. All classifiers and kernels have been implemented within the Kernel-based Learning Platform called KeLP. Our primary submission ranked first in Subtask A, third in Subtask B and second in Subtask C. These ranks are based on MAP, which is the referring challenge system score. Our approach outperforms all the other systems with respect to all the other challenge metrics.",https://aclanthology.org/S16-1172,Association for Computational Linguistics,2016,June,Proceedings of the 10th International Workshop on Semantic Evaluation ({S}em{E}val-2016),"Filice, Simone  and
Croce, Danilo  and
Moschitti, Alessandro  and
Basili, Roberto",KeLP at SemEval-2016 Task 3: Learning Semantic Relations between Questions and Answers,10.18653/v1/S16-1172,S16,49
L18-1722,"['Multilingual NLP', 'Knowledge Representation and Reasoning']",,,"In this paper we proceed with a systematic gathering of design requirements for wordnet browsers that permit to consult the content of wordnets. This is undertaken together with a review of the functionalities of existing browsers. On the basis of this analysis, we present a new wordnet browser we developed that meets these requirements and thus complies with the most ample range of design features. This is an open source browser that is freely distributed and can be reused by anyone interested in doing research on or just using wordnets. We also introduce the notion of a pluricentric global wordnet, for whose undertaking this new advanced browser appears as an important instrument and motivation. This is a promising operative conception for a bootstrapped yet effective process towards the ultimate global wordnet, where all individual wordnets from all languages are meant to eventually converge together, in spite of the plurality of their formats, licenses, depth, etc. that is intrinsic to an inherently plural endeavor undertaken by multiple actors under multiple constraints across the world.",https://aclanthology.org/L18-1722,European Language Resources Association (ELRA),2018,May,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),"Branco, Ant{\'o}nio  and
Branco, Ruben  and
Saedi, Chakaveh  and
Silva, Jo{\~a}o","Browsing and Supporting Pluricentric Global Wordnet, or just your Wordnet of Interest",,L18,1325
2022.bionlp-1.16,"['Learning Paradigms', 'Domain-specific NLP', 'Information Extraction', 'Data Management and Generation']","['Medical and Clinical NLP', 'Data Preparation', 'Supervised Learning', 'Relation Extraction']",['Biomedical NLP'],"This paper proposes novel drug-protein relation extraction models that indirectly utilize distant supervision data. Concretely, instead of adding distant supervision data to the manually annotated training data, our models incorporate distantly supervised models that are relation extraction models trained with distant supervision data. Distantly supervised learning has been proposed to generate a large amount of pseudo-training data at low cost. However, there is still a problem of low prediction performance due to the inclusion of mislabeled data. Therefore, several methods have been proposed to suppress the effects of noisy cases by utilizing some manually annotated training data. However, their performance is lower than that of supervised learning on manually annotated data because mislabeled data that cannot be fully suppressed becomes noise when training the model. To overcome this issue, our methods indirectly utilize distant supervision data with manually annotated training data. The experimental results on the DrugProt corpus in the BioCreative VII Track 1 showed that our proposed model can consistently improve the supervised models in different settings.",https://aclanthology.org/2022.bionlp-1.16,Association for Computational Linguistics,2022,May,Proceedings of the 21st Workshop on Biomedical Language Processing,"Iinuma, Naoki  and
Miwa, Makoto  and
Sasaki, Yutaka",Improving Supervised Drug-Protein Relation Extraction with Distantly Supervised Models,10.18653/v1/2022.bionlp-1.16,bionlp,1046
2022.findings-acl.100,"['Text Generation', 'Learning Paradigms', 'Data Management and Generation', 'Model Architectures']","['Transformer Models', 'Data Preparation', 'Supervised Learning']",['Annotation Processes'],"This paper presents the first multi-objective transformer model for constructing open cloze tests that exploits generation and discrimination capabilities to improve performance. Our model is further enhanced by tweaking its loss function and applying a post-processing reranking algorithm that improves overall test structure. Experiments using automatic and human evaluation show that our approach can achieve up to 82% accuracy according to experts, outperforming previous work and baselines. We also release a collection of highquality open cloze tests along with sample system output and human annotations that can serve as a future benchmark.",https://aclanthology.org/2022.findings-acl.100,Association for Computational Linguistics,2022,May,Findings of the Association for Computational Linguistics: ACL 2022,"Felice, Mariano  and
Taslimipoor, Shiva  and
Buttery, Paula",Constructing Open Cloze Tests Using Generation and Discrimination Capabilities of Transformers,10.18653/v1/2022.findings-acl.100,findings,1353
2020.peoples-1.2,"['Data Management and Generation', 'Low-resource Languages', 'Classification Applications', 'Domain-specific NLP']","['Data Preparation', 'Personality Trait Prediction', 'NLP for News and Media']",['NLP for Social Media'],"As a contribution to personality detection in languages other than English, we rely on distant supervision to create Personal-ITY, a novel corpus of YouTube comments in Italian, where authors are labelled with personality traits. The traits are derived from one of the mainstream personality theories in psychology research, named MBTI. Using personality prediction experiments, we i study the task of personality prediction in itself on our corpus as well as on TWISTY, a Twitter dataset also annotated with MBTI labels; ii carry out an extensive, in-depth analysis of the features used by the classifier, and view them specifically under the light of the original theory that we used to create the corpus in the first place. We observe that no single model is best at personality detection, and that while some traits are easier than others to detect, and also to match back to theory, for other, less frequent traits the picture is much more blurred.",https://aclanthology.org/2020.peoples-1.2,Association for Computational Linguistics,2020,December,"Proceedings of the Third Workshop on Computational Modeling of People's Opinions, Personality, and Emotion's in Social Media","Bassignana, Elisa  and
Nissim, Malvina  and
Patti, Viviana",Matching Theory and Data with Personal-ITY: What a Corpus of Italian YouTube Comments Reveals About Personality,10.48550/arxiv.2011.07009,peoples,822
2020.vlsp-1.12,"['Learning Paradigms', 'Parsing', 'Low-resource Languages', 'Model Architectures']","['Syntactic Parsing', 'Supervised Learning']",['Dependency Parsing'],"This paper presents our approach to resolve the Vietnamese Universal Dependency Parsing task in VLSP 2020 Evaluation Campaign. On the basis of Deep Biaffine Attention for Neural Dependency ParsingDozat and Manning, 2017, we adapted the dependency parser for Vietnamese. Our best model obtained a pretty good performance on the test datasets, achieving 84.08% UAS score and 75.64% LAS on average for the ConLL-U dataset. On the raw text data-set, the results we reached still quite limited, on average 74.47% of UAS and 65.3% of LAS.",https://aclanthology.org/2020.vlsp-1.12,Association for Computational Lingustics,2020,December,Proceedings of the 7th International Workshop on Vietnamese Language and Speech Processing,"Nguyen, Lien",Implementing Bi-LSTM-based deep biaffine neural dependency parser for Vietnamese Universal Dependency Parsing,,vlsp,54
W16-2213,"['Machine Translation (MT)', 'Low-resource Languages']",['Statistical MT (SMT)'],,"We study the relationship between word order freedom and preordering in statistical machine translation. To assess word order freedom, we first introduce a novel entropy measure which quantifies how difficult it is to predict word order given a source sentence and its syntactic analysis. We then address preordering for two target languages at the far ends of the word order freedom spectrum, German and Japanese, and argue that for languages with more word order freedom, attempting to predict a unique word order given source clues only is less justified. Subsequently, we examine lattices of n-best word order predictions as a unified representation for languages from across this broad spectrum and present an effective solution to a resulting technical issue, namely how to select a suitable source word order from the lattice during training. Our experiments show that lattices are crucial for good empirical performance for languages with freer word order English-German and can provide additional improvements for fixed word order languages English-Japanese.",https://aclanthology.org/W16-2213,Association for Computational Linguistics,2016,August,"Proceedings of the First Conference on Machine Translation: Volume 1, Research Papers","Daiber, Joachim  and
Stanojevi{\'c}, Milo{\v{s}}  and
Aziz, Wilker  and
Sima{'}an, Khalil",Examining the Relationship between Preordering and Word Order Freedom in Machine Translation,10.18653/v1/W16-2213,W16,1132
R19-1038,"['Learning Paradigms', 'Automatic Text Summarization']","['Unsupervised Learning', 'Extractive Text Summarization']",,"Given the peculiar structure of songs, applying generic text summarization methods to lyrics can lead to the generation of highly redundant and incoherent text. In this paper, we propose to enhance state-of-the-art text summarization approaches with a method inspired by audio thumbnailing. Instead of searching for the thumbnail clues in the audio of the song, we identify equivalent clues in the lyrics. We then show how these summaries that take into account the audio nature of the lyrics outperform the generic methods according to both an automatic evaluation and human judgments.",https://aclanthology.org/R19-1038,INCOMA Ltd.,2019,September,Proceedings of the International Conference on Recent Advances in Natural Language Processing (RANLP 2019),"Fell, Michael  and
Cabrio, Elena  and
Gandon, Fabien  and
Giboin, Alain",Song Lyrics Summarization Inspired by Audio Thumbnailing,10.26615/978-954-452-056-4_038,R19,1168
P17-1063,"['Error Detection and Correction', 'Dialogue Systems']",['Response Generation'],,"The referring expressions REs produced by a natural language generation NLG system can be misunderstood by the hearer, even when they are semantically correct. In an interactive setting, the NLG system can try to recognize such misunderstandings and correct them. We present an algorithm for generating corrective REs that use contrastive focus ""no, the BLUE button"" to emphasize the information the hearer most likely misunderstood. We show empirically that these contrastive REs are preferred over REs without contrast marking.",https://aclanthology.org/P17-1063,Association for Computational Linguistics,2017,July,Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),"Villalba, Mart{\'\i}n  and
Teichmann, Christoph  and
Koller, Alexander",Generating Contrastive Referring Expressions,10.18653/v1/P17-1063,P17,711
2020.nl4xai-1.13,['Learning Paradigms'],['Reinforcement Learning'],,"We discuss the relationship between explainability and knowledge transfer in reinforcement learning. We argue that explainability methods, in particular methods that use counterfactuals, might help increasing sample efficiency. For this, we present a computational approach to optimize the learner's performance using explanations of another agent and discuss our results in light of effective natural language explanations for both agents and humans.",https://aclanthology.org/2020.nl4xai-1.13,Association for Computational Linguistics,2020,November,2nd Workshop on Interactive Natural Language Technology for Explainable Artificial Intelligence,"Tulli, Silvia  and
Wallk{\""o}tter, Sebastian  and
Paiva, Ana  and
Melo, Francisco S.  and
Chetouani, Mohamed",Learning from Explanations and Demonstrations: A Pilot Study,,nl4xai,1486
2021.nlp4convai-1.2,"['Learning Paradigms', 'Data Management and Generation', 'Classification Applications']","['Few-shot Learning', 'Intent Detection', 'Data Augmentation']",,"Zhang et al. 2020 proposed to formulate fewshot intent classification as natural language inference NLI between query utterances and examples in the training set. The method is known as discriminative nearest neighbor classification or DNNC. Inspired by this work, we propose to simplify the NLI-style classification pipeline to be the entailment prediction on the utterance-semantic-label-pair USLP. The semantic information in the labels can thus been infused into the classification process. Compared with DNNC, our proposed method is more efficient in both training and serving since it is based upon the entailment between query utterance and labels instead of all the training examples. The DNNC method requires more than one example per intent while the USLP approach does not have such constraint. In the 1-shot experiments on the CLINC150 Larson et al., 2019 dataset, the USLP method outperforms traditional classification approach by >20 points in-domain accuracy. We also find that longer and semantically meaningful labels tend to benefit model performance, however, the benefit shrinks as more training data is available.",https://aclanthology.org/2021.nlp4convai-1.2,Association for Computational Linguistics,2021,November,Proceedings of the 3rd Workshop on Natural Language Processing for Conversational AI,"Qu, Jin  and
Hashimoto, Kazuma  and
Liu, Wenhao  and
Xiong, Caiming  and
Zhou, Yingbo",Few-Shot Intent Classification by Gauging Entailment Relationship Between Utterance and Semantic Label,10.18653/v1/2021.nlp4convai-1.2,nlp4convai,1448
2021.nlp4if-1.18,"['Domain-specific NLP', 'Classification Applications']","['Misinformation Detection', 'Medical and Clinical NLP', 'NLP for News and Media']",['NLP for Social Media'],"This paper describes the TOKOFOU system, an ensemble model for misinformation detection tasks based on six different transformer-based pre-trained encoders, implemented in the context of the COVID-19 Infodemic Shared Task for English. We fine tune each model on each of the task's questions and aggregate their prediction scores using a majority voting approach. TOKOFOU obtains an overall F1 score of 89.7%, ranking first.",https://aclanthology.org/2021.nlp4if-1.18,Association for Computational Linguistics,2021,June,"Proceedings of the Fourth Workshop on NLP for Internet Freedom: Censorship, Disinformation, and Propaganda","Tziafas, Georgios  and
Kogkalidis, Konstantinos  and
Caselli, Tommaso",Fighting the COVID-19 Infodemic with a Holistic BERT Ensemble,10.18653/v1/2021.nlp4if-1.18,nlp4if,287
2020.cmlc-1.4,"['Information Extraction', 'Domain-specific NLP', 'Data Management and Generation']","['Named Entity Recognition (NER)', 'Data Analysis']",,"This paper describes work in progress on devising automatic and parallel methods for geoparsing large digital historical textual data by combining the strengths of three natural language processing NLP tools, the Edinburgh Geoparser, spaCy and defoe, and employing different tokenisation and named entity recognition NER techniques. We apply these tools to a large collection of nineteenth century Scottish geographical dictionaries, and describe preliminary results obtained when processing this data.",https://aclanthology.org/2020.cmlc-1.4,European Language Ressources Association,2020,May,Proceedings of the 8th Workshop on Challenges in the Management of Large Corpora,"Filgueira, Rosa  and
Grover, Claire  and
Terras, Melissa  and
Alex, Beatrice",Geoparsing the historical Gazetteers of Scotland: accurately computing location in mass digitised texts,,cmlc,633
2022.slpat-1.2,"['Information Extraction', 'Domain-specific NLP', 'Error Detection and Correction']",['Medical and Clinical NLP'],,"Many people with severely limited muscle control can only communicate through augmentative and alternative communication AAC systems with a small number of buttons. In this paper, we present the design for Color-Code, which is an AAC system with two buttons that uses Bayesian inference to determine what the user wishes to communicate. Our information-theoretic analysis of ColorCode simulations shows that it is efficient in extracting information from the user, even in the presence of errors, achieving nearly optimal error correction. ColorCode is provided as open source software https://github.com/ mrdaly/ColorCode.",https://aclanthology.org/2022.slpat-1.2,Association for Computational Linguistics,2022,May,Ninth Workshop on Speech and Language Processing for Assistive Technologies (SLPAT-2022),"Daly, Matthew",ColorCode: A Bayesian Approach to Augmentative and Alternative Communication with Two Buttons,10.18653/v1/2022.slpat-1.2,slpat,704
2020.trac-1.23,"['Ethics', 'Domain-specific NLP', 'Classification Applications', 'Data Management and Generation']","['Data Preparation', 'Hate and Offensive Speech Detection', 'NLP for News and Media']",['NLP for Social Media'],"Nowadays, the amount of users' activities on online social media is growing dramatically. These online environments provide excellent opportunities for communication and knowledge sharing. However, some people misuse them to harass and bully others online, a phenomenon called cyberbullying. Due to its harmful effects on people, especially youth, it is imperative to detect cyberbullying as early as possible before it causes irreparable damages to victims. Most of the relevant available resources are not explicitly designed to detect cyberbullying, but related content, such as hate speech and abusive language. In this paper, we propose a new approach to create a corpus suited for cyberbullying detection. We also investigate the possibility of designing a framework to monitor the streams of users' online messages and detects the signs of cyberbullying as early as possible.",https://aclanthology.org/2020.trac-1.23,European Language Resources Association (ELRA),2020,May,"Proceedings of the Second Workshop on Trolling, Aggression and Cyberbullying","Safi Samghabadi, Niloofar  and
L{\'o}pez Monroy, Adri{\'a}n Pastor  and
Solorio, Thamar",Detecting Early Signs of Cyberbullying in Social Media,,trac,423
2022.dadc-1.2,"['Ethics', 'Classification Applications']",['Hate and Offensive Speech Detection'],,"Digital harms can manifest across any interface. Key problems in addressing these harms include the high individuality of harms and the fast-changing nature of digital systems. We put forth GreaseVision, a collaborative human-inthe-loop learning framework that enables endusers to analyze their screenomes to annotate harms as well as render overlay interventions. We evaluate HITL intervention development with a set of completed tasks in a cognitive walkthrough, and test scalability with one-shot element removal and fine-tuning hate speech classification models. The contribution of the framework and tool allow individual end-users to study their usage history and create personalized interventions. Our contribution also enables researchers to study the distribution of multi-modal harms and interventions at scale.",https://aclanthology.org/2022.dadc-1.2,Association for Computational Linguistics,2022,July,Proceedings of the First Workshop on Dynamic Adversarial Data Collection,"Datta, Siddhartha  and
Kollnig, Konrad  and
Shadbolt, Nigel",GreaseVision: Rewriting the Rules of the Interface,10.18653/v1/2022.dadc-1.2,dadc,1194
2021.acl-long.365,"['Learning Paradigms', 'Knowledge Representation and Reasoning', 'Model Architectures']","['Reinforcement Learning', 'Knowledge Graphs', 'Graph Neural Networks (GNNs)']",,"Temporal Knowledge Graphs TKGs have been developed and used in many different areas. Reasoning on TKGs that predicts potential facts events in the future brings great challenges to existing models. When facing a prediction task, human beings usually search useful historical information i.e., clues in their memories and then reason for future meticulously. Inspired by this mechanism, we propose CluSTeR to predict future facts in a two-stage manner, Clue Searching and Temporal Reasoning, accordingly. Specifically, at the clue searching stage, CluSTeR learns a beam search policy via reinforcement learning RL to induce multiple clues from historical facts. At the temporal reasoning stage, it adopts a graph convolution network based sequence method to deduce answers from clues. Experiments on four datasets demonstrate the substantial advantages of CluSTeR compared with the state-of-the-art methods. Moreover, the clues found by CluSTeR further provide interpretability for the results.",https://aclanthology.org/2021.acl-long.365,Association for Computational Linguistics,2021,August,Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),"Li, Zixuan  and
Jin, Xiaolong  and
Guan, Saiping  and
Li, Wei  and
Guo, Jiafeng  and
Wang, Yuanzhuo  and
Cheng, Xueqi",Search from History and Reason for Future: Two-stage Reasoning on Temporal Knowledge Graphs,10.18653/v1/2021.acl-long.365,acl,975
2020.vardial-1.6,"['Embeddings', 'Bilingual Lexicon Induction (BLI)', 'Knowledge Representation and Reasoning', 'Learning Paradigms', 'Low-resource Languages']","['Transfer Learning', 'Word Embeddings']",,"Bilingual lexicons are a vital tool for under-resourced languages and recent state-of-the-art approaches to this leverage pretrained monolingual word embeddings using supervised or semisupervised approaches. However, these approaches require cross-lingual information such as seed dictionaries to train the model and find a linear transformation between the word embedding spaces. Especially in the case of low-resourced languages, seed dictionaries are not readily available, and as such, these methods produce extremely weak results on these languages. In this work, we focus on the Dravidian languages, namely Tamil, Telugu, Kannada, and Malayalam, which are even more challenging as they are written in unique scripts. To take advantage of orthographic information and cognates in these languages, we bring the related languages into a single script. Previous approaches have used linguistically sub-optimal measures such as the Levenshtein edit distance to detect cognates, whereby we demonstrate that the longest common sub-sequence is linguistically more sound and improves the performance of bilingual lexicon induction. We show that our approach can increase the accuracy of bilingual lexicon induction methods on these languages many times, making bilingual lexicon induction approaches feasible for such under-resourced languages.",https://aclanthology.org/2020.vardial-1.6,International Committee on Computational Linguistics (ICCL),2020,December,"Proceedings of the 7th Workshop on NLP for Similar Languages, Varieties and Dialects","Chakravarthi, Bharathi Raja  and
Rajasekaran, Navaneethan  and
Arcan, Mihael  and
McGuinness, Kevin  and
E. O{'}Connor, Noel  and
McCrae, John P.",Bilingual Lexicon Induction across Orthographically-distinct Under-Resourced Dravidian Languages,,vardial,1200
2019.icon-1.21,"['Data Management and Generation', 'Low-resource Languages']",['Data Analysis'],,"This paper proposes a metric to quantify lexical complexity in Malayalam. The metric utilizes word frequency, orthography and morphology as the three factors affecting visual word recognition in Malayalam. Malayalam differs from other Indian languages due to its agglutinative morphology and orthography, which are incorporated into our model. The predictions made by our model are then evaluated against reaction times in a lexical decision task. We find that reaction times are predicted by frequency, morphological complexity and script complexity. We also explore the interactions between morphological complexity with frequency and script in our results. To the best of our knowledge, this is the first study on lexical complexity in Malayalam.",https://aclanthology.org/2019.icon-1.21,NLP Association of India,2019,December,Proceedings of the 16th International Conference on Natural Language Processing,"Shallam, Richard  and
Vaidya, Ashwini",Towards measuring lexical complexity in Malayalam,,icon,341
Y18-1082,['Data Management and Generation'],['Data Analysis'],,"This paper re-examines the debatable issue about adverbial clause A,CL whether it indexes a complex discourse of contemporary English and how it distributes across Speech S and Writing W with different subtypes of structures by investigating the internal structure variations. A Finite-State-Machine model is adopted for retrieving the internal structures. Empirical results show that A,CL prevails in W than in S with a higher occurrence rate W: 31.20% vs. S: 14.80%, which confirms its function of indexing a complex discourse; but the standard token-type-ratio of its internal structures shows an opposite distribution W: 12.89% vs. S: 16.66%, which suggests a higher structural density/variation of the spoken mode; besides, five subtypes of internal structures are identified with various distributions across S and W: S employs a higher proportion of subordinator-overt subordinating A,CL, while W adopts more infinitive A,CL, including to-infinitives, present and past participles; coordinated embeddings and subordinator-covert finite A,CL are commonly found in both modes. Despite of the individual variance of internal subtypes, statistical test indicates a less noticed fact that the overall structural variation of A,CL between S and W is not significant p-value = 0.5245.",https://aclanthology.org/Y18-1082,Association for Computational Linguistics,2018,1{--}3 December,"Proceedings of the 32nd Pacific Asia Conference on Language, Information and Computation","Wan, Mingyu  and
Fang, Alex Chengyu",A Re-examination of Syntactic Complexity by Investigating the Internal Structure Variations of Adverbial Clauses across Speech and Writing,,Y18,1023
2021.codi-main.15,"['Text Preprocessing', 'Parsing', 'Multilingual NLP', 'Data Management and Generation', 'Low-resource Languages', 'Model Architectures']","['Transformer Models', 'Data Augmentation', 'Discourse Parsing', 'Text Segmentation']",,"Text discourse parsing weighs importantly in understanding information flow and argumentative structure in natural language, making it beneficial for downstream tasks. While previous work significantly improves the performance of RST discourse parsing, they are not readily applicable to practical use cases: 1 EDU segmentation is not integrated into most existing tree parsing frameworks, thus it is not straightforward to apply such models on newly-coming data. 2 Most parsers cannot be used in multilingual scenarios, because they are developed only in English. 3 Parsers trained from single-domain treebanks do not generalize well on out-of-domain inputs. In this work, we propose a document-level multilingual RST discourse parsing framework, which conducts EDU segmentation and discourse tree parsing jointly. Moreover, we propose a cross-translation augmentation strategy to enable the framework to support multilingual parsing and improve its domain generality. Experimental results show that our model achieves state-of-the-art performance on document-level multilingual RST parsing in all sub-tasks.",https://aclanthology.org/2021.codi-main.15,Association for Computational Linguistics,2021,November,Proceedings of the 2nd Workshop on Computational Approaches to Discourse,"Liu, Zhengyuan  and
Shi, Ke  and
Chen, Nancy",DMRST: A Joint Framework for Document-Level Multilingual RST Discourse Segmentation and Parsing,10.18653/v1/2021.codi-main.15,codi,1229
Q16-1034,['Model Architectures'],,,"Efficient methods for storing and querying are critical for scaling high-order m-gram language models to large corpora. We propose a language model based on compressed suffix trees, a representation that is highly compact and can be easily held in memory, while supporting queries needed in computing language model probabilities on-the-fly. We present several optimisations which improve query runtimes up to 2500×, despite only incurring a modest increase in construction time and memory usage. For large corpora and high Markov orders, our method is highly competitive with the state-of-the-art KenLM package. It imposes much lower memory requirements, often by orders of magnitude, and has runtimes that are either similar for training or comparable for querying.",https://aclanthology.org/Q16-1034,MIT Press,2016,,,"Shareghi, Ehsan  and
Petri, Matthias  and
Haffari, Gholamreza  and
Cohn, Trevor","Fast, Small and Exact: Infinite-order Language Modelling with Compressed Suffix Trees",10.1162/tacl_a_00112,Q16,916
2021.disrpt-1.4,"['Multilingual NLP', 'Parsing', 'Low-resource Languages', 'Classification Applications']",['Discourse Parsing'],,"This paper describes our participating system for the Shared Task on Discourse Segmentation and Connective Identification across Formalisms and Languages. Key features of the presented approach are the formulation as a clause-level classification task, a languageindependent feature inventory based on Universal Dependencies grammar, and compositeverb-form analysis. The achieved F1 is 92% for German and English and lower for other languages. The paper also presents a clauselevel tagger for grammatical tense, aspect, mood, voice and modality in 11 languages.",https://aclanthology.org/2021.disrpt-1.4,Association for Computational Linguistics,2021,November,Proceedings of the 2nd Shared Task on Discourse Relation Parsing and Treebanking (DISRPT 2021),"D{\""o}nicke, Tillmann","Delexicalised Multilingual Discourse Segmentation for DISRPT 2021 and Tense, Mood, Voice and Modality Tagging for 11 Languages",10.18653/v1/2021.disrpt-1.4,disrpt,840
2017.mtsummit-papers.3,"['Model Architectures', 'Low-resource Languages', 'Machine Translation (MT)']",,,"Hybrid machine translation (HMT) takes advantage of different types of machine translation (MT) systems to improve translation performance. Neural machine translation (NMT) can produce more fluent translations while phrase-based statistical machine translation (PB-SMT) can produce adequate results primarily due to the contribution of the translation model. In this paper, we propose a cascaded hybrid framework to combine NMT and PB-SMT to improve translation quality. Specifically, we first use the trained NMT system to pre-translate the training data, and then employ the pre-translated training data to build an SMT system and tune parameters using the pre-translated development set. Finally, the SMT system is utilised as a post-processing step to re-decode the pre-translated test set and produce the final result. Experiments conducted on Japanese→English and Chinese→English show that the proposed cascaded hybrid framework can significantly improve performance by 2.38 BLEU points and 4.22 BLEU points, respectively, compared to the baseline NMT system.",https://aclanthology.org/2017.mtsummit-papers.3,,2017,September 18 {--} September 22,Proceedings of Machine Translation Summit XVI: Research Track,"Du, Jinhua  and
Way, Andy",Neural Pre-Translation for Hybrid Machine Translation,,mtsummit,1480
2020.aacl-main.90,"['Data Management and Generation', 'Question Answering (QA)', 'Low-resource Languages', 'Learning Paradigms', 'Cross-lingual Application', 'Multilingual NLP', 'Model Architectures']","['Visual QA (VQA)', 'Data Preparation', 'Multimodal Learning']",,"In this paper, we propose an effective deep learning framework for multilingual and codemixed visual question answering. The proposed model is capable of predicting answers from the questions in Hindi, English or Codemixed Hinglish: Hindi-English languages. The majority of the existing techniques on Visual Question Answering VQA focus on English questions only. However, many applications such as medical imaging, tourism, visual assistants require a multilinguality-enabled module for their widespread usages. As there is no available dataset in English-Hindi VQA, we firstly create Hindi and Code-mixed VQA datasets by exploiting the linguistic properties of these languages. We propose a robust technique capable of handling the multilingual and code-mixed question to provide the answer against the visual information image. To better encode the multilingual and code-mixed questions, we introduce a hierarchy of shared layers. We control the behaviour of these shared layers by an attention-based soft layer sharing mechanism, which learns how shared layers are applied in different ways for the different languages of the question. Further, our model uses bi-linear attention with a residual connection to fuse the language and image features. We perform extensive evaluation and ablation studies for English, Hindi and Codemixed VQA. The evaluation shows that the proposed multilingual model achieves state-ofthe-art performance in all these settings.",https://aclanthology.org/2020.aacl-main.90,Association for Computational Linguistics,2020,December,Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing,"Gupta, Deepak  and
Lenka, Pabitra  and
Ekbal, Asif  and
Bhattacharyya, Pushpak",A Unified Framework for Multilingual and Code-Mixed Visual Question Answering,10.18653/v1/2020.aacl-main.90,aacl,1078
2020.challengehml-1.4,"['Model Architectures', 'Classification Applications', 'Learning Paradigms']","['Multimodal Learning', 'Emotion Detection', 'Transformer Models', 'Sentiment Analysis (SA)']",,"Our senses individually work in a coordinated fashion to express our emotional intentions. In this work, we experiment with modeling modality-specific sensory signals to attend to our latent multimodal emotional intentions and vice versa expressed via lowrank multimodal fusion and multimodal transformers. The low-rank factorization of multimodal fusion amongst the modalities helps represent approximate multiplicative latent signal interactions. Motivated by the work of Tsai  et al., 2019  and Liu et al., 2018 , we present our transformer-based cross-fusion architecture without any over-parameterization of the model. The low-rank fusion helps represent the latent signal interactions while the modality-specific attention helps focus on relevant parts of the signal. We present two methods for the Multimodal Sentiment and Emotion Recognition results on CMU-MOSEI, CMU-MOSI, and IEMOCAP datasets and show that our models have lesser parameters, train faster and perform comparably to many larger fusion-based architectures.",https://aclanthology.org/2020.challengehml-1.4,Association for Computational Linguistics,2020,July,Second Grand-Challenge and Workshop on Multimodal Language (Challenge-HML),"Sahay, Saurav  and
Okur, Eda  and
H Kumar, Shachi  and
Nachman, Lama",Low Rank Fusion based Transformers for Multimodal Sequences,10.18653/v1/2020.challengehml-1.4,challengehml,174
2018.iwslt-1.2,"['Embeddings', 'Multilingual NLP', 'Low-resource Languages', 'Learning Paradigms']","['Unsupervised Learning', 'Word Embeddings']",,"Mining parallel sentences from comparable corpora is of great interest for many downstream tasks. In the BUCC 2017 shared task, systems performed well by training on gold standard parallel sentences. However, we often want to mine parallel sentences without bilingual supervision. We present a simple approach relying on bilingual word embeddings trained in an unsupervised fashion. We incorporate orthographic similarity in order to handle words with similar surface forms. In addition, we propose a dynamic threshold method to decide if a candidate sentence-pair is parallel which eliminates the need to fine tune a static value for different datasets. Since we do not employ any language specific engineering our approach is highly generic. We show that our approach is effective, on three language-pairs, without the use of any bilingual signal which is important because parallel sentence mining is most useful in low resource scenarios.",https://aclanthology.org/2018.iwslt-1.2,International Conference on Spoken Language Translation,2018,October 29-30,Proceedings of the 15th International Conference on Spoken Language Translation,"Hangya, Viktor  and
Braune, Fabienne  and
Kalasouskaya, Yuliya  and
Fraser, Alexander",Unsupervised Parallel Sentence Extraction from Comparable Corpora,,iwslt,1144
2020.peoples-1.4,"['Domain-specific NLP', 'Classification Applications', 'Discourse Analysis']",['NLP for News and Media'],,"News editorials aim to shape the opinions of their readership and the general public on timely controversial issues. The impact of an editorial on the reader's opinion does not only depend on its content and style, but also on the reader's profile. Previous work has studied the effect of editorial style depending on general political ideologies liberals vs. conservatives. In our work, we dig deeper into the persuasiveness of both content and style, exploring the role of the intensity of an ideology lean vs. extreme and the reader's personality traits agreeableness, conscientiousness, extraversion, neuroticism, and openness. Concretely, we train content-and style-based models on New York Times editorials for different ideology-and personality-specific groups. Our results suggest that particularly readers with extreme ideology and non ""role model"" personalities are impacted by style. We further analyze the importance of various text features with respect to the editorials' impact, the readers' profile, and the editorials' geographical scope.",https://aclanthology.org/2020.peoples-1.4,Association for Computational Linguistics,2020,December,"Proceedings of the Third Workshop on Computational Modeling of People's Opinions, Personality, and Emotion's in Social Media","El Baff, Roxanne  and
Al Khatib, Khalid  and
Stein, Benno  and
Wachsmuth, Henning",Persuasiveness of News Editorials depending on Ideology and Personality,,peoples,646
2020.nuse-1.4,"['Domain-specific NLP', 'Classification Applications', 'Knowledge Representation and Reasoning']",['Emotion Detection'],,"Identifying emotions as expressed in text a.k.a. text emotion recognition has received a lot of attention over the past decade. Narratives often involve a great deal of emotional expression, and so emotion recognition on narrative text is of great interest to computational approaches to narrative understanding. Prior work by Kim et al. 2010 was the work with the highest reported emotion detection performance, on a corpus of fairy tales texts. Close inspection of that work, however, revealed significant reproducibility problems, and we were unable to reimplement Kim's approach as described. As a consequence, we implemented a framework inspired by Kim's approach, where we carefully evaluated the major design choices. We identify the highestperforming combination, which outperforms Kim's reported performance by 7.6 F 1 points on average. Close inspection of the annotated data revealed numerous missing and incorrect emotion terms in the relevant lexicon, Word-NetAffect WNA; Strapparava and Valitutti, 2004 , which allowed us to augment it in a useful way. More generally, this showed that numerous clearly emotive words and phrases are missing from WNA, which suggests that effort invested in augmenting or refining emotion ontologies could be useful for improving the performance of emotion recognition systems. We release our code and data to definitely enable future reproducibility of this work.",https://aclanthology.org/2020.nuse-1.4,Association for Computational Linguistics,2020,July,"Proceedings of the First Joint Workshop on Narrative Understanding, Storylines, and Events","Zad, Samira  and
Finlayson, Mark",Systematic Evaluation of a Framework for Unsupervised Emotion Recognition for Narrative Text,10.18653/v1/2020.nuse-1.4,nuse,1134
2020.mmw-1.4,"['Knowledge Representation and Reasoning', 'Low-resource Languages']",,,"Wordnets are lexical databases where the semantic relations of words and concepts are established. These resources are useful for many NLP tasks, such as automatic text classification, word-sense disambiguation or machine translation. In comparison with other wordnets, the Basque version is smaller and some PoS are underrepresented or missing e.g. adjectives and adverbs. In this work, we explore a novel approach to enrich the Basque WordNet, focusing on the adjectives. We want to prove the use and and effectiveness of sentiment lexicons to enrich the resource without the need of starting from scratch. Using as complementary resources, one dictionary and the sentiment valences of the words, we check if the word of the lexicon matches with the meaning of the synset, and if it matches we add the word as variant to the Basque WordNet. Following this methodology, we describe the most frequent adjectives with positive and negative valence, the matches and the possible solutions for the non-matches.",https://aclanthology.org/2020.mmw-1.4,The European Language Resources Association (ELRA),2020,May,Proceedings of the LREC 2020 Workshop on Multimodal Wordnets (MMW2020),"Gonzalez-Dios, Itziar  and
Alkorta, Jon",Exploring the Enrichment of Basque WordNet with a Sentiment Lexicon,,mmw,1032
R19-1075,"['Embeddings', 'Knowledge Representation and Reasoning']",['Word Embeddings'],,"Language is used to describe concepts, and many of these concepts are hierarchical. Moreover, this hierarchy should be compatible with forming phrases and sentences. We use linear-algebraic methods that allow us to encode words as collections of vectors. The representations we use have an ordering, related to subspace inclusion, which we interpret as modelling hierarchical information. The word representations built can be understood within a compositional distributional semantic framework, providing methods for composing words to form phrase and sentence level representations. The resulting representations give competitive results on simple sentencelevel entailment datasets.",https://aclanthology.org/R19-1075,INCOMA Ltd.,2019,September,Proceedings of the International Conference on Recent Advances in Natural Language Processing (RANLP 2019),"Lewis, Martha",Compositional Hyponymy with Positive Operators,10.26615/978-954-452-056-4_075,R19,1455
N19-1145,"['Domain-specific NLP', 'Information Extraction', 'Knowledge Representation and Reasoning', 'Model Architectures']","['Event Extraction', 'Recurrent Neural Networks (RNNs)', 'Medical and Clinical NLP']","['Biomedical NLP', 'Long Short-Term Memory (LSTM) Models']","Event extraction for the biomedical domain is more challenging than that in the general news domain since it requires broader acquisition of domain-specific knowledge and deeper understanding of complex contexts. To better encode contextual information and external background knowledge, we propose a novel knowledge base KB-driven treestructured long short-term memory networks Tree-LSTM framework, incorporating two new types of features: 1 dependency structures to capture wide contexts; 2 entity properties types and category descriptions from external ontologies via entity linking. We evaluate our approach on the BioNLP shared task with Genia dataset and achieve a new stateof-the-art result. In addition, both quantitative and qualitative studies demonstrate the advancement of the Tree-LSTM and the external knowledge representation for biomedical event extraction.",https://aclanthology.org/N19-1145,Association for Computational Linguistics,2019,June,"Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)","Li, Diya  and
Huang, Lifu  and
Ji, Heng  and
Han, Jiawei",Biomedical Event Extraction based on Knowledge-driven Tree-LSTM,10.18653/v1/N19-1145,N19,1136
2020.lt4hala-1.15,"['Data Management and Generation', 'Multilingual NLP', 'Classification Applications', 'Low-resource Languages']",['Data Preparation'],,"Fictional prose can be broadly divided into narrative and discursive forms with direct speech being central to any discourse representation alongside indirect reported speech and free indirect discourse. This distinction is crucial in digital literary studies and enables interesting forms of narratological or stylistic analysis. The difficulty of automatically detecting direct speech, however, is currently under-estimated. Rule-based systems that work reasonably well for modern languages struggle with the lack of typographical conventions in 19th-century literature. While machine learning approaches to sequence modeling can be applied to solve the task, they typically face a severed skewness in the availability of training material, especially for lesser resourced languages. In this paper, we report the result of a multilingual approach to direct speech detection in a diverse corpus of 19th-century fiction in 9 European languages. The proposed method fine-tunes a transformer architecture with multilingual sentence embedder on a minimal amount of annotated training in each language, and improves performance across languages with ambiguous direct speech marking, in comparison to a carefully constructed regular expression baseline.",https://aclanthology.org/2020.lt4hala-1.15,European Language Resources Association (ELRA),2020,May,Proceedings of LT4HALA 2020 - 1st Workshop on Language Technologies for Historical and Ancient Languages,"Byszuk, Joanna  and
Wo{\'z}niak, Micha{\l}  and
Kestemont, Mike  and
Le{\'s}niak, Albert  and
{\L}ukasik, Wojciech  and
{\v{S}}e{\c{l}}a, Artjoms  and
Eder, Maciej",Detecting Direct Speech in Multilingual Collection of 19th-century Novels,,lt4hala,498
2021.sigdial-1.31,"['Automatic Text Summarization', 'Discourse Analysis', 'Domain-specific NLP', 'Data Management and Generation', 'Model Architectures']","['Transformer Models', 'Extractive Text Summarization', 'Data Preparation', 'Medical and Clinical NLP']",['NLP for Mental Health'],"Regular physical activity is associated with a reduced risk of chronic diseases such as type 2 diabetes and improved mental well-being. Yet, more than half of the US population is insufficiently active. Health coaching has been successful in promoting healthy behaviors. In this paper, we present our work towards assisting health coaches by extracting the physical activity goal the user and coach negotiate via text messages. We show that information captured by dialogue acts can help to improve the goal extraction results. We employ both traditional and transformer-based machine learning models for dialogue acts prediction and find them statistically indistinguishable in performance on our health coaching dataset. Moreover, we discuss the feedback provided by the health coaches when evaluating the correctness of the extracted goal summaries. This work is a step towards building a virtual assistant health coach to promote a healthy lifestyle.",https://aclanthology.org/2021.sigdial-1.31,Association for Computational Linguistics,2021,July,Proceedings of the 22nd Annual Meeting of the Special Interest Group on Discourse and Dialogue,"Gupta, Itika  and
Di Eugenio, Barbara  and
Ziebart, Brian D.  and
Liu, Bing  and
Gerber, Ben S.  and
Sharp, Lisa K.",Summarizing Behavioral Change Goals from SMS Exchanges to Support Health Coaches,10.18653/v1/2021.sigdial-1.31,sigdial,145
2020.sigdial-1.26,"['Domain-specific NLP', 'Data Management and Generation', 'Dialogue Systems', 'Discourse Analysis']",['Data Analysis'],,"The present study aims to examine the prevalent notion that people entrain to the vocabulary of a dialogue system. Although previous research shows that people will replace their choice of words with simple substitutes, studies using more challenging substitutions are sparse. In this paper, we investigate whether people adapt their speech to the vocabulary of a dialogue system when the system's suggested words are not direct synonyms. 32 participants played a geographythemed game with a remote-controlled agent and were primed by referencing strategies rather than individual terms introduced in follow-up questions. Our results suggest that context-appropriate substitutes support convergence and that the convergence has a lasting effect within a dialogue session if the system's wording is more consistent with the norms of the domain than the original wording of the speaker.",https://aclanthology.org/2020.sigdial-1.26,Association for Computational Linguistics,2020,July,Proceedings of the 21th Annual Meeting of the Special Interest Group on Discourse and Dialogue,"Bergqvist, Amanda  and
Manuvinakurike, Ramesh  and
Karkada, Deepthi  and
Paetzel, Maike",Nontrivial Lexical Convergence in a Geography-Themed Game,10.18653/v1/2020.sigdial-1.26,sigdial,1258
2020.argmining-1.13,"['Argument Mining', 'Data Management and Generation']",['Data Preparation'],['Annotation Processes'],"Computational models of argument quality AQ have focused primarily on assessing the overall quality or just one specific characteristic of an argument, such as its convincingness or its clarity. However, previous work has claimed that assessment based on theoretical dimensions of argumentation could benefit writers, but developing such models has been limited by the lack of annotated data. In this work, we describe GAQCorpus, the first large, domain-diverse annotated corpus of theory-based AQ. We discuss how we designed the annotation task to reliably collect a large number of judgments with crowdsourcing, formulating theory-based guidelines that helped make subjective judgments of AQ more objective. We demonstrate how to identify arguments and adapt the annotation task for three diverse domains. Our work will inform research on theory-based argumentation annotation and enable the creation of more diverse corpora to support computational AQ assessment.",https://aclanthology.org/2020.argmining-1.13,Association for Computational Linguistics,2020,December,Proceedings of the 7th Workshop on Argument Mining,"Ng, Lily  and
Lauscher, Anne  and
Tetreault, Joel  and
Napoles, Courtney",Creating a Domain-diverse Corpus for Theory-based Argument Quality Assessment,10.48550/arxiv.2011.01589,argmining,992
2022.gebnlp-1.20,"['Biases in NLP', 'Evaluation Techniques', 'Data Management and Generation', 'Classification Applications', 'Model Architectures']","['Gender Bias', 'Data Preparation', 'Transformer Models', 'Sentiment Analysis (SA)', 'Data Analysis']",,"Pretrained language models are publicly available and constantly finetuned for various reallife applications. As they become capable of grasping complex contextual information, harmful biases are likely increasingly intertwined with those models. This paper analyses gender bias in BERT models with two main contributions: First, a novel bias measure is introduced, defining biases as the difference in sentiment valuation of female and male sample versions. Second, we comprehensively analyse BERT's biases on the example of a realistic IMDB movie classifier. By systematically varying elements of the training pipeline, we can conclude regarding their impact on the final model bias. Seven different public BERT models in nine training conditions, i.e. 63 models in total, are compared. Almost all conditions yield significant gender biases. Results indicate that reflected biases stem from public BERT models rather than task-specific data, emphasising the weight of responsible usage.",https://aclanthology.org/2022.gebnlp-1.20,Association for Computational Linguistics,2022,July,Proceedings of the 4th Workshop on Gender Bias in Natural Language Processing (GeBNLP),"Jentzsch, Sophie  and
Turan, Cigdem",Gender Bias in BERT - Measuring and Analysing Biases through Sentiment Rating in a Realistic Downstream Classification Task,10.18653/v1/2022.gebnlp-1.20,gebnlp,674
2022.dadc-1.4,"['Ethics', 'Learning Paradigms', 'Data Management and Generation', 'Biases in NLP']","['Data Preparation', 'Adversarial Learning']",['Annotation Processes'],"Adversarial data collection has shown promise as a method for building models which are more robust to the spurious correlations that generally appear in naturalistic data. However, adversarially-collected data may itself be subject to biases, particularly with regard to ambiguous or arguable labeling judgments. Searching for examples where an annotator disagrees with a model might over-sample ambiguous inputs, and filtering the results for high inter-annotator agreement may under-sample them. In either case, training a model on such data may produce predictable and unwanted biases. In this work, we investigate whether models trained on adversarially-collected data are miscalibrated with respect to the ambiguity of their inputs. Using Natural Language Inference models as a testbed, we find no clear difference in accuracy between naturalistically and adversarially trained models, but our model trained only on adversarially-sourced data is considerably more overconfident of its predictions and demonstrates worse calibration, especially on ambiguous inputs. This effect is mitigated, however, when naturalistic and adversarial training data are combined.",https://aclanthology.org/2022.dadc-1.4,Association for Computational Linguistics,2022,July,Proceedings of the First Workshop on Dynamic Adversarial Data Collection,"Li, Margaret  and
Michael, Julian",Overconfidence in the Face of Ambiguity with Adversarial Data,10.18653/v1/2022.dadc-1.4,dadc,221
2022.findings-acl.6,"['Machine Translation (MT)', 'Evaluation Techniques', 'Data Management and Generation', 'Low-resource Languages', 'Model Architectures']","['Transformer Models', 'Data Analysis']",,"What can pre-trained multilingual sequenceto-sequence models like mBART contribute to translating low-resource languages? We conduct a thorough empirical experiment in 10 languages to ascertain this, considering five factors: 1 the amount of fine-tuning data, 2 the noise in the fine-tuning data, 3 the amount of pre-training data in the model, 4 the impact of domain mismatch, and 5 language typology. In addition to yielding several heuristics, the experiments form a framework for evaluating the data sensitivities of machine translation systems. While mBART is robust to domain differences, its translations for unseen and typologically distant languages remain below 3.0 BLEU. In answer to our title's question, mBART is not a low-resource panacea; we therefore encourage shifting the emphasis from new models to new data 1 .",https://aclanthology.org/2022.findings-acl.6,Association for Computational Linguistics,2022,May,Findings of the Association for Computational Linguistics: ACL 2022,"Lee, En-Shiun  and
Thillainathan, Sarubi  and
Nayak, Shravan  and
Ranathunga, Surangika  and
Adelani, David  and
Su, Ruisi  and
McCarthy, Arya",Pre-Trained Multilingual Sequence-to-Sequence Models: A Hope for Low-Resource Language Translation?,10.18653/v1/2022.findings-acl.6,findings,481
D18-1020,"['Text Preprocessing', 'Domain-specific NLP', 'Information Extraction', 'Learning Paradigms', 'Low-resource Languages', 'Model Architectures']","['Part-of-Speech (POS) Tagging', 'Named Entity Recognition (NER)', 'NLP for News and Media']",['NLP for Social Media'],"We introduce a family of multitask variational methods for semi-supervised sequence labeling. Our model family consists of a latentvariable generative model and a discriminative labeler. The generative models use latent variables to define the conditional probability of a word given its context, drawing inspiration from word prediction objectives commonly used in learning word embeddings. The labeler helps inject discriminative information into the latent space. We explore several latent variable configurations, including ones with hierarchical structure, which enables the model to account for both label-specific and word-specific information. Our models consistently outperform standard sequential baselines on 8 sequence labeling datasets, and improve further with unlabeled data.",https://aclanthology.org/D18-1020,Association for Computational Linguistics,2018,October-November,Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing,"Chen, Mingda  and
Tang, Qingming  and
Livescu, Karen  and
Gimpel, Kevin",Variational Sequential Labelers for Semi-Supervised Learning,10.18653/v1/D18-1020,D18,1184
Y16-2010,"['Machine Translation (MT)', 'Low-resource Languages']",,,"The Hierarchical Sub-Sentential Alignment HSSA method is a method to obtain aligned binary tree structures for two aligned sentences in translation correspondence. We propose to use the binary aligned tree structures delivered by this method as training data for preordering prior to machine translation. For that, we learn a Bracketing Transduction Grammar BTG from these binary aligned tree structures. In two oracle experiments in English to Japanese and Japanese to English translation, we show that it is theoretically possible to outperform a baseline system with a default distortion limit of 6, by about 2.5 and 5 BLEU points and, 7 and 10 RIBES points respectively, when preordering the source sentences using the learnt preordering model and using a distortion limit of 0. An attempt at learning a preordering model and its results are also reported.",https://aclanthology.org/Y16-2010,,2016,October,"Proceedings of the 30th Pacific Asia Conference on Language, Information and Computation: Oral Papers","Zhang, Yujia  and
Wang, Hao  and
Lepage, Yves",HSSA tree structures for BTG-based preordering in machine translation,,Y16,855
2020.wildre-1.10,"['Multilingual NLP', 'Data Management and Generation', 'Low-resource Languages', 'Knowledge Representation and Reasoning']",['Data Preparation'],,"In the paper we present our methodology with the intention to propose it as a reference for creating lexicon-grammars. We share our long-term experience gained during research projects past and on-going concerning the description of Polish using this approach. The above-mentioned methodology, linking semantics and syntax, has revealed useful for various IT applications. Among other, we address this paper to researchers working on ""less"" or ""middle-resourced"" Indo-European languages as a proposal of a long term academic cooperation in the field. We believe that the confrontation of our lexicon-grammar methodology with other languages -Indo-European, but also Non-Indo-European languages of India, Ugro-Finish or Turkic languages in Eurasiawill allow for better understanding of the level of versatility of our approach and, last but not least, will create opportunities to intensify comparative studies. The reason of presenting some our works on language resources within the Wildre workshop is the intention not only to take up the challenge thrown down in the CFP of this workshop which is: ""To provide opportunity for researchers from India to collaborate with researchers from other parts of the world"", but also to generalize this challenge to other languages.",https://aclanthology.org/2020.wildre-1.10,European Language Resources Association (ELRA),2020,May,Proceedings of the WILDRE5{--} 5th Workshop on Indian Language Data: Resources and Evaluation,"Vetulani, Zygmunt  and
Vetulani, Gra{\.z}yna",Polish Lexicon-Grammar Development Methodology as an Example for Application to other Languages,,wildre,285
2022.cmcl-1.14,"['Multilingual NLP', 'Learning Paradigms', 'Low-resource Languages', 'Cross-lingual Application']",['Transfer Learning'],,"We present the second shared task on eyetracking data prediction of the Cognitive Modeling and Computational Linguistics Workshop CMCL. Differently from the previous edition, participating teams are asked to predict eyetracking features from multiple languages, including a surprise language for which there were no available training data. Moreover, the task also included the prediction of standard deviations of feature values in order to account for individual differences between readers. A total of six teams registered to the task. For the first subtask on multilingual prediction, the winning team proposed a regression model based on lexical features, while for the second subtask on cross-lingual prediction, the winning team used a hybrid model based on a multilingual transformer embeddings as well as statistical features.",https://aclanthology.org/2022.cmcl-1.14,Association for Computational Linguistics,2022,May,Proceedings of the Workshop on Cognitive Modeling and Computational Linguistics,"Hollenstein, Nora  and
Chersoni, Emmanuele  and
Jacobs, Cassandra  and
Oseki, Yohei  and
Pr{\'e}vot, Laurent  and
Santus, Enrico",CMCL 2022 Shared Task on Multilingual and Crosslingual Prediction of Human Reading Behavior,10.18653/v1/2022.cmcl-1.14,cmcl,110
N19-1129,"['Domain-specific NLP', 'Classification Applications', 'Data Management and Generation']","['NLP for Bibliometrics and Scientometrics', 'Data Preparation', 'Data Analysis']",,"RPB in Figure 2 Yuyal Pinter found that the abbreviation ""RPB"" in Figure 2 was not defined. RPB stands for replicability. It is an aspect to rate in the review template of ACL 2018. Peer pressure Nihar Shah pointed out a problem with the term ""peer pressure"" and aspects of correlation/causality in our paper, which indeed allow room for interpretation and might be misleading to some degree. We make the following clarifications.",https://aclanthology.org/N19-1129,Association for Computational Linguistics,2019,June,"Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)","Gao, Yang  and
Eger, Steffen  and
Kuznetsov, Ilia  and
Gurevych, Iryna  and
Miyao, Yusuke",Does My Rebuttal Matter? Insights from a Major NLP Conference,10.18653/v1/N19-1129,N19,1098
2021.disrpt-1.2,"['Text Preprocessing', 'Parsing', 'Multilingual NLP', 'Low-resource Languages', 'Model Architectures']","['Text Segmentation', 'Discourse Parsing']",,"Discourse parsing, which involves understanding the structure, information flow, and modeling the coherence of a given text, is an important task in natural language processing. It forms the basis of several natural language processing tasks such as question-answering, text summarization, and sentiment analysis. Discourse unit segmentation is one of the fundamental tasks in discourse parsing and refers to identifying the elementary units of text that combine to form a coherent text. In this paper, we present a transformer based approach towards the automated identification of discourse unit segments and connectives. Early approaches towards segmentation relied on rule-based systems using POS tags and other syntactic information to identify discourse segments. Recently, transformer based neural systems have shown promising results in this domain. Our system, SegFormers, employs this transformer based approach to perform multilingual discourse segmentation and connective identification across 16 datasets encompassing 11 languages and 3 different annotation frameworks. We evaluate the system based on F1 scores for both tasks, with the best system reporting the highest F1 score of 97.02% for the treebanked English RST-DT dataset.",https://aclanthology.org/2021.disrpt-1.2,Association for Computational Linguistics,2021,November,Proceedings of the 2nd Shared Task on Discourse Relation Parsing and Treebanking (DISRPT 2021),"Bakshi, Sahil  and
Sharma, Dipti",A Transformer Based Approach towards Identification of Discourse Unit Segments and Connectives,10.18653/v1/2021.disrpt-1.2,disrpt,468
S19-2199,"['Question Answering (QA)', 'Classification Applications', 'Model Architectures']","['Claim Verification', 'Community QA']",,"Community Question Answering cQA is one of the popular Natural Language Processing NLP problems being targeted by researchers across the globe. Couple of the unanswered questions in the domain of cQA are 'can we label the questions/answers as factual or not?' and 'Is the given answer by the user to a particular factual question is correct and if it is correct, can we measure the correctness and factuality of the given answer?'. We have participated in SemEval-2019 Task 8 which deals with these questions. In this paper, we present the features used, approaches followed for feature engineering, models experimented with and finally the results. Our primary submission with accuracy official metric for Se-mEval Task 8 of 0.65 in Subtask B Answer Classification and 0.63 in Subtask A Question Classification stood at 6 th and 16 th places respectively.",https://aclanthology.org/S19-2199,Association for Computational Linguistics,2019,June,Proceedings of the 13th International Workshop on Semantic Evaluation,"Avvaru, Adithya  and
Pandey, Anupam",CodeForTheChange at SemEval-2019 Task 8: Skip-Thoughts for Fact Checking in Community Question Answering,10.18653/v1/S19-2199,S19,359
2022.trustnlp-1.2,"['Ethics', 'Data Management and Generation', 'Model Architectures', 'Biases in NLP']",['Data Preparation'],,"The widespread use of Artificial Intelligence AI in consequential domains, such as healthcare and parole decision-making systems, has drawn intense scrutiny on the fairness of these methods. However, ensuring fairness is often insufficient as the rationale for a contentious decision needs to be audited, understood, and defended. We propose that the attention mechanism can be used to ensure fair outcomes while simultaneously providing feature attributions to account for how a decision was made. Toward this goal, we design an attention-based model that can be leveraged as an attribution framework. It can identify features responsible for both performance and fairness of the model through attention interventions and attention weight manipulation. Using this attribution framework, we then design a post-processing bias mitigation strategy and compare it with a suite of baselines. We demonstrate the versatility of our approach by conducting experiments on two distinct data types, tabular and textual.",https://aclanthology.org/2022.trustnlp-1.2,Association for Computational Linguistics,2022,July,Proceedings of the 2nd Workshop on Trustworthy Natural Language Processing (TrustNLP 2022),"Mehrabi, Ninareh  and
Gupta, Umang  and
Morstatter, Fred  and
Steeg, Greg Ver  and
Galstyan, Aram",Attributing Fair Decisions with Attention Interventions,10.18653/v1/2022.trustnlp-1.2,trustnlp,753
2021.americasnlp-1.23,"['Machine Translation (MT)', 'Multilingual NLP', 'Low-resource Languages']","['Neural MT (NMT)', 'Statistical MT (SMT)']",,"This paper presents the results of the 2021 Shared Task on Open Machine Translation for Indigenous Languages of the Americas. The shared task featured two independent tracks, and participants submitted machine translation systems for up to 10 indigenous languages. Overall, 8 teams participated with a total of 214 submissions. We provided training sets consisting of data collected from various sources, as well as manually translated sentences for the development and test sets. An official baseline trained on this data was also provided. Team submissions featured a variety of architectures, including both statistical and neural models, and for the majority of languages, many teams were able to considerably improve over the baseline. The best performing systems achieved 12.97 ChrF higher than baseline, when averaged across languages.",https://aclanthology.org/2021.americasnlp-1.23,Association for Computational Linguistics,2021,June,Proceedings of the First Workshop on Natural Language Processing for Indigenous Languages of the Americas,"Mager, Manuel  and
Oncevay, Arturo  and
Ebrahimi, Abteen  and
Ortega, John  and
Rios, Annette  and
Fan, Angela  and
Gutierrez-Vasques, Ximena  and
Chiruzzo, Luis  and
Gim{\'e}nez-Lugo, Gustavo  and
Ramos, Ricardo  and
Meza Ruiz, Ivan Vladimir  and
Coto-Solano, Rolando  and
Palmer, Alexis  and
Mager-Hois, Elisabeth  and
Chaudhary, Vishrav  and
Neubig, Graham  and
Vu, Ngoc Thang  and
Kann, Katharina",Findings of the AmericasNLP 2021 Shared Task on Open Machine Translation for Indigenous Languages of the Americas,10.18653/v1/2021.americasnlp-1.23,americasnlp,420
S19-2198,"['Question Answering (QA)', 'Data Management and Generation', 'Classification Applications', 'Model Architectures']","['Large Language Models (LLMs)', 'Data Augmentation', 'Community QA', 'Transformer Models']",,"Since the resources of Community Question Answering are abundant and information sharing becomes universal, it will be increasingly difficult to find factual information for questioners in massive messages. SemEval 2019 task 8 is focusing on these issues. We participate in the task and use Generative Pretrained Transformer OpenAI GPT as our system. Our innovations are data extension, feature extraction, and input transformation. For contextual knowledge enhancement, we extend the training set of subtask A, use several features to improve the results of our system and adapt the input formats to be more suitable for this task. We demonstrate the effectiveness of our approaches, which achieves 81.95% of subtask A and 61.08% of subtask B in accuracy on the SemEval 2019 task 8.",https://aclanthology.org/S19-2198,Association for Computational Linguistics,2019,June,Proceedings of the 13th International Workshop on Semantic Evaluation,"Xie, Wanying  and
Que, Mengxi  and
Yang, Ruoyao  and
Liu, Chunhua  and
Yu, Dong",BLCU\_NLP at SemEval-2019 Task 8: A Contextual Knowledge-enhanced GPT Model for Fact Checking,10.18653/v1/S19-2198,S19,1140
2020.textgraphs-1.1,['Knowledge Representation and Reasoning'],"['Knowledge Graphs', 'Link Prediction']",,"Knowledge graphs KGs of real-world facts about entities and their relationships are useful resources for a variety of natural language processing tasks. However, because knowledge graphs are typically incomplete, it is useful to perform knowledge graph completion or link prediction, i.e. predict whether a relationship not in the knowledge graph is likely to be true. This paper serves as a comprehensive survey of embedding models of entities and relationships for knowledge graph completion, summarizing up-to-date experimental results on standard benchmark datasets and pointing out potential future research directions.",https://aclanthology.org/2020.textgraphs-1.1,Association for Computational Linguistics,2020,December,Proceedings of the Graph-based Methods for Natural Language Processing (TextGraphs),"Nguyen, Dat Quoc",A survey of embedding models of entities and relationships for knowledge graph completion,10.18653/v1/2020.textgraphs-1.1,textgraphs,896
W17-5543,"['Data Management and Generation', 'Model Architectures']","['Data Preparation', 'Recurrent Neural Networks (RNNs)']",['Long Short-Term Memory (LSTM) Models'],"Many genres of natural language text are narratively structured, a testament to our predilection for organizing our experiences as narratives. There is broad consensus that understanding a narrative requires identifying and tracking the goals and desires of the characters and their narrative outcomes. However, to date, there has been limited work on computational models for this problem. We introduce a new dataset, DesireDB, which includes goldstandard labels for identifying statements of desire, textual evidence for desire fulfillment, and annotations for whether the stated desire is fulfilled given the evidence in the narrative context. We report experiments on tracking desire fulfillment using different methods, and show that LSTM Skip-Thought model achieves F-measure of 0.7 on our corpus.",https://aclanthology.org/W17-5543,Association for Computational Linguistics,2017,August,Proceedings of the 18th Annual {SIG}dial Meeting on Discourse and Dialogue,"Rahimtoroghi, Elahe  and
Wu, Jiaqi  and
Wang, Ruimin  and
Anand, Pranav  and
Walker, Marilyn",Modelling Protagonist Goals and Desires in First-Person Narrative,10.18653/v1/W17-5543,W17,748
2020.eval4nlp-1.6,"['Evaluation Techniques', 'Embeddings', 'Text Generation']",['Word Embeddings'],,"Recent advances in automatic evaluation metrics for text have shown that deep contextualized word representations, such as those generated by BERT encoders, are helpful for designing metrics that correlate well with human judgements. At the same time, it has been argued that contextualized word representations exhibit sub-optimal statistical properties for encoding the true similarity between words or sentences. In this paper, we present two techniques for improving encoding representations for similarity metrics: a batch-mean centering strategy that improves statistical properties; and a computationally efficient tempered Word Mover Distance, for better fusion of the information in the contextualized word representations. We conduct numerical experiments that demonstrate the robustness of our techniques, reporting results over various BERTbackbone learned metrics and achieving state of the art correlation with human ratings on several benchmarks.",https://aclanthology.org/2020.eval4nlp-1.6,Association for Computational Linguistics,2020,November,Proceedings of the First Workshop on Evaluation and Comparison of NLP Systems,"Chen, Xi  and
Ding, Nan  and
Levinboim, Tomer  and
Soricut, Radu",Improving Text Generation Evaluation with Batch Centering and Tempered Word Mover Distance,10.18653/v1/2020.eval4nlp-1.6,eval4nlp,622
2020.lrec-1.274,"['Domain-specific NLP', 'Data Management and Generation', 'Knowledge Representation and Reasoning']","['Data Preparation', 'Medical and Clinical NLP']",['Annotation Processes'],"This paper proposes a representation framework for encoding spatial language in radiology based on frame semantics. The framework is adopted from the existing SpatialNet representation in the general domain with the aim to generate more accurate representations of spatial language used by radiologists. We describe Rad-SpatialNet in detail along with illustrating the importance of incorporating domain knowledge in understanding the varied linguistic expressions involved in different radiological spatial relations. This work also constructs a corpus of 400 radiology reports of three examination types chest X-rays, brain MRIs, and babygrams annotated with fine-grained contextual information according to this schema. Spatial trigger expressions and elements corresponding to a spatial frame are annotated. We apply BERT-based models BERTBASE and BERTLARGE to first extract the trigger terms lexical units for a spatial frame and then to identify the related frame elements. The results of BERTLARGE are decent, with F1 of 77.89 for spatial trigger extraction and an overall F1 of 81.61 and 66.25 across all frame elements using gold and predicted spatial triggers respectively. This frame-based resource can be used to develop and evaluate more advanced natural language processing NLP methods for extracting fine-grained spatial information from radiology text in the future.",https://aclanthology.org/2020.lrec-1.274,European Language Resources Association,2020,May,Proceedings of the 12th Language Resources and Evaluation Conference,"Datta, Surabhi  and
Ulinski, Morgan  and
Godfrey-Stovall, Jordan  and
Khanpara, Shekhar  and
Riascos-Castaneda, Roy F.  and
Roberts, Kirk",Rad-SpatialNet: A Frame-based Resource for Fine-Grained Spatial Relations in Radiology Reports,,lrec,232
2021.emnlp-main.289,"['Text Preprocessing', 'Information Retrieval', 'Model Architectures']","['Transformer Models', 'Text Segmentation']",,"In this paper, we propose a new ranking model DR-BERT, which improves the Document Retrieval DR task by a task-adaptive training process and a Segmented Token Recovery Mechanism STRM. In the task-adaptive training, we first pre-train DR-BERT to be domain-adaptive and then make the two-phase fine-tuning. In the first-phase fine-tuning, the model learns query-document matching patterns regarding different query types in a pointwise way. Next, in the second-phase finetuning, the model learns document-level ranking features and ranks documents with regard to a given query in a listwise manner. Such pointwise plus listwise fine-tuning enables the model to minimize errors in the document ranking by incorporating ranking-specific supervisions. Meanwhile, the model derived from pointwise fine-tuning is also used to reduce noise in the training data of the listwise fine-tuning. On the other hand, we present STRM which can compute OOV word representation and contextualization more precisely in BERT-based models. As an effective strategy in DR-BERT, STRM improves the matching perfromance of OOV words between a query and a document. Notably, our DR-BERT model keeps in the top three on the MS MARCO leaderboard since May 20, 2020.",https://aclanthology.org/2021.emnlp-main.289,Association for Computational Linguistics,2021,November,Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,"Sun, Xingwu  and
Cui, Yanling  and
Tang, Hongyin  and
Zhang, Fuzheng  and
Jin, Beihong  and
Wang, Shi",Enhancing Document Ranking with Task-adaptive Training and Segmented Token Recovery Mechanism,10.18653/v1/2021.emnlp-main.289,emnlp,668
2021.calcs-1.15,"['Multilingual NLP', 'Data Management and Generation', 'Low-resource Languages']",['Data Preparation'],,"Codeswitching is an omnipresent phe nomenon in multilingual communities all around the world but remains a challenge for NLP systems due to the lack of proper data and processing techniques. HindiEnglish codeswitched text on social media is often transliterated to the Roman script which prevents from utilizing monolingual resources available in the native Devanagari script. In this paper, we propose a method to nor malize and backtransliterate codeswitched HindiEnglish text. In addition, we present a graphemetophoneme G2P conversion technique for romanized Hindi data. We also release a dataset of scriptcorrected HindiEnglish codeswitched sentences labeled for the named entity recognition and partofspeech tagging tasks to facilitate further research in this area.",https://aclanthology.org/2021.calcs-1.15,Association for Computational Linguistics,2021,June,Proceedings of the Fifth Workshop on Computational Approaches to Linguistic Code-Switching,"Parikh, Dwija  and
Solorio, Thamar",Normalization and Back-Transliteration for Code-Switched Data,10.18653/v1/2021.calcs-1.15,calcs,227
Q18-1018,['Question Answering (QA)'],,,"Based on theoretical reasoning it has been suggested that the reliability of findings published in the scientific literature decreases with the popularity of a research field"" Pfeiffer and Hoffmann, 2009 . As we know, deep learning is very popular and the ability to reproduce results is an important part of science. There is growing concern within the deep learning community about the reproducibility of results that are presented. In this paper we present a number of controllable, yet unreported, effects that can substantially change the effectiveness of a sample model, and thusly the reproducibility of those results. Through these environmental effects we show that the commonly held belief that distribution of source code is all that is needed for reproducibility is not enough. Source code without a reproducible environment does not mean anything at all. In addition the range of results produced from these effects can be larger than the majority of incremental improvement reported.",https://aclanthology.org/Q18-1018,MIT Press,2018,,,"Crane, Matt",Questionable Answers in Question Answering Research: Reproducibility and Variability of Published Results,10.1162/tacl_a_00018,Q18,1411
2021.humeval-1.13,"['Machine Translation (MT)', 'Evaluation Techniques']",,,"This paper provides a quick overview of possible methods how to detect that reference translations were actually created by post-editing an MT system. Two methods based on automatic metrics are presented: BLEU difference between the suspected MT and some other good MT and BLEU difference using additional references. These two methods revealed a suspicion that the WMT 2020 Czech reference is based on MT. The suspicion was confirmed in a manual analysis by finding concrete proofs of the post-editing procedure in particular sentences. Finally, a typology of post-editing changes is presented where typical errors or changes made by the post-editor or errors adopted from the MT are classified.",https://aclanthology.org/2021.humeval-1.13,Association for Computational Linguistics,2021,April,Proceedings of the Workshop on Human Evaluation of NLP Systems (HumEval),"Kloudov{\'a}, V{\v{e}}ra  and
Bojar, Ond{\v{r}}ej  and
Popel, Martin",Detecting Post-Edited References and Their Effect on Human Evaluation,,humeval,848
2020.lrec-1.124,"['Text Clustering', 'Multilingual NLP', 'Low-resource Languages']",,,"We present in this work a universal, character-based method for representing sentences so that one can thereby calculate the distance between any two sentence pair. With a small alphabet, it can function as a proxy of phonemes, and as one of its main uses, we carry out dialect clustering: cluster a dialect/sub-language mixed corpus into sub-groups and see if they coincide with the conventional boundaries of dialects and sub-languages. By using data with multiple Japanese dialects and multiple Slavic languages, we report how well each group clusters, in a manner to partially respond to the question of what separates languages from dialects.",https://aclanthology.org/2020.lrec-1.124,European Language Resources Association,2020,May,Proceedings of the 12th Language Resources and Evaluation Conference,"Sato, Yo  and
Heffernan, Kevin",Dialect Clustering with Character-Based Metrics: in Search of the Boundary of Language and Dialect,,lrec,439
2020.tacl-1.6,"['Text Preprocessing', 'Parsing', 'Learning Paradigms', 'Low-resource Languages', 'Model Architectures']","['Text Segmentation', 'Syntactic Parsing']","['Word Segmentation', 'Dependency Parsing']","Chinese word segmentation and dependency parsing are two fundamental tasks for Chinese natural language processing. The dependency parsing is defined at the word-level. Therefore word segmentation is the precondition of dependency parsing, which makes dependency parsing suffer from error propagation and unable to directly make use of character-level pre-trained language models such as BERT. In this paper, we propose a graph-based model to integrate Chinese word segmentation and dependency parsing. Different from previous transition-based joint models, our proposed model is more concise, which results in fewer efforts of feature engineering. Our graph-based joint model achieves better performance than previous joint models and state-of-the-art results in both Chinese word segmentation and dependency parsing. Additionally, when BERT is combined, our model can substantially reduce the performance gap of dependency parsing between joint models and gold-segmented word-based models. Our code is publicly available at https://github. com/fastnlp/JointCwsParser.",https://aclanthology.org/2020.tacl-1.6,MIT Press,2020,,,"Yan, Hang  and
Qiu, Xipeng  and
Huang, Xuanjing",A Graph-based Model for Joint Chinese Word Segmentation and Dependency Parsing,10.1162/tacl_a_00301,tacl,1464
W17-6302,"['Parsing', 'Low-resource Languages']",['Syntactic Parsing'],['Dependency Parsing'],"In this paper, we present an approach to improve the accuracy of a strong transition-based dependency parser by exploiting dependency language models that are extracted from a large parsed corpus. We integrated a small number of features based on the dependency language models into the parser. To demonstrate the effectiveness of the proposed approach, we evaluate our parser on standard English and Chinese data where the base parser could achieve competitive accuracy scores. Our enhanced parser achieved state-of-the-art accuracy on Chinese data and competitive results on English data. We gained a large absolute improvement of one point UAS on Chinese and 0.5 points for English.",https://aclanthology.org/W17-6302,Association for Computational Linguistics,2017,September,Proceedings of the 15th International Conference on Parsing Technologies,"Yu, Juntao  and
Bohnet, Bernd",Dependency Language Models for Transition-based Dependency Parsing,10.48550/arxiv.1607.04982,W17,121
2020.peoples-1.10,"['Domain-specific NLP', 'Data Management and Generation', 'Classification Applications', 'Discourse Analysis', 'Low-resource Languages']","['Data Preparation', 'Medical and Clinical NLP', 'Data Analysis', 'Sentiment Analysis (SA)']",,"Medical discourse within the professional community has undeservingly received very sparse researchers' attention. Medical professional discourse exists offline and online. We carried out sentiment analysis on titles and text descriptions of materials published on the Russian portal Mir Vracha 90,000 word forms approximately. The texts were generated by and for physicians. The materials include personal narratives describing participants' professional experience, participants' opinions about pandemic news and events in the professional sphere, and Russian reviews and discussion of papers published in international journals in English. We present the first results and discussion of the sentiment analysis of Russian online medical discourse. Based on the results of sentiment analysis and discourse analysis, we described the emotions expressed in the forum and the linguistic means the forum participants used to verbalise their attitudes and emotions while discussing the Covid-19 pandemic. The results showed prevalence of neutral texts in the publications since the medical professionals are interested in research materials and outcomes. In the discussions and personal narratives, the forum participants expressed negative sentiments by colloquial words and figurative language.",https://aclanthology.org/2020.peoples-1.10,Association for Computational Linguistics,2020,December,"Proceedings of the Third Workshop on Computational Modeling of People's Opinions, Personality, and Emotion's in Social Media","Ovchinnikova, Irina  and
Ermakova, Liana  and
Nurbakova, Diana",Sentiments in Russian Medical Professional Discourse during the Covid-19 Pandemic,,peoples,266
2021.iwcs-1.22,"['Data Management and Generation', 'Information Extraction', 'Classification Applications', 'Knowledge Representation and Reasoning']",['Data Preparation'],,"In this paper, we measure variation in framing as a function of foregrounding and backgrounding in a co-referential corpus with a range of temporal distance. In one type of experiment, frame-annotated corpora grouped under event types were contrasted, resulting in a ranking of frames with typicality rates. In contrasting between publication dates, a different ranking of frames emerged for documents that are close to or far from the event instance. In the second type of analysis, we trained a diagnostic classifier with frame occurrences in order to let it differentiate documents based on their temporal distance class close to or far from the event instance. The classifier performs above chance and outperforms models with words.",https://aclanthology.org/2021.iwcs-1.22,Association for Computational Linguistics,2021,June,Proceedings of the 14th International Conference on Computational Semantics (IWCS),"Remijnse, Levi  and
Postma, Marten  and
Vossen, Piek",Variation in framing as a function of temporal reporting distance,,iwcs,265
2020.nlposs-1.6,"['Question Answering (QA)', 'Information Retrieval']",,,"Our objective is to introduce to the NLP community an existing k-NN search library NMSLIB, a new retrieval toolkit FlexNeuART, as well as their integration capabilities. NMSLIB, while being one the fastest k-NN search libraries, is quite generic and supports a variety of distance/similarity functions. Because the library relies on the distance-based structure-agnostic algorithms, it can be further extended by adding new distances. FlexNeuART is a modular, extendible and flexible toolkit for candidate generation in IR and QA applications, which supports mixing of classic and neural ranking signals. FlexNeuART can efficiently retrieve mixed dense and sparse representations with weights learned from training data, which is achieved by extending NMSLIB. In that, other retrieval systems work with purely sparse representations e.g., Lucene, purely dense representations e.g., FAISS and Annoy, or only perform mixing at the re-ranking stage.",https://aclanthology.org/2020.nlposs-1.6,Association for Computational Linguistics,2020,November,Proceedings of Second Workshop for NLP Open Source Software (NLP-OSS),"Boytsov, Leonid  and
Nyberg, Eric",Flexible retrieval with NMSLIB and FlexNeuART,10.18653/v1/2020.nlposs-1.6,nlposs,1477
2020.mwe-1.20,"['Multilingual NLP', 'Parsing', 'Low-resource Languages', 'Classification Applications']",['Morphological Parsing'],,"In this paper, we present MultiVitaminBooster, a system implemented for the PARSEME shared task on semi-supervised identification of verbal multiword expressions -edition 1.2. For our approach, we interpret detecting verbal multiword expressions as a token classification task aiming to decide whether a token is part of a verbal multiword expression or not. For this purpose, we train gradient boosting-based models. We encode tokens as feature vectors combining multilingual contextualized word embeddings provided by the XLM-RoBERTa language model Conneau et al., 2019 with a more traditional linguistic feature set relying on context windows and dependency relations. Our system was ranked 7th in the official open track ranking of the shared task evaluations with an encoding-related bug distorting the results. For this reason we carry out further unofficial evaluations. Unofficial versions of our systems would have achieved higher ranks.",https://aclanthology.org/2020.mwe-1.20,Association for Computational Linguistics,2020,December,Proceedings of the Joint Workshop on Multiword Expressions and Electronic Lexicons,"Gombert, Sebastian  and
Bartsch, Sabine",MultiVitaminBooster at PARSEME Shared Task 2020: Combining Window- and Dependency-Based Features with Multilingual Contextualised Word Embeddings for VMWE Detection,,mwe,354
2021.cinlp-1.8,"['Domain-specific NLP', 'Data Management and Generation']","['Medical and Clinical NLP', 'Data Preparation', 'Data Analysis', 'NLP for News and Media']","['NLP for Mental Health', 'NLP for Social Media']","Every day, individuals post suicide notes on social media asking for support, resources, and reasons to live. Some posts receive few comments while others receive many. While prior studies have analyzed whether specific responses are more or less helpful, it is not clear if the quantity of comments received is beneficial in reducing symptoms or in keeping the user engaged with the platform and hence with life. In the present study, we create a large dataset of users' first r/SuicideWatch SW posts from Reddit N=21,274, collect the comments as well as the user's subsequent posts N=1,615,699 to determine whether they post in SW again in the future. We use propensity score stratification, a causal inference method for observational data, and estimate whether the amount of comments -as a measure of social support-increases or decreases the likelihood of posting again on SW. One hypothesis is that receiving more comments may decrease the likelihood of the user posting in SW in the future, either by reducing symptoms or because comments from untrained peers may be harmful. On the contrary, we find that receiving more comments increases the likelihood a user will post in SW again. We discuss how receiving more comments is helpful, not by permanently relieving symptoms since users make another SW post and their second posts have similar mentions of suicidal ideation, but rather by reinforcing users to seek support and remain engaged with the platform. Furthermore, since receiving only 1 comment -the most common case-decreases the likelihood of posting again by 14% on average depending on the time window, it is important to develop systems that encourage more commenting.",https://aclanthology.org/2021.cinlp-1.8,Association for Computational Linguistics,2021,November,Proceedings of the First Workshop on Causal Inference and NLP,"Low, Daniel  and
Zuromski, Kelly  and
Kessler, Daniel  and
Ghosh, Satrajit S.  and
Nock, Matthew K.  and
Dempsey, Walter",It's quality and quantity: the effect of the amount of comments on online suicidal posts,10.18653/v1/2021.cinlp-1.8,cinlp,21
2021.bucc-1.6,"['Data Management and Generation', 'Information Extraction']","['Data Preparation', 'Relation Extraction']",['Annotation Processes'],"Creating datasets manually by human annotators is a laborious task that can lead to biased and inhomogeneous labels. We propose a flexible, semi-automatic framework for labeling data for relation extraction. Furthermore, we provide a dataset of preprocessed sentences from the requirements engineering domain, including a set of automatically created as well as hand-crafted labels. In our case study, we compare the human and automatic labels and show that there is a substantial overlap between both annotations.",https://aclanthology.org/2021.bucc-1.6,INCOMA Ltd.,2021,September,Proceedings of the 14th Workshop on Building and Using Comparable Corpora (BUCC 2021),"Bohn, Jeremias  and
Fischbach, Jannik  and
Schmitt, Martin  and
Sch{\""u}tze, Hinrich  and
Vogelsang, Andreas",Semi-Automated Labeling of Requirement Datasets for Relation Extraction,10.48550/arxiv.2109.02050,bucc,724
2020.repl4nlp-1.11,"['Learning Paradigms', 'Parsing', 'Classification Applications', 'Model Architectures']","['Transformer Models', 'Semantic Parsing']",['Semantic Role Labeling'],End-to-end models trained on natural language inference NLI datasets show low generalization on out-of-distribution evaluation sets. The models tend to learn shallow heuristics due to dataset biases. The performance decreases dramatically on diagnostic sets measuring compositionality or robustness against simple heuristics. Existing solutions for this problem employ dataset augmentation which has the drawbacks of being applicable to only a limited set of adversaries and at worst hurting the model performance on other adversaries not included in the augmentation set. Our proposed solution is to improve sentence understanding hence out-of-distribution generalization with joint learning of explicit semantics. We show that a BERT based model trained jointly on English semantic role labeling SRL and NLI achieves significantly higher performance on external evaluation sets measuring generalization performance.,https://aclanthology.org/2020.repl4nlp-1.11,Association for Computational Linguistics,2020,July,Proceedings of the 5th Workshop on Representation Learning for NLP,"Cengiz, Cemil  and
Yuret, Deniz",Joint Training with Semantic Role Labeling for Better Generalization in Natural Language Inference,10.18653/v1/2020.repl4nlp-1.11,repl4nlp,954
2021.acl-long.559,"['Parsing', 'Learning Paradigms', 'Model Architectures']","['Transformer Models', 'Unsupervised Learning', 'Syntactic Parsing']","['Constituency Parsing', 'Dependency Parsing']","There are two major classes of natural language grammars -the dependency grammar that models one-to-one correspondences between words and the constituency grammar that models the assembly of one or several corresponded words. While previous unsupervised parsing methods mostly focus on only inducing one class of grammars, we introduce a novel model, StructFormer, that can simultaneously induce dependency and constituency structure. To achieve this, we propose a new parsing framework that can jointly generate a constituency tree and dependency graph. Then we integrate the induced dependency relations into the transformer, in a differentiable manner, through a novel dependency-constrained self-attention mechanism. Experimental results show that our model can achieve strong results on unsupervised constituency parsing, unsupervised dependency parsing, and masked language modeling at the same time.",https://aclanthology.org/2021.acl-long.559,Association for Computational Linguistics,2021,August,Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),"Shen, Yikang  and
Tay, Yi  and
Zheng, Che  and
Bahri, Dara  and
Metzler, Donald  and
Courville, Aaron",StructFormer: Joint Unsupervised Induction of Dependency and Constituency Structure from Masked Language Modeling,10.18653/v1/2021.acl-long.559,acl,102
2020.ccl-1.78,"['Data Management and Generation', 'Classification Applications', 'Low-resource Languages', 'Multilingual NLP', 'Model Architectures']",['Data Augmentation'],,"Neural network based models have achieved impressive results on the sentence classification task. However, most of previous work focuses on designing more sophisticated network or effective learning paradigms on monolingual data, which often suffers from insufficient discriminative knowledge for classification. In this paper, we investigate to improve sentence classification by multilingual data augmentation and consensus learning. Comparing to previous methods, our model can make use of multilingual data generated by machine translation and mine their language-share and language-specific knowledge for better representation and classification. We evaluate our model using English i.e., source language and Chinese i.e., target language data on several sentence classification tasks. Very positive classification performance can be achieved by our proposed model.",https://aclanthology.org/2020.ccl-1.78,Chinese Information Processing Society of China,2020,October,Proceedings of the 19th Chinese National Conference on Computational Linguistics,"Wang, Yanfei  and
Chen, Yangdong  and
Zhang, Yuejie",Improving Sentence Classification by Multilingual Data Augmentation and Consensus Learning,10.1007/978-3-030-63031-7_3,ccl,955
2021.case-1.1,"['Biases in NLP', 'Domain-specific NLP', 'Information Extraction', 'Classification Applications', 'Multilingual NLP']","['NLP for News and Media', 'Bias Detection', 'Event Extraction']",,"This workshop is the fourth issue of a series of workshops on automatic extraction of sociopolitical events from news, organized by the Emerging Market Welfare Project, with the support of the Joint Research Centre of the European Commission and with contributions from many other prominent scholars in this field. The purpose of this series of workshops is to foster research and development of reliable, valid, robust, and practical solutions for automatically detecting descriptions of sociopolitical events, such as protests, riots, wars and armed conflicts, in text streams. This year workshop contributors make use of the stateof-the-art NLP technologies, such as Deep Learning, Word Embeddings and Transformers and cover a wide range of topics from text classification to news bias detection. Around 40 teams have registered and 15 teams contributed to three tasks that are i multilingual protest news detection, ii fine-grained classification of socio-political events, and iii discovering Black Lives Matter protest events. The workshop also highlights two keynote and four invited talks about various aspects of creating event data sets and multi-and cross-lingual machine learning in few-and zero-shot settings.",https://aclanthology.org/2021.case-1.1,Association for Computational Linguistics,2021,August,Proceedings of the 4th Workshop on Challenges and Applications of Automated Extraction of Socio-political Events from Text (CASE 2021),"H{\""u}rriyeto{\u{g}}lu, Ali  and
Tanev, Hristo  and
Zavarella, Vanni  and
Piskorski, Jakub  and
Yeniterzi, Reyyan  and
Mutlu, Osman  and
Yuret, Deniz  and
Villavicencio, Aline",Challenges and Applications of Automated Extraction of Socio-political Events from Text CASE 2021: Workshop and Shared Task Report,10.18653/v1/2021.case-1.1,case,536
J18-2001,"['Parsing', 'Evaluation Techniques', 'Discourse Analysis']",,,"Computational text-level discourse analysis mostly happens within Rhetorical Structure Theory RST, whose structures have classically been presented as constituency trees, and relies on data from the RST Discourse Treebank RST-DT; as a result, the RST discourse parsing community has largely borrowed from the syntactic constituency parsing community. The standard evaluation procedure for RST discourse parsers is thus a simplified variant of PARSEVAL, and most RST discourse parsers use techniques that originated in syntactic constituency parsing. In this article, we isolate a number of conceptual and computational problems with the constituency hypothesis. We then examine the consequences, for the implementation and evaluation of RST discourse parsers, of adopting a dependency perspective on RST structures, a view advocated so far only by a few approaches to discourse parsing. While doing that, we show the importance of the notion of headedness of RST structures. We analyze RST discourse parsing as dependency parsing by adapting to RST a recent proposal in syntactic parsing that relies on head-ordered dependency trees, a representation isomorphic to headed constituency trees. We show how to convert the original trees from the RST corpus, RST-DT, and their binarized versions used by all existing RST parsers to head-ordered dependency trees. We also propose a way to convert existing simple dependency parser output to constituent Submission",https://aclanthology.org/J18-2001,MIT Press,2018,June,,"Morey, Mathieu  and
Muller, Philippe  and
Asher, Nicholas",A Dependency Perspective on RST Discourse Parsing and Evaluation,10.1162/COLI_a_00314,J18,619
2020.nlpmc-1.1,"['Domain-specific NLP', 'Data Management and Generation', 'Classification Applications']","['Data Preparation', 'Medical and Clinical NLP']",['Annotation Processes'],"Electronic consult eConsult systems allow specialists more flexibility to respond to referrals more efficiently, thereby increasing access in under-resourced healthcare settings like safety net systems. Understanding the usage patterns of eConsult system is an important part of improving specialist efficiency. In this work, we develop and apply classifiers to a dataset of eConsult questions from primary care providers to specialists, classifying the messages for how they were triaged by the specialist office, and the underlying type of clinical question posed by the primary care provider. We show that pre-trained transformer models are strong baselines, with improving performance from domain-specific training and shared representations.",https://aclanthology.org/2020.nlpmc-1.1,Association for Computational Linguistics,2020,July,Proceedings of the First Workshop on Natural Language Processing for Medical Conversations,"Ding, Xiyu  and
Barnett, Michael  and
Mehrotra, Ateev  and
Miller, Timothy",Methods for Extracting Information from Messages from Primary Care Providers to Specialists,10.18653/v1/2020.nlpmc-1.1,nlpmc,877
2021.eval4nlp-1.2,"['Data Management and Generation', 'Information Extraction']","['Data Preparation', 'Named Entity Recognition (NER)']",['Annotation Processes'],"Data annotation plays a crucial role in ensuring your named entity recognition NER projects are trained with the correct information to learn from. Producing the most accurate labels is a challenge due to the complexity involved with annotation. Label inconsistency between multiple subsets of data annotation e.g., training set and test set, or multiple training subsets is an indicator of label mistakes. In this work, we present an empirical method to explore the relationship between label in-consistency and NER model performance. It can be used to validate the label consistency or catch the inconsistency in multiple sets of NER data annotation. In experiments, our method identified the label inconsistency of test data in SCIERC and CoNLL03 datasets with 26.7% and 5.4% label mistakes. It validated the consistency in the corrected version of both datasets.",https://aclanthology.org/2021.eval4nlp-1.2,Association for Computational Linguistics,2021,November,Proceedings of the 2nd Workshop on Evaluation and Comparison of NLP Systems,"Zeng, Qingkai  and
Yu, Mengxia  and
Yu, Wenhao  and
Jiang, Tianwen  and
Jiang, Meng",Validating Label Consistency in NER Data Annotation,10.18653/v1/2021.eval4nlp-1.2,eval4nlp,101
L16-1603,"['Parsing', 'Data Management and Generation', 'Low-resource Languages', 'Knowledge Representation and Reasoning']","['Data Preparation', 'Semantic Parsing']","['Semantic Role Labeling', 'Annotation Processes']","We present here a general set of semantic frames to annotate causal expressions, with a rich lexicon in French and an annotated corpus of about 4000 instances of causal lexical items with their corresponding semantic frames. The aim of our project is to have both the largest possible coverage of causal phenomena in French, across all parts of speech, and have it linked to a general semantic framework such as FN, to benefit in particular from the relations between other semantic frames, e.g., temporal ones or intentional ones, and the underlying upper lexical ontology that enables some forms of reasoning. This is part of the larger ASFALDA French FrameNet project, which focuses on a few different notional domains which are interesting in their own right Djemaa et al., 2016 , including cognitive positions and communication frames. In the process of building the French lexicon and preparing the annotation of the corpus, we had to remodel some of the frames proposed in FN based on English data, with hopefully more precise frame definitions to facilitate human annotation. This includes semantic clarifications of frames and frame elements, redundancy elimination, and added coverage. The result is arguably a significant improvement of the treatment of causality in FN itself.",https://aclanthology.org/L16-1603,European Language Resources Association (ELRA),2016,May,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),"Vieu, Laure  and
Muller, Philippe  and
Candito, Marie  and
Djemaa, Marianne",A General Framework for the Annotation of Causality Based on FrameNet,,L16,540
2020.intexsempar-1.4,"['Parsing', 'Learning Paradigms']",['Semantic Parsing'],,"Our goal is to create an interactive natural language interface that efficiently and reliably learns from users to complete tasks in simulated robotics settings. We introduce a neural semantic parsing system that learns new high-level abstractions through decomposition: users interactively teach the system by breaking down high-level utterances describing novel behavior into low-level steps that it can understand. Unfortunately, existing methods either rely on grammars which parse sentences with limited flexibility, or neural sequence-to-sequence models that do not learn efficiently or reliably from individual examples. Our approach bridges this gap, demonstrating the flexibility of modern neural systems, as well as the one-shot reliable generalization of grammar-based methods. Our crowdsourced interactive experiments suggest that over time, users complete complex tasks more efficiently while using our system by leveraging what they just taught. At the same time, getting users to trust the system enough to be incentivized to teach high-level utterances is still an ongoing challenge. We end with a discussion of some of the obstacles we need to overcome to fully realize the potential of the interactive paradigm.",https://aclanthology.org/2020.intexsempar-1.4,Association for Computational Linguistics,2020,November,Proceedings of the First Workshop on Interactive and Executable Semantic Parsing,"Karamcheti, Siddharth  and
Sadigh, Dorsa  and
Liang, Percy",Learning Adaptive Language Interfaces through Decomposition,10.18653/v1/2020.intexsempar-1.4,intexsempar,44
2021.cinlp-1.5,"['Domain-specific NLP', 'Data Management and Generation']",['Data Analysis'],,"We introduce a procedure to examine a textas-mediator problem from a novel randomized experiment that studied the effect of conversations on political polarization. In this randomized experiment, Americans from the Democratic and Republican parties were either randomly paired with one-another to have an anonymous conversation about politics or alternatively not assigned to a conversationchange in political polarization over time was measured for all participants. This paper analyzes the text of the conversations to identify potential mediators of depolarization and is faced with a unique challenge, necessitated by the primary research hypothesis, that individuals in the control condition do not have conversations and so lack observed text data. We highlight the importance of using domain knowledge to perform dimension reduction on the text data, and describe a procedure to characterize indirect effects via text when the text is only observed in one arm of the experiment.",https://aclanthology.org/2021.cinlp-1.5,Association for Computational Linguistics,2021,November,Proceedings of the First Workshop on Causal Inference and NLP,"Tierney, Graham  and
Volfovsky, Alexander",Sensitivity Analysis for Causal Mediation through Text: an Application to Political Polarization,10.18653/v1/2021.cinlp-1.5,cinlp,156
S18-1049,"['Domain-specific NLP', 'Low-resource Languages', 'Classification Applications', 'Model Architectures']","['Sentiment Analysis (SA)', 'Emotion Detection', 'NLP for News and Media']",['NLP for Social Media'],"This paper discusses on task 1, ""Affect in Tweets"" sharedtask, conducted in SemEval-2018. This task comprises of various subtasks, which required participants to analyse over different emotions and sentiments based on the provided tweet data and also measure the intensity of these emotions for subsequent subtasks. Our approach is to come up with a model for all the subtasks on count based representation and use machine learning techniques for regression and classification related tasks. In this work, we use bag of words technique for supervised text classification and regression. . Further, fine tuning on various parameters for the bag of word, representation model we acquired better scores over various other baseline models Vinayan et al. participated in the sharedtask.",https://aclanthology.org/S18-1049,Association for Computational Linguistics,2018,June,Proceedings of The 12th International Workshop on Semantic Evaluation,"J R, Naveen  and
Ganesh H. B., Barathi  and
Kumar M, Anand  and
K P, Soman",CENNLP at SemEval-2018 Task 1: Constrained Vector Space Model in Affects in Tweets,10.18653/v1/S18-1049,S18,672
N18-1125,"['Machine Translation (MT)', 'Low-resource Languages', 'Model Architectures']",['Neural MT (NMT)'],,"in neural machine translation, an attention model is used to identify the aligned source words for a target word target foresight wordin order to select translation context, but it does not make use of any information of this target foresight word at all. previous work proposed an approach to improve the attention model by explicitly accessing this target foresight word and demonstrated the substantial gains in alignment task. however, this approach is useless in machine translation task on which the target foresight word is unavailable. in this paper, we propose a new attention model enhanced by the implicit information of target foresight word oriented to both alignment and translation tasks. empirical experiments on chineseto-english and japanese-to-english datasets show that the proposed attention model delivers significant improvements in terms of both alignment error rate and bleu.",https://aclanthology.org/N18-1125,Association for Computational Linguistics,2018,June,"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)","Li, Xintong  and
Liu, Lemao  and
Tu, Zhaopeng  and
Shi, Shuming  and
Meng, Max",Target Foresight Based Attention for Neural Machine Translation,10.18653/v1/N18-1125,N18,1344
P16-1033,"['Parsing', 'Learning Paradigms', 'Low-resource Languages', 'Data Management and Generation']","['Syntactic Parsing', 'Data Preparation', 'Active Learning']","['Dependency Parsing', 'Annotation Processes']","Different from traditional active learning based on sentence-wise full annotation FA, this paper proposes active learning with dependency-wise partial annotation PA as a finer-grained unit for dependency parsing. At each iteration, we select a few most uncertain words from an unlabeled data pool, manually annotate their syntactic heads, and add the partial trees into labeled data for parser retraining. Compared with sentence-wise FA, dependency-wise PA gives us more flexibility in task selection and avoids wasting time on annotating trivial tasks in a sentence. Our work makes the following contributions. First, we are the first to apply a probabilistic model to active learning for dependency parsing, which can 1 provide tree probabilities and dependency marginal probabilities as principled uncertainty metrics, and 2 directly learn parameters from PA based on a forest-based training objective. Second, we propose and compare several uncertainty metrics through simulation experiments on both Chinese and English. Finally, we conduct human annotation experiments to compare FA and PA on real annotation time and quality.",https://aclanthology.org/P16-1033,Association for Computational Linguistics,2016,August,Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),"Li, Zhenghua  and
Zhang, Min  and
Zhang, Yue  and
Liu, Zhanyi  and
Chen, Wenliang  and
Wu, Hua  and
Wang, Haifeng",Active Learning for Dependency Parsing with Partial Annotation,10.18653/v1/P16-1033,P16,881
2020.iwpt-1.24,"['Learning Paradigms', 'Parsing', 'Low-resource Languages', 'Model Architectures']","['Supervised Learning', 'Semantic Parsing', 'Recurrent Neural Networks (RNNs)']",['Long Short-Term Memory (LSTM) Models'],"We describe the ADAPT system for the 2020 IWPT Shared Task on parsing enhanced Universal Dependencies in 17 languages. We implement a pipeline approach using UDPipe and UDPipe-future to provide initial levels of annotation. The enhanced dependency graph is either produced by a graph-based semantic dependency parser or is built from the basic tree using a small set of heuristics. Our results show that, for the majority of languages, a semantic dependency parser can be successfully applied to the task of parsing enhanced dependencies. Unfortunately, we did not ensure a connected graph as part of our pipeline approach and our competition submission relied on a last-minute fix to pass the validation script which harmed our official evaluation scores significantly. Our submission ranked eighth in the official evaluation with a macro-averaged coarse ELAS F1 of 67.23 and a treebank average of 67.49. We later implemented our own graph-connecting fix which resulted in a score of 79.53 language average or 79.76 treebank average, which would have placed fourth in the competition evaluation.",https://aclanthology.org/2020.iwpt-1.24,Association for Computational Linguistics,2020,July,Proceedings of the 16th International Conference on Parsing Technologies and the IWPT 2020 Shared Task on Parsing into Enhanced Universal Dependencies,"Barry, James  and
Wagner, Joachim  and
Foster, Jennifer",The ADAPT Enhanced Dependency Parser at the IWPT 2020 Shared Task,10.18653/v1/2020.iwpt-1.24,iwpt,1222
2021.cmcl-1.6,"['Evaluation Techniques', 'Model Architectures']",['Large Language Models (LLMs)'],,"We advance a novel explanation of similaritybased interference effects in subject-verb and reflexive pronoun agreement processing, grounded in surprisal values computed from a pretrained large-scale Transformer model, GPT-2. Specifically, we show that surprisal of the verb or reflexive pronoun predicts facilitatory interference effects in ungrammatical sentences, where a distractor noun that matches in number with the verb or pronoun leads to faster reading times, despite the distractor not participating in the agreement relation. We review the human empirical evidence for such effects, including recent metaanalyses and large-scale studies. We also show that attention patterns indexed by entropy and other measures in the Transformer show patterns of diffuse attention in the presence of similar distractors, consistent with cue-based retrieval models of parsing. But in contrast to these models, the attentional cues and memory representations are learned entirely from the simple self-supervised task of predicting the next word.",https://aclanthology.org/2021.cmcl-1.6,Association for Computational Linguistics,2021,June,Proceedings of the Workshop on Cognitive Modeling and Computational Linguistics,"Ryu, Soo Hyun  and
Lewis, Richard",Accounting for Agreement Phenomena in Sentence Comprehension with Transformer Language Models: Effects of Similarity-based Interference on Surprisal and Attention,10.18653/v1/2021.cmcl-1.6,cmcl,413
W19-1412,"['Low-resource Languages', 'Classification Applications', 'Model Architectures']",,,"Automatic dialect identification is a more challenging task than language identification, as it requires the ability to discriminate between varieties of one language. In this paper, we propose an ensemble based system, which combines traditional machine learning models trained on bag of n-gram fetures, with deep learning models trained on word embeddings, to solve the Discriminating between Mainland and Taiwan Variation of Mandarin Chinese DMT shared task at VarDial 2019. Our experiments show that a character bigramtrigram combination based Naive Bayes is a very strong model for identifying varieties of Mandarin Chinense. Through further ensemble of Navie Bayes and BiLSTM, our system team: itsalexyang achived an macroaveraged F1 score of 0.8530 and 0.8687 in two tracks.",https://aclanthology.org/W19-1412,Association for Computational Linguistics,2019,June,"Proceedings of the Sixth Workshop on {NLP} for Similar Languages, Varieties and Dialects","Yang, Li  and
Xiang, Yang",Naive Bayes and BiLSTM Ensemble for Discriminating between Mainland and Taiwan Variation of Mandarin Chinese,10.18653/v1/W19-1412,W19,1290
2022.hcinlp-1.3,['Domain-specific NLP'],['Medical and Clinical NLP'],,"As digital social platforms and mobile technologies become more prevalent and robust, the use of Artificial Intelligence AI in facilitating human communication will grow. This, in turn, will encourage development of intuitive, adaptive, and effective empathic AI interfaces that better address the needs of socially and culturally diverse communities. In this paper, we present several design considerations of an intelligent digital interface intended to guide the clinicians toward more empathetic communication. This approach allows various communities of practice to investigate how AI, on one side, and human communication and healthcare needs, on the other, can contribute to each other's development.",https://aclanthology.org/2022.hcinlp-1.3,Association for Computational Linguistics,2022,July,Proceedings of the Second Workshop on Bridging Human--Computer Interaction and Natural Language Processing,"Girju, Roxana  and
Girju, Marina",Design Considerations for an NLP-Driven Empathy and Emotion Interface for Clinician Training via Telemedicine,10.18653/v1/2022.hcinlp-1.3,hcinlp,520
2022.computel-1.6,"['Data Management and Generation', 'Low-resource Languages']",['Data Preparation'],['Annotation Processes'],"Many archival recordings of speech from endangered languages remain unannotated and inaccessible to community members and language learning programs. One bottleneck is the time-intensive nature of annotation. An even narrower bottleneck occurs for recordings with access constraints, such as language that must be vetted or filtered by authorised community members before annotation can begin. We propose a privacy-preserving workflow to widen both bottlenecks for recordings where speech in the endangered language is intermixed with a more widely-used language such as English for meta-linguistic commentary and questions e.g. What is the word for 'tree'?. We integrate voice activity detection VAD, spoken language identification SLI, and automatic speech recognition ASR to transcribe the metalinguistic content, which an authorised person can quickly scan to triage recordings that can be annotated by people with lower levels of access. We report workin-progress processing 136 hours archival audio containing a mix of English and Muruwari. Our collaborative work with the Muruwari custodian of the archival materials show that this workflow reduces metalanguage transcription time by 20% even with minimal amounts of annotated training data: 10 utterances per language for SLI and for ASR at most 39 minutes, and possibly as little as 39 seconds.",https://aclanthology.org/2022.computel-1.6,Association for Computational Linguistics,2022,May,Proceedings of the Fifth Workshop on the Use of Computational Methods in the Study of Endangered Languages,"San, Nay  and
Bartelds, Martijn  and
Ogunremi, Tolulope  and
Mount, Alison  and
Thompson, Ruben  and
Higgins, Michael  and
Barker, Roy  and
Helen Simpson, Jane  and
Jurafsky, Dan",Automated speech tools for helping communities process restricted-access corpora for language revival efforts,10.18653/v1/2022.computel-1.6,computel,1234
2021.cmcl-1.17,['Model Architectures'],['Recurrent Neural Networks (RNNs)'],['Long Short-Term Memory (LSTM) Models'],"This system description paper describes our participation in CMCL 2021 shared task on predicting human reading patterns. Our focus in this study is making use of well-known, traditional oculomotor control models and machine learning systems. We present experiments with a traditional oculomotor control model the EZ Reader and two machine learning models a linear regression model and a recurrent network model, as well as combining the two different models. In all experiments we test effects of features well-known in the literature for predicting reading patterns, such as frequency, word length and word predictability. Our experiments support the earlier findings that such features are useful when combined. Furthermore, we show that although machine learning models perform better in comparison to traditional models, combination of both gives a consistent improvement for predicting multiple eye tracking variables during reading.",https://aclanthology.org/2021.cmcl-1.17,Association for Computational Linguistics,2021,June,Proceedings of the Workshop on Cognitive Modeling and Computational Linguistics,"Balkoca, Alisan  and
Algan, Abdullah  and
Acarturk, Cengiz  and
{\c{C}}{\""o}ltekin, {\c{C}}a{\u{g}}r{\i}",Team ReadMe at CMCL 2021 Shared Task: Predicting Human Reading Patterns by Traditional Oculomotor Control Models and Machine Learning,10.18653/v1/2021.cmcl-1.17,cmcl,1446
2021.emnlp-main.103,"['Evaluation Techniques', 'Information Extraction', 'Model Architectures']","['Transformer Models', 'Coreference Resolution']",,"Despite recent promising results achieved by span-based approaches to event coreference resolution, there is a lack of understanding of what has been improved. We present an empirical analysis of our state-of-the-art span-based event coreference resolver Lu and Ng, 2021 with the goal of providing the general NLP audience with a better understanding of the state of the art and coreference researchers with directions for future research.",https://aclanthology.org/2021.emnlp-main.103,Association for Computational Linguistics,2021,November,Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,"Lu, Jing  and
Ng, Vincent",Conundrums in Event Coreference Resolution: Making Sense of the State of the Art,10.18653/v1/2021.emnlp-main.103,emnlp,13
2020.readi-1.6,"['Data Management and Generation', 'Low-resource Languages', 'Text Generation']","['Data Preparation', 'Text Simplification']",['Annotation Processes'],"Literature in psycholinguistics and neurosciences has showed that abstract and concrete concepts are perceived differently by our brain, and that the abstractness of a word can cause difficulties in reading. In order to integrate this parameter into an automatic text simplification ATS system for French readers, an annotated list with 7,898 abstract and concrete nouns has been semi-automatically developed. Our aim was to obtain abstract and concrete nouns from an initial manually annotated short list by using two distributional approaches: nearest neighbors and syntactic co-occurrences. The results of this experience have enabled to shed light on the different behaviors of concrete and abstract nouns in context. Besides, the final list, a resource per se in French available on demand, provides a valuable contribution since annotated resources based on cognitive variables such as concreteness or abstractness are scarce and very difficult to obtain. In future work, the list will be enlarged and integrated into an existing lexicon with ranked synonyms for the identification of complex words in text simplification applications.",https://aclanthology.org/2020.readi-1.6,European Language Resources Association,2020,May,Proceedings of the 1st Workshop on Tools and Resources to Empower People with REAding DIfficulties (READI),"Goriachun, Daria  and
Gala, N{\'u}ria",Identifying Abstract and Concrete Words in French to Better Address Reading Difficulties,,readi,437
W16-2522,"['Data Management and Generation', 'Knowledge Representation and Reasoning']",['Data Preparation'],,"The way humans define words is a powerful way of representing them. In this work, we propose to measure word similarity by comparing the overlap in their definition. This highlights linguistic phenomena that are complementary to the information extracted from standard context-based representation learning techniques. To acquire a large amount of word definitions in a cost-efficient manner, we designed a simple interactive word game, Word Sheriff. As a byproduct of game play, it generates short word sequences that can be used to uniquely identify words. These sequences can not only be used to evaluate the quality of word representations, but it could ultimately give an alternative way of learning them, as it overcomes some of the limitations of the distributional hypothesis. Moreover, inspecting player behaviour reveals interesting aspects about human strategies and knowledge acquisition beyond those of simple word association games, due to the conversational nature of the game. Lastly, we outline a vision of a communicative evaluation setting, where systems are evaluated based on how well a given representation allows a system to communicate with human and computer players.",https://aclanthology.org/W16-2522,Association for Computational Linguistics,2016,August,Proceedings of the 1st Workshop on Evaluating Vector-Space Representations for {NLP},"Parasca, Iuliana-Elena  and
Rauter, Andreas Lukas  and
Roper, Jack  and
Rusinov, Aleksandar  and
Bouchard, Guillaume  and
Riedel, Sebastian  and
Stenetorp, Pontus",Defining Words with Words: Beyond the Distributional Hypothesis,10.18653/v1/W16-2522,W16,319
2020.scil-1.43,"['Data Management and Generation', 'Language Change Analysis']",['Data Analysis'],,"We perform statistical analysis of the phenomenon of neology, the process by which new words emerge in a language, using large diachronic corpora of English. We investigate the importance of two factors, semantic sparsity and frequency growth rates of semantic neighbors, formalized in the distributional semantics paradigm. We show that both factors are predictive of word emergence although we find more support for the latter hypothesis. Besides presenting a new linguistic application of distributional semantics, this study tackles the linguistic question of the role of languageinternal factors in our case, sparsity in language change motivated by language-external factors reflected in frequency growth. 1",https://aclanthology.org/2020.scil-1.43,Association for Computational Linguistics,2020,January,Proceedings of the Society for Computation in Linguistics 2020,"Ryskina, Maria  and
Rabinovich, Ella  and
Berg-Kirkpatrick, Taylor  and
Mortensen, David  and
Tsvetkov, Yulia",Where New Words Are Born: Distributional Semantic Analysis of Neologisms and Their Semantic Neighborhoods,10.48550/arxiv.2001.07740,scil,1394
2020.emnlp-main.712,['Question Answering (QA)'],,,"Has there been real progress in multi-hop question-answering? Models often exploit dataset artifacts to produce correct answers, without connecting information across multiple supporting facts. This limits our ability to measure true progress and defeats the purpose of building multi-hop QA datasets. We make three contributions towards addressing this. First, we formalize such undesirable behavior as disconnected reasoning across subsets of supporting facts. This allows developing a model-agnostic probe for measuring how much any model can cheat via disconnected reasoning. Second, using a notion of contrastive support sufficiency, we introduce an automatic transformation of existing datasets that reduces the amount of disconnected reasoning. Third, our experiments 1 suggest that there hasn't been much progress in multifact QA in the reading comprehension setting. For a recent large-scale model XLNet, we show that only 18 points out of its answer F1 score of 72 on HotpotQA are obtained through multifact reasoning, roughly the same as that of a simpler RNN baseline. Our transformation substantially reduces disconnected reasoning 19 points in answer F1. It is complementary to adversarial approaches, yielding further reductions in conjunction.",https://aclanthology.org/2020.emnlp-main.712,Association for Computational Linguistics,2020,November,Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),"Trivedi, Harsh  and
Balasubramanian, Niranjan  and
Khot, Tushar  and
Sabharwal, Ashish",Is Multihop QA in DiRe Condition? Measuring and Reducing Disconnected Reasoning,10.18653/v1/2020.emnlp-main.712,emnlp,523
2022.sigmorphon-1.7,"['Embeddings', 'Learning Paradigms', 'Low-resource Languages', 'Cross-lingual Application']",['Transfer Learning'],,"Word embeddings are growing to be a crucial resource in the field of NLP for any language. This work introduces a novel technique for static subword embeddings transfer for Indic languages from a relatively higher resource language to a genealogically related low resource language. We primarily work with Hindi-Marathi, simulating a low-resource scenario for Marathi, and confirm observed trends on Nepali. We demonstrate the consistent benefits of unsupervised morphemic segmentation on both source and target sides over the treatment performed by fastText. Our best-performing approach uses an EM-style approach to learning bilingual subword embeddings; we also show, for the first time, that a trivial ""copyand-paste"" embeddings transfer based on even perfect bilingual lexicons is inadequate in capturing language-specific relationships. We find that our approach substantially outperforms the fastText baselines for both Marathi and Nepali on the Word Similarity task as well as WordNet-Based Synonymy Tests; on the former task, its performance for Marathi is close to that of pretrained fastText embeddings that use three orders of magnitude more Marathi data.",https://aclanthology.org/2022.sigmorphon-1.7,Association for Computational Linguistics,2022,July,"Proceedings of the 19th SIGMORPHON Workshop on Computational Research in Phonetics, Phonology, and Morphology","Bafna, Niyata  and
{\v{Z}}abokrtsk{\'y}, Zden{\v{e}}k",Subword-based Cross-lingual Transfer of Embeddings from Hindi to Marathi and Nepali,10.18653/v1/2022.sigmorphon-1.7,sigmorphon,577
2021.wat-1.2,"['Machine Translation (MT)', 'Domain-specific NLP', 'Low-resource Languages']",['Neural MT (NMT)'],,"This paper describes the system of our team NHK for the WAT 2021 Japanese↔English restricted machine translation task. In this task, the aim is to improve quality while maintaining consistent terminology for scientific paper translation. This task has a unique feature, where some words in a target sentence are given in addition to a source sentence. In this paper, we use a lexically-constrained neural machine translation NMT, which concatenates the source sentence and constrained words with a special token to input them into the encoder of NMT. The key to the successful lexically-constrained NMT is the way to extract constraints from a target sentence of training data. We propose two extraction methods: proper-noun constraint and mistranslated-word constraint. These two methods consider the importance of words and fallibility of NMT, respectively. The evaluation results demonstrate the effectiveness of our lexical-constraint method.",https://aclanthology.org/2021.wat-1.2,Association for Computational Linguistics,2021,August,Proceedings of the 8th Workshop on Asian Translation (WAT2021),"Mino, Hideya  and
Kinugawa, Kazutaka  and
Ito, Hitoshi  and
Goto, Isao  and
Yamada, Ichiro  and
Tokunaga, Takenobu",NHK's Lexically-Constrained Neural Machine Translation at WAT 2021,10.18653/v1/2021.wat-1.2,wat,884
Y17-1018,"['Machine Translation (MT)', 'Low-resource Languages', 'Model Architectures']",['Statistical MT (SMT)'],,"In this paper, we present a novel statistical machine translation method which employs a BTG-based reordering model during decoding. BTG-based reordering models for preordering have been widely explored, aiming to improve the standard phrase-based statistical machine translation system. Less attention has been paid to incorporating such a reordering model into decoding directly. Our reordering model differs from previous models built using a syntactic parser or directly from annotated treebanks. Here, we train without using any syntactic information. The experiment results on an English-Japanese translation task show that our BTG-based decoder achieves comparable or better performance than the more complex state-of-the-art SMT decoders.",https://aclanthology.org/Y17-1018,The National University (Phillippines),2017,November,"Proceedings of the 31st Pacific Asia Conference on Language, Information and Computation","Wang, Hao  and
Lepage, Yves",BTG-based Machine Translation with Simple Reordering Model using Structured Perceptron,,Y17,554
2019.rocling-1.10,"['Information Retrieval', 'Domain-specific NLP', 'Dialogue Systems', 'Classification Applications']","['Information Filtering', 'Intent Detection', 'Chatbots', 'NLP for Finance']",['Recommender Systems'],"In the insurance industry, lots of effort is putting into helping the customer to solve their problems that occurred during and after purchasing cycle and helping telemarketers to practice selling skills. Chat bots and assistant bots are widely used in these business scenarios, but building a bot application from scratch is expensive. In this paper, a human-machine interaction platform specially designed for intelligent bot applications in insurance industry that combined the technologies of Question Answering (QA), task-oriented dialogue and chit-chat was proposed and we demonstrate the architecture design of this platform, key technologies and the scenario of applications in real-world insurance industry. It has been supporting many intelligent bot applications of insurance industry already, such as Intelligent Coach Bot (ICB) which helps telemarketers to practice their selling skills, Intelligent Customer Service Bot (ICSB) which provides after-sales services and Insurance Advisor Bot (IAB) which helps customer to purchase the most suitable insurance product. Currently, these bot applications serve millions of users per day and are able to solve 80% of the online problems.",https://aclanthology.org/2019.rocling-1.10,The Association for Computational Linguistics and Chinese Language Processing (ACLCLP),2019,October,Proceedings of the 31st Conference on Computational Linguistics and Speech Processing (ROCLING 2019),"Tan, Wei  and
Chang, Chia-Hao  and
Mo, Yang  and
Jiang, Lian-Xin  and
Li, Gen  and
Hou, Xiao-Long  and
Chen, Chu  and
Huang, Yu-Sheng  and
Huang, Meng-Yuan  and
Shen, Jian-Ping",A Real-World Human-Machine Interaction Platform in Insurance Industry,,rocling,1329
2021.argmining-1.21,"['Argument Mining', 'Model Architectures']",['Transformer Models'],,"We present the system description for our submission towards the Key Point Analysis Shared Task at ArgMining 2021. Track 1 of the shared task requires participants to develop methods to predict the match score between each pair of arguments and keypoints, provided they belong to the same topic under the same stance. We leveraged existing state of the art pre-trained language models along with incorporating additional data and features extracted from the inputs topics, key points, and arguments to improve performance. We were able to achieve mAP strict and mAP relaxed score of 0.872 and 0.966 respectively in the evaluation phase, securing 5th place 1 on the leaderboard. In the post evaluation phase, we achieved a mAP strict and mAP relaxed score of 0.921 and 0.982 respectively. All the codes to generate reproducible results on our models are available on Github 2 .",https://aclanthology.org/2021.argmining-1.21,Association for Computational Linguistics,2021,November,Proceedings of the 8th Workshop on Argument Mining,"Kapadnis, Manav  and
Patnaik, Sohan  and
Panigrahi, Siba  and
Madhavan, Varun  and
Nandy, Abhilash",Team Enigma at ArgMining-EMNLP 2021: Leveraging Pre-trained Language Models for Key Point Matching,10.18653/v1/2021.argmining-1.21,argmining,1336
K17-3016,"['Parsing', 'Low-resource Languages', 'Model Architectures']","['Syntactic Parsing', 'Recurrent Neural Networks (RNNs)']","['Dependency Parsing', 'Long Short-Term Memory (LSTM) Models']","The LyS-FASTPARSE team presents BIST-COVINGTON, a neural implementation of the Covington  2001  algorithm for non-projective dependency parsing. The bidirectional LSTM approach by Kiperwasser and Goldberg  2016  is used to train a greedy parser with a dynamic oracle to mitigate error propagation. The model participated in the CoNLL 2017 UD Shared Task. In spite of not using any ensemble methods and using the baseline segmentation and PoS tagging, the parser obtained good results on both macro-average LAS and UAS in the big treebanks category 55 languages, ranking 7th out of 33 teams. In the all treebanks category LAS and UAS we ranked 16th and 12th. The gap between the all and big categories is mainly due to the poor performance on four parallel PUD treebanks, suggesting that some 'suffixed' treebanks e.g. Spanish-AnCora perform poorly on cross-treebank settings, which does not occur with the corresponding 'unsuffixed' treebank e.g. Spanish. By changing that, we obtain the 11th best LAS among all runs official and unofficial. The code is made available at https://github.com/CoNLL-UD-2017/LyS-FASTPARSE",https://aclanthology.org/K17-3016,Association for Computational Linguistics,2017,August,Proceedings of the {C}o{NLL} 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies,"Vilares, David  and
G{\'o}mez-Rodr{\'\i}guez, Carlos",A non-projective greedy dependency parser with bidirectional LSTMs,10.18653/v1/K17-3016,K17,675
2020.alta-1.8,"['Domain-specific NLP', 'Data Management and Generation', 'Classification Applications', 'Learning Paradigms']","['Supervised Learning', 'Data Preparation', 'Multilabel Text Classification']",,"Around 60% of doctoral graduates worldwide ended up working in industry rather than academia. There have been calls to more closely align the PhD curriculum with the needs of industry, but an evidence base is lacking to inform these changes. We need to find better ways to understand what industry employers really want from doctoral graduates. One good source of data is job advertisements where employers provide a 'wish list' of skills and expertise. In this paper, a machine learning-natural language processing ML-NLP based approach was used to explore and extract skill requirements from research intensive job advertisements, suitable for PhD graduates. The model developed for detecting skill requirements in job ads was driven by SVM. Our preliminary results showed that ML-NLP approach had the potential to replicate manual efforts in understanding job requirements of PhD graduates. Our model offers a new perspective to look at PhD-level job skill requirements.",https://aclanthology.org/2020.alta-1.8,Australasian Language Technology Association,2020,December,Proceedings of the The 18th Annual Workshop of the Australasian Language Technology Association,"Chen, Li{'}An  and
Mewburn, Inger  and
Suonimen, Hanna",A machine-learning based model to identify PhD-level skills in job ads,,alta,1225
2020.globalex-1.18,"['Low-resource Languages', 'Machine Translation (MT)']",,,"This paper describes four different strategies proposed to the TIAD 2020 Shared Task for automatic translation inference across dictionaries. The proposed strategies are based on the analysis of Apertium RDF graph, taking advantage of characteristics such as translation using multiple paths, synonyms and similarities between lexical entries from different lexicons and cardinality of possible translations through the graph. The four strategies were trained and validated on the Apertium RDF EN ↔ ES dictionary, showing promising results. Finally, the strategies, applied together, obtained an F-measure of 0.43 in the task of inferring the dictionaries proposed in the shared task, ranking thus third with respect to the other new systems presented to the TIAD 2020 Shared Task. No system presented to the shared task exceeded the baseline proposed by the TIAD organizers.",https://aclanthology.org/2020.globalex-1.18,European Language Resources Association,2020,May,Proceedings of the 2020 Globalex Workshop on Linked Lexicography,"Dranca, Lacramioara",Multi-Strategy system for translation inference across dictionaries,,globalex,310
2020.globalex-1.5,"['Data Management and Generation', 'Knowledge Representation and Reasoning', 'Low-resource Languages']",['Data Preparation'],,"This extended abstract presents on-going work consisting in interlinking and merging the Open Dutch WordNet and generic lexicographic resources for Dutch, focusing for now on the Dutch and English versions of Wiktionary and using the Algemeen Nederlands Woordenboek as a quality checking instance. As the Open Dutch WordNet is already equipped with a relevant number of complex lexical units, we are aiming at expanding it and proposing a new representational framework for the encoding of the interlinked and integrated data. The longer term goal of the work is to investigate if and on how senses can be restricted to particular morphological variations of Dutch lexical entries, and how to represent this information in a Linguistic Linked Open Data compliant format.",https://aclanthology.org/2020.globalex-1.5,European Language Resources Association,2020,May,Proceedings of the 2020 Globalex Workshop on Linked Lexicography,"Declerck, Thierry",Towards an Extension of the Linking of the Open Dutch WordNet with Dutch Lexicographic Resources,,globalex,1266
2020.parlaclarin-1.4,"['Domain-specific NLP', 'Data Management and Generation', 'Low-resource Languages']",['Data Preparation'],,"The Parliament of the Czech Republic consists of two chambers: the Chamber of Deputies Lower House and the Senate Upper House. In our work, we focus on agenda and documents that relate to the Chamber of Deputies. Namely, we pay particular attention to stenographic protocols that record the Chamber of Deputies' meetings. Our overall goal is to continually compile the protocols into the TEI encoded corpus ParCzech and make the corpus accessible in a more user friendly way than the Parliament publishes the protocols. In the very first stage of the compilation, the ParCzech corpus consists of the 2013+ protocols that we make accessible and searchable in the TEITOK web-based platform.",https://aclanthology.org/2020.parlaclarin-1.4,European Language Resources Association,2020,May,Proceedings of the Second ParlaCLARIN Workshop,"Hladka, Barbora  and
Kopp, Maty{\'a}{\v{s}}  and
Stra{\v{n}}{\'a}k, Pavel",Compiling Czech Parliamentary Stenographic Protocols into a Corpus,,parlaclarin,1189
2020.semeval-1.182,"['Domain-specific NLP', 'Multilingual NLP', 'Low-resource Languages', 'Classification Applications', 'Model Architectures']","['Sentiment Analysis (SA)', 'NLP for News and Media']",['NLP for Social Media'],"Mixing languages are widely used in social media, especially in multilingual societies like India. Detecting the emotions contained in these languages, which is of great significance to the development of society and political trends. In this paper, we propose an ensemble of pseudo-label based Bert model and TFIDF based SGDClassifier model to identify the sentiments of Hindi-English Hi-En code-mixed data. The ensemble model combines the strengths of rich semantic information from the Bert model and word frequency information from the probabilistic ngram model to predict the sentiment of a given code-mixed tweet. Finally, our team got an average F1 score of 0.686 on the final leaderboard, and our codalab username is will go. Introduction The rapid development of modern social media makes it possible for people to express their opinions almost at any time. Detecting the sentiments of these views can roughly analyze the social and economic development of the user's current region, the psychological characteristics, interests and hobbies of people of different ages, so as to facilitate the company to better deliver information. It is also possible to judge the emotion and psychological characteristics of a certain type of users according to their statements, so as to eliminate some adverse factors to the society. Many companies put efforts to concern customer feedback to achieve better product optimization, and then win the trust of consumers. However, in social media, speech is not required to conform to certain norms, so there are differences in language and format. In some multilingual societies, when different languages are mixed, it is even more difficult to detect the sentiments. In India and Spain, a number of bi-lingual hybrids produced many texts. The process of switching text between two or more languages is called code-mixing and a significant part is the mixing of the native language and English. The difference of language habits makes the problem even harder. A quite number of people prefer to use nonstandard words like coooool rather than cool, thx instead of thanks, which causes obstacles. Code mixing has always been one of the most important directions of natural language processing, and there is a lot of valuable research and a lot of good results in language identifying, POS tagging and Named Entity Recognition of code-mixed from a lot of researchers Bali et al., 2014; Kumar et al., 2018 . LSTM model shows great results in the sentiment analysis of code-mixing Prabhu et al., 2016 . However, the noise of the code mixed data exists. The model we propose in this paper combines multi-sample-dropout and pseudo label based on the BERT. Besides, TDIDF is also adopted to get better performance. We compare different models to get a higher F1 and the results show that the model BERT with pseudo label performs well when the size of batch is 16. When the n-gram ranges from 1 to 3 helps the model more accurate. Overall solutions can be seen in Patwa et al., 2020 The rest of the paper is organized as follows. The overview of sentiment analysis in code-mixed data is shown in section 2. And in section 3, we explain the details of our model and the corresponding frames. Section 4 shows the results of our models.",https://aclanthology.org/2020.semeval-1.182,International Committee for Computational Linguistics,2020,December,Proceedings of the Fourteenth Workshop on Semantic Evaluation,"Bao, Wei  and
Chen, Weilong  and
Bai, Wei  and
Zhuang, Yan  and
Cheng, Mingyuan  and
Ma, Xiangyu",Will\_go at SemEval-2020 Task 9: An Accurate Approach for Sentiment Analysis on Hindi-English Tweets Based on Bert and Pesudo Label Strategy,10.18653/v1/2020.semeval-1.182,semeval,648
2021.mtsummit-at4ssl.11,['Low-resource Languages'],,,"This paper addresses the tasks of sign segmentation and segment-meaning mapping in the context of sign language SL recognition. It aims to give an overview of the linguistic properties of SL, such as coarticulation and simultaneity, which make these tasks complex. A better understanding of SL structure is the necessary ground for the design and development of SL recognition and segmentation methodologies, which are fundamental for machine translation of these languages. Based on this preliminary exploration, a proposal for mapping segments to meaning in the form of an agglomerate of lexical and non-lexical information is introduced.",https://aclanthology.org/2021.mtsummit-at4ssl.11,Association for Machine Translation in the Americas,2021,August,Proceedings of the 1st International Workshop on Automatic Translation for Signed and Spoken Languages (AT4SSL),"De Sisto, Mirella  and
Shterionov, Dimitar  and
Murtagh, Irene  and
Vermeerbergen, Myriam  and
Leeson, Lorraine",Defining meaningful units. Challenges in sign segmentation and segment-meaning mapping short paper,,mtsummit,210
U16-1021,"['Learning Paradigms', 'Information Extraction', 'Domain-specific NLP']","['NLP for News and Media', 'Supervised Learning', 'Entity Linking']",,"This paper describes system details and results of team ""EOF"" from the University of Melbourne in the shared task of ALTA 2016, which addresses the use of cross document coreference resolution to determine whether two URLs refer to the same underlying entity. In our submission, we develop a two stage system which first identifies the underlying entity for a given URL using entity-level features by ranking the entity mentions present in the crawled text with the help of logistic regression. This is followed by disambiguating entities present in the given pair of URLs using a tree ensemble model to classify if both URLs refer to the same underlying entity. Our system achieved a final F1-score of 86.02% on the private leaderboard 1 , which is the best score among all the participating systems.",https://aclanthology.org/U16-1021,,2016,December,Proceedings of the Australasian Language Technology Association Workshop 2016,"Khirbat, Gitansh  and
Qi, Jianzhong  and
Zhang, Rui",Disambiguating Entities Referred by Web Endpoints using Tree Ensembles,,U16,1108
2022.semeval-1.177,"['Question Answering (QA)', 'Learning Paradigms', 'Domain-specific NLP']","['Transfer Learning', 'Multimodal Learning', 'Visual QA (VQA)']",,"This paper presents the second place system for the R2VQ: competence-based multimodal question answering shared task. The task consisted in building question answering systems that could process procedural recipes involving both text and image, and enriched with semantic and cooking roles. We tackled the task by using a text-to-text generative model based on the transformer architecture, with the aim of generalising across different question types. Our proposed architecture incorporates a novel approach for enriching input texts by incorporating semantic and cooking role labels through what we call Label-Enclosed Generative Question Answering LEG-QA. Our model achieves a score of 91.3, with a significant improvement over the baseline 65.34 and close to the top-ranked system 92.5. After describing the submitted system, we analyse the impact of the different components of LEG-QA as well as perform an error analysis.",https://aclanthology.org/2022.semeval-1.177,Association for Computational Linguistics,2022,July,Proceedings of the 16th International Workshop on Semantic Evaluation (SemEval-2022),"Zhai, Weihe  and
Feng, Mingqiang  and
Zubiaga, Arkaitz  and
Liu, Bingquan",HIT\&QMUL at SemEval-2022 Task 9: Label-Enclosed Generative Question Answering LEG-QA,10.18653/v1/2022.semeval-1.177,semeval,66
R19-2008,"['Text Preprocessing', 'Embeddings', 'Information Extraction', 'Low-resource Languages', 'Model Architectures']","['Part-of-Speech (POS) Tagging', 'Word Embeddings', 'Named Entity Recognition (NER)']",,"This paper reports on experiments with different stacks of word embeddings and evaluation of their usefulness for Bulgarian downstream tasks such as Named Entity Recognition and Classification NERC and Part-of-speech POS Tagging. Word embeddings stay in the core of the development of NLP, with several key language models being created over the last two years like FastText Bojanowski et al., 2017 , ElMo Peters et al.,  2018, BERT Devlin et al., 2018  and Flair Akbik et al., 2018 . Stacking or combining different word embeddings is another technique used in this paper and still not reported for Bulgarian NERC. Well-established architecture is used for the sequence tagging task such as BI-LSTM-CRF, and different pre-trained language models are combined in the embedding layer to decide which combination of them scores better.",https://aclanthology.org/R19-2008,INCOMA Ltd.,2019,September,Proceedings of the Student Research Workshop Associated with RANLP 2019,"Marinova, Iva",Evaluation of Stacked Embeddings for Bulgarian on the Downstream Tasks POS and NERC,10.26615/issn.2603-2821.2019_008,R19,1333
2021.wat-1.16,"['Machine Translation (MT)', 'Multilingual NLP', 'Image and Video Processing', 'Learning Paradigms', 'Low-resource Languages', 'Model Architectures']","['Multimodal Learning', 'Transformer Models', 'Image Captioning', 'Neural MT (NMT)', 'Recurrent Neural Networks (RNNs)']",['Long Short-Term Memory (LSTM) Models'],"This paper provides the description of shared tasks to the WAT 2021 by our team ""NLPHut"". We have participated in the English→Hindi Multimodal translation task, English→Malayalam Multimodal translation task, and Indic Multilingual translation task. We have used the state-of-the-art Transformer model with language tags in different settings for the translation task and proposed a novel ""region-specific"" caption generation approach using a combination of image CNN and LSTM for the Hindi and Malayalam image captioning. Our submission tops in English→Malayalam Multimodal translation task text-only translation, and Malayalam caption, and ranks secondbest in English→Hindi Multimodal translation task text-only translation, and Hindi caption. Our submissions have also performed well in the Indic Multilingual translation tasks.",https://aclanthology.org/2021.wat-1.16,Association for Computational Linguistics,2021,August,Proceedings of the 8th Workshop on Asian Translation (WAT2021),"Parida, Shantipriya  and
Panda, Subhadarshi  and
Kotwal, Ketan  and
Dash, Amulya Ratna  and
Dash, Satya Ranjan  and
Sharma, Yashvardhan  and
Motlicek, Petr  and
Bojar, Ond{\v{r}}ej",NLPHut's Participation at WAT2021,10.18653/v1/2021.wat-1.16,wat,732
2022.naacl-demo.4,"['Learning Paradigms', 'Information Extraction', 'Data Management and Generation']",['Data Preparation'],['Annotation Processes'],"The current workflow for Information Extraction IE analysts involves the definition of the entities/relations of interest and a training corpus with annotated examples. In this demonstration we introduce a new workflow where the analyst directly verbalizes the entities/relations, which are then used by a Textual Entailment model to perform zero-shot IE. We present the design and implementation of a toolkit with a user interface, as well as experiments on four IE tasks that show that the system achieves very good performance at zero-shot learning using only 5-15 minutes per type of a user's effort. Our demonstration system is open-sourced at https:// github.com/BBN-E/ZS4IE. A demonstration video is available at https:// vimeo.com/676138340.",https://aclanthology.org/2022.naacl-demo.4,Association for Computational Linguistics,2022,July,Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: System Demonstrations,"Sainz, Oscar  and
Qiu, Haoling  and
Lopez de Lacalle, Oier  and
Agirre, Eneko  and
Min, Bonan",ZS4IE: A toolkit for Zero-Shot Information Extraction with simple Verbalizations,10.18653/v1/2022.naacl-demo.4,naacl,1255
2021.smm4h-1.1,"['Domain-specific NLP', 'Classification Applications', 'Data Management and Generation']","['NLP for News and Media', 'Sentiment Analysis (SA)', 'Data Analysis', 'Medical and Clinical NLP']",['NLP for Social Media'],"In the midst of a global pandemic, understanding the public's opinion of their government's policy-level, non-pharmaceutical interventions NPIs is a crucial component of the health-policy-making process. Prior work on COVID-19 NPI sentiment analysis by the epidemiological community has proceeded without a method for properly attributing sentiment changes to events, an ability to distinguish the influence of various events across time, a coherent model for predicting the public's opinion of future events of the same sort, nor even a means of conducting significance tests. We argue here that this urgently needed evaluation method does already exist. In the financial sector, event studies of the fluctuations in a publicly traded company's stock price are commonplace for determining the effects of earnings announcements, product placements, etc. The same method is suitable for analysing temporal sentiment variation in the light of policy-level NPIs. We provide a case study of Twitter sentiment towards policy-level NPIs in Canada. Our results confirm a generally positive connection between the announcements of NPIs and Twitter sentiment, and we document a promising correlation between the results of this study and a public-health survey of popular compliance with NPIs.",https://aclanthology.org/2021.smm4h-1.1,Association for Computational Linguistics,2021,June,Proceedings of the Sixth Social Media Mining for Health ({\#}SMM4H) Workshop and Shared Task,"Niu, Jingcheng  and
Rees, Erin  and
Ng, Victoria  and
Penn, Gerald",Statistically Evaluating Social Media Sentiment Trends towards COVID-19 Non-Pharmaceutical Interventions with Event Studies,10.18653/v1/2021.smm4h-1.1,smm4h,1459
E17-1102,"['Domain-specific NLP', 'Bilingual Lexicon Induction (BLI)', 'Data Management and Generation', 'Low-resource Languages', 'Classification Applications', 'Model Architectures']","['Data Preparation', 'Recurrent Neural Networks (RNNs)', 'Medical and Clinical NLP']",['Long Short-Term Memory (LSTM) Models'],"We study the problem of bilingual lexicon induction BLI in a setting where some translation resources are available, but unknown translations are sought for certain, possibly domain-specific terminology. We frame BLI as a classification problem for which we design a neural network based classification architecture composed of recurrent long short-term memory and deep feed forward networks. The results show that word-and character-level representations each improve state-of-the-art results for BLI, and the best results are obtained by exploiting the synergy between these wordand character-level representations in the classification model.",https://aclanthology.org/E17-1102,Association for Computational Linguistics,2017,April,"Proceedings of the 15th Conference of the {E}uropean Chapter of the Association for Computational Linguistics: Volume 1, Long Papers","Heyman, Geert  and
Vuli{\'c}, Ivan  and
Moens, Marie-Francine",Bilingual Lexicon Induction by Learning to Combine Word-Level and Character-Level Representations,10.18653/v1/e17-1102,E17,1242
W18-0703,['Information Extraction'],['Anaphora Resolution'],,"We present two systems for bridging resolution, which we submitted to the CRAC shared task on bridging anaphora resolution in the ARRAU corpus track 2: a rulebased approach following Hou et al.  2014  and a learning-based approach. The reimplementation of Hou et al.  2014  achieves very poor performance when being applied to ARRAU. We found that the reason for this lies in the different bridging annotations: whereas the rule-based system suggests many referential bridging pairs, ARRAU contains mostly lexical bridging. We describe the differences between these two types of bridging and adapt the rule-based approach to be able to handle lexical bridging. The modified rule-based approach achieves reasonable performance on all sub-tasks and outperforms a simple learningbased approach.",https://aclanthology.org/W18-0703,Association for Computational Linguistics,2018,June,"Proceedings of the First Workshop on Computational Models of Reference, Anaphora and Coreference","Roesiger, Ina",Rule- and Learning-based Methods for Bridging Resolution in the ARRAU Corpus,10.18653/v1/W18-0703,W18,1112
2021.mrqa-1.3,"['Question Answering (QA)', 'Domain-specific NLP', 'Evaluation Techniques', 'Data Management and Generation']","['Data Analysis', 'Medical and Clinical NLP']",['Biomedical NLP'],"Medical question answering QA systems have the potential to answer clinicians' uncertainties about treatment and diagnosis ondemand, informed by the latest evidence. However, despite the significant progress in general QA made by the NLP community, medical QA systems are still not widely used in clinical environments. One likely reason for this is that clinicians may not readily trust QA system outputs, in part because transparency, trustworthiness, and provenance have not been key considerations in the design of such models. In this paper we discuss a set of criteria that, if met, we argue would likely increase the utility of biomedical QA systems, which may in turn lead to adoption of such systems in practice. We assess existing models, tasks, and datasets with respect to these criteria, highlighting shortcomings of previously proposed approaches and pointing toward what might be more usable QA systems.",https://aclanthology.org/2021.mrqa-1.3,Association for Computational Linguistics,2021,November,Proceedings of the 3rd Workshop on Machine Reading for Question Answering,"Kell, Gregory  and
Marshall, Iain  and
Wallace, Byron  and
Jaun, Andre",What Would it Take to get Biomedical QA Systems into Practice?,10.18653/v1/2021.mrqa-1.3,mrqa,1345
2022.ecnlp-1.13,"['Question Answering (QA)', 'Data Management and Generation', 'Text Generation', 'Domain-specific NLP']","['Data Augmentation', 'Data Preparation']",['Annotation Processes'],"It is of great value to answer product questions based on heterogeneous information sources available on web product pages, e.g., semistructured attributes, text descriptions, userprovided contents, etc. However, these sources have different structures and writing styles, which poses challenges for 1 evidence ranking, 2 source selection, and 3 answer generation. In this paper, we build a benchmark with annotations for both evidence selection and answer generation covering 6 information sources. Based on this benchmark, we conduct a comprehensive study and present a set of best practices. We show that all sources are important and contribute to answering questions. Handling all sources within one single model can produce comparable confidence scores across sources and combining multiple sources for training always helps, even for sources with totally different structures. We further propose a novel data augmentation method to iteratively create training samples for answer generation, which achieves close-to-human performance with only a few thousand annotations. Finally, we perform an in-depth error analysis of model predictions and highlight the challenges for future research.",https://aclanthology.org/2022.ecnlp-1.13,Association for Computational Linguistics,2022,May,Proceedings of The Fifth Workshop on e-Commerce and NLP (ECNLP 5),"Shen, Xiaoyu  and
Barlacchi, Gianni  and
Del Tredici, Marco  and
Cheng, Weiwei  and
Byrne, Bill  and
Gispert, Adri{\`a}",Product Answer Generation from Heterogeneous Sources: A New Benchmark and Best Practices,10.18653/v1/2022.ecnlp-1.13,ecnlp,1236
E17-1097,"['Learning Paradigms', 'Information Extraction', 'Classification Applications', 'Data Management and Generation']","['Data Preparation', 'Relation Extraction']",,"We investigate the task of open domain opinion relation extraction. Given a large number of unlabelled texts, we propose an efficient distantly supervised framework based on pattern matching and neural network classifiers. The patterns are designed to automatically generate training data, and the deep learning model is designed to capture various lexical and syntactic features. The result algorithm is fast and scalable on large-scale corpus. We test the system on the Amazon online review dataset, and show that the proposed model is able to achieve promising performances without any human annotations.",https://aclanthology.org/E17-1097,Association for Computational Linguistics,2017,April,"Proceedings of the 15th Conference of the {E}uropean Chapter of the Association for Computational Linguistics: Volume 1, Long Papers","Sun, Changzhi  and
Wu, Yuanbin  and
Lan, Man  and
Sun, Shiliang  and
Zhang, Qi",Large-scale Opinion Relation Extraction with Distantly Supervised Neural Network,10.18653/v1/e17-1097,E17,712
2021.woah-1.6,"['Data Management and Generation', 'Low-resource Languages', 'Classification Applications', 'Domain-specific NLP']","['Data Preparation', 'Hate and Offensive Speech Detection', 'NLP for News and Media']","['NLP for Social Media', 'Annotation Processes']","As socially unacceptable language become pervasive in social media platforms, the need for automatic content moderation become more pressing. This contribution introduces the Dutch Abusive Language Corpus DALC v1.0, a new dataset with tweets manually annotated for abusive language. The resource address a gap in language resources for Dutch and adopts a multi-layer annotation scheme modeling the explicitness and the target of the abusive messages. Baselines experiments on all annotation layers have been conducted, achieving a macro F1 score of 0.748 for binary classification of the explicitness layer and .489 for target classification.",https://aclanthology.org/2021.woah-1.6,Association for Computational Linguistics,2021,August,Proceedings of the 5th Workshop on Online Abuse and Harms (WOAH 2021),"Caselli, Tommaso  and
Schelhaas, Arjan  and
Weultjes, Marieke  and
Leistra, Folkert  and
van der Veen, Hylke  and
Timmerman, Gerben  and
Nissim, Malvina",DALC: the Dutch Abusive Language Corpus,10.18653/v1/2021.woah-1.6,woah,320
2022.autosimtrans-1.5,"['Audio Generation and Processing', 'Machine Translation (MT)', 'Data Management and Generation', 'Low-resource Languages', 'Model Architectures']","['Transformer Models', 'Data Augmentation', 'Neural MT (NMT)', 'Automatic Speech Recognition (ASR)']",,"This paper describes the system submitted to AutoSimTrans 2022 from Huawei Noah's Ark Lab, which won the first place in the audio input track of the Chinese-English translation task. Our system is based on RealTranS, an end-to-end simultaneous speech translation model. We enhance the model with pretraining, by initializing the acoustic encoder with ASR encoder, and the semantic encoder and decoder with NMT encoder and decoder, respectively. To relieve the data scarcity, we further construct pseudo training corpus as a kind of knowledge distillation with ASR data and the pretrained NMT model. Meanwhile, we also apply several techniques to improve the robustness and domain generalizability, including punctuation removal, tokenlevel knowledge distillation and multi-domain finetuning. Experiments show that our system significantly outperforms the baselines at all latency and also verify the effectiveness of our proposed methods.",https://aclanthology.org/2022.autosimtrans-1.5,Association for Computational Linguistics,2022,July,Proceedings of the Third Workshop on Automatic Simultaneous Translation,"Zeng, Xingshan  and
Li, Pengfei  and
Li, Liangyou  and
Liu, Qun",End-to-End Simultaneous Speech Translation with Pretraining and Distillation: Huawei Noah's System for AutoSimTranS 2022,10.18653/v1/2022.autosimtrans-1.5,autosimtrans,340
2020.framenet-1.12,"['Domain-specific NLP', 'Data Management and Generation', 'Parsing', 'Low-resource Languages', 'Knowledge Representation and Reasoning', 'Multilingual NLP']","['Data Preparation', 'Semantic Parsing']",,"The methodology developed within the FrameNet project is being used to compile resources in an increasing number of specialized fields of knowledge. The methodology along with the theoretical principles on which it is based, i.e. Frame Semantics, are especially appealing as they allow domain-specific resources to account for the conceptual background of specialized knowledge and to explain the linguistic properties of terms against this background. This paper presents a methodology for building a multilingual resource that accounts for terms of the environment. After listing some lexical and conceptual differences that need to be managed in such a resource, we explain how the FrameNet methodology is adapted for describing terms in different languages. We first applied our methodology to French and then extended it to English. Extensions to Spanish, Portuguese and Chinese were made more recently. Up to now, we have defined 190 frames: 112 frames are new; 38 are used as such; and 40 are slightly different a different number of obligatory participants; a significant alternation, etc. when compared to Berkeley FrameNet.",https://aclanthology.org/2020.framenet-1.12,European Language Resources Association,2020,May,"Proceedings of the International FrameNet Workshop 2020: Towards a Global, Multilingual FrameNet","L{'} Homme, Marie-Claude  and
Robichaud, Beno{\^\i}t  and
Subirats, Carlos",Building Multilingual Specialized Resources Based on FrameNet: Application to the Field of the Environment,,framenet,288
2020.eamt-1.28,"['Domain-specific NLP', 'Data Management and Generation', 'Low-resource Languages', 'Machine Translation (MT)']","['Neural MT (NMT)', 'NLP for the Legal Domain']",,"The mission of the Directorate General for Translation DGT is to provide highquality translation to help the European Commission communicate with EU citizens. To this end DGT employs almost 2000 translators from all EU official languages. But while the demand for translation has been continuously growing, following a global trend, the number of translators has decreased. To cope with the demand, DGT extensively uses a CAT environment encompassing translation memories, terminology databases and recently also machine translation. This paper examines the benefits and risks of using neural machine translation to augment the productivity of in-house DGT translators for the English-Polish language pair. Based on the analysis of a sample of NMT-translated texts and on the observations of the working practices of Polish translators it is concluded that the possible productivity gain is still modest, while the risks to quality are quite substantial.",https://aclanthology.org/2020.eamt-1.28,European Association for Machine Translation,2020,November,Proceedings of the 22nd Annual Conference of the European Association for Machine Translation,"Stefaniak, Karolina",Evaluating the usefulness of neural machine translation for the Polish translators in the European Commission,,eamt,1174
2021.reinact-1.3,"['Classification Applications', 'Model Architectures']",,,"Starting from an existing account of semantic classification and learning from interaction formulated in a Probabilistic Type Theory with Records, encompassing Bayesian inference and learning with a frequentist flavour, we observe some problems with this account and provide an alternative account of classification learning that addresses the observed problems. The proposed account is also broadly Bayesian in nature but instead uses a linear transformation model for classification and learning.",https://aclanthology.org/2021.reinact-1.3,Association for Computational Linguistics,2021,October,Proceedings of the Reasoning and Interaction Conference (ReInAct 2021),"Larsson, Staffan  and
Bernardy, Jean-Philippe",Semantic Classification and Learning Using a Linear Tranformation Model in a Probabilistic Type Theory with Records,,reinact,485
2020.readi-1.5,"['Domain-specific NLP', 'Data Management and Generation', 'Low-resource Languages', 'Text Generation']","['Medical and Clinical NLP', 'Text Simplification', 'Data Analysis']",,"The objective of this work is to introduce text simplification as a potential reading aid to help improve the poor reading performance experienced by visually impaired individuals. As a first step, we explore what makes a text especially complex when read with low vision, by assessing the individual effect of three word properties frequency, orthographic similarity and length on reading speed in the presence of Central visual Field Loss CFL. Individuals with bilateral CFL induced by macular diseases read pairs of French sentences displayed with the self-paced reading method. For each sentence pair, sentence n contained a target word matched with a synonym word of the same length included in sentence n+1. Reading time was recorded for each target word. Given the corpus we used, our results show that 1 word frequency has a significant effect on reading time the more frequent the faster the reading speed with larger amplitude in the range of seconds compared to normal vision; 2 word neighborhood size has a significant effect on reading time the more neighbors the slower the reading speed, this effect being rather small in amplitude, but interestingly reversed compared to normal vision; 3 word length has no significant effect on reading time. Supporting the development of new and more effective assistive technology to help low vision is an important and timely issue, with massive potential implications for social and rehabilitation practices. The end goal of this project will be to use our findings to custom text simplification to this specific population and use it as an optimal and efficient reading aid.",https://aclanthology.org/2020.readi-1.5,European Language Resources Association,2020,May,Proceedings of the 1st Workshop on Tools and Resources to Empower People with REAding DIfficulties (READI),"Sauvan, Lauren  and
Stolowy, Natacha  and
Aguilar, Carlos  and
Fran{\c{c}}ois, Thomas  and
Gala, N{\'u}ria  and
Matonti, Fr{\'e}d{\'e}ric  and
Castet, Eric  and
Calabr{\`e}se, Aur{\'e}lie",Text Simplification to Help Individuals with Low Vision Read More Fluently,,readi,1089
2021.ranlp-1.140,"['Learning Paradigms', 'Domain-specific NLP', 'Classification Applications', 'Model Architectures']","['Transformer Models', 'Supervised Learning', 'NLP for News and Media']",,"Hyperpartisan news show an extreme manipulation of reality based on an underlying and extreme ideological orientation. Because of its harmful effects at reinforcing one's bias and the posterior behavior of people, hyperpartisan news detection has become an important task for computational linguists. In this paper, we evaluate two different approaches to detect hyperpartisan news. First, a text masking technique that allows us to compare style vs. topicrelated features in a different perspective from previous work. Second, the transformer-based models BERT, XLM-RoBERTa, and M-BERT, known for their ability to capture semantic and syntactic patterns in the same representation. Our results corroborate previous research on this task in that topic-related features yield better results than style-based ones, although they also highlight the relevance of using higherlength n-grams. Furthermore, they show that transformer-based models are more effective than traditional methods, but this at the cost of greater computational complexity and lack of transparency. Based on our experiments, we conclude that the beginning of the news show relevant information for the transformers at distinguishing effectively between left-wing, mainstream, and right-wing orientations.",https://aclanthology.org/2021.ranlp-1.140,INCOMA Ltd.,2021,September,Proceedings of the International Conference on Recent Advances in Natural Language Processing (RANLP 2021),"S{\'a}nchez-Junquera, Javier  and
Rosso, Paolo  and
Montes-y-G{\'o}mez, Manuel  and
Ponzetto, Simone Paolo",Masking and Transformer-based Models for Hyperpartisanship Detection in News,10.26615/978-954-452-072-4_140,ranlp,182
Q16-1006,"['Multilingual NLP', 'Low-resource Languages', 'Classification Applications']",,,"Algorithmic decipherment is a prime example of a truly unsupervised problem. The first step in the decipherment process is the identification of the encrypted language. We propose three methods for determining the source language of a document enciphered with a monoalphabetic substitution cipher. The best method achieves 97% accuracy on 380 languages. We then present an approach to decoding anagrammed substitution ciphers, in which the letters within words have been arbitrarily transposed. It obtains the average decryption word accuracy of 93% on a set of 50 ciphertexts in 5 languages. Finally, we report the results on the Voynich manuscript, an unsolved fifteenth century cipher, which suggest Hebrew as the language of the document.",https://aclanthology.org/Q16-1006,MIT Press,2016,,,"Hauer, Bradley  and
Kondrak, Grzegorz",Decoding Anagrammed Texts Written in an Unknown Language and Script,10.1162/tacl_a_00084,Q16,419
2021.blackboxnlp-1.36,"['Domain-specific NLP', 'Data Management and Generation', 'Embeddings', 'Adversarial Attacks and Robustness']","['Data Preparation', 'Data Augmentation']",,"Recently, some studies have shown that text classification tasks are vulnerable to poisoning and evasion attacks. However, little work has investigated attacks against decision-making algorithms that use text embeddings, and their output is a ranking. In this paper, we focus on ranking algorithms for the recruitment process that employ text embeddings for ranking applicants' resumes when compared to a job description. We demonstrate both white-box and black-box attacks that identify text items that, based on their location in embedding space, have a significant contribution in increasing the similarity score between a resume and a job description. The adversary then uses these text items to improve the ranking of their resume among others. We tested recruitment algorithms that use the similarity scores obtained from Universal Sentence Encoder USE and Term Frequency-Inverse Document Frequency TF-IDF vectors. Our results show that in both adversarial settings, on average the attacker is successful. We also found that attacks against TF-IDF are more successful compared to USE.",https://aclanthology.org/2021.blackboxnlp-1.36,Association for Computational Linguistics,2021,November,Proceedings of the Fourth BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP,"Samadi, Anahita  and
Banerjee, Debapriya  and
Nilizadeh, Shirin",Attacks against Ranking Algorithms with Text Embeddings: A Case Study on Recruitment Algorithms,10.18653/v1/2021.blackboxnlp-1.36,blackboxnlp,391
2021.nodalida-main.5,"['Machine Translation (MT)', 'Data Management and Generation', 'Learning Paradigms', 'Low-resource Languages', 'Cross-lingual Application']","['Multimodal Learning', 'Data Preparation', 'Data Augmentation', 'Transfer Learning', 'Neural MT (NMT)']",,"An effective method to improve extremely low-resource neural machine translation is multilingual training, which can be improved by leveraging monolingual data to create synthetic bilingual corpora using the back-translation method. This work focuses on closely related languages from the Uralic language family: from Estonian and Finnish geographical regions. We find that multilingual learning and synthetic corpora increase the translation quality in every language pair for which we have data. We show that transfer learning and fine-tuning are very effective for doing low-resource machine translation and achieve the best results. We collected new parallel data for Võro, North and South Saami and present first results of neural machine translation for these languages.",https://aclanthology.org/2021.nodalida-main.5,"Link{\""o}ping University Electronic Press, Sweden",2021,May 31--2 June,Proceedings of the 23rd Nordic Conference on Computational Linguistics (NoDaLiDa),"Tars, Maali  and
T{\""a}ttar, Andre  and
Fi{\v{s}}el, Mark",Extremely low-resource machine translation for closely related languages,10.48550/arxiv.2105.13065,nodalida,1395
2020.deelio-1.10,"['Commonsense Reasoning', 'Model Architectures']",['Transformer Models'],,"In this work, we present our empirical attempt to identify the proper strategy of using Transformer Language Models to identify sentences consistent with commonsense. We tackle the first two tasks from the ComVE Wang et al., 2020a competition. The starting point for our work is the BERT assumption according to which a large number of NLP tasks can be solved with pre-trained Transformers with no substantial task-specific changes of the architecture. However, our experiments show that the encoding strategy can have a great impact on the quality of the fine-tuning. The combination between cross-encoding and multi-input models worked better than one cross-encoder and allowed us to achieve comparable results with the state-of-the-art without the use of any external data.",https://aclanthology.org/2020.deelio-1.10,Association for Computational Linguistics,2020,November,Proceedings of Deep Learning Inside Out (DeeLIO): The First Workshop on Knowledge Extraction and Integration for Deep Learning Architectures,"Cibu, Sonia  and
Marginean, Anca",Commonsense Statements Identification and Explanation with Transformer-based Encoders,10.18653/v1/2020.deelio-1.10,deelio,1062
2022.autosimtrans-1.2,"['Machine Translation (MT)', 'Evaluation Techniques']",,,"Simultaneous speech translation SimulST systems aim at generating their output with the lowest possible latency, which is normally computed in terms of Average Lagging AL. In this paper we highlight that, despite its widespread adoption, AL provides underestimated scores for systems that generate longer predictions compared to the corresponding references. We also show that this problem has practical relevance, as recent SimulST systems have indeed a tendency to over-generate. As a solution, we propose LAAL Length-Adaptive Average Lagging, a modified version of the metric that takes into account the over-generation phenomenon and allows for unbiased evaluation of both under-/over-generating systems.",https://aclanthology.org/2022.autosimtrans-1.2,Association for Computational Linguistics,2022,July,Proceedings of the Third Workshop on Automatic Simultaneous Translation,"Papi, Sara  and
Gaido, Marco  and
Negri, Matteo  and
Turchi, Marco",Over-Generation Cannot Be Rewarded: Length-Adaptive Average Lagging for Simultaneous Speech Translation,10.18653/v1/2022.autosimtrans-1.2,autosimtrans,1084
2020.mwe-1.8,"['Domain-specific NLP', 'Data Management and Generation', 'Low-resource Languages']","['NLP for News and Media', 'Data Analysis']",['NLP for Social Media'],"We evaluate manually five lexical association measurements as regards the discovery of Modern Greek verb multiword expressions with two or more lexicalised components using mwetoolkit3 Ramisch et al., 2010. We use Twitter corpora and compare our findings with previous work on fiction corpora. The results of LL, MLE and T-score were found to overlap significantly in both the fiction and the Twitter corpora, while the results of PMI and Dice do not. We find that MWEs with two lexicalised components are more frequent in Twitter than in fiction corpora and that lean syntactic patterns help retrieve them more efficiently than richer ones. Our work i supports the enrichment of the lexicographical database for Modern Greek MWEs 'IDION' Markantonatou et al., 2019 and ii highlights aspects of the usage of five association measurements on specific text genres for best MWE discovery results.",https://aclanthology.org/2020.mwe-1.8,Association for Computational Linguistics,2020,December,Proceedings of the Joint Workshop on Multiword Expressions and Electronic Lexicons,"Stamou, Vivian  and
Xylogianni, Artemis  and
Malli, Marilena  and
Takorou, Penny  and
Markantonatou, Stella",VMWE discovery: a comparative analysis between Literature and Twitter Corpora,,mwe,561
2020.lt4gov-1.5,"['Data Management and Generation', 'Low-resource Languages', 'Domain-specific NLP']",,,"The Austrian Language Resource Portal Sprachressourcenportal Österreichs is Austria's central platform for language resources in the area of public administration. It focuses on language resources in the Austrian variety of the German language. As a product of the cooperation between a public administration body and a university, the Portal contains various language resources terminological resources in the public administration domain, a language guide, named entities based on open public data, translation memories, etc.. German is a pluricentric language that considerably varies in the domain of public administration due to different public administration systems. Therefore, the Austrian Language Resource Portal stresses the importance of language resources specific to a language variety, thus paving the way for the re-use of variety-specific language data for human language technology, such as machine translation training, for the Austrian standard variety.",https://aclanthology.org/2020.lt4gov-1.5,European Language Resources Association,2020,May,Proceedings of the 1st Workshop on Language Technologies for Government and Public Administration (LT4Gov),"Heinisch, Barbara  and
Lu{\v{s}}icky, Vesna",The Austrian Language Resource Portal for the Use and Provision of Language Resources in a Language Variety by Public Administration -- a Showcase for Collaboration between Public Administration and a University,,lt4gov,1057
2020.inlg-1.27,"['Machine Translation (MT)', 'Text Generation']",,,"Massive digital disinformation is one of the main risks of modern society. Hundreds of models and linguistic analyses have been done to compare and contrast misleading and credible content online. However, most models do not remove the confounding factor of a topic or narrative when training, so the resulting models learn a clear topical separation for misleading versus credible content. We study the feasibility of using two strategies to disentangle the topic bias from the models to understand and explicitly measure linguistic and stylistic properties of content from misleading versus credible content. First, we develop conditional generative models to create news content that is characteristic of different credibility levels. We perform multi-dimensional evaluation of model performance on mimicking both the style and linguistic differences that distinguish news of different credibility using machine translation metrics and classification models. We show that even though generative models are able to imitate both the style and language of the original content, additional conditioning on both the news category and the topic leads to reduced performance. In a second approach, we perform deception style ""transfer"" by translating deceptive content into the style of credible content and vice versa. Extending earlier studies, we demonstrate that, when conditioned on a topic, deceptive content is shorter, less readable, more biased, and more subjective than credible content, and transferring the style from deceptive to credible content is more challenging than the opposite direction.",https://aclanthology.org/2020.inlg-1.27,Association for Computational Linguistics,2020,December,Proceedings of the 13th International Conference on Natural Language Generation,"Saldanha, Emily  and
Garimella, Aparna  and
Volkova, Svitlana",Understanding and Explicitly Measuring Linguistic and Stylistic Properties of Deception via Generation and Translation,10.18653/v1/2020.inlg-1.27,inlg,1403
2021.findings-acl.428,"['Text Generation', 'Learning Paradigms', 'Model Architectures']","['Transformer Models', 'Adversarial Learning']",,"Conventional autoregressive models have achieved great success in text generation but suffer from the exposure bias problem in that token sequences in the training and in the generation stages are mismatched. While generative adversarial networks GANs can remedy this problem, existing implementations of GANs directly on discrete outputs tend to be unstable and lack diversity. In this work, we propose TILGAN, a Transformerbased Implicit Latent GAN, which combines a Transformer autoencoder and GAN in the latent space with a novel design and distribution matching based on the Kullback-Leibler KL divergence. Specifically, to improve local and global coherence, we explicitly introduce a multi-scale discriminator to capture the semantic information at varying scales among the sequence of hidden representations encoded by Transformer. Moreover, the decoder is enhanced by an additional KL loss to be consistent with the latent-generator. Experimental results on three benchmark datasets demonstrate the validity and effectiveness of our model, by obtaining significant improvements and a better quality-diversity trade-off in automatic and human evaluation for both unconditional and conditional generation tasks. 1",https://aclanthology.org/2021.findings-acl.428,Association for Computational Linguistics,2021,August,Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021,"Diao, Shizhe  and
Shen, Xinwei  and
Shum, Kashun  and
Song, Yan  and
Zhang, Tong",TILGAN: Transformer-based Implicit Latent GAN for Diverse and Coherent Text Generation,10.18653/v1/2021.findings-acl.428,findings,607
P17-2005,"['Evaluation Techniques', 'Domain-specific NLP', 'Automatic Text Summarization']","['Extractive Text Summarization', 'Document Summarization', 'NLP for News and Media']",,"We present a new framework for evaluating extractive summarizers, which is based on a principled representation as optimization problem. We prove that every extractive summarizer can be decomposed into an objective function and an optimization technique. We perform a comparative analysis and evaluation of several objective functions embedded in wellknown summarizers regarding their correlation with human judgments. Our comparison of these correlations across two datasets yields surprising insights into the role and performance of objective functions in the different summarizers.",https://aclanthology.org/P17-2005,Association for Computational Linguistics,2017,July,Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),"Peyrard, Maxime  and
Eckle-Kohler, Judith",A Principled Framework for Evaluating Summarizers: Comparing Models of Summary Quality against Human Judgments,10.18653/v1/P17-2005,P17,198
2020.iwpt-1.5,"['Learning Paradigms', 'Parsing', 'Model Architectures']",['Syntactic Parsing'],['Dependency Parsing'],"We propose an end-to-end variational autoencoding parsing VAP model for semisupervised graph-based projective dependency parsing. It encodes the input using continuous latent variables in a sequential manner by deep neural networks DNN that can utilize the contextual information, and reconstruct the input using a generative model. The VAP model admits a unified structure with different loss functions for labeled and unlabeled data with shared parameters. We conducted experiments on the WSJ data sets, showing the proposed model can use the unlabeled data to increase the performance on a limited amount of labeled data, on a par with a recently proposed semi-supervised parser with faster inference.",https://aclanthology.org/2020.iwpt-1.5,Association for Computational Linguistics,2020,July,Proceedings of the 16th International Conference on Parsing Technologies and the IWPT 2020 Shared Task on Parsing into Enhanced Universal Dependencies,"Zhang, Xiao  and
Goldwasser, Dan",Semi-supervised Parsing with a Variational Autoencoding Parser,10.18653/v1/2020.iwpt-1.5,iwpt,660
S16-1093,"['Classification Applications', 'Model Architectures']",,,"We present a description of the system submitted to the Semantic Textual Similarity STS shared task at SemEval 2016. The task is to assess the degree to which two sentences carry the same meaning. We have designed two different methods to automatically compute a similarity score between sentences. The first method combines a variety of semantic similarity measures as features in a machine learning model. In our second approach, we employ training data from the Interpretable Similarity subtask to create a combined wordsimilarity measure and assess the importance of both aligned and unaligned words. Finally, we combine the two methods into a single hybrid model. Our best-performing run attains a score of 0.7732 on the 2015 STS evaluation data and 0.7488 on the 2016 STS evaluation data.",https://aclanthology.org/S16-1093,Association for Computational Linguistics,2016,June,Proceedings of the 10th International Workshop on Semantic Evaluation ({S}em{E}val-2016),"Przyby{\l}a, Piotr  and
Nguyen, Nhung T. H.  and
Shardlow, Matthew  and
Kontonatsios, Georgios  and
Ananiadou, Sophia",NaCTeM at SemEval-2016 Task 1: Inferring sentence-level semantic similarity from an ensemble of complementary lexical and sentence-level features,10.18653/v1/S16-1093,S16,146
2021.internlp-1.5,"['Information Retrieval', 'Learning Paradigms', 'Data Management and Generation']","['Unsupervised Learning', 'Data Preparation']",,"Dynamic faceted search DFS, an interactive query refinement technique, is a form of Human-computer information retrieval HCIR approach. It allows users to narrow down search results through facets, where the facetsdocuments mapping is determined at runtime based on the context of user query instead of pre-indexing the facets statically. In this paper, we propose a new unsupervised approach for dynamic facet generation, namely optimistic facets, which attempts to generate the best possible subset of facets, hence maximizing expected Discounted Cumulative Gain DCG, a measure of ranking quality that uses a graded relevance scale. We also release code to generate a new evaluation dataset. Through empirical results on two datasets, we show that the proposed DFS approach considerably improves the document ranking in the search results.",https://aclanthology.org/2021.internlp-1.5,Association for Computational Linguistics,2021,August,Proceedings of the First Workshop on Interactive Learning for Natural Language Processing,"Glass, Michael  and
Chowdhury, Md Faisal Mahbub  and
Deng, Yu  and
Mahindru, Ruchi  and
Fauceglia, Nicolas Rodolfo  and
Gliozzo, Alfio  and
Mihindukulasooriya, Nandana",Dynamic Facet Selection by Maximizing Graded Relevance,10.18653/v1/2021.internlp-1.5,internlp,467
P19-1181,"['Evaluation Techniques', 'Learning Paradigms', 'Data Management and Generation']","['Data Augmentation', 'Reinforcement Learning']",,"Advances in learning and representations have reinvigorated work that connects language to other modalities. A particularly exciting direction is Vision-and-Language Navigation VLN, in which agents interpret natural language instructions and visual scenes to move through environments and reach goals. Despite recent progress, current research leaves unclear how much of a role language understanding plays in this task, especially because dominant evaluation metrics have focused on goal completion rather than the sequence of actions corresponding to the instructions. Here, we highlight shortcomings of current metrics for the Room-to-Room dataset Anderson et al., 2018b and propose a new metric, Coverage weighted by Length Score CLS. We also show that the existing paths in the dataset are not ideal for evaluating instruction following because they are direct-to-goal shortest paths. We join existing short paths to form more challenging extended paths to create a new data set, Room-for-Room R4R. Using R4R and CLS, we show that agents that receive rewards for instruction fidelity outperform agents that focus on goal completion.",https://aclanthology.org/P19-1181,Association for Computational Linguistics,2019,July,Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,"Jain, Vihan  and
Magalhaes, Gabriel  and
Ku, Alexander  and
Vaswani, Ashish  and
Ie, Eugene  and
Baldridge, Jason",Stay on the Path: Instruction Fidelity in Vision-and-Language Navigation,10.18653/v1/P19-1181,P19,256
2021.naacl-srw.5,"['Domain-specific NLP', 'Data Management and Generation', 'Information Extraction', 'Knowledge Representation and Reasoning', 'Low-resource Languages']","['Data Preparation', 'Event Extraction']",,"In this thesis proposal, we explore event extraction and event representation on literary texts. Due to its variety of genres and varying document length, literature is a challenging domain, yet the representation of literary content has received relatively little attention. As most individual events contribute little to the overall semantics of literary documents, we model events at different granularities. On the conceptual level, we adapt the previous definition of schemas as sequences of events, all describing a single process connected through shared participants, and extend the notion to allow modeling a document's content using sequences of schemas. Technically, the segmentation of event sequences into schemas is approached by modeling such sequences, making use of the narrative cloze task, which is the prediction of masked events in event sequence contexts. We propose building on sequences of event embeddings to form schema representations, thereby summarizing sections of documents using a fixed-size representation. This approach will give rise to comparisons of sections such as chapters up to the comparison of entire literary works on the level of their schema structure, paving the way to a computational approach to quantitative literary research.",https://aclanthology.org/2021.naacl-srw.5,Association for Computational Linguistics,2021,June,Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Student Research Workshop,"Hatzel, Hans Ole  and
Biemann, Chris",Towards Layered Events and Schema Representations in Long Documents,10.18653/v1/2021.naacl-srw.5,naacl,39
2020.blackboxnlp-1.15,"['Data Management and Generation', 'Embeddings']","['Word Embeddings', 'Data Analysis']",,"The recent paradigm shift to contextual word embeddings has seen tremendous success across a wide range of down-stream tasks. However, little is known on how the emergent relation of context and semantics manifests geometrically. We investigate polysemous words as one particularly prominent instance of semantic organization. Our rigorous quantitative analysis of linear separability and cluster organization in embedding vectors produced by BERT shows that semantics do not surface as isolated clusters but form seamless structures, tightly coupled with sentiment and syntax.",https://aclanthology.org/2020.blackboxnlp-1.15,Association for Computational Linguistics,2020,November,Proceedings of the Third BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP,"Yenicelik, David  and
Schmidt, Florian  and
Kilcher, Yannic",How does BERT capture semantics? A closer look at polysemous words,10.18653/v1/2020.blackboxnlp-1.15,blackboxnlp,97
O16-1027,"['Domain-specific NLP', 'Data Management and Generation', 'Low-resource Languages', 'Classification Applications', 'Model Architectures']","['Data Preparation', 'Sarcasm Detection', 'NLP for News and Media']",['NLP for Social Media'],"Based on the assumption that comment with positive sentimental polarity to a negative issue has high probability to be a sarcasm, we propose a simple yet efficient method to collect sarcastic textual data by crowdsourcing with social media and merging game with a purpose approach. Taking advantage of Facebook's reaction button, posts triggering strong negative emotion are collected. Next, by using PTT's search engine, we successfully connect PTT's comments to the collected posts in Facebook and build the sarcasm corpus. Based on the corpus data, the performance comparison of sarcasm detection between SVM with naïve features and Convolutional Neural Network models is conducted. An impressive accuracy rate and great potentials of the corpus are demonstrated.",https://aclanthology.org/O16-1027,The Association for Computational Linguistics and Chinese Language Processing (ACLCLP),2016,October,Proceedings of the 28th Conference on Computational Linguistics and Speech Processing ({ROCLING} 2016),"Lin, Shih-Kai  and
Hsieh, Shu-Kai",Sarcasm Detection in Chinese Using a Crowdsourced Corpus,,O16,141
2022.fl4nlp-1.3,"['Ethics', 'Learning Paradigms', 'Model Architectures']",['Transformer Models'],,"Although differential privacy DP can protect language models from leaking privacy, its indiscriminative protection on all data points reduces its practical utility. Previous works improve DP training by discriminating private and non-private data. But these works rely on datasets with prior privacy information, which is not available in real-world scenarios. In this paper, we propose an Adaptive Differential Privacy ADP framework for language modeling without resorting to prior privacy information. We estimate the probability that a linguistic item contains privacy based on a language model. We further propose a new Adam algorithm that adjusts the degree of differential privacy noise injected to the language model according to the estimated privacy probabilities. Experiments demonstrate that our ADP improves differentially private language modeling to achieve good protection from canary attackers.",https://aclanthology.org/2022.fl4nlp-1.3,Association for Computational Linguistics,2022,May,Proceedings of the First Workshop on Federated Learning for Natural Language Processing (FL4NLP 2022),"Wu, Xinwei  and
Gong, Li  and
Xiong, Deyi",Adaptive Differential Privacy for Language Model Training,10.18653/v1/2022.fl4nlp-1.3,fl4nlp,1481
2021.paclic-1.25,"['Machine Translation (MT)', 'Data Management and Generation', 'Learning Paradigms', 'Low-resource Languages', 'Model Architectures']","['Transformer Models', 'Transfer Learning', 'Neural MT (NMT)', 'Data Analysis']",,"Recently, several studies have proposed pretrained encoder-decoder models using monolingual data, such as BART, which can improve the accuracy of seq2seq tasks via finetuning with task-specific data. However, the effectiveness of pre-training using monolingual data requires further verification, as previous experiments on machine translation have focused on specific languages with overlapping vocabularies and particular translation directions. Additionally, we hypothesize that the effects of pre-trained models differ depending on the syntactic similarity between languages for pre-training and fine-tuning, as in transfer learning. To this end, we analyze BART fine-tuned with languages exhibiting different syntactic proximities to the source language in terms of the translation accuracy and network representations. Our experiments show that 1 BART realizes consistent improvements regardless of language pairs and translation directions. Contrary to our hypothesis, there is no significant difference in the translation accuracy based on the syntactic similarity. However, when syntactically similar, BART achieves approximately twice the accuracy of our baseline model in the initial epoch. Furthermore, we demonstrate that 2 syntactic similarity correlates with closeness of the encoder representations; in a syntactically similar language pair, the representations of the encoder do not change after fine-tuning. The code used in our experiments has been published. 1",https://aclanthology.org/2021.paclic-1.25,Association for Computational Lingustics,2021,11,"Proceedings of the 35th Pacific Asia Conference on Language, Information and Computation","Kim, Hwichan  and
Komachi, Mamoru",Can Monolingual Pre-trained Encoder-Decoder Improve NMT for Distant Language Pairs?,,paclic,86
U18-1004,"['Embeddings', 'Learning Paradigms']","['Unsupervised Learning', 'Word Embeddings']",,"It has been demonstrated that vector-based representations of words trained on large text corpora encode linguistic regularities that may be exploited via the use of vector space arithmetic. This capability has been extensively explored and is generally measured via tasks which involve the automated completion of linguistic proportional analogies. The question remains, however, as to what extent it is possible to induce relations from word embeddings in a principled and systematic way, without the provision of exemplars or seed terms. In this paper we propose an extensible and efficient framework for inducing relations via the use of constraint satisfaction. The method is efficient, unsupervised and can be customized in various ways. We provide both quantitative and qualitative analysis of the results.",https://aclanthology.org/U18-1004,,2018,December,Proceedings of the Australasian Language Technology Association Workshop 2018,"De Vine, Lance  and
Geva, Shlomo  and
Bruza, Peter",Unsupervised Mining of Analogical Frames by Constraint Satisfaction,,U18,456
Y16-3018,"['Data Management and Generation', 'Low-resource Languages']",['Data Analysis'],,"This paper adopts a comparable corpus-based approach to light verb variations in two varieties of Mandarin Chinese and proposes a transitivity Hopper and Thompson 1980 based theoretical account. Light verbs are highly grammaticalized and lack strong collocation restrictions; hence it has been a challenge to empirical accounts. It is even more challenging to consider their variations between different varieties e.g. Taiwan and Mainland Mandarin. This current study follows the research paradigm set up in Lin et al. 2014 for differentiating different light verbs and Huang et al.  2014  for automatic discovery of light verb variations. In our study, a corpus-based statistical approach is adopted to show that both internal variety differences between light verbs and external differences between different variants can be detected effectively. The distributional differences between Mainland and Taiwan can also shed light on the re-classification of syntactic types of the taken complement. We further argue that the variations in selection of arguments of light verb in two Mandarin variants can in fact be accounted for in terms of their different degree of transitivity. Higher degree of transitivity in Taiwan Mandarin in fact show that light verbs are less grammaticalized and hence consistent with the generalization that varieties away from the main speaking community should be more conservative.",https://aclanthology.org/Y16-3018,,2016,October,"Proceedings of the 30th Pacific Asia Conference on Language, Information and Computation: Posters","Jiang, Menghan  and
Shi, Dingxu  and
Huang, Chu-Ren",Transitivity in Light Verb Variations in Mandarin Chinese -- A Comparable Corpus-based Statistical Approach,,Y16,108
N16-1083,"['Classification Applications', 'Cross-lingual Application', 'Model Architectures']",,,"In many languages, sparse availability of resources causes numerous challenges for textual analysis tasks. Text classification is one of such standard tasks that is hindered due to limited availability of label information in lowresource languages. Transferring knowledge i.e. label information from high-resource to low-resource languages might improve text classification as compared to the other approaches like machine translation. We introduce BRAVE Bilingual paRAgraph VEctors, a model to learn bilingual distributed representations i.e. embeddings of words without word alignments either from sentencealigned parallel or label-aligned non-parallel document corpora to support cross-language text classification. Empirical analysis shows that classification models trained with our bilingual embeddings outperforms other stateof-the-art systems on three different crosslanguage text classification tasks.",https://aclanthology.org/N16-1083,Association for Computational Linguistics,2016,June,Proceedings of the 2016 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies,"Mogadala, Aditya  and
Rettinger, Achim",Bilingual Word Embeddings from Parallel and Non-parallel Corpora for Cross-Language Text Classification,10.18653/v1/N16-1083,N16,856
2020.semeval-1.38,"['Embeddings', 'Multilingual NLP', 'Model Architectures', 'Low-resource Languages']",['Word Embeddings'],,"Natural Language Processing NLP has been widely used in the semantic analysis in recent years. Our paper mainly discusses a methodology to analyze the effect that context has on human perception of similar words, which is the third task of SemEval 2020. We apply several methods in calculating the distance between two embedding vector generated by Bidirectional Encoder Representation from Transformer BERT. Our team will go won the 1st place in Finnish language track of subtask1, the second place in English track of subtask1.",https://aclanthology.org/2020.semeval-1.38,International Committee for Computational Linguistics,2020,December,Proceedings of the Fourteenth Workshop on Semantic Evaluation,"Bao, Wei  and
Che, Hongshu  and
Zhang, Jiandong",Will\_Go at SemEval-2020 Task 3: An Accurate Model for Predicting the Graded Effect of Context in Word Similarity Based on BERT,10.18653/v1/2020.semeval-1.38,semeval,854
K19-1046,"['Domain-specific NLP', 'Data Management and Generation', 'Information Extraction', 'Information Retrieval', 'Classification Applications']","['Claim Verification', 'Misinformation Detection', 'Stance Detection', 'Data Preparation', 'NLP for News and Media']",['Annotation Processes'],"Automated fact-checking based on machine learning is a promising approach to identify false information distributed on the web. In order to achieve satisfactory performance, machine learning methods require a large corpus with reliable annotations for the different tasks in the fact-checking process. Having analyzed existing fact-checking corpora, we found that none of them meets these criteria in full. They are either too small in size, do not provide detailed annotations, or are limited to a single domain. Motivated by this gap, we present a new substantially sized mixed-domain corpus with annotations of good quality for the core fact-checking tasks: document retrieval, evidence extraction, stance detection, and claim validation. To aid future corpus construction, we describe our methodology for corpus creation and annotation, and demonstrate that it results in substantial inter-annotator agreement. As baselines for future research, we perform experiments on our corpus with a number of model architectures that reach high performance in similar problem settings. Finally, to support the development of future models, we provide a detailed error analysis for each of the tasks. Our results show that the realistic, multi-domain setting defined by our data poses new challenges for the existing models, providing opportunities for considerable improvement by future systems.",https://aclanthology.org/K19-1046,Association for Computational Linguistics,2019,November,Proceedings of the 23rd Conference on Computational Natural Language Learning (CoNLL),"Hanselowski, Andreas  and
Stab, Christian  and
Schulz, Claudia  and
Li, Zile  and
Gurevych, Iryna",A Richly Annotated Corpus for Different Tasks in Automated Fact-Checking,10.18653/v1/K19-1046,K19,1192
2021.triton-1.13,"['Machine Translation (MT)', 'Domain-specific NLP', 'Low-resource Languages', 'Image and Video Processing']","['Sign Language and Fingerspelling Recognition', 'Medical and Clinical NLP']",,"Communication between healthcare professionals and deaf patients is challenging, and the current COVID-19 pandemic makes this issue even more acute. Sign language interpreters can often not enter hospitals and face masks make lipreading impossible. To address this urgent problem, we developed a system which allows healthcare professionals to translate sentences that are frequently used in the diagnosis and treatment of COVID-19 into Sign Language of the Netherlands NGT. Translations are displayed by means of videos and avatar animations. The architecture of the system is such that it could be extended to other applications and other sign languages in a relatively straightforward way.",https://aclanthology.org/2021.triton-1.13,INCOMA Ltd.,2021,July,Proceedings of the Translation and Interpreting Technology Online Conference,"Roelofsen, Floris  and
Esselink, Lyke  and
Mende-Gillings, Shani  and
Smeijers, Anika",Sign Language Translation in a Healthcare Setting,10.26615/978-954-452-071-7_013,triton,432
2021.paclic-1.47,"['Audio Generation and Processing', 'Data Management and Generation', 'Learning Paradigms', 'Low-resource Languages', 'Cross-lingual Application']","['Transfer Learning', 'Data Preparation']",,"Speaker verification is an essential task in speech processing with great authentication and surveillance applications. Large-scale datasets have hugely contributed to the success of neural networks for speaker verification. However, in low-resource languages, building such massive datasets is infeasible. This paper aims at proposing a speaker verification model for low-resource scenarios, with the baseline from Clova's H/ASP system in VoxSRC 2020. The proposed method adopts transfer learning to utilize the knowledge from the model trained with VoxCeleb, an English large-scale dataset. For network optimization, Stochastic Gradient Descent is employed instead of Adam because of its superior generalization. This work also proposes a novel marginal variant of Angular Prototypical AP Loss, i.e. Angular Margin Prototypical AMP Loss, which encourages a more discriminative embedding space. To experiment with the proposed model, we investigated building a public speaker verification dataset for Vietnamese. A processing pipeline is proposed to enhance the quality of the dataset. After being collected from sources, noisy speakers and noisy utterances are removed using selfsimilarity matrix analysis. Speakers with the same identity are then unified. The experimental results show that the proposed model achieves an Equal Error Rate EER of 3.1% which outperforms the baseline with 7.6% EER on the collected dataset.",https://aclanthology.org/2021.paclic-1.47,Association for Computational Lingustics,2021,11,"Proceedings of the 35th Pacific Asia Conference on Language, Information and Computation","Thanh, Dat Vi  and
Viet, Thanh Pham  and
Thu, Trang Nguyen Thi",Deep Speaker Verification Model for Low-Resource Languages and Vietnamese Dataset,,paclic,898
L18-1497,"['Data Management and Generation', 'Low-resource Languages', 'Classification Applications']","['Data Augmentation', 'Data Preparation']",,"Discriminating between similar languages DSL on conversational texts is a challenging task. This paper aims at discriminating between limited-resource languages on short conversational texts, like Uyghur and Kazakh. Considering that Uyghur and Kazakh data are severely imbalanced, we leverage an effective compensation strategy to build a balanced Uyghur and Kazakh corpus. Then we construct a maximum entropy classifier based on morphological features to discriminate between the two languages and investigate the contribution of each feature. Empirical results suggest that our system achieves an accuracy of 95.7% on our Uyghur and Kazakh dataset, which is higher than that of the CNN classifier. We also apply our system to the out-of-domain subtasks of VarDial'2016 DSL shared tasks to test the system's performance on short conversational texts of other similar languages. Though with much less preprocessing, our system outperforms the champions on both test sets B1 and B2.",https://aclanthology.org/L18-1497,European Language Resources Association (ELRA),2018,May,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),"He, Junqing  and
Huang, Xian  and
Zhao, Xuemin  and
Zhang, Yan  and
Yan, Yonghong",Discriminating between Similar Languages on Imbalanced Conversational Texts,,L18,1245
W17-5019,['Error Detection and Correction'],['Grammatical Error Correction (GEC)'],,"The field of grammatical error correction GEC has made tremendous bounds in the last ten years, but new questions and obstacles are revealing themselves. In this position paper, we discuss the issues that need to be addressed and provide recommendations for the field to continue to make progress, and propose a new shared task. We invite suggestions and critiques from the audience to make the new shared task a community-driven venture.",https://aclanthology.org/W17-5019,Association for Computational Linguistics,2017,September,Proceedings of the 12th Workshop on Innovative Use of {NLP} for Building Educational Applications,"Sakaguchi, Keisuke  and
Napoles, Courtney  and
Tetreault, Joel",GEC into the future: Where are we going and how do we get there?,10.18653/v1/W17-5019,W17,1276
2020.nlpcovid19-acl.17,"['Domain-specific NLP', 'Data Management and Generation', 'Classification Applications', 'Learning Paradigms']","['Transfer Learning', 'Medical and Clinical NLP', 'Misinformation Detection', 'Multilabel Text Classification', 'Data Preparation']",,"We present a simple NLP methodology for detecting COVID-19 misinformation videos on YouTube by leveraging user comments. We use transfer learning pre-trained models to generate a multi-label classifier that can categorize conspiratorial content. We use the percentage of misinformation comments on each video as a new feature for video classification. We show that the inclusion of this feature in simple models yields an accuracy of up to 82.2%. Furthermore, we verify the significance of the feature by performing a Bayesian analysis. Finally, we show that adding the first hundred comments as tf-idf features increases the video classifier accuracy by up to 89.4%.",https://aclanthology.org/2020.nlpcovid19-acl.17,Association for Computational Linguistics,2020,July,Proceedings of the 1st Workshop on {NLP} for {COVID-19} at {ACL} 2020,"Medina Serrano, Juan Carlos  and
Papakyriakopoulos, Orestis  and
Hegelich, Simon",NLP-based Feature Extraction for the Detection of COVID-19 Misinformation Videos on YouTube,,nlpcovid19,857
U17-1011,"['Learning Paradigms', 'Classification Applications', 'Model Architectures']",['Supervised Learning'],,"Ensemble techniques are powerful approaches that combine several weak learners to build a stronger one. As a meta learning framework, ensemble techniques can easily be applied to many machine learning techniques. In this paper we propose a neural network extended with an ensemble loss function for text classification. The weight of each weak loss function is tuned within the training phase through the gradient propagation optimization method of the neural network. The approach is evaluated on several text classification datasets. We also evaluate its performance in various environments with several degrees of label noise. Experimental results indicate an improvement of the results and strong resilience against label noise in comparison with other methods.",https://aclanthology.org/U17-1011,,2017,December,Proceedings of the Australasian Language Technology Association Workshop 2017,"Hajiabadi, Hamideh  and
Molla-Aliod, Diego  and
Monsefi, Reza",On Extending Neural Networks with Loss Ensembles for Text Classification,10.48550/arxiv.1711.05170,U17,731
2020.nlp4call-1.3,"['Dialogue Systems', 'Low-resource Languages']",['Chatbots'],,"This paper explores the impact on language proficiency of comprehensible output applied in computer assisted language learning CALL. Targeting speakers of intermediate level, we adapted a visually-grounded dialogue task, optimizing for language acquisition. The task was implemented as a mobile application where learners are organized in pairs and write short texts to play an imageguessing game, producing samples in a wide variety of languages. Following a framework for CALL evaluation, we conducted an analysis of the game and players' gains through time, including the measure of pre-trained XLM-r cross-lingual transformers' acceptability score of the samples. The results confirm the intended fit for intermediate speakers as well as reveal possible benefits for other levels. This research provides a successful case study of a multilingual CALL design where users have the autonomy to generate output creatively.",https://aclanthology.org/2020.nlp4call-1.3,LiU Electronic Press,2020,November,Proceedings of the 9th Workshop on NLP for Computer Assisted Language Learning,"da Cruz Dalcol, Etiene  and
Poesio, Massimo",Polygloss - A conversational agent for language practice,10.3384/ecp2017521,nlp4call,1355
2020.wosp-1.5,['Information Retrieval'],['Information Filtering'],['Recommender Systems'],"Effectiveness of a recommendation in an Information Retrieval IR system is determined by relevancy scores of retrieved results. Term weighting is responsible for computing the relevance scores and consequently differentiating between the terms in a document. However, current term weighting formula like TF-IDF weigh terms only based on term frequency and inverse document frequency irrespective of other important factors. This results in uncertainty in cases when both TF and IDF values are same for more than one document, hence resulting in same term weight values. In this paper, we propose a modification of TF-IDF and other term-weighting schemes that weights terms additionally based on the recency of a term, i.e. the metric based on the year the term occurred for the first time and the document frequency. We modified the term weighting schemes TF-IDF, BM25 and Universal Sentence Encoder USE to additionally consider the recency of a term and evaluated them on three datasets. Our modified TF-IDF outperformed the standard TF-IDF on all three datasets; the modified USE outperformed the standard USE on two of the three datasets; the modified BM25 did not outperform the standard BM25 term-weighting scheme.",https://aclanthology.org/2020.wosp-1.5,Association for Computational Linguistics,2020,05-Aug,Proceedings of the 8th International Workshop on Mining Scientific Publications,"Marwah, Divyanshu  and
Beel, Joeran","Term-Recency for TF-IDF, BM25 and USE Term Weighting",,wosp,225
2020.smm4h-1.1,"['Domain-specific NLP', 'Data Management and Generation']","['Medical and Clinical NLP', 'Data Analysis', 'NLP for News and Media']",['NLP for Social Media'],"Social media platforms offer extensive information about the development of the COVID-19 pandemic and the current state of public health. In recent years, the Natural Language Processing community has developed a variety of methods to extract health-related information from posts on social media platforms. In order for these techniques to be used by a broad public, they must be aggregated and presented in a user-friendly way. We have aggregated ten methods to analyze tweets related to the COVID-19 pandemic, and present interactive visualizations of the results on our online platform, the COVID-19 Twitter Monitor. In the current version of our platform, we offer distinct methods for the inspection of the dataset, at different levels: corpuswide, single post, and spans within each post. Besides, we allow the combination of different methods to enable a more selective acquisition of knowledge. Through the visual and interactive combination of various methods, interconnections in the different outputs can be revealed.",https://aclanthology.org/2020.smm4h-1.1,Association for Computational Linguistics,2020,December,Proceedings of the Fifth Social Media Mining for Health Applications Workshop {\&} Shared Task,"Cornelius, Joseph  and
Ellendorff, Tilia  and
Furrer, Lenz  and
Rinaldi, Fabio",COVID-19 Twitter Monitor: Aggregating and Visualizing COVID-19 Related Trends in Social Media,,smm4h,435
2021.scil-1.56,"['Data Management and Generation', 'Low-resource Languages']",['Data Preparation'],['Annotation Processes'],"We present in-progress annotation of semantic relations expressed through adpositions and case markers in a Hindi corpus. We used the multilingual SNACS annotation scheme, which has been applied to a variety of typologically diverse languages. Annotation problems in Hindi are examined and used to suggest changes to SNACS. We look towards finalizing the corpus and using it for future work in typology and semantic role-dependent tasks.",https://aclanthology.org/2021.scil-1.56,Association for Computational Linguistics,2021,February,Proceedings of the Society for Computation in Linguistics 2021,"Arora, Aryaman  and
Venkateswaran, Nitin  and
Schneider, Nathan",SNACS Annotation of Case Markers and Adpositions in Hindi,10.7275/hfme-g116,scil,1110
2021.clpsych-1.1,"['Information Extraction', 'Domain-specific NLP', 'Data Management and Generation', 'Ethics']","['NLP for News and Media', 'Data Preparation', 'Medical and Clinical NLP']","['NLP for Social Media', 'Annotation Processes', 'NLP for Mental Health']","Recently, research on mental health conditions using public online data, including Reddit, has surged in NLP and health research but has not reported user characteristics, which are important to judge generalisability of findings. This paper shows how existing NLP methods can yield information on clinical, demographic, and identity characteristics of almost 20K Reddit users who self-report a bipolar disorder diagnosis. This population consists of slightly more feminine-than masculinegendered mainly young or middle-aged USbased adults who often report additional mental health diagnoses, which is compared with general Reddit statistics and epidemiological studies. Additionally, this paper carefully evaluates all methods and discusses ethical issues.",https://aclanthology.org/2021.clpsych-1.1,Association for Computational Linguistics,2021,June,Proceedings of the Seventh Workshop on Computational Linguistics and Clinical Psychology: Improving Access,"Jagfeld, Glorianna  and
Lobban, Fiona  and
Rayson, Paul  and
Jones, Steven",Understanding who uses Reddit: Profiling individuals with a self-reported bipolar disorder diagnosis,10.18653/v1/2021.clpsych-1.1,clpsych,100
U19-1022,"['Text Preprocessing', 'Domain-specific NLP', 'Model Architectures']","['Transformer Models', 'Text Segmentation', 'Medical and Clinical NLP']","['Word Segmentation', 'Biomedical NLP']","Transformer-based models have been popular recently and have improved performance for many Natural Language Processing NLP Tasks, including those in the biomedical field. Previous research suggests that, when using these models, an in-domain vocabulary is more suitable than using an open-domain vocabulary. We investigate the effects of a specialised in-domain vocabulary trained from scratch on a biomedical corpus. Our research suggests that, although the in-domain vocabulary is useful, it is usually constrained by the corpora size because these models needs to be trained from scratch. Instead, it is more useful to have more data, perform additional pretraining steps with a corpus-specific vocabulary. 1",https://aclanthology.org/U19-1022,Australasian Language Technology Association,2019,4--6 December,Proceedings of the The 17th Annual Workshop of the Australasian Language Technology Association,"Nguyen, Vincent  and
Karimi, Sarvnaz  and
Xing, Zhenchang",Investigating the Effect of Lexical Segmentation in Transformer-based Models on Medical Datasets,,U19,1085
C18-1034,"['Text Generation', 'Data Management and Generation', 'Low-resource Languages']","['Data Preparation', 'Text Simplification']",,"Effective textual communication depends on readers being proficient enough to comprehend texts, and texts being clear enough to be understood by the intended audience, in a reading task. When the meaning of textual information and instructions is not well conveyed, many losses and damages may occur. Among the solutions to alleviate this problem is the automatic evaluation of sentence readability, task which has been receiving a lot of attention due to its large applicability. However, a shortage of resources, such as corpora for training and evaluation, hinders the full development of this task. In this paper, we generate a nontrivial sentence corpus in Portuguese. We evaluate three scenarios for building it, taking advantage of a parallel corpus of simplification, in which each sentence triplet is aligned and has simplification operations annotated, being ideal for justifying possible mistakes of future methods. The best scenario of our corpus PorSimplesSent is composed of 4,888 pairs, which is bigger than a similar corpus for English; all the three versions of it are publicly available. We created four baselines for PorSimplesSent and made available a pairwise ranking method, using 17 linguistic and psycholinguistic features, which correctly identifies the ranking of sentence pairs with an accuracy of 74.2%.",https://aclanthology.org/C18-1034,Association for Computational Linguistics,2018,August,Proceedings of the 27th International Conference on Computational Linguistics,"Leal, Sidney Evaldo  and
Duran, Magali Sanches  and
Alu{\'\i}sio, Sandra Maria",A Nontrivial Sentence Corpus for the Task of Sentence Readability Assessment in Portuguese,,C18,29
2021.tacl-1.8,"['Multilingual NLP', 'Parsing', 'Low-resource Languages', 'Model Architectures']","['Transformer Models', 'Syntactic Parsing']",['Dependency Parsing'],"We propose the Recursive Non-autoregressive Graph-to-Graph Transformer architecture RNGTr for the iterative refinement of arbitrary graphs through the recursive application of a non-autoregressive Graph-to-Graph Transformer and apply it to syntactic dependency parsing. We demonstrate the power and effectiveness of RNGTr on several dependency corpora, using a refinement model pre-trained with BERT. We also introduce Syntactic Transformer SynTr, a non-recursive parser similar to our refinement model. RNGTr can improve the accuracy of a variety of initial parsers on 13 languages from the Universal Dependencies Treebanks, English and Chinese Penn Treebanks, and the German CoNLL2009 corpus, even improving over the new state-ofthe-art results achieved by SynTr, significantly improving the state-of-the-art for all corpora tested.",https://aclanthology.org/2021.tacl-1.8,MIT Press,2021,,,"Mohammadshahi, Alireza  and
Henderson, James",Recursive Non-Autoregressive Graph-to-Graph Transformer for Dependency Parsing with Iterative Refinement,10.1162/tacl_a_00358,tacl,124
2020.computerm-1.10,"['Error Detection and Correction', 'Information Extraction', 'Low-resource Languages']",,,"Terminology extraction procedure usually consists of selecting candidates for terms and ordering them according to their importance for the given text or set of texts. Depending on the method used, a list of candidates contains different fractions of grammatically incorrect, semantically odd and irrelevant sequences. The aim of this work was to improve term candidate selection by reducing the number of incorrect sequences using a dependency parser for Polish.",https://aclanthology.org/2020.computerm-1.10,European Language Resources Association,2020,May,Proceedings of the 6th International Workshop on Computational Terminology,"Marciniak, Malgorzata  and
Rychlik, Piotr  and
Mykowiecka, Agnieszka",Supporting terminology extraction with dependency parses,,computerm,15
2020.sdp-1.7,"['Information Retrieval', 'Embeddings', 'Domain-specific NLP']",['NLP for Bibliometrics and Scientometrics'],,"Expert search aims to find and rank experts based on a user's query. In academia, retrieving experts is an efficient way to navigate through a large amount of academic knowledge. Here, we study how different distributed representations of academic papers i.e. embeddings impact academic expert retrieval. We use the Microsoft Academic Graph dataset and experiment with different configurations of a document-centric voting model for retrieval. In particular, we explore the impact of the use of contextualized embeddings on search performance. We also present results for paper embeddings that incorporate citation information through retrofitting. Additionally, experiments are conducted using different techniques for assigning author weights based on author order. We observe that using contextual embeddings produced by a transformer model trained for sentence similarity tasks produces the most effective paper representations for document-centric expert retrieval. However, retrofitting the paper embeddings and using elaborate author contribution weighting strategies did not improve retrieval performance.",https://aclanthology.org/2020.sdp-1.7,Association for Computational Linguistics,2020,November,Proceedings of the First Workshop on Scholarly Document Processing,"Berger, Mark  and
Zavrel, Jakub  and
Groth, Paul",Effective distributed representations for academic expert search,10.18653/v1/2020.sdp-1.7,sdp,1435
J16-4011,['Domain-specific NLP'],['NLP for News and Media'],['NLP for Social Media'],"Today, social media refers to a wide range of Web sites and Internet-based services that allow users to create content and interact with other users. Some of these tools, such as multi-party chats, discussion forums, blogs, and online reviews, have been a focus of natural language processing NLP research for quite some time now. But within the last decade, NLP work has expanded rapidly to cover an immense variety of new social media content-microblogs such as Twitter, social networks such as Facebook, comments on news articles, captions on user-contributed images such as on Flickr, and forums dedicated to specialized topics and needs e.g., health and online education. Simultaneously, many other research communities are carrying out work using social media data-information science, information retrieval, network science, social media analytics, social science, psychology, and corpus linguistics. Today, a large number of businesses are also centered on, or benefit from, analytics performed on social media. Given these myriad research and commercial interests in the social media domain, we are at a time where we should seek to clearly understand what role NLP has in the field of social media analysis, both in terms of the key and interesting language questions, as well as contributions NLP can make to the research carried out in other fields. In this context, this short book by Farzindar and Inkpen is timely and exciting, filling an obvious gap.",https://aclanthology.org/J16-4011,MIT Press,2016,December,,"Louis, Annie",Book Reviews: Natural Language Processing for Social Media by Atefeh Farzindar and Diana Inkpen,10.1162/COLI_r_00270,J16,1252
2020.globalex-1.2,"['Data Management and Generation', 'Knowledge Representation and Reasoning', 'Low-resource Languages']",['Data Preparation'],,"This paper reports on an extended version of a synonym verb class lexicon, newly called SynSemClass formerly CzEngClass. This lexicon stores cross-lingual semantically similar verb senses in synonym classes extracted from a richly annotated parallel corpus, the Prague Czech-English Dependency Treebank. When building the lexicon, we make use of predicate-argument relations valency and link them to semantic roles; in addition, each entry is linked to several external lexicons of more or less ""semantic"" nature, namely FrameNet, WordNet, VerbNet, OntoNotes and PropBank, and Czech VALLEX. The aim is to provide a linguistic resource that can be used to compare semantic roles and their syntactic properties and features across languages within and across synonym groups classes, or 'synsets', as well as gold standard data for automatic NLP experiments with such synonyms, such as synonym discovery, feature mapping, etc. However, perhaps the most important goal is to eventually build an event type ontology that can be referenced and used as a human-readable and human-understandable ""database"" for all types of events, processes and states. While the current paper describes primarily the content of the lexicon, we are also presenting a preliminary design of a format compatible with Linked Data, on which we are hoping to get feedback during discussions at the workshop. Once the resource in whichever form is applied to corpus annotation, deep analysis will be possible using such combined resources as training data.",https://aclanthology.org/2020.globalex-1.2,European Language Resources Association,2020,May,Proceedings of the 2020 Globalex Workshop on Linked Lexicography,"Uresova, Zdenka  and
Fucikova, Eva  and
Hajicova, Eva  and
Hajic, Jan",SynSemClass Linked Lexicon: Mapping Synonymy between Languages,,globalex,0
W17-2348,"['Question Answering (QA)', 'Learning Paradigms', 'Domain-specific NLP']","['Multimodal Learning', 'Medical and Clinical NLP']",['Biomedical NLP'],"This paper describes the participation of USTB PRIR team in the 2017 BioASQ 5B on question answering, including document retrieval, snippet retrieval and concept retrieval task. We introduce different multimodal query processing strategies to enrich query terms and assign different weights to them. Specifically, sequential dependence model SDM, pseudo relevance feedback PRF, fielded sequential dependence model FSDM and Divergence from Randomness model D-FRM are respectively performed on different fields of PubMed articles, sentences extracted from relevant articles, the five terminologies or ontologies MeSH, GO, Jochem, Uniprot and DO to achieve better search performances. Preliminary results show that our systems outperform others in the document and snippet retrieval task in the first two batches.",https://aclanthology.org/W17-2348,Association for Computational Linguistics,2017,August,{B}io{NLP} 2017,"Jin, Zan-Xia  and
Zhang, Bo-Wen  and
Fang, Fan  and
Zhang, Le-Le  and
Yin, Xu-Cheng",A Multi-strategy Query Processing Approach for Biomedical Question Answering: USTB\_PRIR at BioASQ 2017 Task 5B,10.18653/v1/W17-2348,W17,12
2020.msr-1.5,"['Knowledge Representation and Reasoning', 'Classification Applications', 'Low-resource Languages']",['Sentiment Analysis (SA)'],,"Languages with large vocabularies pose a challenge for lexical induction especially if they are too low in resources to train sophisticated language models. The situation becomes even more challenging if the language lacks a standard orthography, such that spelling variants cannot be referenced with a standard format for lexical simplification. We propose a simple approach to induce morphological and orthographic forms in low-resourced NLP and prove its potential on a non-standard language as a use case.",https://aclanthology.org/2020.msr-1.5,Association for Computational Linguistics,2020,December,Proceedings of the Third Workshop on Multilingual Surface Realisation,"Tobaili, Taha",Lexical Induction of Morphological and Orthographic Forms for Low-Resourced Languages,,msr,749
2022.lchange-1.9,"['Learning Paradigms', 'Low-resource Languages', 'Classification Applications']",['Supervised Learning'],,"Computational approaches in historical linguistics have been increasingly applied during the past decade and many new methods that implement parts of the traditional comparative method have been proposed. Despite these increased efforts, there are not many easy-to-use and fast approaches for the task of phonological reconstruction. Here we present a new framework that combines state-of-the-art techniques for automated sequence comparison with novel techniques for phonetic alignment analysis and sound correspondence pattern detection to allow for the supervised reconstruction of word forms in ancestral languages. We test the method on a new dataset covering six groups from three different language families. The results show that our method yields promising results while at the same time being not only fast but also easy to apply and expand.",https://aclanthology.org/2022.lchange-1.9,Association for Computational Linguistics,2022,May,Proceedings of the 3rd Workshop on Computational Approaches to Historical Language Change,"List, Johann-Mattis  and
Forkel, Robert  and
Hill, Nathan",A New Framework for Fast Automated Phonological Reconstruction Using Trimmed Alignments and Sound Correspondence Patterns,10.18653/v1/2022.lchange-1.9,lchange,772
2022.wordplay-1.4,"['Question Answering (QA)', 'Learning Paradigms', 'Model Architectures']","['Transformer Models', 'Reinforcement Learning']",,"Interactive Question Answering IQA requires an intelligent agent to interact with a dynamic environment in order to gather information necessary to answer a question. IQA tasks have been proposed as means of training systems to develop language or visual comprehension abilities. To this end, the Question Answering with Interactive Text QAit task was created to produce and benchmark interactive agents capable of seeking information and answering questions in unseen environments. While prior work has exclusively focused on IQA as a reinforcement learning problem, such methods suffer from low sample efficiency and poor accuracy in zero-shot evaluation. In this paper, we propose the use of the recently proposed Decision Transformer architecture to provide improvements upon prior baselines. By utilising a causally masked GPT-2 Transformer for command generation and a BERT model for question answer prediction, we show that the Decision Transformer achieves performance greater than or equal to current state-of-the-art RL baselines on the QAit task in a sample efficient manner. In addition, these results are achievable by training on sub-optimal random trajectories, therefore not requiring the use of online agents to gather data.",https://aclanthology.org/2022.wordplay-1.4,Association for Computational Linguistics,2022,July,Proceedings of the 3rd Wordplay: When Language Meets Games Workshop (Wordplay 2022),"Furman, Gregory  and
Toledo, Edan  and
Shock, Jonathan  and
Buys, Jan",A Sequence Modelling Approach to Question Answering in Text-Based Games,10.18653/v1/2022.wordplay-1.4,wordplay,388
Y16-1002,['Domain-specific NLP'],['Medical and Clinical NLP'],['Biomedical NLP'],"Large amounts of biomedical corpora have emerged from different sources, including scientific literature, lab notes, patents and electronic health records. Most of the efforts in biomedical text mining have focused on the extraction and linkage of specific facts, such as molecular interactions, links between genes and diseases, or patients' symptoms. Such facts are rarely contextualised using the associated scientific or professional methodology e.g. what methods were used to detect particular interaction, or to diagnose a particular disease. However, methods are the vital, but often neglected, under-pinning of science and practice. Given enough data, the ability to extract methodological knowledge would allow us to ""infer"" common and possibly best practice for a given task, and thus indeed learn from vast amount of text. This is obviously a complex task that involves identification, representation and linking of steps in associated methods, requiring a series of NLP methods such as temporal information extraction and discourse analysis. In this talk we will explore finding out what methods are being used to do what experiment from the literature, or to infer what clinical pathways patients have followed, based on the notes in their electronic health records. We will illustrate some of the work in the context of bioinformatics e.g. recovering a general view of the methods described in the literature and clinical practice e.g. reconstruction of patient journeys. We will also discuss how feasible this task is given the known issues with the lack of reported details needed for understanding and reproducibility of associated methods i.e. how much of a method is indeed present in the literate or clinical records.",https://aclanthology.org/Y16-1002,,2016,October,"Proceedings of the 30th Pacific Asia Conference on Language, Information and Computation: Keynote Speeches and Invited Talks","Nenadic, Goran",Inferring Methodological Meta-knowledge from Large Biomedical Corpora,,Y16,687
2020.wnut-1.76,"['Domain-specific NLP', 'Information Extraction']","['Event Extraction', 'NLP for News and Media']",['NLP for Social Media'],"The competition of extracting COVID-19 events from Twitter is to develop systems that can automatically extract related events from tweets. The built system should identify different pre-defined slots for each event, in order to answer important questions e.g., Who is tested positive? What is the age of the person? Where is he/she?. To tackle these challenges, we propose the Joint Event Multitask Learning JOELIN model. Through a unified global learning framework, we make use of all the training data across different events to learn and fine-tune the language model. Moreover, we implement a type-aware post-processing procedure using named entity recognition NER to further filter the predictions. JOELIN outperforms the BERT baseline by 17.2% in micro F1. 1",https://aclanthology.org/2020.wnut-1.76,Association for Computational Linguistics,2020,November,Proceedings of the Sixth Workshop on Noisy User-generated Text (W-NUT 2020),"Chen, Chacha  and
Huang, Chieh-Yang  and
Hou, Yaqi  and
Shi, Yang  and
Dai, Enyan  and
Wang, Jiaqi",TEST\_POSITIVE at W-NUT 2020 Shared Task-3: Cross-task modeling,10.18653/v1/2020.wnut-1.76,wnut,447
2021.depling-1.10,"['Data Management and Generation', 'Information Extraction', 'Low-resource Languages']","['Data Preparation', 'Data Analysis', 'Coreference Resolution']",['Annotation Processes'],"We present an empirical study that compares mention heads as annotated manually in four coreference datasets for Dutch, English, Polish, and Russian on one hand, with heads induced from dependency trees parsed automatically, on the other hand. For parsing, we used UDPipe 2.6, a modern parser trained using the Universal Dependencies collection. We show that majority of mismatches 64%-94% can be attributed to several classes of systematic differences in how the notion of head is treated in the respective data resources, while mismatches caused by parsing errors are relatively rare 4%-15%. Our conclusion is that consistency would be gained in and across coreference resources after migration to UD-style mention heads, without losing substantial information. This can be achieved with sufficient accuracy using modern dependency parsers even for coreference corpora that lack manual head annotation.",https://aclanthology.org/2021.depling-1.10,Association for Computational Linguistics,2021,December,"Proceedings of the Sixth International Conference on Dependency Linguistics (Depling, SyntaxFest 2021)","Nedoluzhko, Anna  and
Nov{\'a}k, Michal  and
Popel, Martin  and
{\v{Z}}abokrtsk{\'y}, Zden{\v{e}}k  and
Zeman, Daniel",Is one head enough? Mention heads in coreference annotations compared with UD-style heads,10.18653/v1/2023.crac-sharedtask,depling,1004
2021.isa-1.6,"['Audio Generation and Processing', 'Evaluation Techniques', 'Low-resource Languages']",['Speech Synthesis'],,"This paper presents work carried out to transform glosses of a fable in Italian Sign Language LIS into a text which is then read by a TTS synthesizer from an SSML modified version of the same text. Whereas many systems exist that generate sign language from a text, we decided to do the reverse operation and generate text from LIS. For that purpose we used a version of the fable The Tortoise and the Hare, signed and made available on Youtube by ALBA cooperativa sociale, which was annotated manually by second author for her master's thesis. In order to achieve our goal, we converted the multilayer glosses into linear Prolog terms to be fed to the generator. In the paper we focus on the main problems encountered in the transformation of the glosses into a semantically and pragmatically consistent representation. The main problems have been caused by the complexities of a text like a fable which requires coreference mechanisms and speech acts to be implemented in the representation which are often unexpressed and constitute implicit information.",https://aclanthology.org/2021.isa-1.6,Association for Computational Linguistics,2021,June,Proceedings of the 17th Joint ACL - ISO Workshop on Interoperable Semantic Annotation,"Delmonte, Rodolfo  and
Trolvi, Serena  and
Stiffoni, Francesco",Converting Multilayer Glosses into Semantic and Pragmatic forms with GENLIS,,isa,1104
K19-1087,['Model Architectures'],,,"In this paper, we focus on quantifying model stability as a function of random seed by investigating the effects of the induced randomness on model performance and the robustness of the model in general. We specifically perform a controlled study on the effect of random seeds on the behaviour of attention, gradientbased and surrogate model based LIME interpretations. Our analysis suggests that random seeds can adversely affect the consistency of models resulting in counterfactual interpretations. We propose a technique called Aggressive Stochastic Weight Averaging ASWA and an extension called Norm-filtered Aggressive Stochastic Weight Averaging NASWA which improves the stability of models over random seeds. With our ASWA and NASWA based optimization, we are able to improve the robustness of the original model, on average reducing the standard deviation of the model's performance by 72%.",https://aclanthology.org/K19-1087,Association for Computational Linguistics,2019,November,Proceedings of the 23rd Conference on Computational Natural Language Learning (CoNLL),"Madhyastha, Pranava  and
Jain, Rishabh",On Model Stability as a Function of Random Seed,10.18653/v1/K19-1087,K19,597
L16-1341,"['Data Management and Generation', 'Image and Video Processing']",['Data Preparation'],,"We present a newly crowd-sourced data set of natural language references to objects anchored in complex urban scenes In short: The REAL Corpus -Referring Expressions Anchored Language. The REAL corpus contains a collection of images of real-world urban scenes together with verbal descriptions of target objects generated by humans, paired with data on how successful other people were able to identify the same object based on these descriptions. In total, the corpus contains 32 images with on average 27 descriptions per image and 3 verifications for each description. In addition, the corpus is annotated with a variety of linguistically motivated features. The paper highlights issues posed by collecting data using crowd-sourcing with an unrestricted input format, as well as using real-world urban scenes. The corpus will be released via the ELRA repository as part of this submission.",https://aclanthology.org/L16-1341,European Language Resources Association (ELRA),2016,May,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),"Bartie, Phil  and
Mackaness, William  and
Gkatzia, Dimitra  and
Rieser, Verena",The REAL Corpus: A Crowd-Sourced Corpus of Human Generated and Evaluated Spatial References to Real-World Urban Scenes,,L16,1244
2020.readi-1.14,"['Information Extraction', 'Text Generation', 'Data Management and Generation', 'Low-resource Languages']","['Text Simplification', 'Data Preparation', 'Coreference Resolution']",,"Text simplification aims at adapting documents to make them easier to read by a given audience. Usually, simplification systems consider only lexical and syntactic levels, and, moreover, are often evaluated at the sentence level. Thus, studies on the impact of simplification in text cohesion are lacking. Some works add coreference resolution in their pipeline to address this issue. In this paper, we move forward in this direction and present a rule-based system for automatic text simplification, aiming at adapting French texts for dyslexic children. The architecture of our system takes into account not only lexical and syntactic but also discourse information, based on coreference chains. Our system has been manually evaluated in terms of grammaticality and cohesion. We have also built and used an evaluation corpus containing multiple simplification references for each sentence. It has been annotated by experts following a set of simplification guidelines, and can be used to run automatic evaluation of other simplification systems. Both the system and the evaluation corpus are freely available.",https://aclanthology.org/2020.readi-1.14,European Language Resources Association,2020,May,Proceedings of the 1st Workshop on Tools and Resources to Empower People with REAding DIfficulties (READI),"Wilkens, Rodrigo  and
Oberle, Bruno  and
Todirascu, Amalia",Coreference-Based Text Simplification,,readi,661
Q16-1023,"['Parsing', 'Low-resource Languages', 'Model Architectures']","['Syntactic Parsing', 'Recurrent Neural Networks (RNNs)']","['Dependency Parsing', 'Long Short-Term Memory (LSTM) Models']","We present a simple and effective scheme for dependency parsing which is based on bidirectional-LSTMs BiLSTMs. Each sentence token is associated with a BiLSTM vector representing the token in its sentential context, and feature vectors are constructed by concatenating a few BiLSTM vectors. The BiLSTM is trained jointly with the parser objective, resulting in very effective feature extractors for parsing. We demonstrate the effectiveness of the approach by applying it to a greedy transition-based parser as well as to a globally optimized graph-based parser. The resulting parsers have very simple architectures, and match or surpass the state-of-the-art accuracies on English and Chinese.",https://aclanthology.org/Q16-1023,MIT Press,2016,,,"Kiperwasser, Eliyahu  and
Goldberg, Yoav",Simple and Accurate Dependency Parsing Using Bidirectional LSTM Feature Representations,10.1162/tacl_a_00101,Q16,374
K19-1069,"['Dialogue Systems', 'Model Architectures']",['Chatbots'],,"We consider the importance of different utterances in the context for selecting the response usually depends on the current query. 1 In this paper, we propose the model TripleNet to fully model the task with the triple context, query, response instead of context, response in previous works. The heart of TripeNet is a novel attention mechanism named triple attention to model the relationships within the triple at four levels. The new mechanism updates the representation for each element based on the attention with the other two concurrently and symmetrically. We match the triple C, Q, R centered on the response from char to context level for prediction. Experimental results on two large-scale multi-turn response selection datasets show that the proposed model can significantly outperform the state-of-the-art methods. 2",https://aclanthology.org/K19-1069,Association for Computational Linguistics,2019,November,Proceedings of the 23rd Conference on Computational Natural Language Learning (CoNLL),"Ma, Wentao  and
Cui, Yiming  and
Shao, Nan  and
He, Su  and
Zhang, Wei-Nan  and
Liu, Ting  and
Wang, Shijin  and
Hu, Guoping",TripleNet: Triple Attention Network for Multi-Turn Response Selection in Retrieval-Based Chatbots,10.18653/v1/K19-1069,K19,18
2020.nlpbt-1.4,"['Domain-specific NLP', 'Information Extraction', 'Data Management and Generation', 'Learning Paradigms', 'Image and Video Processing']","['Unsupervised Learning', 'Supervised Learning', 'Data Preparation', 'Multimodal Learning']",,"Watching instructional videos are often used to learn about procedures. Video captioning is one way of automatically collecting such knowledge. However, it provides only an indirect, overall evaluation of multimodal models with no finer-grained quantitative measure of what they have learned. We propose instead, a benchmark of structured procedural knowledge extracted from cooking videos. This work is complementary to existing tasks, but requires models to produce interpretable structured knowledge in the form of verb-argument tuples. Our manually annotated open-vocabulary resource includes 356 instructional cooking videos and 15,523 video clip/sentence-level annotations. Our analysis shows that the proposed task is challenging and standard modeling approaches like unsupervised segmentation, semantic role labeling, and visual action detection perform poorly when forced to predict every action of a procedure in structured form.",https://aclanthology.org/2020.nlpbt-1.4,Association for Computational Linguistics,2020,November,Proceedings of the First International Workshop on Natural Language Processing Beyond Text,"Xu, Frank F.  and
Ji, Lei  and
Shi, Botian  and
Du, Junyi  and
Neubig, Graham  and
Bisk, Yonatan  and
Duan, Nan",A Benchmark for Structured Procedural Knowledge Extraction from Cooking Videos,10.18653/v1/2020.nlpbt-1.4,nlpbt,918
2021.unimplicit-1.1,"['Learning Paradigms', 'Classification Applications', 'Discourse Analysis']",['Few-shot Learning'],,"In implicit discourse relation classification, we want to predict the relation between adjacent sentences in the absence of any overt discourse connectives. This is challenging even for humans, leading to shortage of annotated data, a fact that makes the task even more difficult for supervised machine learning approaches. In the current study, we perform implicit discourse relation classification without relying on any labeled implicit relation. We sidestep the lack of data through explicitation of implicit relations to reduce the task to two subproblems: language modeling and explicit discourse relation classification, a much easier problem. Our experimental results show that this method can even marginally outperform the state-of-the-art, in spite of being much simpler than alternative models of comparable performance. Moreover, we show that the achieved performance is robust across domains as suggested by the zero-shot experiments on a completely different domain. This indicates that recent advances in language modeling have made language models sufficiently good at capturing inter-sentence relations without the help of explicit discourse markers.",https://aclanthology.org/2021.unimplicit-1.1,Association for Computational Linguistics,2021,August,Proceedings of the 1st Workshop on Understanding Implicit and Underspecified Language,"Kurfal{\i}, Murathan  and
{\""O}stling, Robert",Let's be explicit about that: Distant supervision for implicit discourse relation classification via connective prediction,10.18653/v1/2021.unimplicit-1.1,unimplicit,1385
L16-1561,"['Domain-specific NLP', 'Machine Translation (MT)', 'Data Management and Generation', 'Multilingual NLP', 'Low-resource Languages']","['Data Preparation', 'Statistical MT (SMT)']",,"This paper describes the creation process and statistics of the official United Nations Parallel Corpus, the first parallel corpus composed from United Nations documents published by the original data creator. The parallel corpus presented consists of manually translated UN documents from the last 25 years 1990 to 2014 for the six official UN languages, Arabic, Chinese, English, French, Russian, and Spanish. The corpus is freely available for download under a liberal license. Apart from the pairwise aligned documents, a fully aligned subcorpus for the six official UN languages is distributed. We provide baseline BLEU scores of our Moses-based SMT systems trained with the full data of language pairs involving English and for all possible translation directions of the six-way subcorpus.",https://aclanthology.org/L16-1561,European Language Resources Association (ELRA),2016,May,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),"Ziemski, Micha{\l}  and
Junczys-Dowmunt, Marcin  and
Pouliquen, Bruno",The United Nations Parallel Corpus v1.0,,L16,507
2022.findings-acl.291,"['Parsing', 'Data Management and Generation', 'Low-resource Languages', 'Model Architectures']","['Data Augmentation', 'Semantic Parsing']",,"We introduce a novel setup for low-resource task-oriented semantic parsing which incorporates several constraints that may arise in real-world scenarios: 1 lack of similar datasets/models from a related domain, 2 inability to sample useful logical forms directly from a grammar, and 3 privacy requirements for unlabeled natural utterances. Our goal is to improve a low-resource semantic parser using utterances collected through user interactions. In this highly challenging but realistic setting, we investigate data augmentation approaches involving generating a set of structured canonical utterances corresponding to logical forms, before simulating corresponding natural language and filtering the resulting pairs. We find that such approaches are effective despite our restrictive setup: in a lowresource setting on the complex SMCalFlow calendaring dataset Andreas et al., 2020, we observe 33% relative improvement over a nondata-augmented baseline in top-1 match.",https://aclanthology.org/2022.findings-acl.291,Association for Computational Linguistics,2022,May,Findings of the Association for Computational Linguistics: ACL 2022,"Yang, Kevin  and
Deng, Olivia  and
Chen, Charles  and
Shin, Richard  and
Roy, Subhro  and
Van Durme, Benjamin",Addressing Resource and Privacy Constraints in Semantic Parsing Through Data Augmentation,10.18653/v1/2022.findings-acl.291,findings,1037
2022.repl4nlp-1.16,"['Classification Applications', 'Adversarial Attacks and Robustness', 'Model Architectures']",,,"State-of-the-art machine learning models are prone to adversarial attacks: Maliciously crafted inputs to fool the model into making a wrong prediction, often with high confidence. While defense strategies have been extensively explored in the computer vision domain, research in natural language processing still lacks techniques to make models resilient to adversarial text inputs. We adapt a technique from computer vision to detect word-level attacks targeting text classifiers. This method relies on training an adversarial detector leveraging Shapley additive explanations and outperforms the current state-of-the-art on two benchmarks. Furthermore, we prove the detector requires only a low amount of training samples and, in some cases, generalizes to different datasets without needing to retrain.",https://aclanthology.org/2022.repl4nlp-1.16,Association for Computational Linguistics,2022,May,Proceedings of the 7th Workshop on Representation Learning for NLP,"Huber, Lukas  and
K{\""u}hn, Marc Alexander  and
Mosca, Edoardo  and
Groh, Georg",Detecting Word-Level Adversarial Text Attacks via SHapley Additive exPlanations,10.18653/v1/2022.repl4nlp-1.16,repl4nlp,801
L18-1164,"['Learning Paradigms', 'Information Extraction', 'Low-resource Languages']","['Unsupervised Learning', 'Word Sense Disambiguation (WSD)']",,"In this paper, we present Watasense, an unsupervised system for word sense disambiguation. Given a sentence, the system chooses the most relevant sense of each input word with respect to the semantic similarity between the given sentence and the synset constituting the sense of the target word. Watasense has two modes of operation. The sparse mode uses the traditional vector space model to estimate the most similar word sense corresponding to its context. The dense mode, instead, uses synset embeddings to cope with the sparsity problem. We describe the architecture of the present system and also conduct its evaluation on three different lexical semantic resources for Russian. We found that the dense mode substantially outperforms the sparse one on all datasets according to the adjusted Rand index.",https://aclanthology.org/L18-1164,European Language Resources Association (ELRA),2018,May,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),"Ustalov, Dmitry  and
Teslenko, Denis  and
Panchenko, Alexander  and
Chernoskutov, Mikhail  and
Biemann, Chris  and
Ponzetto, Simone Paolo",An Unsupervised Word Sense Disambiguation System for Under-Resourced Languages,10.48550/arxiv.1804.10686,L18,657
2021.acl-long.223,"['Machine Translation (MT)', 'Learning Paradigms', 'Low-resource Languages', 'Model Architectures']","['Adversarial Learning', 'Transformer Models', 'Transfer Learning', 'Reinforcement Learning', 'Neural MT (NMT)']",,"Although teacher forcing has become the main training paradigm for neural machine translation, it usually makes predictions only conditioned on past information, and hence lacks global planning for the future. To address this problem, we introduce another decoder, called seer decoder, into the encoder-decoder framework during training, which involves future information in target predictions. Meanwhile, we force the conventional decoder to simulate the behaviors of the seer decoder via knowledge distillation. In this way, at test the conventional decoder can perform like the seer decoder without the attendance of it. Experiment results on the Chinese-English, English-German and English-Romanian translation tasks show our method can outperform competitive baselines significantly and achieves greater improvements on the bigger data sets. Besides, the experiments also prove knowledge distillation the best way to transfer knowledge from the seer decoder to the conventional decoder compared to adversarial learning and L2 regularization.",https://aclanthology.org/2021.acl-long.223,Association for Computational Linguistics,2021,August,Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),"Feng, Yang  and
Gu, Shuhao  and
Guo, Dengji  and
Yang, Zhengxin  and
Shao, Chenze",Guiding Teacher Forcing with Seer Forcing for Neural Machine Translation,10.18653/v1/2021.acl-long.223,acl,1334
2021.iwpt-1.11,"['Learning Paradigms', 'Parsing', 'Low-resource Languages']",['Syntactic Parsing'],['Dependency Parsing'],"We review two features of mixture of experts MoE models which we call averaging and clustering effects in the context of graph-based dependency parsers learned in a supervised probabilistic framework. Averaging corresponds to the ensemble combination of parsers and is responsible for variance reduction which helps stabilizing and improving parsing accuracy. Clustering describes the capacity of MoE models to give more credit to experts believed to be more accurate given an input. Although promising, this is difficult to achieve, especially without additional data. We design an experimental set-up to study the impact of these effects.Whereas averaging is always beneficial, clustering requires good initialization and stabilization techniques, but its advantages over mere averaging seem to eventually vanish when enough experts are present. As a by product, we show how this leads to state-of-the-art results on the PTB and the CoNLL09 Chinese treebank, with low variance across experiments. 89.04 ±0.12",https://aclanthology.org/2021.iwpt-1.11,Association for Computational Linguistics,2021,August,Proceedings of the 17th International Conference on Parsing Technologies and the IWPT 2021 Shared Task on Parsing into Enhanced Universal Dependencies (IWPT 2021),"Zhang, Xudong  and
Le Roux, Joseph  and
Charnois, Thierry",Strength in Numbers: Averaging and Clustering Effects in Mixture of Experts for Graph-Based Dependency Parsing,10.18653/v1/2021.iwpt-1.11,iwpt,762
2022.acl-long.381,"['Parsing', 'Text Generation', 'Data Management and Generation']","['Data Preparation', 'Text-to-SQL']",,"Text-to-SQL parsers map natural language questions to programs that are executable over tables to generate answers, and are typically evaluated on large-scale datasets like SPIDER Yu et al., 2018 . We argue that existing benchmarks fail to capture a certain out-of-domain generalization problem that is of significant practical importance: matching domain specific phrases to composite operations over columns. To study this problem, we propose a synthetic dataset and a re-purposed train/test split of the SQUALL dataset Shi et al., 2020 as new benchmarks to quantify domain generalization over column operations. Our results indicate that existing state-of-the-art parsers struggle in these benchmarks. We propose to address this problem by incorporating prior domain knowledge by preprocessing table schemas, and design a method that consists of two components: schema expansion and schema pruning. This method can be easily applied to multiple existing base parsers, and we show that it significantly outperforms baseline parsers on this domain generalization problem, boosting the underlying parsers' overall performance by up to 13.8% relative accuracy gain 5.1% absolute on the new SQUALL data split.",https://aclanthology.org/2022.acl-long.381,Association for Computational Linguistics,2022,May,Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),"Zhao, Chen  and
Su, Yu  and
Pauls, Adam  and
Platanios, Emmanouil Antonios",Bridging the Generalization Gap in Text-to-SQL Parsing with Schema Expansion,10.18653/v1/2022.acl-long.381,acl,1318
C18-1169,"['Domain-specific NLP', 'Information Extraction', 'Data Management and Generation']","['Data Preparation', 'Named Entity Recognition (NER)', 'NLP for News and Media']",['NLP for Social Media'],"Extracting location names from informal and unstructured social media data requires the identification of referent boundaries and partitioning compound names. Variability, particularly systematic variability in location names Carroll, 1983 , challenges the identification task. Some of this variability can be anticipated as operations within a statistical language model, in this case drawn from gazetteers such as OpenStreetMap OSM, Geonames, and DBpedia. This permits evaluation of an observed n-gram in Twitter targeted text as a legitimate location name variant from the same location-context. Using n-gram statistics and location-related dictionaries, our Location Name Extraction tool LNEx handles abbreviations and automatically filters and augments the location names in gazetteers handling name contractions and auxiliary contents to help detect the boundaries of multi-word location names and thereby delimit them in texts. We evaluated our approach on 4,500 event-specific tweets from three targeted streams to compare the performance of LNEx against that of ten state-of-the-art taggers that rely on standard semantic, syntactic and/or orthographic features. LNEx improved the average F-Score by 33-179%, outperforming all taggers. Further, LNEx is capable of stream processing. 1  This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http:// creativecommons.org/licenses/by/4.0/ 1 Data and the tool is available at https://github.com/halolimat/LNEx 2 We define a targeted stream as a set of tweets that has the potential to satisfy an event-related information need Piskorski and Ehrmann, 2013 crawled using keywords and hashtags, to contextualize the event e.g., ""#HarveyFlood"".",https://aclanthology.org/C18-1169,Association for Computational Linguistics,2018,August,Proceedings of the 27th International Conference on Computational Linguistics,"Al-Olimat, Hussein  and
Thirunarayan, Krishnaprasad  and
Shalin, Valerie  and
Sheth, Amit",Location Name Extraction from Targeted Text Streams using Gazetteer-based Statistical Language Models,,C18,701
2021.wanlp-1.51,"['Learning Paradigms', 'Domain-specific NLP', 'Low-resource Languages', 'Classification Applications']","['Sarcasm Detection', 'Sentiment Analysis (SA)', 'Supervised Learning', 'NLP for News and Media']",['NLP for Social Media'],"Within the last few years, the number of Arabic internet users and Arabic online content is in exponential growth. Dealing with Arabic datasets and the usage of non-explicit sentences to express an opinion are considered to be the major challenges in the field of natural language processing. Hence, sarcasm and sentiment analysis has gained a major interest from the research community, especially in this language. Automatic sarcasm detection and sentiment analysis can be applied using three approaches, namely supervised, unsupervised and hybrid approach. In this paper, a model based on a supervised machine learning algorithm called Support Vector Machine SVM has been used for this process. The proposed model has been evaluated using ArSarcasm-v2 dataset. The performance of the proposed model has been compared with other models submitted to sentiment analysis and sarcasm detection shared task.",https://aclanthology.org/2021.wanlp-1.51,Association for Computational Linguistics,2021,April,Proceedings of the Sixth Arabic Natural Language Processing Workshop,"Nayel, Hamada  and
Amer, Eslam  and
Allam, Aya  and
Abdallah, Hanya",Machine Learning-Based Model for Sentiment and Sarcasm Detection,,wanlp,514
2020.crac-1.10,"['Learning Paradigms', 'Information Extraction', 'Model Architectures']","['Coreference Resolution', 'Recurrent Neural Networks (RNNs)']",['Long Short-Term Memory (LSTM) Models'],"Learning to detect entity mentions without using syntactic information can be useful for integration and joint optimization with other tasks. However, it is common to have partially annotated data for this problem. Here, we investigate two approaches to deal with partial annotation of mentions: weighted loss and soft-target classification. We also propose two neural mention detection approaches: a sequence tagging, and an exhaustive search. We evaluate our methods with coreference resolution as a downstream task, using multitask learning. The results show that the recall and F1 score improve for all methods.",https://aclanthology.org/2020.crac-1.10,Association for Computational Linguistics,2020,December,"Proceedings of the Third Workshop on Computational Models of Reference, Anaphora and Coreference","Miculicich, Lesly  and
Henderson, James",Partially-supervised Mention Detection,10.48550/arxiv.1908.09507,crac,889
2019.nsurl-1.8,"['Data Management and Generation', 'Model Architectures', 'Classification Applications', 'Low-resource Languages']","['Recurrent Neural Networks (RNNs)', 'Data Augmentation']",['Long Short-Term Memory (LSTM) Models'],"In this paper, we describe our team's effort on the semantic text question similarity task of NSURL 2019. Our top performing system utilizes several innovative data augmentation techniques to enlarge the training data. Then, it takes ELMo pre-trained contextual embeddings of the data and feeds them into an ON-LSTM network with self-attention. This results in sequence representation vectors that are used to predict the relation between the question pairs. The model is ranked in the 1st place with 96.499 F1score same as the second place F1-score and the 2nd place with 94.848 F1-score differs by 1.076 F1-score from the first place on the public and private leaderboards, respectively.",https://aclanthology.org/2019.nsurl-1.8,Association for Computational Linguistics,2019,11--12 September,Proceedings of The First International Workshop on NLP Solutions for Under Resourced Languages (NSURL 2019) co-located with ICNLSP 2019 - Short Papers,"Fadel, Ali  and
Tuffaha, Ibraheem  and
Al-Ayyoub, Mahmoud",Tha3aroon at NSURL-2019 Task 8: Semantic Question Similarity in Arabic,10.48550/arxiv.1912.12514,nsurl,190
2021.woah-1.1,"['Learning Paradigms', 'Classification Applications', 'Model Architectures', 'Domain-specific NLP']","['Transformer Models', 'Transfer Learning', 'Hate and Offensive Speech Detection', 'NLP for News and Media']",['NLP for Social Media'],"Offensive language detection OLD has received increasing attention due to its societal impact. Recent work shows that bidirectional transformer based methods obtain impressive performance on OLD. However, such methods usually rely on large-scale well-labeled OLD datasets for model training. To address the issue of data/label scarcity in OLD, in this paper, we propose a simple yet effective domain adaptation approach to train bidirectional transformers. Our approach introduces domain adaptation DA training procedures to ALBERT, such that it can effectively exploit auxiliary data from source domains to improve the OLD performance in a target domain. Experimental results on benchmark datasets show that our approach, AL-BERT DA, obtains the state-of-the-art performance in most cases. Particularly, our approach significantly benefits underrepresented and under-performing classes, with a significant improvement over ALBERT.",https://aclanthology.org/2021.woah-1.1,Association for Computational Linguistics,2021,August,Proceedings of the 5th Workshop on Online Abuse and Harms (WOAH 2021),"Singh, Sumer  and
Li, Sheng",Exploiting Auxiliary Data for Offensive Language Detection with Bidirectional Transformers,10.18653/v1/2021.woah-1.1,woah,248
2021.louhi-1.8,"['Topic Modeling', 'Domain-specific NLP', 'Data Management and Generation']","['Data Preparation', 'Medical and Clinical NLP', 'NLP for News and Media']",['NLP for Social Media'],"This paper applies topic modeling to understand maternal health topics, concerns, and questions expressed in online communities on social networking sites. We examine latent Dirichlet analysis LDA and two state-of-theart methods: neural topic model with knowledge distillation KD and Embedded Topic Model ETM on maternal health texts collected from Reddit. The models are evaluated on topic quality and topic inference, using both auto-evaluation metrics and human assessment. We analyze a disconnect between automatic metrics and human evaluations. While LDA performs the best overall with the auto-evaluation metrics NPMI and Coherence, Neural Topic Model with Knowledge Distillation is favourable by expert evaluation. We also create a new partially expert annotated gold-standard maternal health topic modeling dataset for future research.",https://aclanthology.org/2021.louhi-1.8,Association for Computational Linguistics,2021,April,Proceedings of the 12th International Workshop on Health Text Mining and Information Analysis,"Gao, Shuang  and
Pandya, Shivani  and
Agarwal, Smisha  and
Sedoc, Jo{\~a}o",Topic Modeling for Maternal Health Using Reddit,,louhi,321
2020.eval4nlp-1.10,"['Evaluation Techniques', 'Data Management and Generation']",,,"Recognizing Textual Entailment RTE was proposed as a unified evaluation framework to compare semantic understanding of different NLP systems. In this survey paper, we provide an overview of different approaches for evaluating and understanding the reasoning capabilities of NLP systems. We then focus our discussion on RTE by highlighting prominent RTE datasets as well as advances in RTE dataset that focus on specific linguistic phenomena that can be used to evaluate NLP systems on a fine-grained level. We conclude by arguing that when evaluating NLP systems, the community should utilize newly introduced RTE datasets that focus on specific linguistic phenomena.",https://aclanthology.org/2020.eval4nlp-1.10,Association for Computational Linguistics,2020,November,Proceedings of the First Workshop on Evaluation and Comparison of NLP Systems,"Poliak, Adam",A survey on Recognizing Textual Entailment as an NLP Evaluation,10.18653/v1/2020.eval4nlp-1.10,eval4nlp,251
U16-1005,"['Learning Paradigms', 'Domain-specific NLP', 'Language Change Analysis']",['Medical and Clinical NLP'],,"Syndromic Surveillance has been performed using machine learning and other statistical methods to detect disease outbreaks. These methods are largely dependent on the availability of historical data to train the machine learning-based surveillance system. However, relevant training data may differ from region to region due to geographical and seasonal trends, meaning that the syndromic surveillance designed for one area may not be effective for another. We proposed and analyse a semi-supervised method for syndromic surveillance from emergency department chief complaint textual notes that avoids the need for large training data. Our new method is based on identification of lexical shifts in the language of Chief Complaints of patients, as recorded by triage nurses, that we believe can be used to monitor disease distributions and possible outbreaks over time. The results we obtained demonstrate that effective lexical syndromic surveillance can be approached when distinctive lexical items are available to describe specific syndromes.",https://aclanthology.org/U16-1005,,2016,December,Proceedings of the Australasian Language Technology Association Workshop 2016,"Aamer, Hafsah  and
Ofoghi, Bahadorreza  and
Verspoor, Karin",Syndromic Surveillance through Measuring Lexical Shift in Emergency Department Chief Complaint Texts,,U16,1302
U16-1014,"['Learning Paradigms', 'Classification Applications', 'Domain-specific NLP']",['Medical and Clinical NLP'],['Biomedical NLP'],"In this paper, we develop a weakly supervised version of logistic regression to help to improve biomedical text classification performance when there is limited annotated data. We learn cascaded latent variable models for the classification tasks. First, with a large number of unlabelled but limited amount of labelled biomedical text, we will bootstrap and semi-automate the annotation task with partially and weakly annotated data. Second, both coarse-grained document and fine-grained sentence levels of each individual biomedical report will be taken into consideration. Our experimental work shows this achieves higher classification results.",https://aclanthology.org/U16-1014,,2016,December,Proceedings of the Australasian Language Technology Association Workshop 2016,"Liu, Ming  and
Haffari, Gholamreza  and
Buntine, Wray",Learning cascaded latent variable models for biomedical text classification,,U16,1414
2021.icnlsp-1.8,"['Learning Paradigms', 'Information Extraction', 'Model Architectures']","['Transformer Models', 'Relation Extraction']",,"Multiple instance learning has become the standard learning paradigm for distantly supervised relation extraction. However, relation extraction is performed at bag level in this learning paradigm and thus has significant hardware requirements for training when coupled with large sentence encoders such as deep transformer neural networks. In this paper, we propose a novel sample-based training method for distantly supervised relation extraction that relaxes these hardware requirements. In the proposed method, we limit the number of sentences in a batch by randomly sampling sentences from the bags in the batch. However, this comes at the cost of losing valid sentences from bags. To alleviate the issues caused by random sampling, we use an ensemble of trained models for prediction. We demonstrate the effectiveness of our approach by using our proposed learning setting to finetuning BERT on the widely NYT dataset. Our approach significantly outperforms previous state-of-the-art methods in terms of AUC and P@N metrics.",https://aclanthology.org/2021.icnlsp-1.8,Association for Computational Linguistics,2021,12--13 November,Proceedings of The Fourth International Conference on Natural Language and Speech Processing (ICNLSP 2021),"Nasser, Mehrdad  and
Sajadi, Mohamad Bagher  and
Minaei-Bidgoli, Behrouz",A Sample-Based Training Method for Distantly Supervised Relation Extraction with Pre-Trained Transformers,10.48550/arxiv.2104.07512,icnlsp,571
2021.ltedi-1.1,"['Biases in NLP', 'Ethics', 'Domain-specific NLP', 'Classification Applications']","['NLP for Bibliometrics and Scientometrics', 'Gender Bias', 'Medical and Clinical NLP']",,"This study sheds light on the effects of COVID-19 in the particular field of Computational Linguistics and Natural Language Processing within Artificial Intelligence. We provide an inter-sectional study on gender, contribution, and experience that considers one school year from August 2019 to August 2020 as a pandemic year. August is included twice for the purpose of an inter-annual comparison. While there has been an increasing trend in publications during the crisis, the results show that the ratio between female authors' publications and male authors' publications decreased. This only reduces the importance of the female role in the scientific contributions of computational linguistics it is now far below its peak of 0.24. The pandemic has a significantly negative effect on female senior researchers' production in the first position of authors maximum work, followed by the female junior researchers in the last position of authors supervision or collaborative work.",https://aclanthology.org/2021.ltedi-1.1,Association for Computational Linguistics,2021,April,"Proceedings of the First Workshop on Language Technology for Equality, Diversity and Inclusion","Basta, Christine  and
Costa-jussa, Marta R.","Impact of COVID-19 in Natural Language Processing Publications: a Disaggregated Study in Gender, Contribution and Experience",10.5821/dissertation-2117-379361,ltedi,396
2016.lilt-13.1,"['Data Management and Generation', 'Information Extraction']","['Data Preparation', 'Coreference Resolution']",,"Verb phrase VP ellipsis is the omission of a verb phrase whose meaning can be reconstructed from the linguistic or real-world context. It is licensed in English by auxiliary verbs, often modal auxiliaries: She can go to Hawaii but he can't e. This paper describes a system called ViPER VP Ellipsis Resolver that detects and resolves VP ellipsis, relying on linguistic principles such as syntactic parallelism, modality correlations, and the delineation of core vs. peripheral sentence constituents. The key insight guiding the work is that not all cases of ellipsis are equally difficult: some can be detected and resolved with high confidence even before we are able to build systems with human-level semantic and pragmatic understanding of text.",https://aclanthology.org/2016.lilt-13.1,CSLI Publications,2016,,"Linguistic Issues in Language Technology, Volume 13, 2016","McShane, Marjorie  and
Babkin, Petr",Detection and Resolution of Verb Phrase Ellipsis,10.33011/lilt.v13i.1385,lilt,1153
L16-1033,"['Error Detection and Correction', 'Low-resource Languages']",['Grammatical Error Correction (GEC)'],,"Automated grammatical error detection, which helps users improve their writing, is an important application in NLP. Recently more and more people are learning Chinese, and an automated error detection system can be helpful for the learners. This paper proposes n-gram features, dependency count features, dependency bigram features, and single-character features to determine if a Chinese sentence contains word usage errors, in which a word is written as a wrong form or the word selection is inappropriate. With marking potential errors on the level of sentence segments, typically delimited by punctuation marks, the learner can try to correct the problems without the assistant of a language teacher. Experiments on the HSK corpus show that the classifier combining all sets of features achieves an accuracy of 0.8423. By utilizing certain combination of the sets of features, we can construct a system that favours precision or recall. The best precision we achieve is 0.9536, indicating that our system is reliable and seldom produces misleading results.",https://aclanthology.org/L16-1033,European Language Resources Association (ELRA),2016,May,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),"Shiue, Yow-Ting  and
Chen, Hsin-Hsi",Detecting Word Usage Errors in Chinese Sentences for Learning Chinese as a Foreign Language,,L16,564
2021.fnp-1.9,"['Learning Paradigms', 'Domain-specific NLP', 'Information Extraction', 'Model Architectures']","['NLP for Finance', 'Supervised Learning', 'Relation Extraction']",['Causality Relations Extraction'],"In this paper, we report on our system for FIN-CAUSAL 2021 Financial Document Causality Detection Task. In this task, the aim is to identify, in a causal sentence or text block, the causal elements and the consequential ones. We propose a system that uses a pre-trained model, fine-tuned on the extended dataset, and task-specific post-processing of the model's inputs to improve the quality of the results. We tried two types of approaches: 1 a fine-tuned T5-model that generated cause and effect spans 2 and a sequence-to-sequence model based on XLNet that solved the task as token classification. The best result of our XLNet-large is 0.946 F1 on the test set while T5-model got the F1 score of 0.835 which may be due to the lower number of exact matches.",https://aclanthology.org/2021.fnp-1.9,Association for Computational Linguistics,2021,15-16 September,Proceedings of the 3rd Financial Narrative Processing Workshop,"Davletov, Adis  and
Pletenev, Sergey  and
Gordeev, Denis",LIORI at the FinCausal 2021 Shared task: Transformer ensembles are not enough to win,,fnp,22
2021.depling-1.11,"['Data Management and Generation', 'Low-resource Languages']",['Data Analysis'],,"In this paper, we research the interaction of number agreement, dependency length, and word order between the subject and the verb in Finnish traditional dialects. While in standard Finnish the verb always agrees with the subject in person and number, in traditional dialects it does not always agree in number with a third person plural subject. We approach this variation with data from The Finnish Dialect Syntax Archive, focusing here on plural lexical subjects. We use generalized linear mixed effects modelling to model variation in number agreement and use as as a predictor the dependency length between the subject and the verb, building in word order as part of this measure. Variation across lemmas, individuals, and dialects is addressed via random grouping factors. Finite verb and the main lexical verb are considered as alternative reference points for dependency length and agreement. The results suggest that the probability of number agreement increases as the distance of the preverbal subject from the verb increases, but the trend is the opposite for postverbal subjects so that the probability of number agreement decreases as the distance of the subject from the verb increases. 'children are eating' b. lapse-t child-PL.NOM syö-ø/vät eat-3SG/3PL 'children are eating' This work is licensed under a Creative Commons Attribution 4.0 International Licence. Licence details: http:// creativecommons.org/licenses/by/4.0/.",https://aclanthology.org/2021.depling-1.11,Association for Computational Linguistics,2021,December,"Proceedings of the Sixth International Conference on Dependency Linguistics (Depling, SyntaxFest 2021)","Sinnem{\""a}ki, Kaius  and
Takaki, Akira","Number agreement, dependency length, and word order in Finnish traditional dialects",,depling,127
Y16-2019,"['Information Extraction', 'Low-resource Languages']",,,"Comparable corpus is the most important resource in several NLP tasks. However, it is very expensive to collect manually. Lexical borrowing happened in almost all languages. We can use the loanwords to detect useful bilingual knowledge and expand the size of donor-recipient / recipient-donor comparable corpora. In this paper, we propose a recurrent neural network RNN based framework to identify loanwords in Uyghur. Additionally, we suggest two features: inverse language model feature and collocation feature to improve the performance of our model. Experimental results show that our approach outperforms several sequence labeling baselines.",https://aclanthology.org/Y16-2019,,2016,October,"Proceedings of the 30th Pacific Asia Conference on Language, Information and Computation: Oral Papers","Mi, Chenggang  and
Yang, Yating  and
Zhou, Xi  and
Wang, Lei  and
Li, Xiao  and
Jiang, Tonghai",Recurrent Neural Network Based Loanwords Identification in Uyghur,,Y16,1454
P17-1038,"['Data Management and Generation', 'Information Extraction', 'Knowledge Representation and Reasoning', 'Model Architectures']","['Data Preparation', 'Event Extraction']",['Annotation Processes'],"Modern models of event extraction for tasks like ACE are based on supervised learning of events from small hand-labeled data. However, hand-labeled training data is expensive to produce, in low coverage of event types, and limited in size, which makes supervised methods hard to extract large scale of events for knowledge base population. To solve the data labeling problem, we propose to automatically label training data for event extraction via world knowledge and linguistic knowledge, which can detect key arguments and trigger words for each event type and employ them to label events in texts automatically. The experimental results show that the quality of our large scale automatically labeled data is competitive with elaborately human-labeled data. And our automatically labeled data can incorporate with human-labeled data, then improve the performance of models learned from these data.",https://aclanthology.org/P17-1038,Association for Computational Linguistics,2017,July,Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),"Chen, Yubo  and
Liu, Shulin  and
Zhang, Xiang  and
Liu, Kang  and
Zhao, Jun",Automatically Labeled Data Generation for Large Scale Event Extraction,10.18653/v1/P17-1038,P17,1039
S17-2105,"['Embeddings', 'Learning Paradigms', 'Classification Applications', 'Domain-specific NLP']","['Word Embeddings', 'Supervised Learning', 'NLP for News and Media']",['NLP for Social Media'],"We present a simple supervised text classification system that combines sparse and dense vector representations of words, and the generalized representations of words via clusters. The sparse vectors are generated from word n-gram sequences 1-3. The dense vector representations of words embeddings are learned by training a neural network to predict neighboring words in a large unlabeled dataset. To classify a text segment, the different vector representations of it are concatenated, and the classification is performed using Support Vector Machines SVMs. Our system is particularly intended for use by nonexperts of natural language processing and machine learning, and, therefore, the system does not require any manual tuning of parameters or weights. Given a training set, the system automatically generates the training vectors, optimizes the relevant hyper-parameters for the SVM classifier, and trains the classification model. We evaluated this system on the SemEval-2017 English sentiment analysis task. In terms of average F1-Score, our system obtained 8 th position out of 39 submissions F1-Score: 0.632, average recall: 0.637, accuracy: 0.646.",https://aclanthology.org/S17-2105,Association for Computational Linguistics,2017,August,Proceedings of the 11th International Workshop on Semantic Evaluation ({S}em{E}val-2017),"Sarker, Abeed  and
Gonzalez, Graciela","HLP@UPenn at SemEval-2017 Task 4A: A simple, self-optimizing text classification system combining dense and sparse vectors",10.18653/v1/S17-2105,S17,505
2022.wnu-1.5,['Evaluation Techniques'],,,"Decision making theories such as Fuzzy-Trace Theory FTT suggest that individuals tend to rely on gist, or bottom-line meaning, in the text when making decisions. In this work, we delineate the process of developing GisPy, an opensource tool in Python for measuring the Gist Inference Score GIS in text. Evaluation of GisPy on documents in three benchmarks from the news and scientific text domains demonstrates that scores generated by our tool significantly distinguish low vs. high gist documents.",https://aclanthology.org/2022.wnu-1.5,Association for Computational Linguistics,2022,July,Proceedings of the 4th Workshop of Narrative Understanding (WNU2022),"Hosseini, Pedram  and
Wolfe, Christopher  and
Diab, Mona  and
Broniatowski, David",GisPy: A Tool for Measuring Gist Inference Score in Text,10.18653/v1/2022.wnu-1.5,wnu,1027
P17-1108,"['Automatic Text Summarization', 'Model Architectures']","['Abstractive Text Summarization', 'Graph Neural Networks (GNNs)', 'Document Summarization']",,"Abstractive summarization is the ultimate goal of document summarization research, but previously it is less investigated due to the immaturity of text generation techniques. Recently impressive progress has been made to abstractive sentence summarization using neural models. Unfortunately, attempts on abstractive document summarization are still in a primitive stage, and the evaluation results are worse than extractive methods on benchmark datasets. In this paper, we review the difficulties of neural abstractive document summarization, and propose a novel graph-based attention mechanism in the sequence-to-sequence framework. The intuition is to address the saliency factor of summarization, which has been overlooked by prior works. Experimental results demonstrate our model is able to achieve considerable improvement over previous neural abstractive models. The data-driven neural abstractive method is also competitive with state-of-the-art extractive methods.",https://aclanthology.org/P17-1108,Association for Computational Linguistics,2017,July,Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),"Tan, Jiwei  and
Wan, Xiaojun  and
Xiao, Jianguo",Abstractive Document Summarization with a Graph-Based Attentional Neural Model,10.18653/v1/P17-1108,P17,1298
S16-1164,"['Learning Paradigms', 'Classification Applications', 'Data Management and Generation']","['Data Analysis', 'Supervised Learning']",,"We introduce a word frequency-based classifier for the SemEval 2016 complex word identification task #11. Words with lower frequency are predicted as complex based on a threshold optimized for G-score. We examine three different corpora for calculating frequencies and find English Wikipedia to perform best ranked 13th on the SemEval task, followed by the Google Web Corpus and lastly Simple English Wikipedia. Bagging is also shown to slightly improve the performance of the classifier. Overall, we find word frequency to be a strong predictor of complexity. On the SemEval ""test"" set, a frequency classifier that uses the optimal frequency threshold performs on-par with the best submitted system and a system trained using only 500 labeled examples split from the test set achieves results that are only slightly below the best submitted system.",https://aclanthology.org/S16-1164,Association for Computational Linguistics,2016,June,Proceedings of the 10th International Workshop on Semantic Evaluation ({S}em{E}val-2016),"Kauchak, David",Pomona at SemEval-2016 Task 11: Predicting Word Complexity Based on Corpus Frequency,10.18653/v1/S16-1164,S16,212
2020.alvr-1.5,"['Question Answering (QA)', 'Image and Video Processing', 'Model Architectures', 'Learning Paradigms']",['Multimodal Learning'],,"We propose a novel alignment mechanism to deal with procedural reasoning on a newly released multimodal QA dataset, named RecipeQA. Our model is solving the textual cloze task which is a reading comprehension on a recipe containing images and instructions. We exploit the power of attention networks, cross-modal representations, and a latent alignment space between instructions and candidate answers to solve the problem. We introduce constrained max-pooling which refines the max-pooling operation on the alignment matrix to impose disjoint constraints among the outputs of the model. Our evaluation result indicates a 19% improvement over the baselines.",https://aclanthology.org/2020.alvr-1.5,Association for Computational Linguistics,2020,July,Proceedings of the First Workshop on Advances in Language and Vision Research,"Rajaby Faghihi, Hossein  and
Mirzaee, Roshanak  and
Paliwal, Sudarshan  and
Kordjamshidi, Parisa",Latent Alignment of Procedural Concepts in Multimodal Recipes,10.18653/v1/2020.alvr-1.5,alvr,1018
Y18-1053,"['Text Generation', 'Knowledge Representation and Reasoning', 'Model Architectures']",['Recurrent Neural Networks (RNNs)'],['Long Short-Term Memory (LSTM) Models'],"Word problem solving has always been a challenging task as it involves reasoning across sentences, identification of operations and their order of application on relevant operands. Most of the earlier systems attempted to solve word problems with tailored features for handling each category of problems. In this paper, we present a new approach to solve simple arithmetic problems. Through this work we introduce a novel method where we first learn a dense representation of the problem description conditioned on the question in hand. We leverage this representation to generate the operands and operators in the appropriate order. Our approach improves upon the state-of-the-art system by 3% in one benchmark dataset while ensuring comparable accuracies in other datasets.",https://aclanthology.org/Y18-1053,Association for Computational Linguistics,2018,1{--}3 December,"Proceedings of the 32nd Pacific Asia Conference on Language, Information and Computation","Mishra, Pruthwik  and
Kurisinkel, Litton J  and
Sharma, Dipti Misra  and
Varma, Vasudeva",EquGener: A Reasoning Network for Word Problem Solving by Generating Arithmetic Equations,,Y18,815
2020.onion-1.2,"['Data Management and Generation', 'Low-resource Languages']",['Data Analysis'],,"We conducted preliminary comparison of human-robot HR interaction with human-human HH interaction conducted in English and in Japanese. As the result, body gestures increased in HR, while hand and head gestures decreased in HR. Concerning hand gesture, they were composed of more diverse and complex forms, trajectories and functions in HH than in HR. Moreover, English speakers produced 6 times more hand gestures than Japanese speakers in HH. Regarding head gesture, even though there was no difference in the frequency of head gestures between English speakers and Japanese speakers in HH, Japanese speakers produced slightly more nodding during the robot's speaking than English speakers in HR. Furthermore, positions of nod were different depending on the language. Concerning body gesture, participants produced body gestures mostly to regulate appropriate distance with the robot in HR. Additionally, English speakers produced slightly more body gestures than Japanese speakers.",https://aclanthology.org/2020.onion-1.2,European Language Resources Association (ELRA),2020,May,"Proceedings of LREC2020 Workshop ``People in language, vision and the mind'' (ONION2020)","Mori, Taiga  and
Jokinen, Kristiina  and
Den, Yasuharu",Analysis of Body Behaviours in Human-Human and Human-Robot Interactions,,onion,1357
2020.semeval-1.299,"['Domain-specific NLP', 'Model Architectures', 'Classification Applications']","['NLP for News and Media', 'Hate and Offensive Speech Detection', 'Transformer Models']",['NLP for Social Media'],"This paper presents six document classification models using the latest transformer encoders and a high-performing ensemble model for a task of offensive language identification in social media. For the individual models, deep transformer layers are applied to perform multi-head attentions. For the ensemble model, the utterance representations taken from those individual models are concatenated and fed into a linear decoder to make the final decisions. Our ensemble model outperforms the individual models and shows up to 8.6% improvement over the individual models on the development set. On the test set, it achieves macro-F1 of 90.9% and becomes one of the high performing systems among 85 participants in the sub-task A of this shared task. Our analysis shows that although the ensemble model significantly improves the accuracy on the development set, the improvement is not as evident on the test set.",https://aclanthology.org/2020.semeval-1.299,International Committee for Computational Linguistics,2020,December,Proceedings of the Fourteenth Workshop on Semantic Evaluation,"Dong, Xiangjue  and
Choi, Jinho D.",XD at SemEval-2020 Task 12: Ensemble Approach to Offensive Language Identification in Social Media Using Transformer Encoders,10.18653/v1/2020.semeval-1.299,semeval,422
2020.evalnlgeval-1.2,"['Biases in NLP', 'Data Management and Generation']",['Data Analysis'],,"Despite recent efforts reviewing current human evaluation practices for natural language generation NLG research, the lack of reported question wording and potential for framing effects or cognitive biases influencing results has been widely overlooked. In this opinion paper, we detail three possible framing effects and cognitive biases that could be imposed on human evaluation in NLG. Based on this, we make a call for increased transparency for human evaluation in NLG and propose the concept of human evaluation statements. We make several recommendations for design details to report that could potentially influence results, such as question wording, and suggest that reporting pertinent design details can help increase comparability across studies as well as reproducibility of results.",https://aclanthology.org/2020.evalnlgeval-1.2,Association for Computational Linguistics,2020,December,Proceedings of the 1st Workshop on Evaluating NLG Evaluation,"Schoch, Stephanie  and
Yang, Diyi  and
Ji, Yangfeng","``This is a Problem, Don't You Agree?'' Framing and Bias in Human Evaluation for Natural Language Generation",,evalnlgeval,1091
Q17-1004,"['Learning Paradigms', 'Parsing', 'Low-resource Languages', 'Model Architectures']","['Supervised Learning', 'Syntactic Parsing', 'Recurrent Neural Networks (RNNs)']","['Constituency Parsing', 'Long Short-Term Memory (LSTM) Models']","Transition-based models can be fast and accurate for constituent parsing. Compared with chart-based models, they leverage richer features by extracting history information from a parser stack, which consists of a sequence of non-local constituents. On the other hand, during incremental parsing, constituent information on the right hand side of the current word is not utilized, which is a relative weakness of shift-reduce parsing. To address this limitation, we leverage a fast neural model to extract lookahead features. In particular, we build a bidirectional LSTM model, which leverages full sentence information to predict the hierarchy of constituents that each word starts and ends. The results are then passed to a strong transition-based constituent parser as lookahead features. The resulting parser gives 1.3% absolute improvement in WSJ and 2.3% in CTB compared to the baseline, giving the highest reported accuracies for fullysupervised parsing.",https://aclanthology.org/Q17-1004,MIT Press,2017,,,"Liu, Jiangming  and
Zhang, Yue",Shift-Reduce Constituent Parsing with Neural Lookahead Features,10.1162/tacl_a_00045,Q17,747
2020.ccl-1.106,"['Domain-specific NLP', 'Model Architectures', 'Classification Applications']","['NLP for News and Media', 'Transformer Models']",,"Clickbait is a form of web content designed to attract attention and entice users to click on specific hyperlinks. The detection of clickbaits is an important task for online platforms to improve the quality of web content and the satisfaction of users. Clickbait detection is typically formed as a binary classification task based on the title and body of a webpage, and existing methods are mainly based on the content of title and the relevance between title and body. However, these methods ignore the stylistic patterns of titles, which can provide important clues on identifying clickbaits. In addition, they do not consider the interactions between the contexts within title and body, which are very important for measuring their relevance for clickbait detection. In this paper, we propose a clickbait detection approach with style-aware title modeling and coattention. Specifically, we use Transformers to learn content representations of title and body, and respectively compute two content-based clickbait scores for title and body based on their representations. In addition, we propose to use a character-level Transformer to learn a style-aware title representation by capturing the stylistic patterns of title, and we compute a title stylistic score based on this representation. Besides, we propose to use a co-attention network to model the relatedness between the contexts within title and body, and further enhance their representations by encoding the interaction information. We compute a title-body matching score based on the representations of title and body enhanced by their interactions. The final clickbait score is predicted by a weighted summation of the aforementioned four kinds of scores. Extensive experiments on two benchmark datasets show that our approach can effectively improve the performance of clickbait detection and consistently outperform many baseline methods.",https://aclanthology.org/2020.ccl-1.106,Chinese Information Processing Society of China,2020,October,Proceedings of the 19th Chinese National Conference on Computational Linguistics,"Wu, Chuhan  and
Wu, Fangzhao  and
Qi, Tao  and
Huang, Yongfeng",Clickbait Detection with Style-aware Title Modeling and Co-attention,10.1007/978-3-030-63031-7_31,ccl,234
2021.privatenlp-1.4,"['Ethics', 'Learning Paradigms', 'Information Extraction', 'Model Architectures']","['Recurrent Neural Networks (RNNs)', 'Named Entity Recognition (NER)', 'Supervised Learning']",['Long Short-Term Memory (LSTM) Models'],"To build machine learning-based applications for sensitive domains like medical, legal, etc. where the digitized text contains private information, anonymization of text is required for preserving privacy. Sequence tagging, e.g. as used for Named Entity Recognition NER, can help to detect private information. However, to train sequence tagging models, a sufficient amount of labeled data are required but for privacy-sensitive domains, such labeled data also can not be shared directly. In this paper, we investigate the applicability of a privacy-preserving framework for sequence tagging tasks, specifically NER. Hence, we analyze a framework for the NER task, which incorporates two levels of privacy protection. Firstly, we deploy a federated learning FL framework where the labeled data are neither shared with the centralized server nor with the peer clients. Secondly, we apply differential privacy DP while the models are being trained in each client instance. While both privacy measures are suitable for privacy-aware models, their combination results in unstable models. To our knowledge, this is the first study of its kind on privacy-aware sequence tagging models.",https://aclanthology.org/2021.privatenlp-1.4,Association for Computational Linguistics,2021,June,Proceedings of the Third Workshop on Privacy in Natural Language Processing,"Jana, Abhik  and
Biemann, Chris",An Investigation towards Differentially Private Sequence Tagging in a Federated Framework,10.18653/v1/2021.privatenlp-1.4,privatenlp,1263
2021.gem-1.7,"['Ethics', 'Text Generation']",,,"We propose an approach to automatically test for originality in generation tasks where no standard automatic measures exist. Our proposal addresses original uses of language, not necessarily original ideas. We provide an algorithm for our approach and a run-time analysis. The algorithm, which finds all of the original fragments in a ground-truth corpus and can reveal whether a generated fragment copies an original without attribution, has a run-time complexity of θn log n where n is the number of sentences in the ground truth.",https://aclanthology.org/2021.gem-1.7,Association for Computational Linguistics,2021,August,"Proceedings of the 1st Workshop on Natural Language Generation, Evaluation, and Metrics (GEM 2021)","Brooks, Jennifer  and
Youssef, Abdou",GOT: Testing for Originality in Natural Language Generation,10.18653/v1/2021.gem-1.7,gem,582
2021.wat-1.6,"['Machine Translation (MT)', 'Low-resource Languages', 'Model Architectures']","['Neural MT (NMT)', 'Recurrent Neural Networks (RNNs)']",,"In this paper, we report the experimental results of Machine Translation models conducted by a NECTEC team Team-ID: NECTEC for the WAT-2021 Myanmar-English translation task Nakazawa et al., 2021 . Basically, our models are based on neural methods for both directions of English-Myanmar and Myanmar-English language pairs. Most of the existing Neural Machine Translation NMT models mainly focus on the conversion of sequential data and do not directly use syntactic information. However, we conduct multisource neural machine translation NMT models using the multilingual corpora such as string data corpus, tree data corpus, or POS-tagged data corpus. The multisource translation is an approach to exploit multiple inputs e.g. in two different formats to increase translation accuracy. The RNN-based encoder-decoder model with attention mechanism and transformer architectures have been carried out for our experiment. The experimental results showed that the proposed models of RNNbased architecture outperform the baseline model for the English-to-Myanmar translation task, and the multi-source and sharedmulti-source transformer models yield better translation results than the baseline.",https://aclanthology.org/2021.wat-1.6,Association for Computational Linguistics,2021,August,Proceedings of the 8th Workshop on Asian Translation (WAT2021),"Hlaing, Zar Zar  and
Thu, Ye Kyaw  and
Myint Oo, Thazin  and
Ei San, Mya  and
Usanavasin, Sasiporn  and
Netisopakul, Ponrudee  and
Supnithi, Thepchai",NECTEC's Participation in WAT-2021,10.18653/v1/2021.wat-1.6,wat,1227
2021.iwclul-1.6,"['Multilingual NLP', 'Error Detection and Correction', 'Low-resource Languages']",,,"We argue that regression testing is necessary to ensure reliability in the continuous development of NLP tools, especially higher level applications like grammar checkers. Our approach is rule-based, building on successful work for a number of low-resourced languages over the last 20 years. Instead of working with a black box, we choose a method that allows us to pinpoint the exact reasons for failures in the system. We present a tool for regression testing for GramDivvun, the rule-based open source North Sámi grammar checker. The regression tool is available for any of the 135 languages in the Giella-LT infrastructure and can be applied when respective tools are built. An evaluation of the system shows how the precision of the regression tests improves with almost 20% over a time span of 1.5 years. We also illustrate that the regression tool can detect undesired effects of rule changes that affect the performance of the grammar checker.",https://aclanthology.org/2021.iwclul-1.6,Association for Computational Linguistics,2021,September,Proceedings of the Seventh International Workshop on Computational Linguistics of Uralic Languages,"Wiechetek, Linda  and
Pirinen, Flammie A  and
Gaup, B{\o}rre  and
Omma, Thomas",No more fumbling in the dark - Quality assurance of high-level NLP tools in a multi-lingual infrastructure,,iwclul,1308
2021.eacl-demos.28,"['Learning Paradigms', 'Data Management and Generation']",['Data Preparation'],['Annotation Processes'],"In this paper, we present Paladin, an opensource web-based annotation tool for creating high-quality multi-label document-level datasets. By integrating active learning and proactive learning to the annotation task, Paladin makes the task less time-consuming and requiring less human effort. Although Paladin is designed for multi-label settings, the system is flexible and can be adapted to other tasks in single-label settings.",https://aclanthology.org/2021.eacl-demos.28,Association for Computational Linguistics,2021,April,Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: System Demonstrations,"Nghiem, Minh-Quoc  and
Baylis, Paul  and
Ananiadou, Sophia",Paladin: an annotation tool based on active and proactive learning,10.18653/v1/2021.eacl-demos.28,eacl,549

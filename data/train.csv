acl_id,Level1,Level2,Level3,abstract,url,publisher,year,month,booktitle,author,title,doi,venue,data_index
2021.nlp4call-1.2,"['Data Management and Generation', 'Information Extraction', 'Model Architectures', 'Domain-specific NLP']",['Data Preparation'],['Annotation Processes'],"Integrating an adaptive Intelligent Tutoring System ITS in real-life school contexts requires coverage of the official curricula, which necessitates a broad range and number of activities to practice the official set of language phenomena. In the context of developing an adaptive ITS for English as a Foreign Language, we propose a method to automatically derive rich activity models from ordinary exercise specifications. The method identifies the language means being covered from the curriculum by processing the language used in the exercise and exemplary answers.",https://aclanthology.org/2021.nlp4call-1.2,LiU Electronic Press,2021,May,Proceedings of the 10th Workshop on NLP for Computer Assisted Language Learning,"Quixal, Mart{\'\i}  and
Rudzewitz, Bj{\""o}rn  and
Bear, Elizabeth  and
Meurers, Detmar",Automatic annotation of curricular language targets to enrich activity models and support both pedagogy and adaptive systems,,nlp4call,1022
L18-1251,"['Audio Generation and Processing', 'Data Management and Generation', 'Image and Video Processing', 'Learning Paradigms', 'Low-resource Languages', 'Classification Applications']","['Multimodal Learning', 'Data Preparation', 'Emotion Detection']",,"The presented study concentrates on the collection of emotional multimodal real-world in-car audio, video and physiological signal recordings while driving. To do so, three sensor systems were integrated in the car and four relevant emotional states of the driver were defined: neutral, positive, frustrated and anxious. To gather as natural as possible emotional data of the driver, the subjects needed to be unbiased and were therefore kept unaware of the detailed research objective. The emotions were induced using so-called Wizard-of-Oz experiments, where the drivers believed to be interacting with an automated technical system, which in fact was controlled by a human. Additionally, on board interviews while driving were conducted by an instructed psychologist. To evaluate the collected data, questionnaires were filled out by the subjects before, during and after the data collection. These included monitoring of the drivers perceived state of emotion, stress, sleepiness and thermal sensation but also detailed questionnaires on their driving experience, attitude towards technology and big five OCEAN personality traits. Afterwards, the data was annotated by expert labelers. Exemplary results of the evaluation of the experiments are given in the result section of this paper. They indicate that the emotional states were successfully induced and the annotation results are consistent for both performed annotation approaches.",https://aclanthology.org/L18-1251,European Language Resources Association (ELRA),2018,May,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),"Lotz, Alicia  and
Ihme, Klas  and
Charnoz, Audrey  and
Maroudis, Pantelis  and
Dmitriev, Ivan  and
Wendemuth, Andreas",Recognizing Behavioral Factors while Driving: A Real-World Multimodal Corpus to Monitor the Driver's Affective State,,L18,117
2022.sigtyp-1.11,"['Multilingual NLP', 'Classification Applications', 'Low-resource Languages']",,,"This work describes an implementation of the ""extended alignment"" model for cognate reflex prediction submitted to the ""SIGTYP 2022 Shared Task on the Prediction of Cognate Reflexes"". Similarly to List et al. 2022a , the technique involves an automatic extension of sequence alignments with multilayered vectors that encode informational tiers on both site-specific traits, such as sound classes and distinctive features, as well as contextual and suprasegmental ones, conveyed by cross-site referrals and replication. The method allows to generalize the problem of cognate reflex prediction as a classification problem, with models trained using a parallel corpus of cognate sets. A model using random forests is trained and evaluated on the shared task for reflex prediction, and the experimental results are presented and discussed along with some differences to other implementations.",https://aclanthology.org/2022.sigtyp-1.11,Association for Computational Linguistics,2022,July,Proceedings of the 4th Workshop on Research in Computational Linguistic Typology and Multilingual NLP,"Tresoldi, Tiago",Approaching Reflex Predictions as a Classification Problem Using Extended Phonological Alignments,10.18653/v1/2022.sigtyp-1.11,sigtyp,560
2022.ltedi-1.41,"['Classification Applications', 'Domain-specific NLP', 'Automatic Text Summarization', 'Data Management and Generation']","['Abstractive Text Summarization', 'Data Augmentation', 'NLP for News and Media', 'Medical and Clinical NLP']","['NLP for Mental Health', 'NLP for Social Media']","Depression is a common mental disorder that severely affects the quality of life, and can lead to suicide. When diagnosed in time, mild, moderate, and even severe depression can be treated. This is why it is vital to detect signs of depression in time. One possibility for this is the use of text classification models on social media posts. Transformers have achieved state-of-theart performance on a variety of similar text classification tasks. One drawback, however, is that when the dataset is imbalanced, the performance of these models may be negatively affected. Because of this, in this paper, we examine the effect of balancing a depression detection dataset using data augmentation. In particular, we use abstractive summarization techniques for data augmentation. We examine the effect of this method on the LT-EDI-ACL2022 task. Our results show that when increasing the multiplicity of the minority classes to the right degree, this data augmentation method can in fact improve classification scores on the task.",https://aclanthology.org/2022.ltedi-1.41,Association for Computational Linguistics,2022,May,"Proceedings of the Second Workshop on Language Technology for Equality, Diversity and Inclusion","Nilsson, Filip  and
Kov{\'a}cs, Gy{\""o}rgy",FilipN@LT-EDI-ACL2022-Detecting signs of Depression from Social Media: Examining the use of summarization methods as data augmentation for text classification,10.18653/v1/2022.ltedi-1.41,ltedi,1348
2022.udfestbr-1.5,"['Parsing', 'Data Management and Generation', 'Low-resource Languages', 'Domain-specific NLP']","['Data Preparation', 'Syntactic Parsing']","['Dependency Parsing', 'Annotation Processes']","We report the process of annotating verbal arguments and adjuncts in PetroGold, a treebank of the oil & gas domain. The corpus follows the dependencies approach of the Universal Dependencies multilingual project. The argument-adjunct distinction in UD is not a relevant one, and it is up to the contributors of each language to decide how to annotate it in some particular cases. After consulting Portuguese grammars to assist in the annotation of the adverbial adjunct and indirect object relations, we propose a semantic-discursively oriented approach, which was used in the PetroGold annotation and affected 14.8% of the sentences in the treebank. Finally, we present a visualization of the results, showing the distribution of verbs by transitivity in the corpus.",https://aclanthology.org/2022.udfestbr-1.5,Association for Computational Linguistics,2022,March,Proceedings of the {U}niversal {D}ependencies {B}razilian Festival,"Souza, Elvis  and
Freitas, Claudia",Still on arguments and adjuncts: the status of the indirect object and the adverbial adjunct relations in Universal Dependencies for Portuguese,,udfestbr,1042
I17-2044,"['Text Preprocessing', 'Data Management and Generation', 'Low-resource Languages']",['Data Augmentation'],,"In this study, we investigated the effectiveness of augmented data for encoderdecoder-based neural normalization models. Attention based encoder-decoder models are greatly effective in generating many natural languages. In general, we have to prepare for a large amount of training data to train an encoderdecoder model. Unlike machine translation, there are few training data for textnormalization tasks. In this paper, we propose two methods for generating augmented data. The experimental results with Japanese dialect normalization indicate that our methods are effective for an encoder-decoder model and achieve higher BLEU score than that of baselines. We also investigated the oracle performance and revealed that there is sufficient room for improving an encoder-decoder model.",https://aclanthology.org/I17-2044,Asian Federation of Natural Language Processing,2017,November,Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 2: Short Papers),"Saito, Itsumi  and
Suzuki, Jun  and
Nishida, Kyosuke  and
Sadamitsu, Kugatsu  and
Kobashikawa, Satoshi  and
Masumura, Ryo  and
Matsumoto, Yuji  and
Tomita, Junji",Improving Neural Text Normalization with Data Augmentation at Character- and Morphological Levels,,I17,545
2022.bea-1.13,"['Audio Generation and Processing', 'Data Management and Generation', 'Domain-specific NLP']","['Data Preparation', 'Automatic Speech Recognition (ASR)']",,"A supportive environment is vital for overall cognitive development in children. Challenges with direct observation and limitations of access to data driven approaches often hinder teachers or practitioners in early childhood research to modify or enhance classroom structures. Deploying sensor based tools in naturalistic preschool classrooms will thereby help teachers/practitioners to make informed decisions and better support student learning needs. In this study, two elements of eco-behavioral assessment: conversational speech and realtime location are fused together. While various challenges remain in developing Automatic Speech Recognition systems for spontaneous preschool children speech, efforts are made to develop a hybrid ASR engine reporting an effective Word-Error-Rate of 40%. The ASR engine further supports recognition of spoken words, WH-words, and verbs in various activity learning zones in a naturalistic preschool classroom scenario. Activity areas represent various locations within the physical ecology of an early childhood setting, each of which is suited for knowledge and skill enhancement in young children. Capturing children's communication engagement in such areas could help teachers/practitioners fine-tune their daily activities, without the need for direct observation. This investigation provides evidence of the use of speech technology in educational settings to better support such early childhood intervention.",https://aclanthology.org/2022.bea-1.13,Association for Computational Linguistics,2022,July,Proceedings of the 17th Workshop on Innovative Use of NLP for Building Educational Applications (BEA 2022),"Dutta, Satwik  and
Irvin, Dwight  and
Buzhardt, Jay  and
Hansen, John H.L.",Activity focused Speech Recognition of Preschool Children in Early Childhood Classrooms,10.18653/v1/2022.bea-1.13,bea,168
2021.splurobonlp-1.7,['Text Generation'],,,"Ideally, people who navigate together in a complex indoor space share a mental model that facilitates explanation. This paper reports on a robot control system whose cognitive world model is based on spatial affordances that generalize over its perceptual data. Given a target, the control system formulates multiple plans, each with a model-relevant metric, and selects among them. As a result, it can provide readily understandable natural language about the robot's intentions and confidence, and generate diverse, contrastive explanations that reference the acquired spatial model. Empirical results in large, complex environments demonstrate the robot's ability to provide humanfriendly explanations in natural language.",https://aclanthology.org/2021.splurobonlp-1.7,Association for Computational Linguistics,2021,August,Proceedings of Second International Combined Workshop on Spatial Language Understanding and Grounded Communication for Robotics,"Korpan, Raj  and
Epstein, Susan L.",Plan Explanations that Exploit a Cognitive Spatial Model,10.18653/v1/2021.splurobonlp-1.7,splurobonlp,601
2021.wanlp-1.52,"['Domain-specific NLP', 'Low-resource Languages', 'Classification Applications', 'Model Architectures']","['Transformer Models', 'Sarcasm Detection', 'Sentiment Analysis (SA)', 'NLP for News and Media']",['NLP for Social Media'],"Sarcasm is one of the main challenges for sentiment analysis systems due to using implicit indirect phrasing for expressing opinions, especially in Arabic. This paper presents the system we submitted to the Sarcasm and Sentiment Detection task of WANLP-2021 that is capable of dealing with both two subtasks. We first perform fine-tuning on two kinds of pre-trained language models PLMs with different training strategies. Then an effective stacking mechanism is applied on top of the fine-tuned PLMs to obtain the final prediction. Experimental results on ArSarcasm-v2 dataset show the effectiveness of our method and we rank third and second for subtask 1 and 2.",https://aclanthology.org/2021.wanlp-1.52,Association for Computational Linguistics,2021,April,Proceedings of the Sixth Arabic Natural Language Processing Workshop,"Song, Bingyan  and
Pan, Chunguang  and
Wang, Shengguang  and
Luo, Zhipeng",DeepBlueAI at WANLP-EACL2021 task 2: A Deep Ensemble-based Method for Sarcasm and Sentiment Detection in Arabic,,wanlp,378
2020.coling-main.547,"['Domain-specific NLP', 'Classification Applications', 'Data Management and Generation']",['Data Preparation'],,"In this paper we present the first work on the automated scoring of mindreading ability in middle childhood and early adolescence. We create MIND-CA, a new corpus of 11,311 question-answer pairs in English from 1,066 children aged 7 to 14. We perform machine learning experiments and carry out extensive quantitative and qualitative evaluation. We obtain promising results, demonstrating the applicability of state-of-the-art NLP solutions to a new domain and task.",https://aclanthology.org/2020.coling-main.547,International Committee on Computational Linguistics,2020,December,Proceedings of the 28th International Conference on Computational Linguistics,"Kovatchev, Venelin  and
Smith, Phillip  and
Lee, Mark  and
Grumley Traynor, Imogen  and
Luque Aguilera, Irene  and
Devine, Rory",``What is on your mind?'' Automated Scoring of Mindreading in Childhood and Early Adolescence,10.18653/v1/2020.coling-main.547,coling,58
2020.sustainlp-1.15,"['Information Extraction', 'Learning Paradigms']","['Transfer Learning', 'Named Entity Recognition (NER)']",,"Transfer learning is a popular technique to learn a task using less training data and fewer compute resources. However, selecting the correct source model for transfer learning is a challenging task. We demonstrate a novel predictive method that determines which existing source model would minimize error for transfer learning to a given target. This technique does not require learning for prediction, and avoids computational costs of trial-and-error. We have evaluated this technique on nine datasets across diverse domains, including newswire, user forums, air flight booking, cybersecurity news, etc. We show that it performs better than existing techniques such as fine-tuning over vanilla BERT, or curriculum learning over the largest dataset on top of BERT, resulting in average F 1 score gains in excess of 3%. Moreover, our technique consistently selects the best model using fewer tries.",https://aclanthology.org/2020.sustainlp-1.15,Association for Computational Linguistics,2020,November,Proceedings of SustaiNLP: Workshop on Simple and Efficient Natural Language Processing,"Awasthy, Parul  and
Bhattacharjee, Bishwaranjan  and
Kender, John  and
Florian, Radu",Predictive Model Selection for Transfer Learning in Sequence Labeling Tasks,10.18653/v1/2020.sustainlp-1.15,sustainlp,1439
S17-1019,"['Learning Paradigms', 'Domain-specific NLP', 'Information Extraction', 'Data Management and Generation']","['Unsupervised Learning', 'Data Preparation', 'NLP for News and Media']",['NLP for Social Media'],"We present a simple method for evergrowing extraction of predicate paraphrases from news headlines in Twitter. Analysis of the output of ten weeks of collection shows that the accuracy of paraphrases with different support levels is estimated between 60-86%. We also demonstrate that our resource is to a large extent complementary to existing resources, providing many novel paraphrases. Our resource is publicly available, continuously expanding based on daily news.",https://aclanthology.org/S17-1019,Association for Computational Linguistics,2017,August,Proceedings of the 6th Joint Conference on Lexical and Computational Semantics (*{SEM} 2017),"Shwartz, Vered  and
Stanovsky, Gabriel  and
Dagan, Ido",Acquiring Predicate Paraphrases from News Tweets,10.18653/v1/S17-1019,S17,658
2021.splurobonlp-1.5,"['Learning Paradigms', 'Multi-agent Communication Systems', 'Model Architectures']","['Reinforcement Learning', 'Intelligent Agents', 'Recurrent Neural Networks (RNNs)']",['Long Short-Term Memory (LSTM) Models'],"We deal with the navigation problem where the agent follows natural language instructions while observing the environment. Focusing on language understanding, we show the importance of spatial semantics in grounding navigation instructions into visual perceptions. We propose a neural agent that uses the elements of spatial configurations and investigate their influence on the navigation agent's reasoning ability. Moreover, we model the sequential execution order and align visual objects with spatial configurations in the instruction. Our neural agent improves strong baselines on the seen environments and shows competitive performance on the unseen environments. Additionally, the experimental results demonstrate that explicit modeling of spatial semantic elements in the instructions can improve the grounding and spatial reasoning of the model.",https://aclanthology.org/2021.splurobonlp-1.5,Association for Computational Linguistics,2021,August,Proceedings of Second International Combined Workshop on Spatial Language Understanding and Grounded Communication for Robotics,"Zhang, Yue  and
Guo, Quan  and
Kordjamshidi, Parisa",Towards Navigation by Reasoning over Spatial Configurations,10.18653/v1/2021.splurobonlp-1.5,splurobonlp,703
2021.wanlp-1.41,"['Domain-specific NLP', 'Low-resource Languages', 'Classification Applications']","['Sarcasm Detection', 'Sentiment Analysis (SA)', 'NLP for News and Media']",['NLP for Social Media'],We present three methods developed for the Shared Task on Sarcasm and Sentiment Detection in Arabic. We present a baseline that uses character n-gram features. We also propose two more sophisticated methods: a recurrent neural network with a word level representation and an ensemble classifier relying on word and character-level features. We chose to present results from an ensemble classifier but it was not very successful as compared to the best systems : 22th/37 on sarcasm detection and 15th/22 on sentiment detection. It finally appeared that our baseline could have easily been tuned and achieve much better results.,https://aclanthology.org/2021.wanlp-1.41,Association for Computational Linguistics,2021,April,Proceedings of the Sixth Arabic Natural Language Processing Workshop,"Ghoul, Dhaou  and
Lejeune, Ga{\""e}l",Sarcasm and Sentiment Detection in Arabic: investigating the interest of character-level features,,wanlp,1306
2020.findings-emnlp.59,"['Model Architectures', 'Classification Applications']",,,"Transformers have shown great success in learning representations for language modelling. However, an open challenge still remains on how to systematically aggregate semantic information word embedding with positional or temporal information word orders. In this work, we propose a new architecture to aggregate the two sources of information using cascaded semantic and positional self-attention network CSPAN in the context of document classification. The CSPAN uses a semantic self-attention layer cascaded with Bi-LSTM to process the semantic and positional information in a sequential manner, and then adaptively combine them together through a residual connection. Compared with commonly used positional encoding schemes, CSPAN can exploit the interaction between semantics and word positions in a more interpretable and adaptive manner, and the classification performance can be notably improved while simultaneously preserving a compact model size and high convergence rate. We evaluate the CSPAN model on several benchmark data sets for document classification with careful ablation studies, and demonstrate the encouraging results compared with state of the art.",https://aclanthology.org/2020.findings-emnlp.59,Association for Computational Linguistics,2020,November,Findings of the Association for Computational Linguistics: EMNLP 2020,"Jiang, Juyong  and
Zhang, Jie  and
Zhang, Kai",Cascaded Semantic and Positional Self-Attention Network for Document Classification,10.18653/v1/2020.findings-emnlp.59,findings,476
2020.findings-emnlp.170,"['Audio Generation and Processing', 'Data Management and Generation', 'Learning Paradigms', 'Model Architectures']","['Transformer Models', 'Multimodal Learning', 'Data Preparation', 'Adversarial Learning']",,"We study relationships between spoken language and co-speech gestures in context of two key challenges. First, distributions of text and gestures are inherently skewed making it important to model the long tail. Second, gesture predictions are made at a subword level, making it important to learn relationships between language and acoustic cues. We introduce Adversarial Importance Sampled Learning or AISLe, which combines adversarial learning with importance sampling to strike a balance between precision and coverage. We propose the use of a multimodal multiscale attention block to perform subword alignment without the need of explicit alignment between language and acoustic cues. Finally, to empirically study the importance of language in this task, we extend the dataset proposed in Ahuja et al.  2020  with automatically extracted transcripts for audio signals. We substantiate the effectiveness of our approach through large-scale quantitative and user studies, which show that our proposed methodology significantly outperforms previous state-of-the-art approaches for gesture generation. Link to code, data and videos: https: //github.com/chahuja/aisle",https://aclanthology.org/2020.findings-emnlp.170,Association for Computational Linguistics,2020,November,Findings of the Association for Computational Linguistics: EMNLP 2020,"Ahuja, Chaitanya  and
Lee, Dong Won  and
Ishii, Ryo  and
Morency, Louis-Philippe",No Gestures Left Behind: Learning Relationships between Spoken Language and Freeform Gestures,10.18653/v1/2020.findings-emnlp.170,findings,187
J17-2002,"['Embeddings', 'Parsing', 'Multilingual NLP', 'Low-resource Languages', 'Model Architectures']","['Syntactic Parsing', 'Word Embeddings', 'Recurrent Neural Networks (RNNs)']","['Dependency Parsing', 'Long Short-Term Memory (LSTM) Models']","We introduce a greedy transition-based parser that learns to represent parser states using recurrent neural networks. Our primary innovation that enables us to do this efficiently is a new control structure for sequential neural networks-the stack long short-term memory unit LSTM. Like the conventional stack data structures used in transition-based parsers, elements can be pushed to or popped from the top of the stack in constant time, but, in addition, an LSTM maintains a continuous space embedding of the stack contents. Our model captures three facets of the parser's state: i unbounded look-ahead into the buffer of incoming words, ii the complete history of transition actions taken by the parser, and iii the complete contents of the stack of partially built tree fragments, including their internal structures. In addition, we compare two different word representations: i standard word vectors based on look-up tables and ii character-based models of words. Although standard word embedding models work well in all languages, the character-based models improve the handling of out-of-vocabulary words, particularly in morphologically rich languages. Finally, we discuss the use of dynamic oracles in training the parser. During training, dynamic oracles alternate between sampling parser states from the training data and from the model as it is being learned, making the model more robust to the kinds of errors that will be made at test time. Training our model with dynamic oracles yields a linear-time greedy parser with very competitive performance.",https://aclanthology.org/J17-2002,MIT Press,2017,June,,"Ballesteros, Miguel  and
Dyer, Chris  and
Goldberg, Yoav  and
Smith, Noah A.",Greedy Transition-Based Dependency Parsing with Stack LSTMs,10.1162/COLI_a_00285,J17,1297
2021.repl4nlp-1.13,"['Parsing', 'Learning Paradigms', 'Low-resource Languages', 'Cross-lingual Application']","['Few-shot Learning', 'Transfer Learning', 'Semantic Parsing']",,"Task-oriented compositional semantic parsing TCSP handles complex nested user queries and serves as an essential component of virtual assistants. Current TCSP models rely on numerous training data to achieve decent performance but fail to generalize to low-resource target languages or domains. In this paper, we present X2Parser, a transferable Crosslingual and Cross-domain Parser for TCSP. Unlike previous models that learn to generate the hierarchical representations for nested intents and slots, we propose to predict flattened intents and slots representations separately and cast both prediction tasks into sequence labeling problems. After that, we further propose a fertility-based slot predictor that first learns to dynamically detect the number of labels for each token, and then predicts the slot types. Experimental results illustrate that our model can significantly outperform existing strong baselines in cross-lingual and cross-domain settings, and our model can also achieve a good generalization ability on target languages of target domains. Furthermore, our model tackles the problem in an efficient nonautoregressive way that reduces the latency by up to 66% compared to the generative model. 1",https://aclanthology.org/2021.repl4nlp-1.13,Association for Computational Linguistics,2021,August,Proceedings of the 6th Workshop on Representation Learning for NLP (RepL4NLP-2021),"Liu, Zihan  and
Winata, Genta Indra  and
Xu, Peng  and
Fung, Pascale",X2Parser: Cross-Lingual and Cross-Domain Framework for Task-Oriented Compositional Semantic Parsing,10.18653/v1/2021.repl4nlp-1.13,repl4nlp,785
C16-1103,"['Text Generation', 'Model Architectures', 'Finite State Machines']",['Recurrent Neural Networks (RNNs)'],['Long Short-Term Memory (LSTM) Models'],"Recently Wen et al. 2015 have proposed a Recurrent Neural Network RNN approach to the generation of utterances from dialog acts, and shown that although their model requires less effort to develop than a rule-based system, it is able to improve certain aspects of the utterances, in particular their naturalness. However their system employs generation at the word-level, which requires one to pre-process the data by substituting named entities with placeholders. This preprocessing prevents the model from handling some contextual effects and from managing multiple occurrences of the same attribute. Our approach uses a character-level model, which unlike the word-level model makes it possible to learn to ""copy"" information from the dialog act to the target without having to pre-process the input. In order to avoid generating non-words and inventing information not present in the input, we propose a method for incorporating prior knowledge into the RNN in the form of a weighted finite-state automaton over character sequences. Automatic and human evaluations show improved performance over baselines on several evaluation criteria.",https://aclanthology.org/C16-1103,The COLING 2016 Organizing Committee,2016,December,"Proceedings of {COLING} 2016, the 26th International Conference on Computational Linguistics: Technical Papers","Goyal, Raghav  and
Dymetman, Marc  and
Gaussier, Eric",Natural Language Generation through Character-based RNNs with Finite-state Prior Knowledge,,C16,84
2021.konvens-1.14,"['Text Preprocessing', 'Domain-specific NLP', 'Low-resource Languages', 'Classification Applications']","['Hate and Offensive Speech Detection', 'NLP for News and Media']",['NLP for Social Media'],"Preprocessing is essential for creating more effective features and reducing noise in classification, especially in user-generated data e.g. Twitter. How each individual preprocessing decision changes an individual classifier's behavior is not universal. We perform a series of ablation experiments in which we examine how classifiers behave based on individual preprocessing steps when detecting offensive language in German. While preprocessing decisions for traditional classifier approaches are not as varied, we note that pre-trained BERT models are far more sensitive to each decision and do not behave identically to each other. We find that the cause of much variation between classifiers has to do with the interactions specific preprocessing steps have on the overall vocabulary distributions, and, in the case of BERT models, how this interacts with the WordPiece tokenization.",https://aclanthology.org/2021.konvens-1.14,KONVENS 2021 Organizers,2021,6--9 September,Proceedings of the 17th Conference on Natural Language Processing (KONVENS 2021),"Reimann, Sebastian  and
Dakota, Daniel",Examining the Effects of Preprocessing on the Detection of Offensive Language in German Tweets,,konvens,1392
2020.pam-1.7,"['Data Management and Generation', 'Learning Paradigms', 'Classification Applications', 'Model Architectures']","['Few-shot Learning', 'Transfer Learning', 'Data Preparation', 'Supervised Learning']",,"The major shortcomings of using neural networks with situated agents are that in incremental interaction very few learning examples are available and that their visual sensory representations are quite different from image caption datasets. In this work we adapt and evaluate a few-shot learning approach, Matching Networks Vinyals et al., 2016 , to conversational strategies of a robot interacting with a human tutor in order to efficiently learn to categorise objects that are presented to it and also investigate to what degree transfer learning from pre-trained models on images from different contexts can improve its performance. We discuss the implications of such learning on the nature of semantic representations the system has learned.",https://aclanthology.org/2020.pam-1.7,Association for Computational Linguistics,2020,June,Proceedings of the Probability and Meaning Conference (PaM 2020),"Cano Sant{\'\i}n, Jos{\'e} Miguel  and
Dobnik, Simon  and
Ghanimifard, Mehdi",Fast visual grounding in interaction: bringing few-shot learning with neural networks to an interactive robot,,pam,1116
2020.findings-emnlp.140,['Dialogue Systems'],['Response Generation'],,"Generating responses following a desired style has great potentials to extend applications of open-domain dialogue systems, yet is refrained by lacking of parallel data for training. In this work, we explore the challenging task with pre-trained language models that have brought breakthrough to various natural language tasks. To this end, we introduce a KL loss and a style classifier to the fine-tuning step in order to steer response generation towards the target style in both a word-level and a sentence-level. Comprehensive empirical studies with two public datasets indicate that our model can significantly outperform stateof-the-art methods in terms of both style consistency and contextual coherence.",https://aclanthology.org/2020.findings-emnlp.140,Association for Computational Linguistics,2020,November,Findings of the Association for Computational Linguistics: EMNLP 2020,"Yang, Ze  and
Wu, Wei  and
Xu, Can  and
Liang, Xinnian  and
Bai, Jiaqi  and
Wang, Liran  and
Wang, Wei  and
Li, Zhoujun",StyleDGPT: Stylized Response Generation with Pre-trained Language Models,10.18653/v1/2020.findings-emnlp.140,findings,493
2021.nllp-1.15,"['Domain-specific NLP', 'Parsing', 'Data Management and Generation', 'Image and Video Processing', 'Learning Paradigms', 'Low-resource Languages', 'Classification Applications']","['NLP for the Legal Domain', 'Multimodal Learning', 'Data Preparation', 'Document Layout Analysis (DLA)']",,"While many NLP pipelines assume raw, clean texts, many texts we encounter in the wild, including a vast majority of legal documents, are not so clean, with many of them being visually structured documents VSDs such as PDFs. Conventional preprocessing tools for VSDs mainly focused on word segmentation and coarse layout analysis, whereas finegrained logical structure analysis such as identifying paragraph boundaries and their hierarchies of VSDs is underexplored. To that end, we proposed to formulate the task as prediction of transition labels between text fragments that maps the fragments to a tree, and developed a feature-based machine learning system that fuses visual, textual and semantic cues. Our system is easily customizable to different types of VSDs and it significantly outperformed baselines in identifying different structures in VSDs. For example, our system obtained a paragraph boundary detection F1 score of 0.953 which is significantly better than a popular PDF-to-text tool with an F1 score of 0.739.",https://aclanthology.org/2021.nllp-1.15,Association for Computational Linguistics,2021,November,Proceedings of the Natural Legal Language Processing Workshop 2021,"Koreeda, Yuta  and
Manning, Christopher",Capturing Logical Structure of Visually Structured Documents with Multimodal Transition Parser,10.18653/v1/2021.nllp-1.15,nllp,490
E17-1043,"['Data Management and Generation', 'Domain-specific NLP', 'Information Extraction', 'Model Architectures']",['Data Preparation'],,"In this paper we tackle a unique and important problem of extracting a structured order from the conversation a customer has with an order taker at a restaurant. This is motivated by an actual system under development to assist in the order taking process. We develop a sequence-tosequence model that is able to map from unstructured conversational input to the structured form that is conveyed to the kitchen and appears on the customer receipt. This problem is critically different from other tasks like machine translation where sequence-to-sequence models have been used: the input includes two sides of a conversation; the output is highly structured; and logical manipulations must be performed, for example when the customer changes his mind while ordering. We present a novel sequence-to-sequence model that incorporates a special attention-memory gating mechanism and conversational role markers. The proposed model improves performance over both a phrase-based machine translation approach and a standard sequence-to-sequence model. Hi, how can I help you ? We'd like a large cheese pizza. Any toppings? Yeah, how about pepperoni and two diet cokes. What size? Uh, medium and make that three cokes. Anything else? A small Caesar salad with the dressing on the side Sure, is that it? Yes, that's all, thanks.",https://aclanthology.org/E17-1043,Association for Computational Linguistics,2017,April,"Proceedings of the 15th Conference of the {E}uropean Chapter of the Association for Computational Linguistics: Volume 1, Long Papers","Peng, Baolin  and
Seltzer, Michael  and
Ju, Y.C.  and
Zweig, Geoffrey  and
Wong, Kam-Fai",May I take your order? A Neural Model for Extracting Structured Information from Conversations,10.18653/v1/e17-1043,E17,1166
2021.econlp-1.12,"['Topic Modeling', 'Learning Paradigms', 'Domain-specific NLP']","['NLP for Finance', 'Supervised Learning']",,"Estimating the effects of monetary policy is one of the fundamental research questions in monetary economics. Many economies are facing ultra-low interest rate environments ever since the global financial crisis of 2007-9. The Covid pandemic recently reinforced this situation. In the US and Europe, interest rates are close to or even below zero, which limits the scope of traditional monetary policy measures for central banks. Dedicated central bank communication has hence become an increasingly important tool to steer and control market expectations these days. However, incorporating central bank language directly as features into economic models is still a very nascent research area. In particular, the content and effect of central bank speeches has been mostly neglected from monetary policy modelling so far. With our paper, we aim to provide to the research community a novel, monetary policy shock series based on central bank speeches. We use a supervised topic modeling approach that can deal with text as well as numeric covariates to estimate a monetary policy signal dispersion index along three key economic dimensions: GDP, CPI and unemployment. This ""dispersion shock"" series is not only more frequent than series that classically focus on policy announcement dates, it also opens up the possibility of answering new questions that have up until now been difficult to analyse. For example, do markets form different expectations when facing a ""cacophony of policy voices""? Our initial findings for the US point towards the fact that more dispersed or incongruent monetary policy stance communication in the build up to Federal Open Market Committee FOMC meetings might be associated with stronger subsequent market surprises at FOMC policy announcement time.",https://aclanthology.org/2021.econlp-1.12,Association for Computational Linguistics,2021,November,Proceedings of the Third Workshop on Economics and Natural Language Processing,"Ahrens, Maximilian  and
McMahon, Michael",Extracting Economic Signals from Central Bank Speeches,10.18653/v1/2021.econlp-1.12,econlp,1040
2021.case-1.4,"['Learning Paradigms', 'Information Extraction', 'Domain-specific NLP']","['Unsupervised Learning', 'NLP for News and Media', 'Event Extraction']",,"We analyze the effect of further pre-training BERT with different domain specific data as an unsupervised domain adaptation strategy for event extraction. Portability of event extraction models is particularly challenging, with large performance drops affecting data on the same text genres e.g., news. We present PROTEST-ER, a retrained BERT model for protest event extraction. PROTEST-ER outperforms a corresponding generic BERT on outof-domain data of 8.1 points. Our best performing models reach 51.91-46.39 F1 across both domains.",https://aclanthology.org/2021.case-1.4,Association for Computational Linguistics,2021,August,Proceedings of the 4th Workshop on Challenges and Applications of Automated Extraction of Socio-political Events from Text (CASE 2021),"Caselli, Tommaso  and
Mutlu, Osman  and
Basile, Angelo  and
H{\""u}rriyeto{\u{g}}lu, Ali",PROTEST-ER: Retraining BERT for Protest Event Extraction,10.18653/v1/2021.case-1.4,case,816
2021.disrpt-1.6,"['Text Preprocessing', 'Parsing', 'Low-resource Languages', 'Classification Applications']","['Text Segmentation', 'Discourse Parsing']",,"This paper describes our submission to the DISRPT2021 Shared Task on Discourse Unit Segmentation, Connective Detection, and Relation Classification. Our system, called Dis-CoDisCo, is a Transformer-based neural classifier which enhances contextualized word embeddings CWEs with hand-crafted features, relying on tokenwise sequence tagging for discourse segmentation and connective detection, and a feature-rich, encoder-less sentence pair classifier for relation classification. Our results for the first two tasks outperform SOTA scores from the previous 2019 shared task, and results on relation classification suggest strong performance on the new 2021 benchmark. Ablation tests show that including features beyond CWEs are helpful for both tasks, and a partial evaluation of multiple pre-trained Transformer-based language models indicates that models pre-trained on the Next Sentence Prediction NSP task are optimal for relation classification.",https://aclanthology.org/2021.disrpt-1.6,Association for Computational Linguistics,2021,November,Proceedings of the 2nd Shared Task on Discourse Relation Parsing and Treebanking (DISRPT 2021),"Gessler, Luke  and
Behzad, Shabnam  and
Liu, Yang Janet  and
Peng, Siyao  and
Zhu, Yilun  and
Zeldes, Amir","DisCoDisCo at the DISRPT2021 Shared Task: A System for Discourse Segmentation, Classification, and Connective Detection",10.18653/v1/2021.disrpt-1.6,disrpt,768
2021.repl4nlp-1.25,"['Knowledge Representation and Reasoning', 'Model Architectures']","['Transformer Models', 'Knowledge Graphs']",,"Pre-trained language models have emerged as highly successful methods for learning good text representations. However, the amount of structured knowledge retained in such models, and how if at all it can be extracted, remains an open question. In this work, we aim at directly learning text representations which leverage structured knowledge about entities mentioned in the text. This can be particularly beneficial for downstream tasks which are knowledge-intensive. Our approach utilizes self-attention between words in the text and knowledge graph KG entities mentioned in the text. While existing methods require entity-linked data for pre-training, we train using a mention-span masking objective and a candidate ranking objective -which doesn't require any entity-links and only assumes access to an alias table for retrieving candidates, enabling large-scale pre-training. We show that the proposed model learns knowledgeinformed text representations that yield improvements on the downstream tasks over existing methods.",https://aclanthology.org/2021.repl4nlp-1.25,Association for Computational Linguistics,2021,August,Proceedings of the 6th Workshop on Representation Learning for NLP (RepL4NLP-2021),"Thai, Dung  and
Thirukovalluru, Raghuveer  and
Bansal, Trapit  and
McCallum, Andrew",Simultaneously Self-Attending to Text and Entities for Knowledge-Informed Text Representations,10.18653/v1/2021.repl4nlp-1.25,repl4nlp,745
K16-1002,"['Text Generation', 'Model Architectures']",['Recurrent Neural Networks (RNNs)'],,"The standard recurrent neural network language model rnnlm generates sentences one word at a time and does not work from an explicit global sentence representation. In this work, we introduce and study an rnn-based variational autoencoder generative model that incorporates distributed latent representations of entire sentences. This factorization allows it to explicitly model holistic properties of sentences such as style, topic, and high-level syntactic features. Samples from the prior over these sentence representations remarkably produce diverse and well-formed sentences through simple deterministic decoding. By examining paths through this latent space, we are able to generate coherent novel sentences that interpolate between known sentences. We present techniques for solving the difficult learning problem presented by this model, demonstrate its effectiveness in imputing missing words, explore many interesting properties of the model's latent sentence space, and present negative results on the use of the model in language modeling.",https://aclanthology.org/K16-1002,Association for Computational Linguistics,2016,August,Proceedings of The 20th {SIGNLL} Conference on Computational Natural Language Learning,"Bowman, Samuel R.  and
Vilnis, Luke  and
Vinyals, Oriol  and
Dai, Andrew  and
Jozefowicz, Rafal  and
Bengio, Samy",Generating Sentences from a Continuous Space,10.18653/v1/K16-1002,K16,1050
2021.econlp-1.11,"['Data Management and Generation', 'Discourse Analysis', 'Domain-specific NLP']","['NLP for Finance', 'Data Preparation', 'NLP for News and Media']",['NLP for Social Media'],"With 56 million people actively trading and investing in cryptocurrency online and globally in 2020, there is an increasing need for automatic social media analysis tools to help understand trading discourse and behavior. In this work, we present a dual natural language modeling pipeline which leverages language and social network behaviors for the prediction of cryptocurrency day trading actions and their associated framing patterns. This pipeline first predicts if tweets can be used to guide day trading behavior, specifically if a cryptocurrency investor should buy, sell, or hold their cryptocurrencies in order to make a profit. Next, tweets are input to an unsupervised deep clustering approach to automatically detect trading framing patterns. Our contributions include the modeling pipeline for this novel task, a new Cryptocurrency Tweets Dataset compiled from influential accounts, and a Historical Price Dataset. Our experiments show that our approach achieves an 88.78% accuracy for day trading behavior prediction and reveals framing fluctuations prior to and during the COVID-19 pandemic that could be used to guide investment actions. 1 Introduction Beginning with the 2008 introduction of Bitcoin BTC Nakamoto, 2008, a cryptocurrency for a Peer-to-Peer cash system, the use of cryptocurrencies and their corresponding blockchains have increasingly gained in popularity. In 2019, the number of Americans owning cryptocurrency doubled from 7% in 2018 to 14%, representing about 35 million people trading and investing with cryptocurrency Partz, 2019. This increase is largely due to the capability of cryptocurrency to improve various applications ranging from increased security of smart contracts to facilitating less expensive, faster cross-border international payments. Another contributing factor to this growth is that digital coins fulfill the prop-042 erty of storing value similar to other fiat currencies, 043 which are government-issued currencies not backed 044 by physical commodities, e.g., the American dollar 045 or euro. Finally, cryptocurrency popularity can be 046 associated with its high day trading volume. As of 047",https://aclanthology.org/2021.econlp-1.11,Association for Computational Linguistics,2021,November,Proceedings of the Third Workshop on Economics and Natural Language Processing,"Pawlicka Maule, Anna Paula  and
Johnson, Kristen",Cryptocurrency Day Trading and Framing Prediction in Microblog Discourse,10.18653/v1/2021.econlp-1.11,econlp,1360
2021.iwcs-1.13,"['Question Answering (QA)', 'Data Management and Generation']",['Data Preparation'],,"Research in NLP has mainly focused on factoid questions, with the goal of finding quick and reliable ways of matching a query to an answer. However, human discourse involves more than that: it contains non-canonical questions deployed to achieve specific communicative goals. In this paper, we investigate this under-studied aspect of NLP by introducing a targeted task, creating an appropriate corpus for the task and providing baseline models of diverse nature. With this, we are also able to generate useful insights on the task and open the way for future research in this direction.",https://aclanthology.org/2021.iwcs-1.13,Association for Computational Linguistics,2021,June,Proceedings of the 14th International Conference on Computational Semantics (IWCS),"Kalouli, Aikaterini-Lida  and
Kehlbeck, Rebecca  and
Sevastjanova, Rita  and
Deussen, Oliver  and
Keim, Daniel  and
Butt, Miriam",Is that really a question? Going beyond factoid questions in NLP,,iwcs,402
2016.iwslt-1.11,"['Parsing', 'Low-resource Languages', 'Machine Translation (MT)']",['Syntactic Parsing'],,"String-to-tree MT systems translate verbs without lexical or syntactic context on the source side and with limited targetside context. The lack of context is one reason why verb translation recall is as low as 45.5%. We propose a verb lexicon model trained with a feedforward neural network that predicts the target verb conditioned on a wide source-side context. We show that a syntactic context extracted from the dependency parse of the source sentence improves the model's accuracy by 1.5% over a baseline trained on a window context. When used as an extra feature for re-ranking the n-best list produced by the string-to-tree MT system, the verb lexicon model improves verb translation recall by more than 7%.",https://aclanthology.org/2016.iwslt-1.11,International Workshop on Spoken Language Translation,2016,December 8-9,Proceedings of the 13th International Conference on Spoken Language Translation,"N{\u{a}}dejde, Maria  and
Birch, Alexandra  and
Koehn, Philipp",A Neural Verb Lexicon Model with Source-side Syntactic Context for String-to-Tree Machine Translation,,iwslt,300
2020.alw-1.5,"['Domain-specific NLP', 'Model Architectures', 'Classification Applications', 'Learning Paradigms']","['NLP for News and Media', 'Recurrent Neural Networks (RNNs)', 'Supervised Learning', 'Hate and Offensive Speech Detection', 'Transformer Models']","['Long Short-Term Memory (LSTM) Models', 'NLP for Social Media']","The detection of abusive or offensive remarks in social texts has received significant attention in research. In several related shared tasks, BERT has been shown to be the state-of-theart. In this paper, we propose to utilize lexical features derived from a hate lexicon towards improving the performance of BERT in such tasks. We explore different ways to utilize the lexical features in the form of lexicon-based encodings at the sentence level or embeddings at the word level. We provide an extensive dataset evaluation that addresses in-domain as well as cross-domain detection of abusive content to render a complete picture. Our results indicate that our proposed models combining BERT with lexical features help improve over a baseline BERT model in many of our indomain and cross-domain experiments.",https://aclanthology.org/2020.alw-1.5,Association for Computational Linguistics,2020,November,Proceedings of the Fourth Workshop on Online Abuse and Harms,"Koufakou, Anna  and
Pamungkas, Endang Wahyu  and
Basile, Valerio  and
Patti, Viviana",HurtBERT: Incorporating Lexical Features with BERT for the Detection of Abusive Language,10.18653/v1/2020.alw-1.5,alw,1375
2022.dialdoc-1.7,['Information Retrieval'],['Search Engines'],,"We deal with the scenario of conversational search, where user queries are under-specified or ambiguous. This calls for a mixed-initiative setup. User-asks queries and system-answers, as well as system-asks clarification questions and user response, in order to clarify her information needs. We focus on the task of selecting the next clarification question, given the conversation context. Our method leverages passage retrieval from a background content to fine-tune two deep-learning models for ranking candidate clarification questions. We evaluated our method on two different use-cases. The first is an open domain conversational search in a large web collection. The second is a taskoriented customer-support setup. We show that our method performs well on both use-cases.",https://aclanthology.org/2022.dialdoc-1.7,Association for Computational Linguistics,2022,May,Proceedings of the Second DialDoc Workshop on Document-grounded Dialogue and Conversational Question Answering,"Mass, Yosi  and
Cohen, Doron  and
Yehudai, Asaf  and
Konopnicki, David",Conversational Search with Mixed-Initiative - Asking Good Clarification Questions backed-up by Passage Retrieval,10.18653/v1/2022.dialdoc-1.7,dialdoc,181
I17-1065,"['Classification Applications', 'Model Architectures']",['Sentiment Analysis (SA)'],,"Deep learning models have recently been applied successfully in natural language processing, especially sentiment analysis. Each deep learning model has a particular advantage, but it is difficult to combine these advantages into one model, especially in the area of sentiment analysis. In our approach, Convolutional Neural Network CNN and Long Short Term Memory LSTM were utilized to learn sentiment-specific features in a freezing scheme. This scenario provides a novel and efficient way for integrating advantages of deep learning models. In addition, we also grouped documents into clusters by their similarity and applied the prediction score of Naive Bayes SVM NB-SVM method to boost the classification accuracy of each group. The experiments show that our method achieves the stateof-the-art performance on two well-known datasets: IMDB large movie reviews for document level and Pang & Lee movie reviews for sentence level.",https://aclanthology.org/I17-1065,Asian Federation of Natural Language Processing,2017,November,Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 1: Long Papers),"Nguyen, Huy Tien  and
Nguyen, Minh Le",An Ensemble Method with Sentiment Features and Clustering Support,,I17,1410
2021.wassa-1.2,"['Data Management and Generation', 'Classification Applications', 'Model Architectures']","['Data Preparation', 'Recurrent Neural Networks (RNNs)', 'Multilabel Text Classification']",,"The aim of the paper is twofold: 1 to automatically predict the ratings assigned by viewers to 14 categories available for TED talks in a multi-label classification task and 2 to determine what types of features drive classification accuracy for each of the categories. The focus is on features of language usage from five groups pertaining to syntactic complexity, lexical richness, register-based n-gram measures, information-theoretic measures and LIWCstyle measures. We show that a Recurrent Neural Network classifier trained exclusively on within-text distributions of such features can reach relatively high levels of overall accuracy 69% across the 14 categories. We find that features from two groups are strong predictors of the affective ratings across all categories and that there are distinct patterns of language usage for each rating category.",https://aclanthology.org/2021.wassa-1.2,Association for Computational Linguistics,2021,April,"Proceedings of the Eleventh Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis","Kerz, Elma  and
Qiao, Yu  and
Wiechmann, Daniel",Language that Captivates the Audience: Predicting Affective Ratings of TED Talks in a Multi-Label Classification Task,,wassa,989
2021.nlp4call-1.6,"['Data Management and Generation', 'Error Detection and Correction']","['Data Augmentation', 'Grammatical Error Correction (GEC)', 'Data Preparation']",,"One of the issues in automatically evaluating learner input in the context of Intelligent Tutoring Systems is learners' use of incorrect forms and non-standard language. Grammatical Error Correction GEC systems have emerged as a way of automatically correcting grammar and spelling mistakes, often by approaching the task as machine translation of individual sentences from non-standard to standard language. However, due to the inherent lack of context awareness, GEC systems often do not produce a contextually appropriate correction. In this paper, we investigate how current neural GEC systems can be optimized for educationally relevant tasks such as Short Answer Assessment. We build on a recent GEC system and train a reranker based on context e.g. similarity to prompt, task e.g. type and format and answerlevel e.g. language modeling features on a Short Answer Assessment data set augmented with crowd worker corrections. Results show that our approach successfully gives preference to corrections that are closer to the reference.",https://aclanthology.org/2021.nlp4call-1.6,LiU Electronic Press,2021,May,Proceedings of the 10th Workshop on NLP for Computer Assisted Language Learning,"Ziai, Ramon  and
Karnysheva, Anna",Leveraging Task Information in Grammatical Error Correction for Short Answer Assessment through Context-based Reranking,,nlp4call,1048
P19-1594,['Model Architectures'],,,"Spectral models for learning weighted nondeterministic automata have nice theoretical and algorithmic properties. Despite this, it has been challenging to obtain competitive results in language modeling tasks, for two main reasons. First, in order to capture long-range dependencies of the data, the method must use statistics from long substrings, which results in very large matrices that are difficult to decompose. The second is that the loss function behind spectral learning, based on moment matching, differs from the probabilistic metrics used to evaluate language models. In this work we employ a technique for scaling up spectral learning, and use interpolated predictions that are optimized to maximize perplexity. Our experiments in character-based language modeling show that our method matches the performance of stateof-the-art ngram models, while being very fast to train.",https://aclanthology.org/P19-1594,Association for Computational Linguistics,2019,July,Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,"Quattoni, Ariadna  and
Carreras, Xavier",Interpolated Spectral NGram Language Models,10.18653/v1/P19-1594,P19,949
C18-1177,"['Parsing', 'Data Management and Generation', 'Information Extraction', 'Low-resource Languages', 'Model Architectures']","['Morphological Parsing', 'Data Preparation', 'Named Entity Recognition (NER)', 'Recurrent Neural Networks (RNNs)']",['Long Short-Term Memory (LSTM) Models'],"Previous studies have shown that linguistic features of a word such as possession, genitive or other grammatical cases can be employed in word representations of a named entity recognition NER tagger to improve the performance for morphologically rich languages. However, these taggers require external morphological disambiguation MD tools to function which are hard to obtain or non-existent for many languages. In this work, we propose a model which alleviates the need for such disambiguators by jointly learning NER and MD taggers in languages for which one can provide a list of candidate morphological analyses. We show that this can be done independent of the morphological annotation schemes, which differ among languages. Our experiments employing three different model architectures that join these two tasks show that joint learning improves NER performance. Furthermore, the morphological disambiguator's performance is shown to be competitive.",https://aclanthology.org/C18-1177,Association for Computational Linguistics,2018,August,Proceedings of the 27th International Conference on Computational Linguistics,"G{\""u}ng{\""o}r, Onur  and
Uskudarli, Suzan  and
G{\""u}ng{\""o}r, Tunga",Improving Named Entity Recognition by Jointly Learning to Disambiguate Morphological Tags,10.48550/arxiv.1807.06683,C18,1450
2020.nlposs-1.20,['Evaluation Techniques'],,,"For the last 5 years, we have developed and maintained RSMTool -an open-source tool for evaluating NLP systems that automatically score written and spoken responses. RSMTool is designed to be cross-disciplinary, borrowing heavily from NLP, machine learning, and educational measurement. Its crossdisciplinary nature has required us to learn a user-centered development approach in terms of both design and implementation. We share some of these lessons in this paper. 1 https://www.measurementinc.com/ products-services/automated-essay-scoring 2 https://www.ets.org/accelerate/ai-portfolio/ speechrater be evaluated as thoroughly as possible to detect any harmful, systematic biases in their predictions. However, this may prove difficult for an NLP or machine learning researcher since they may be unfamiliar with the required psychometric and statistical checks. RSMTool incorporates a large, diverse set of psychometric and statistical analyses aimed at detecting possible bias in system performance and makes them available in an easy-to-use package. RSMTool is open-source and non-proprietary so that the automated scoring community can not only audit the source code of the already available analyses to ensure their compliance with fairness standards but also contribute new analyses.",https://aclanthology.org/2020.nlposs-1.20,Association for Computational Linguistics,2020,November,Proceedings of Second Workshop for NLP Open Source Software (NLP-OSS),"Madnani, Nitin  and
Loukina, Anastassia",User-centered \& Robust NLP OSS: Lessons Learned from Developing \& Maintaining RSMTool,10.18653/v1/2020.nlposs-1.20,nlposs,478
S16-1200,"['Learning Paradigms', 'Domain-specific NLP', 'Information Extraction']","['Supervised Learning', 'Medical and Clinical NLP']",,"In this paper, we describe the system developed for our participation in the Clinical TempEval task of SemEval 2016 task 12. Our team focused on the subtasks of span and attribute identification from raw text and proposed a system that integrates both statistical and linguistic approaches. Our system is based on Conditional Random Fields with high-precision linguistic features.",https://aclanthology.org/S16-1200,Association for Computational Linguistics,2016,June,Proceedings of the 10th International Workshop on Semantic Evaluation ({S}em{E}val-2016),"Hansart, Charlotte  and
De Meyere, Damien  and
Watrin, Patrick  and
Bittar, Andr{\'e}  and
Fairon, C{\'e}drick",CENTAL at SemEval-2016 Task 12: a linguistically fed CRF model for medical and temporal information extraction,10.18653/v1/S16-1200,S16,774
2020.coling-main.146,"['Embeddings', 'Information Extraction', 'Learning Paradigms', 'Classification Applications', 'Model Architectures']","['Word Embeddings', 'Supervised Learning', 'Relation Extraction']",,"Distantly Supervised Relation Extraction DSRE has proven to be effective to find relational facts from texts, but it still suffers from two main problems: the wrong labeling problem and the long-tail problem. Most of the existing approaches address these two problems through flat classification, which lacks hierarchical information of relations. To leverage the informative relation hierarchies, we formulate DSRE as a hierarchical classification task and propose a novel hierarchical classification framework, which extracts the relation in a top-down manner. Specifically, in our proposed framework, 1 we use a hierarchically-refined representation method to achieve hierarchy-specific representation; 2 a top-down classification strategy is introduced instead of training a set of local classifiers. The experiments on NYT dataset demonstrate that our approach significantly outperforms other state-of-the-art approaches, especially for the long-tail problem.",https://aclanthology.org/2020.coling-main.146,International Committee on Computational Linguistics,2020,December,Proceedings of the 28th International Conference on Computational Linguistics,"Yu, Erxin  and
Han, Wenjuan  and
Tian, Yuan  and
Chang, Yi",ToHRE: A Top-Down Classification Strategy with Hierarchical Bag Representation for Distantly Supervised Relation Extraction,10.18653/v1/2020.coling-main.146,coling,615
D18-1296,"['Audio Generation and Processing', 'Dialogue Systems', 'Model Architectures']","['Automatic Speech Recognition (ASR)', 'Recurrent Neural Networks (RNNs)']",['Long Short-Term Memory (LSTM) Models'],"We propose to generalize language models for conversational speech recognition to allow them to operate across utterance boundaries and speaker changes, thereby capturing conversation-level phenomena such as adjacency pairs, lexical entrainment, and topical coherence. The model consists of a long-shortterm memory LSTM recurrent network that reads the entire word-level history of a conversation, as well as information about turn taking and speaker overlap, in order to predict each next word. The model is applied in a rescoring framework, where the word history prior to the current utterance is approximated with preliminary recognition results. In experiments in the conversational telephone speech domain Switchboard we find that such a model gives substantial perplexity reductions over a standard LSTM-LM with utterance scope, as well as improvements in word error rate.",https://aclanthology.org/D18-1296,Association for Computational Linguistics,2018,October-November,Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing,"Xiong, Wayne  and
Wu, Lingfeng  and
Zhang, Jun  and
Stolcke, Andreas",Session-level Language Modeling for Conversational Speech,10.18653/v1/D18-1296,D18,742
W16-2805,"['Evaluation Techniques', 'Argument Mining', 'Data Management and Generation']",['Data Preparation'],['Annotation Processes'],"Argument mining integrates many distinct computational linguistics tasks, and as a result, reporting agreement between annotators or between automated output and gold standard is particularly challenging. More worrying for the field, agreement and performance are also reported in a wide variety of different ways, making comparison between approaches difficult. To solve this problem, we propose the CASS technique for combining metrics covering different parts of the argument mining task. CASS delivers a justified method of integrating results yielding confusion matrices from which CASS- and CASS-F1 scores can be calculated.",https://aclanthology.org/W16-2805,Association for Computational Linguistics,2016,August,Proceedings of the Third Workshop on Argument Mining ({A}rg{M}ining2016),"Duthie, Rory  and
Lawrence, John  and
Budzynska, Katarzyna  and
Reed, Chris",The CASS Technique for Evaluating the Performance of Argument Mining,10.18653/v1/W16-2805,W16,1207
P18-1195,['Model Architectures'],['Recurrent Neural Networks (RNNs)'],,"Despite the effectiveness of recurrent neural network language models, their maximum likelihood estimation suffers from two limitations. It treats all sentences that do not match the ground truth as equally poor, ignoring the structure of the output space. Second, it suffers from ""exposure bias"": during training tokens are predicted given ground-truth sequences, while at test time prediction is conditioned on generated output sequences. To overcome these limitations we build upon the recent reward augmented maximum likelihood approach i.e. sequence-level smoothing that encourages the model to predict sentences close to the ground truth according to a given performance metric. We extend this approach to token-level loss smoothing, and propose improvements to the sequence-level smoothing approach. Our experiments on two different tasks, image captioning and machine translation, show that token-level and sequence-level loss smoothing are complementary, and significantly improve results.",https://aclanthology.org/P18-1195,Association for Computational Linguistics,2018,July,Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),"Elbayad, Maha  and
Besacier, Laurent  and
Verbeek, Jakob",Token-level and sequence-level loss smoothing for RNN language models,10.18653/v1/P18-1195,P18,791
2020.splu-1.6,"['Domain-specific NLP', 'Information Extraction', 'Classification Applications', 'Model Architectures']","['Transformer Models', 'Medical and Clinical NLP']",,"Radiology reports contain important clinical information about patients which are often tied through spatial expressions. Spatial expressions or triggers are mainly used to describe the positioning of radiographic findings or medical devices with respect to some anatomical structures. As the expressions result from the mental visualization of the radiologist's interpretations, they are varied and complex. The focus of this work is to automatically identify the spatial expression terms from three different radiology sub-domains. We propose a hybrid deep learning-based NLP method that includes -1 generating a set of candidate spatial triggers by exact match with the known trigger terms from the training data, 2 applying domain-specific constraints to filter the candidate triggers, and 3 utilizing a BERTbased classifier to predict whether a candidate trigger is a true spatial trigger or not. The results are promising, with an improvement of 24 points in the average F1 measure compared to a standard BERT-based sequence labeler.",https://aclanthology.org/2020.splu-1.6,Association for Computational Linguistics,2020,November,Proceedings of the Third International Workshop on Spatial Language Understanding,"Datta, Surabhi  and
Roberts, Kirk",A Hybrid Deep Learning Approach for Spatial Trigger Extraction from Radiology Reports,10.18653/v1/2020.splu-1.6,splu,782
2021.dialdoc-1.4,"['Question Answering (QA)', 'Domain-specific NLP', 'Low-resource Languages']",,,"This paper presents a learning assistant that tests one's knowledge and gives feedback that helps a person learn at a faster pace. A learning assistant based on an automated question generation has extensive uses in education, information websites, self-assessment, FAQs, testing ML agents, research, etc. Multiple researchers, and companies have worked on Virtual Assistance, but majorly in English. We",https://aclanthology.org/2021.dialdoc-1.4,Association for Computational Linguistics,2021,August,Proceedings of the 1st Workshop on Document-grounded Dialogue and Conversational Question Answering (DialDoc 2021),"Bommadi, Meghana  and
Terupally, Shreya  and
Mamidi, Radhika",Automatic Learning Assistant in Telugu,10.18653/v1/2021.dialdoc-1.4,dialdoc,315
2020.ngt-1.8,"['Model Architectures', 'Text Generation']",['Recurrent Neural Networks (RNNs)'],['Long Short-Term Memory (LSTM) Models'],"The answer-agnostic question generation is a significant and challenging task, which aims to automatically generate questions for a given sentence but without an answer. In this paper, we propose two new strategies to deal with this task: question type prediction and copy loss mechanism. The question type module is to predict the types of questions that should be asked, which allows our model to generate multiple types of questions for the same source sentence. The new copy loss enhances the original copy mechanism to make sure that every important word in the source sentence has been copied when generating questions. Our integrated model outperforms the state-of-theart approach in answer-agnostic question generation, achieving a BLEU-4 score of 13.9 on SQuAD. Human evaluation further validates the high quality of our generated questions. We will make our code public available for further research.",https://aclanthology.org/2020.ngt-1.8,Association for Computational Linguistics,2020,July,Proceedings of the Fourth Workshop on Neural Generation and Translation,"Wu, Xiuyu  and
Jiang, Nan  and
Wu, Yunfang",A Question Type Driven and Copy Loss Enhanced Frameworkfor Answer-Agnostic Neural Question Generation,10.18653/v1/2020.ngt-1.8,ngt,329
2021.computel-1.6,"['Machine Translation (MT)', 'Data Management and Generation', 'Low-resource Languages', 'Domain-specific NLP']","['Data Augmentation', 'Data Preparation', 'Neural MT (NMT)']",,"Bibles are available in a wide range of languages, which provides valuable parallel text between languages since verses can be aligned accurately between all the different translations. How well can such data be utilized to train good neural machine translation NMT models? We are particularly interested in low-resource languages of high morphological complexity, and attempt to answer this question in the current work by training and evaluating Basque-English and Navajo-English MT models with the Transformer architecture. Different tokenization methods are applied, among which syllabification turns out to be most effective for Navajo and it is also good for Basque. Another additional data resource which can be potentially available for endangered languages is a dictionary of either word or phrase translations, thanks to linguists' work on language documentation. Could this data be leveraged to augment Bible data for better performance? We experiment with different ways to utilize dictionary data, and find that word-to-word mapping translation with a word-pair dictionary is more effective than low-resource techniques such as backtranslation or adding dictionary data directly into the training set, though neither backtranslation nor word-to-word mapping translation produce improvements over using Bible data alone in our experiments.",https://aclanthology.org/2021.computel-1.6,Association for Computational Linguistics,2021,March,Proceedings of the 4th Workshop on the Use of Computational Methods in the Study of Endangered Languages Volume 1 (Papers),"Liu, Ling  and
Ryan, Zach  and
Hulden, Mans",The Usefulness of Bibles in Low-Resource Machine Translation,10.33011/computel.v1i.957,computel,806
2019.lilt-18.4,"['Parsing', 'Data Management and Generation', 'Low-resource Languages']","['Data Analysis', 'Syntactic Parsing']",,"This paper presents a case study of the use of the NINJAL Parsed Corpus of Modern Japanese NPCMJ for syntactic research. NPCMJ is the first phrase structure-based treebank for Japanese that is specifically designed for application in linguistic in addition to NLP research. After discussing some basic methodological issues pertaining to the use of treebanks for theoretical linguistics research, we introduce our case study on the status of the Coordinate Structure Constraint CSC in Japanese, showing that NPCMJ enables us to easily retrieve examples that support one of the key claims of Kubota and Lee 2015 : that the CSC should be viewed as a pragmatic, rather than a syntactic constraint. The corpus-based study we conducted moreover revealed a previously unnoticed tendency that was highly relevant for further clarifying the principles governing the empirical data in question. We conclude the paper by briefly discussing some further methodological issues brought up by our case study pertaining to the relationship between linguistic research and corpus development.",https://aclanthology.org/2019.lilt-18.4,CSLI Publications,2019,July,"Linguistic Issues in Language Technology, Volume 18, 2019 - Exploiting Parsed Corpora: Applications in Research, Pedagogy, and Processing","Kubota, Yusuke  and
Kubota, Ai",Probing the nature of an island constraint with a parsed corpus,10.33011/lilt.v18i.1433,lilt,693
2016.tc-1.14,"['Knowledge Representation and Reasoning', 'Low-resource Languages', 'Machine Translation (MT)']","['Semantic Web', 'Statistical MT (SMT)']",,"In this paper we outline easily implementable procedures to leverage multilingual Linked Open Data LOD resources such as the DBpedia in open-source Statistical Machine Translation SMT systems such as Moses. Using open standards such as RDF Resource Description Framework Schema, NIF Natural language processing Interchange Format, and SPARQL SPARQL Protocol and RDF Query Language queries, we demonstrate the efficacy of translating named entities and thereby improving the quality and consistency of SMT outputs. We also give a brief overview of two funded projects that are actively working on this topic. These are the 1 BMBF funded project DKT Digitale Kuratierungstechnologien on digital curation technologies, and 2 EU Horizon 2020 funded project FREME Open Framework of e-services for Multilingual and Semantic Enrichment of Digital Content. This is a step towards designing a Semantic Web-aware Machine Translation MT system and keeping SMT algorithms up-to-date with the current stage of web development Web 3.0.",https://aclanthology.org/2016.tc-1.14,AsLing,2016,November 17-18,Proceedings of Translating and the Computer 38,"Srivastava, Ankit  and
Sasaki, Felix  and
Moreno-Schneider, Peter Bourgonje. Julian  and
Nehring, Jan  and
Rehm, Georg",How to configure statistical machine translation with linked open data resources,,tc,116
2021.conll-1.43,"['Domain-specific NLP', 'Data Management and Generation', 'Classification Applications']","['NLP for News and Media', 'Data Preparation', 'Hate and Offensive Speech Detection', 'Data Analysis']","['NLP for Social Media', 'Annotation Processes']","As users in online communities suffer from severe side effects of abusive language, many researchers attempted to detect abusive texts from social media, presenting several datasets for such detection. However, none of them contain both comprehensive labels and contextual information, which are essential for thoroughly detecting all kinds of abusiveness from texts, since datasets with such fine-grained features demand a significant amount of annotations, leading to much increased complexity. In this paper, we propose a Comprehensive Abusiveness Detection Dataset CADD, collected from the English Reddit posts, with multifaceted labels and contexts. Our dataset is annotated hierarchically for an efficient annotation through crowdsourcing on a large-scale. We also empirically explore the characteristics of our dataset and provide a detailed analysis for novel insights. The results of our experiments with strong pre-trained natural language understanding models on our dataset show that our dataset gives rise to meaningful performance, assuring its practicality for abusive language detection.",https://aclanthology.org/2021.conll-1.43,Association for Computational Linguistics,2021,November,Proceedings of the 25th Conference on Computational Natural Language Learning,"Song, Hoyun  and
Ryu, Soo Hyun  and
Lee, Huije  and
Park, Jong",A Large-scale Comprehensive Abusiveness Detection Dataset with Multifaceted Labels from Reddit,10.18653/v1/2021.conll-1.43,conll,979
2020.challengehml-1.8,"['Model Architectures', 'Learning Paradigms']",['Multimodal Learning'],,"An artificial intelligenceAI system should be capable of processing the sensory inputs to extract both task-specific and general information about its environment. However, most of the existing algorithms extract only task specific information. In this work, an innovative approach to address the problem of processing visual sensory data is presented by utilizing convolutional neural network CNN. It recognizes and represents the physical and semantic nature of the surrounding in both human readable and machine processable format. This work utilizes the image captioning model to capture the semantics of the input image and a modular design to generate a probability distribution for semantic topics. It gives any autonomous system the ability to process visual information in a human-like way and generates more insights which are hardly possible with a conventional algorithm. Here a model and data collection method are proposed.",https://aclanthology.org/2020.challengehml-1.8,Association for Computational Linguistics,2020,July,Second Grand-Challenge and Workshop on Multimodal Language (Challenge-HML),"Singh, Yuvaram  and
JV, Kameshwar Rao",AI Sensing for Robotics using Deep Learning based Visual and Language Modeling,10.18653/v1/2020.challengehml-1.8,challengehml,1470
2020.inlg-1.9,['Text Generation'],['Data-to-Text Generation'],,"We present a novel approach to data-to-text generation based on iterative text editing. Our approach maximizes the completeness and semantic accuracy of the output text while leveraging the abilities of recent pre-trained models for text editing LASERTAGGER and language modeling GPT-2 to improve the text fluency. To this end, we first transform data items to text using trivial templates, and then we iteratively improve the resulting text by a neural model trained for the sentence fusion task. The output of the model is filtered by a simple heuristic and reranked with an offthe-shelf pre-trained language model. We evaluate our approach on two major data-to-text datasets WebNLG, Cleaned E2E and analyze its caveats and benefits. Furthermore, we show that our formulation of data-to-text generation opens up the possibility for zero-shot domain adaptation using a general-domain dataset for sentence fusion.",https://aclanthology.org/2020.inlg-1.9,Association for Computational Linguistics,2020,December,Proceedings of the 13th International Conference on Natural Language Generation,"Kasner, Zden{\v{e}}k  and
Du{\v{s}}ek, Ond{\v{r}}ej",Data-to-Text Generation with Iterative Text Editing,10.18653/v1/2020.inlg-1.9,inlg,1171
2020.nlp4musa-1.2,"['Text Preprocessing', 'Automatic Text Summarization', 'Text Generation', 'Domain-specific NLP', 'Topic Modeling', 'Data Management and Generation', 'Classification Applications']","['Text Segmentation', 'Lyrics Generation', 'Data Analysis']",,"In this paper we propose lyrics information processing LIP as a research field for technologies focusing on lyrics text, which has both linguistic and musical characteristics. This field could bridge the natural language processing field and the music information retrieval field, leverage technologies developed in those fields, and bring challenges that encourage the development of new technologies. We introduce three main approaches in LIP, 1 lyrics analysis, 2 lyrics generation and writing support, and 3 lyrics-centered applications, and briefly discuss their importance, current approaches, and limitations. 1 Lyrics and poetry are different types of text because lyrics are assumed to be sung along with music. However, some linguistic properties of lyrics and poetry overlap.",https://aclanthology.org/2020.nlp4musa-1.2,Association for Computational Linguistics,2020,16-Oct,Proceedings of the 1st Workshop on NLP for Music and Audio (NLP4MusA),"Watanabe, Kento  and
Goto, Masataka","Lyrics Information Processing: Analysis, Generation, and Applications",,nlp4musa,153
2020.figlang-1.18,"['Figurative Language', 'Embeddings', 'Classification Applications', 'Model Architectures']","['Transformer Models', 'Metaphors']",,"Recent work on automatic sequential metaphor detection has involved recurrent neural networks initialized with different pre-trained word embeddings and which are sometimes combined with hand engineered features. To capture lexical and orthographic information automatically, in this paper we propose to add character based word representation. Also, to contrast the difference between literal and contextual meaning, we utilize a similarity network. We explore these components via two different architectures -a BiLSTM model and a Transformer Encoder model similar to BERT to perform metaphor identification. We participate in the Second Shared Task on Metaphor Detection on both the VUA and TOFEL datasets with the above models. The experimental results demonstrate the effectiveness of our method as it outperforms all the systems which participated in the previous shared task.",https://aclanthology.org/2020.figlang-1.18,Association for Computational Linguistics,2020,July,Proceedings of the Second Workshop on Figurative Language Processing,"Kumar, Tarun  and
Sharma, Yashvardhan",Character aware models with similarity learning for metaphor detection,10.18653/v1/2020.figlang-1.18,figlang,609
2022.lchange-1.4,"['Embeddings', 'Low-resource Languages', 'Language Change Analysis']","['Semantic Change Analysis', 'Word Embeddings']",,"Contextual word embedding techniques for semantic shift detection are receiving more and more attention. In this paper, we present What is Done is Done WiDiD, an incremental approach to semantic shift detection based on incremental clustering techniques and contextual embedding methods to capture the changes over the meanings of a target word along a diachronic corpus. In WiDiD, the word contexts observed in the past are consolidated as a set of clusters that constitute the ""memory"" of the word meanings observed so far. Such a memory is exploited as a basis for subsequent word observations, so that the meanings observed in the present are stratified over the past ones.",https://aclanthology.org/2022.lchange-1.4,Association for Computational Linguistics,2022,May,Proceedings of the 3rd Workshop on Computational Approaches to Historical Language Change,"Periti, Francesco  and
Ferrara, Alfio  and
Montanelli, Stefano  and
Ruskov, Martin",What is Done is Done: an Incremental Approach to Semantic Shift Detection,10.18653/v1/2022.lchange-1.4,lchange,1409
2022.acl-long.85,"['Data Management and Generation', 'Knowledge Representation and Reasoning', 'Model Architectures']","['Transformer Models', 'Data Augmentation', 'Knowledge Graphs']",,"Pre-trained sequence-to-sequence language models have led to widespread success in many natural language generation tasks. However, there has been relatively less work on analyzing their ability to generate structured outputs such as graphs. Unlike natural language, graphs have distinct structural and semantic properties in the context of a downstream NLP task, e.g., generating a graph that is connected and acyclic can be attributed to its structural constraints, while the semantics of a graph can refer to how meaningfully an edge represents the relation between two node concepts. In this work, we study pre-trained language models that generate explanation graphs in an end-to-end manner and analyze their ability to learn the structural constraints and semantics of such graphs. We first show that with limited supervision, pre-trained language models often generate graphs that either violate these constraints or are semantically incoherent. Since curating large amount of humanannotated graphs is expensive and tedious, we propose simple yet effective ways of graph perturbations via node and edge edit operations that lead to structurally and semantically positive and negative graphs. Next, we leverage these graphs in different contrastive learning models with Max-Margin and InfoNCE losses. Our methods lead to significant improvements in both structural and semantic accuracy of explanation graphs and also generalize to other similar graph generation tasks. Lastly, we show that human errors are the best negatives for contrastive learning and also that automatically generating more such human-like nega-",https://aclanthology.org/2022.acl-long.85,Association for Computational Linguistics,2022,May,Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),"Saha, Swarnadeep  and
Yadav, Prateek  and
Bansal, Mohit",Explanation Graph Generation via Pre-trained Language Models: An Empirical Study with Contrastive Learning,10.18653/v1/2022.acl-long.85,acl,1451
2020.findings-emnlp.127,"['Model Architectures', 'Dialogue Systems', 'Information Retrieval', 'Knowledge Representation and Reasoning']","['Recurrent Neural Networks (RNNs)', 'Information Filtering', 'Chatbots']",['Long Short-Term Memory (LSTM) Models'],"The challenges of building knowledgegrounded retrieval-based chatbots lie in how to ground a conversation on its background knowledge and how to match response candidates with both context and knowledge simultaneously. This paper proposes a method named Filtering before Iteratively REferring FIRE for this task. In this method, a context filter and a knowledge filter are first built, which derive knowledge-aware context representations and context-aware knowledge representations respectively by global and bidirectional attention. Besides, the entries irrelevant to the conversation are discarded by the knowledge filter. After that, iteratively referring is performed between context and response representations as well as between knowledge and response representations, in order to collect deep matching features for scoring response candidates. Experimental results show that FIRE outperforms previous methods by margins larger than 2.8% and 4.1% on the PERSONA-CHAT dataset with original and revised personas respectively, and margins larger than 3.1% on the CMU DoG dataset in terms of top-1 accuracy. We also show that FIRE is more interpretable by visualizing the knowledge grounding process.",https://aclanthology.org/2020.findings-emnlp.127,Association for Computational Linguistics,2020,November,Findings of the Association for Computational Linguistics: EMNLP 2020,"Gu, Jia-Chen  and
Ling, Zhenhua  and
Liu, Quan  and
Chen, Zhigang  and
Zhu, Xiaodan",Filtering before Iteratively Referring for Knowledge-Grounded Response Selection in Retrieval-Based Chatbots,10.18653/v1/2020.findings-emnlp.127,findings,72
2022.wassa-1.24,"['Learning Paradigms', 'Domain-specific NLP', 'Classification Applications', 'Model Architectures']","['Transformer Models', 'Emotion Detection', 'Supervised Learning', 'NLP for News and Media']",,"This paper describes team PVG's AI Club's approach to the Emotion Classification shared task held at WASSA 2022. This Track 2 subtask focuses on building models which can predict a multi-class emotion label based on essays from news articles where a person, group or another entity is affected. Baseline transformer models have been demonstrating good results on sequence classification tasks, and we aim to improve this performance with the help of ensembling techniques, and by leveraging two variations of emotion-specific representations. We observe better results than our baseline models and achieve an accuracy of 0.619 and a macro F1 score of 0.520 on the emotion classification task.",https://aclanthology.org/2022.wassa-1.24,Association for Computational Linguistics,2022,May,"Proceedings of the 12th Workshop on Computational Approaches to Subjectivity, Sentiment {\&} Social Media Analysis","Desai, Shaily  and
Kshirsagar, Atharva  and
Sidnerlikar, Aditi  and
Khodake, Nikhil  and
Marathe, Manisha",Leveraging Emotion-Specific features to improve Transformer performance for Emotion Classification,10.18653/v1/2022.wassa-1.24,wassa,937
2020.signlang-1.10,['Image and Video Processing'],,,"Proform constructs such as classifier predicates and size and shape specifiers are essential elements of Sign Language, but have remained a challenge for synthesis due to their highly variable nature. In contrast to frozen signs, which may be pre-animated or recorded, their variability necessitates a new approach both to their linguistic description and to their synthesis in animation. Though the specification and animation of classifier predicates was covered in previous works, size and shape specifiers have to this date remained unaddressed. This paper presents an efficient method for linguistically describing such specifiers using a small number of rules that cover a large range of possible constructs. It continues to show that with a small number of services in a signing avatar, these descriptions can be synthesized in a natural way that captures the essential gestural actions while also including the subtleties of human motion that make the signing legible.",https://aclanthology.org/2020.signlang-1.10,European Language Resources Association (ELRA),2020,May,"Proceedings of the LREC2020 9th Workshop on the Representation and Processing of Sign Languages: Sign Language Resources in the Service of the Language Community, Technological Challenges and Application Perspectives","Filhol, Michael  and
McDonald, John C.",The Synthesis of Complex Shape Deployments in Sign Language,,signlang,357
2020.law-1.6,"['Biases in NLP', 'Data Management and Generation']",,,"The development of linguistic corpora is fraught with various problems of annotation and representation. These constitute a very real challenge for the development and use of annotated corpora, but as yet not much literature exists on how to address the underlying problems. In this paper, we identify and discuss five sources of representation problems, which are independent though interrelated: ambiguity, variation, uncertainty, error and bias. We outline and characterize these sources, discussing how their improper treatment can have stark consequences for research outcomes. Finally, we discuss how an adequate treatment can inform corpus-related linguistic research, both computational and theoretical, improving the reliability of research results and NLP models, as well as informing the more general reproducibility issue.",https://aclanthology.org/2020.law-1.6,Association for Computational Linguistics,2020,December,Proceedings of the 14th Linguistic Annotation Workshop,"Beck, Christin  and
Booth, Hannah  and
El-Assady, Mennatallah  and
Butt, Miriam","Representation Problems in Linguistic Annotations: Ambiguity, Variation, Uncertainty, Error and Bias",,law,527
2020.ccl-1.79,"['Image and Video Processing', 'Information Extraction', 'Model Architectures']",['Graph Neural Networks (GNNs)'],,"Information extraction from documents such as receipts or invoices is a fundamental and crucial step for office automation. Many approaches focus on extracting entities and relationships from plain texts, however, when it comes to document images, such demand becomes quite challenging since visual and layout information are also of great significance to help tackle this problem. In this work, we propose the attention-based graph neural network to combine textual and visual information from document images. Moreover, the global node is introduced in our graph construction algorithm which is used as a virtual hub to collect the information from all the nodes and edges to help improve the performance. Extensive experiments on real-world datasets show that our method outperforms baseline methods by significant margins.",https://aclanthology.org/2020.ccl-1.79,Chinese Information Processing Society of China,2020,October,Proceedings of the 19th Chinese National Conference on Computational Linguistics,"Hua, Yuan  and
Huang, Zheng  and
Guo, Jie  and
Qiu, Weidong",Attention-Based Graph Neural Network with Global Context Awareness for Document Understanding,10.1007/978-3-030-63031-7_4,ccl,641
2021.nlp4posimpact-1.3,"['Ethics', 'Data Management and Generation']","['Data Preparation', 'Data Analysis']",['Annotation Processes'],"The range of works that can be considered as developing NLP for social good NLP4SG is enormous. While many of them target the identification of hate speech or fake news, there are others that address, e.g., text simplification to alleviate consequences of dyslexia, or coaching strategies to fight depression. However, so far, there is no clear picture of what areas are targeted by NLP4SG, who are the actors, which are the main scenarios and what are the topics that have been left aside. In order to obtain a clearer view in this respect, we first propose a working definition of NLP4SG and identify some primary aspects that are crucial for NLP4SG, including, e.g., tackled areas, ethics, privacy and bias. Then, we draw upon a corpus of around 50,000 articles downloaded from the ACL Anthology. Based on a list of keywords retrieved from the literature and revised in view of the task, we retrieve from this corpus articles that can be considered to be on NLP4SG according to our definition and analyze them. The result of the analysis is a map of the current NLP4SG research and insights concerning the white spots on this map.",https://aclanthology.org/2021.nlp4posimpact-1.3,Association for Computational Linguistics,2021,August,Proceedings of the 1st Workshop on NLP for Positive Impact,"Fortuna, Paula  and
P{\'e}rez-Mayos, Laura  and
AbuRa{'}ed, Ahmed  and
Soler-Company, Juan  and
Wanner, Leo","Cartography of Natural Language Processing for Social Good NLP4SG: Searching for Definitions, Statistics and White Spots",10.18653/v1/2021.nlp4posimpact-1.3,nlp4posimpact,1096
W17-1314,"['Domain-specific NLP', 'Low-resource Languages', 'Classification Applications']","['Sentiment Analysis (SA)', 'NLP for News and Media']",['NLP for Social Media'],"Opinion mining in Arabic is a challenging task given the rich morphology of the language. The task becomes more challenging when it is applied to Twitter data, which contains additional sources of noise, such as the use of unstandardized dialectal variations, the nonconformation to grammatical rules, the use of Arabizi and code-switching, and the use of non-text objects such as images and URLs to express opinion. In this paper, we perform an analytical study to observe how such linguistic phenomena vary across different Arab regions. This study of Arabic Twitter characterization aims at providing better understanding of Arabic Tweets, and fostering advanced research on the topic. Furthermore, we explore the performance of the two schools of machine learning on Arabic Twitter, namely the feature engineering approach and the deep learning approach. We consider models that have achieved state-of-the-art performance for opinion mining in English. Results highlight the advantages of using deep learning-based models, and confirm the importance of using morphological abstractions to address Arabic's complex morphology.",https://aclanthology.org/W17-1314,Association for Computational Linguistics,2017,April,Proceedings of the Third {A}rabic Natural Language Processing Workshop,"Baly, Ramy  and
Badaro, Gilbert  and
El-Khoury, Georges  and
Moukalled, Rawan  and
Aoun, Rita  and
Hajj, Hazem  and
El-Hajj, Wassim  and
Habash, Nizar  and
Shaban, Khaled",A Characterization Study of Arabic Twitter Data with a Benchmarking for State-of-the-Art Opinion Mining Models,10.18653/v1/W17-1314,W17,189
2020.semeval-1.139,"['Domain-specific NLP', 'Model Architectures', 'Classification Applications']","['NLP for News and Media', 'Humor Detection']",,"This paper describes the work done by the team UniTuebingenCL for the SemEval 2020 Task 7: ""Assessing the Funniness of Edited News Headlines"". We participated in both sub-tasks: sub-task A, given the original and the edited headline, predicting the mean funniness of the edited headline; and sub-task B, given the original headline and two edited versions, predicting which edited version is the funnier of the two. A Ridge Regression model using Elmo and Glove embeddings as well as Truncated Singular Value Decomposition was used as the final model. A long short term memory model recurrent network LSTM served as another approach for assessing the funniness of a headline. For the first sub-task, we experimented with the extraction of multiple features to achieve lower Root Mean Squared Error. The lowest Root Mean Squared Error achieved was 0.575 for sub-task A, and the highest Accuracy was 0.618 for sub-task B.",https://aclanthology.org/2020.semeval-1.139,International Committee for Computational Linguistics,2020,December,Proceedings of the Fourteenth Workshop on Semantic Evaluation,"Ammer, Charlotte  and
Gr{\""u}ner, Lea",UniTuebingenCL at SemEval-2020 Task 7: Humor Detection in News Headlines,10.18653/v1/2020.semeval-1.139,semeval,1269
D19-1453,"['Machine Translation (MT)', 'Learning Paradigms', 'Low-resource Languages', 'Model Architectures']","['Transformer Models', 'Neural MT (NMT)']",,"The state of the art in machine translation MT is governed by neural approaches, which typically provide superior translation accuracy over statistical approaches. However, on the closely related task of word alignment, traditional statistical word alignment models often remain the go-to solution. In this paper, we present an approach to train a Transformer model to produce both accurate translations and alignments. We extract discrete alignments from the attention probabilities learnt during regular neural machine translation model training and leverage them in a multi-task framework to optimize towards translation and alignment objectives. We demonstrate that our approach produces competitive results compared to GIZA++ trained IBM alignment models without sacrificing translation accuracy and outperforms previous attempts on Transformer model based word alignment. Finally, by incorporating IBM model alignments into our multi-task training, we report significantly better alignment accuracies compared to GIZA++ on three publicly available data sets. Our implementation has been open-sourced 1 .",https://aclanthology.org/D19-1453,Association for Computational Linguistics,2019,November,Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"Garg, Sarthak  and
Peitz, Stephan  and
Nallasamy, Udhyakumar  and
Paulik, Matthias",Jointly Learning to Align and Translate with Transformer Models,10.18653/v1/D19-1453,D19,10
2020.pam-1.8,"['Classification Applications', 'Knowledge Representation and Reasoning']",,,"We present a formal semantics a version of Type Theory with Records which places classifiers of perceptual information at the core of semantics. Using this framework, we present an account of the interpretation and classification of utterances referring to perceptually available situations such as visual scenes. The account improves on previous work by clarifying the role of classifiers in a hybrid semantics combining statistical/neural classifiers with logical/inferential aspects of meaning. The account covers both discrete and probabilistic classification, thereby enabling learning, vagueness and other non-discrete linguistic phenomena.",https://aclanthology.org/2020.pam-1.8,Association for Computational Linguistics,2020,June,Proceedings of the Probability and Meaning Conference (PaM 2020),"Larsson, Staffan",Discrete and Probabilistic Classifier-based Semantics,,pam,247
2021.newsum-1.6,"['Biases in NLP', 'Evaluation Techniques', 'Data Management and Generation', 'Automatic Text Summarization']","['Data Preparation', 'Data Analysis']",,"Summarization systems are ultimately evaluated by human annotators and raters. Usually, annotators and raters do not reflect the demographics of end users, but are recruited through student populations or crowdsourcing platforms with skewed demographics. For two different evaluation scenarios -evaluation against gold summaries and system output ratings -we show that summary evaluation is sensitive to protected attributes. This can severely bias system development and evaluation, leading us to build models that cater for some groups rather than others.",https://aclanthology.org/2021.newsum-1.6,Association for Computational Linguistics,2021,November,Proceedings of the Third Workshop on New Frontiers in Summarization,"J{\o}rgensen, Anna  and
S{\o}gaard, Anders","Evaluation of Summarization Systems across Gender, Age, and Race",10.18653/v1/2021.newsum-1.6,newsum,1391
D19-1041,"['Embeddings', 'Information Extraction', 'Model Architectures']","['Temporal Event Understanding', 'Recurrent Neural Networks (RNNs)', 'Relation Extraction']",['Long Short-Term Memory (LSTM) Models'],"We propose a joint event and temporal relation extraction model with shared representation learning and structured prediction. The proposed method has two advantages over existing work. First, it improves event representation by allowing the event and relation modules to share the same contextualized embeddings and neural representation learner. Second, it avoids error propagation in the conventional pipeline systems by leveraging structured inference and learning methods to assign both the event labels and the temporal relation labels jointly. Experiments show that the proposed method can improve both event extraction and temporal relation extraction over state-of-the-art systems, with the end-to-end F 1 improved by 10% and 6.8% on two benchmark datasets respectively.",https://aclanthology.org/D19-1041,Association for Computational Linguistics,2019,November,Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"Han, Rujun  and
Ning, Qiang  and
Peng, Nanyun",Joint Event and Temporal Relation Extraction with Shared Representations and Structured Prediction,10.18653/v1/D19-1041,D19,235
P16-1111,"['Domain-specific NLP', 'Discourse Analysis', 'Topic Modeling', 'Parsing', 'Classification Applications']",['NLP for Bibliometrics and Scientometrics'],,"Computationally modeling the evolution of science by tracking how scientific topics rise and fall over time has important implications for research funding and public policy. However, little is known about the mechanisms underlying topic growth and decline. We investigate the role of rhetorical framing: whether the rhetorical role or function that authors ascribe to topics as methods, as goals, as results, etc. relates to the historical trajectory of the topics. We train topic models and a rhetorical function classifier to map topic models onto their rhetorical roles in 2.4 million abstracts from the Web of Science from 1991-2010. We find that a topic's rhetorical function is highly predictive of its eventual growth or decline. For example, topics that are rhetorically described as results tend to be in decline, while topics that function as methods tend to be in early phases of growth.",https://aclanthology.org/P16-1111,Association for Computational Linguistics,2016,August,Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),"Prabhakaran, Vinodkumar  and
Hamilton, William L.  and
McFarland, Dan  and
Jurafsky, Dan",Predicting the Rise and Fall of Scientific Topics from Trends in their Rhetorical Framing,10.18653/v1/P16-1111,P16,1145
2021.iwcs-1.3,"['Evaluation Techniques', 'Knowledge Representation and Reasoning', 'Commonsense Reasoning']",['Link Prediction'],,"In this work we leverage commonsense knowledge in the form of knowledge paths to establish connections between sentences, as a form of explicitation of implicit knowledge. Such connections can be direct singlehop paths or require intermediate concepts multihop paths. To construct such paths we combine two model types in a joint framework we call CO-NNECT: a relation classifier that predicts direct connections between concepts; and a target prediction model that generates target or intermediate concepts given a source concept and a relation, which we use to construct multihop paths. Unlike prior work that relies exclusively on static knowledge sources, we leverage language models finetuned on knowledge stored in ConceptNet, to dynamically generate knowledge paths, as explanations of implicit knowledge that connects sentences in texts. As a central contribution we design manual and automatic evaluation settings for assessing the quality of the generated paths. We conduct evaluations on two argumentative datasets and show that a combination of the two model types generates meaningful, high-quality knowledge paths between sentences that reveal implicit knowledge conveyed in text.",https://aclanthology.org/2021.iwcs-1.3,Association for Computational Linguistics,2021,June,Proceedings of the 14th International Conference on Computational Semantics (IWCS),"Becker, Maria  and
Korfhage, Katharina  and
Paul, Debjit  and
Frank, Anette",CO-NNECT: A Framework for Revealing Commonsense Knowledge Paths as Explicitations of Implicit Knowledge in Texts,10.48550/arxiv.2105.03157,iwcs,1173
2022.deeplo-1.5,"['Text Preprocessing', 'Information Retrieval', 'Data Management and Generation']","['Text Segmentation', 'Data Preparation']",,"Text segmentation and extraction from unstructured documents can provide business researchers with a wealth of new information on firms and their behaviors. However, the most valuable text is often difficult to extract consistently due to substantial variations in how content can appear from document to document. Thus, the most successful way to extract this content has been through costly crowdsourcing and training of manual workers. We propose the Assisted Neural Text Segmentation ANTS framework to identify pertinent text in unstructured documents from a small set of labeled examples. ANTS leverages deep learning and transfer learning architectures to empower researchers to identify relevant text with minimal manual coding. Using a real world sample of accounting documents, we identify targeted sections 96% of the time using only 5 training examples.",https://aclanthology.org/2022.deeplo-1.5,Association for Computational Linguistics,2022,July,Proceedings of the Third Workshop on Deep Learning for Low-Resource Natural Language Processing,"Chivers, Brian  and
P. Jiang, Mason  and
Lee, Wonhee  and
Ng, Amy  and
Rapstine, Natalya I.  and
Storer, Alex",ANTS: A Framework for Retrieval of Text Segments in Unstructured Documents,10.18653/v1/2022.deeplo-1.5,deeplo,302
2021.nlp4if-1.11,"['Domain-specific NLP', 'Classification Applications', 'Model Architectures']","['Transformer Models', 'Misinformation Detection', 'NLP for News and Media', 'Medical and Clinical NLP']",,"The explosion of online health news articles runs the risk of the proliferation of low-quality information. Within the existing work on factchecking, however, relatively little attention has been paid to medical news. We present a health news classification task to determine whether medical news articles satisfy a set of review criteria deemed important by medical experts and health care journalists. We present a dataset of 1,119 health news paired with systematic reviews. The review criteria consist of six elements that are essential to the accuracy of medical news. We then present experiments comparing the classical token-based approach with the more recent transformer-based models. Our results show that detecting qualitative lapses is a challenging task with direct ramifications in misinformation, but is an important direction to pursue beyond assigning True or False labels to short claims.",https://aclanthology.org/2021.nlp4if-1.11,Association for Computational Linguistics,2021,June,"Proceedings of the Fourth Workshop on NLP for Internet Freedom: Censorship, Disinformation, and Propaganda","Zuo, Chaoyuan  and
Zhang, Qi  and
Banerjee, Ritwik",An Empirical Assessment of the Qualitative Aspects of Misinformation in Health News,10.18653/v1/2021.nlp4if-1.11,nlp4if,1471
2021.argmining-1.14,"['Argument Mining', 'Learning Paradigms', 'Classification Applications']",,,"Argument role labeling is a fundamental task in Argument Mining research. However, such research often suffers from a lack of largescale datasets labeled for argument roles such as evidence, which is crucial for neural model training. While large pretrained language models have somewhat alleviated the need for massive manually labeled datasets, how much these models can further benefit from self-training techniques hasn't been widely explored in the literature in general and in Argument Mining specifically. In this work, we focus on self-trained language models particularly BERT for evidence detection. We provide a thorough investigation on how to utilize pseudo labels effectively in the selftraining scheme. We also assess whether adding pseudo labels from an out-of-domain source can be beneficial. Experiments on sentence level evidence detection show that selftraining can complement pretrained language models to provide performance improvements.",https://aclanthology.org/2021.argmining-1.14,Association for Computational Linguistics,2021,November,Proceedings of the 8th Workshop on Argument Mining,"Elaraby, Mohamed  and
Litman, Diane",Self-trained Pretrained Language Models for Evidence Detection,10.18653/v1/2021.argmining-1.14,argmining,705
2021.americasnlp-1.22,"['Learning Paradigms', 'Low-resource Languages', 'Cross-lingual Application', 'Multilingual NLP', 'Machine Translation (MT)']","['Neural MT (NMT)', 'Transfer Learning']",,"Peru is a multilingual country with a long history of contact between the indigenous languages and Spanish. Taking advantage of this context for machine translation is possible with multilingual approaches for learning both unsupervised subword segmentation and neural machine translation models. The study proposes the first multilingual translation models for four languages spoken in Peru: Aymara, Ashaninka, Quechua and Shipibo-Konibo, providing both many-to-Spanish and Spanishto-many models and outperforming pairwise baselines in most of them. The task exploited a large English-Spanish dataset for pretraining, monolingual texts with tagged backtranslation, and parallel corpora aligned with English. Finally, by fine-tuning the best models, we also assessed the out-of-domain capabilities in two evaluation datasets for Quechua and a new one for Shipibo-Konibo 1 .",https://aclanthology.org/2021.americasnlp-1.22,Association for Computational Linguistics,2021,June,Proceedings of the First Workshop on Natural Language Processing for Indigenous Languages of the Americas,"Oncevay, Arturo","Peru is Multilingual, Its Machine Translation Should Be Too?",10.18653/v1/2021.americasnlp-1.22,americasnlp,737
2022.dadc-1.8,"['Learning Paradigms', 'Data Management and Generation']","['Data Preparation', 'Adversarial Learning']",,"Large language models increasingly saturate existing task benchmarks, in some cases outperforming humans, leaving little headroom with which to measure further progress. Adversarial dataset creation, which builds datasets using examples that a target system outputs incorrect predictions for, has been proposed as a strategy to construct more challenging datasets, avoiding the more serious challenge of building more precise benchmarks by conventional means. In this work, we study the impact of applying three common approaches for adversarial dataset creation: 1 filtering out easy examples AFLite,  2  perturbing examples TextFooler, and 3 model-in-the-loop data collection ANLI and AdversarialQA, across 18 different adversary models. We find that all three methods can produce more challenging datasets, with stronger adversary models lowering the performance of evaluated models more. However, the resulting ranking of the evaluated models can also be unstable and highly sensitive to the choice of adversary model. Moreover, we find that AFLite oversamples examples with low annotator agreement, meaning that model comparisons hinge on the examples that are most contentious for humans. We recommend that researchers tread carefully when using adversarial methods for building evaluation datasets.",https://aclanthology.org/2022.dadc-1.8,Association for Computational Linguistics,2022,July,Proceedings of the First Workshop on Dynamic Adversarial Data Collection,"Phang, Jason  and
Chen, Angelica  and
Huang, William  and
Bowman, Samuel R.","Adversarially Constructed Evaluation Sets Are More Challenging, but May Not Be Fair",10.18653/v1/2022.dadc-1.8,dadc,342
2020.ngt-1.23,"['Low-resource Languages', 'Text Generation', 'Multilingual NLP', 'Model Architectures', 'Machine Translation (MT)']","['Neural MT (NMT)', 'Paraphrase and Rephrase Generation']",,This paper describes the third place submission to the shared task on simultaneous translation and paraphrasing for language education at the 4th workshop on Neural Generation and Translation WNGT for ACL 2020. The final system leverages pre-trained translation models and uses a Transformer architecture combined with an oversampling strategy to achieve a competitive performance. This system significantly outperforms the baseline on Hungarian 27% absolute improvement in Weighted Macro F1 score and Portuguese 33% absolute improvement languages.,https://aclanthology.org/2020.ngt-1.23,Association for Computational Linguistics,2020,July,Proceedings of the Fourth Workshop on Neural Generation and Translation,"Chada, Rakesh",Simultaneous paraphrasing and translation by fine-tuning Transformer models,10.18653/v1/2020.ngt-1.23,ngt,311
2019.ccnlg-1.6,"['Domain-specific NLP', 'Data Management and Generation', 'Learning Paradigms', 'Text Generation']","['Unsupervised Learning', 'Data Preparation', 'Paraphrase and Rephrase Generation']",,"Nominalization is a common technique in academic writing for producing abstract and formal text. Since it often involves paraphrasing a clause with a verb or adjectival phrase into a noun phrase, an important task is to generate the noun to replace the original verb or adjective. Given that a verb or adjective may have multiple nominalized forms with similar meaning, the system needs to be able to automatically select the most appropriate one. We propose an unsupervised algorithm that makes the selection with BERT, a stateof-the-art neural language model. Experimental results show that it significantly outperforms baselines based on word frequencies, word2vec and doc2vec.",https://aclanthology.org/2019.ccnlg-1.6,Association for Computational Linguistics,2019,29 October--3 November,Proceedings of the 4th Workshop on Computational Creativity in Language Generation,"Saberi, Dariush  and
Lee, John",Noun Generation for Nominalization in Academic Writing,,ccnlg,677
2021.mrqa-1.6,"['Biases in NLP', 'Question Answering (QA)', 'Data Management and Generation', 'Text Generation']",['Data Augmentation'],,"Question answering QA models for reading comprehension have been demonstrated to exploit unintended dataset biases such as question-context lexical overlap. This hinders QA models from generalizing to underrepresented samples such as questions with low lexical overlap. Question generation QG, a method for augmenting QA datasets, can be a solution for such performance degradation if QG can properly debias QA datasets. However, we discover that recent neural QG models are biased towards generating questions with high lexical overlap, which can amplify the dataset bias. Moreover, our analysis reveals that data augmentation with these QG models frequently impairs the performance on questions with low lexical overlap, while improving that on questions with high lexical overlap. To address this problem, we use a synonym replacement-based approach to augment questions with low lexical overlap. We demonstrate that the proposed data augmentation approach is simple yet effective to mitigate the degradation problem with only 70k synthetic examples. 1",https://aclanthology.org/2021.mrqa-1.6,Association for Computational Linguistics,2021,November,Proceedings of the 3rd Workshop on Machine Reading for Question Answering,"Shinoda, Kazutoshi  and
Sugawara, Saku  and
Aizawa, Akiko",Can Question Generation Debias Question Answering Models? A Case Study on Question--Context Lexical Overlap,10.18653/v1/2021.mrqa-1.6,mrqa,829
2022.starsem-1.25,"['Domain-specific NLP', 'Data Management and Generation', 'Learning Paradigms', 'Low-resource Languages', 'Classification Applications', 'Model Architectures']","['Data Preparation', 'Supervised Learning', 'NLP for News and Media']","['Annotation Processes', 'NLP for Social Media']","Recognizing speech acts SA is crucial for capturing meaning beyond what is said, making communicative intentions particularly relevant to identify urgent messages. This paper attempts to measure for the first time the impact of SA on urgency detection during crises, in tweets. We propose a new dataset annotated for both urgency and SA, and develop several deep learning architectures to inject SA into urgency detection while ensuring models generalisability. Our results show that taking speech acts into account in tweet analysis improves information type detection in an out-of-type configuration where models are evaluated in unseen event types during training. These results are encouraging and constitute a first step towards SA-aware disaster management in social media.",https://aclanthology.org/2022.starsem-1.25,Association for Computational Linguistics,2022,July,Proceedings of the 11th Joint Conference on Lexical and Computational Semantics,"Enzo, Laurenti  and
Nils, Bourgon  and
Benamara, Farah  and
Alda, Mari  and
Moriceau, V{\'e}ronique  and
Camille, Courgeon",Speech acts and Communicative Intentions for Urgency Detection,10.18653/v1/2022.starsem-1.25,starsem,751
J18-3002,"['Machine Translation (MT)', 'Evaluation Techniques']",,,"The BLEU metric has been widely used in NLP for over 15 years to evaluate NLP systems, especially in machine translation and natural language generation. I present a structured review of the evidence on whether BLEU is a valid evaluation technique-in other words, whether BLEU scores correlate with real-world utility and user-satisfaction of NLP systems; this review covers 284 correlations reported in 34 papers. Overall, the evidence supports using BLEU for diagnostic evaluation of MT systems which is what it was originally proposed for, but does not support using BLEU outside of MT, for evaluation of individual texts, or for scientific hypothesis testing.",https://aclanthology.org/J18-3002,MIT Press,2018,September,,"Reiter, Ehud",A Structured Review of the Validity of BLEU,10.1162/coli_a_00322,J18,421
2022.gebnlp-1.27,"['Biases in NLP', 'Model Architectures']","['Transformer Models', 'Gender Bias']",,"Knowledge distillation is widely used to transfer the language understanding of a large model to a smaller model. However, after knowledge distillation, it was found that the smaller model is more biased by gender compared to the source large model. This paper studies what causes gender bias to increase after the knowledge distillation process. Moreover, we suggest applying a variant of the mixup on knowledge distillation, which is used to increase generalizability during the distillation process, not for augmentation. By doing so, we can significantly reduce the gender bias amplification after knowledge distillation. We also conduct an experiment on the GLUE benchmark to demonstrate that even if the mixup is applied, it does not have a significant adverse effect on the model's performance.",https://aclanthology.org/2022.gebnlp-1.27,Association for Computational Linguistics,2022,July,Proceedings of the 4th Workshop on Gender Bias in Natural Language Processing (GeBNLP),"Ahn, Jaimeen  and
Lee, Hwaran  and
Kim, Jinhwa  and
Oh, Alice",Why Knowledge Distillation Amplifies Gender Bias and How to Mitigate from the Perspective of DistilBERT,10.18653/v1/2022.gebnlp-1.27,gebnlp,1009
2016.lilt-14.4,"['Data Management and Generation', 'Low-resource Languages']",['Data Preparation'],['Annotation Processes'],"In this paper we present current work on the design and validation of a linguistically-motivated annotation model of modality in English and Spanish in the context of the MULTINOT project. 1 Our annotation model captures four basic modal meanings and their subtypes, on the one hand, and provides a fine-grained characterisation of the syntactic realisations of those meanings in English and Spanish, on the other. We validate the modal tagset proposed through an agreement study performed on a bilingual sample of four hundred sentences extracted from original texts of the MULTINOT corpus, and discuss the difficult cases encountered in the annotation experiment. We also describe current steps in the implementation of the proposed scheme for the large-scale annotation of the bilingual corpus using both automatic and manual procedures. 1 The MULTINOT project is financed by the Spanish Ministry of Economy and Competitiveness, under grant number FF2012-32201. We gratefully acknowledge the support provided by the Spanish authorities. We also thank the comments and suggestions provided by the anonymous reviewers which have helped to improve the current manuscript.",https://aclanthology.org/2016.lilt-14.4,CSLI Publications,2016,sept,"Linguistic Issues in Language Technology, Volume 14, 2016 - Modality: Logic, Semantics, Annotation, and Machine Learning","Lavid, Julia  and
Carretrero, Marta  and
Zamorano-Mansilla, Juan Rafael",A linguistically-motivated annotation model of modality in English and Spanish: Insights from MULTINOT,10.33011/lilt.v14i.1399,lilt,1422
2021.wat-1.24,"['Machine Translation (MT)', 'Multilingual NLP', 'Low-resource Languages']","['Neural MT (NMT)', 'Statistical MT (SMT)']",,This work shows that competitive translation results can be obtained in a constrained setting by incorporating the latest advances in memory and compute optimization. We train and evaluate large multilingual translation models using a single GPU for a maximum of 100 hours and get within 4-5 BLEU points of the top submission on the WAT 2021 leaderboard. We also benchmark standard baselines on the PMI corpus and re-discover well-known shortcomings of current translation metrics.,https://aclanthology.org/2021.wat-1.24,Association for Computational Linguistics,2021,August,Proceedings of the 8th Workshop on Asian Translation (WAT2021),"Aralikatte, Rahul  and
Murrieta Bello, H{\'e}ctor Ricardo  and
de Lhoneux, Miryam  and
Hershcovich, Daniel  and
Bollmann, Marcel  and
S{\o}gaard, Anders",How far can we get with one GPU in 100 hours? CoAStaL at MultiIndicMT Shared Task,10.18653/v1/2021.wat-1.24,wat,659
W16-2354,"['Low-resource Languages', 'Classification Applications', 'Cross-lingual Application']",,,"We present a system submitted to the WMT16 shared task in cross-lingual pronoun prediction, in particular, to the English-to-German and German-to-English sub-tasks. The system is based on a linear classifier making use of features both from the target language model and from linguistically analyzed source and target texts. Furthermore, we apply example weighing in classifier learning, which proved to be beneficial for recall in less frequent pronoun classes. Compared to other shared task participants, our best English-to-German system is able to rank just below the top performing submissions.",https://aclanthology.org/W16-2354,Association for Computational Linguistics,2016,August,"Proceedings of the First Conference on Machine Translation: Volume 2, Shared Task Papers","Nov{\'a}k, Michal",Pronoun Prediction with Linguistic Features and Example Weighing,10.18653/v1/W16-2354,W16,1053
2020.vardial-1.8,"['Text Preprocessing', 'Data Management and Generation', 'Low-resource Languages']",['Data Preparation'],,"This paper analyses the challenge of working with dialectal variation when semi-automatically normalising and analysing historical Basque texts. This work is part of a more general ongoing project for the construction of a morphosyntactically annotated historical corpus of Basque called Basque in the Making BIM: A Historical Look at a European Language Isolate, whose main objective is the systematic and diachronic study of a number of grammatical features. This will be not only the first tagged corpus of historical Basque, but also a means to improve language processing tools by analysing historical Basque varieties more or less distant from present-day standard Basque.",https://aclanthology.org/2020.vardial-1.8,International Committee on Computational Linguistics (ICCL),2020,December,"Proceedings of the 7th Workshop on NLP for Similar Languages, Varieties and Dialects","Estarrona, Ainara  and
Etxeberria, Izaskun  and
Etxepare, Ricardo  and
Padilla-Moyano, Manuel  and
Soraluze, Ander",Dealing with dialectal variation in the construction of the Basque historical corpus,,vardial,1460
W16-1519,['Domain-specific NLP'],['NLP for Bibliometrics and Scientometrics'],['Citation Analysis'],"As a way to tackle Task 1A in CL-SciSumm 2016, we introduce a composite model consisting of TFIDF and Neural Network NN, the latter being a adaptation of the embedding model originally proposed for the Q/A domain 2, 7 . We discuss an experiment using a development data, results thereof, and some remaining issues.",https://aclanthology.org/W16-1519,,2016,June,Proceedings of the Joint Workshop on Bibliometric-enhanced Information Retrieval and Natural Language Processing for Digital Libraries ({BIRNDL}),"Nomoto, Tadashi",NEAL: A Neurally Enhanced Approach to Linking Citation and Reference,,W16,482
2020.textgraphs-1.4,"['Data Management and Generation', 'Model Architectures', 'Classification Applications', 'Learning Paradigms']","['Graph Neural Networks (GNNs)', 'Data Preparation']",,"Graph-based semi-supervised learning is appealing when labels are scarce but large amounts of unlabeled data are available. These methods typically use a heuristic strategy to construct the graph based on some fixed data representation, independently of the available labels. In this paper, we propose to jointly learn a data representation and a graph from both labeled and unlabeled data such that i the learned representation indirectly encodes the label information injected into the graph, and ii the graph provides a smooth topology with respect to the transformed data. Plugging the resulting graph and representation into existing graph-based semi-supervised learning algorithms like label spreading and graph convolutional networks, we show that our approach outperforms standard graph construction methods on both synthetic data and real datasets.",https://aclanthology.org/2020.textgraphs-1.4,Association for Computational Linguistics,2020,December,Proceedings of the Graph-based Methods for Natural Language Processing (TextGraphs),"Vargas-Vieyra, Mariana  and
Bellet, Aur{\'e}lien  and
Denis, Pascal",Joint Learning of the Graph and the Data Representation for Graph-Based Semi-Supervised Learning,10.18653/v1/2020.textgraphs-1.4,textgraphs,1019
2020.acl-main.67,"['Model Architectures', 'Knowledge Representation and Reasoning', 'Text Generation']","['Data-to-Text Generation', 'Graph Neural Networks (GNNs)', 'Abstract Meaning Representation (AMR)']",,"Efficient structure encoding for graphs with labeled edges is an important yet challenging point in many graph-based models. This work focuses on AMR-to-text generation -A graph-to-sequence task aiming to recover natural language from Abstract Meaning Representations AMR. Existing graph-to-sequence approaches generally utilize graph neural networks as their encoders, which have two limitations: 1 The message propagation process in AMR graphs is only guided by the firstorder adjacency information. 2 The relationships between labeled edges are not fully considered. In this work, we propose a novel graph encoding framework which can effectively explore the edge relations. We also adopt graph attention networks with higherorder neighborhood information to encode the rich structure in AMR graphs. Experiment results show that our approach obtains new state-of-the-art performance on English AMR benchmark datasets. The ablation analyses also demonstrate that both edge relations and higher-order information are beneficial to graph-to-sequence modeling.",https://aclanthology.org/2020.acl-main.67,Association for Computational Linguistics,2020,July,Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,"Zhao, Yanbin  and
Chen, Lu  and
Chen, Zhi  and
Cao, Ruisheng  and
Zhu, Su  and
Yu, Kai",Line Graph Enhanced AMR-to-Text Generation with Mix-Order Graph Attention Networks,10.18653/v1/2020.acl-main.67,acl,1388
I17-1030,"['Machine Translation (MT)', 'Text Generation', 'Data Management and Generation', 'Model Architectures']","['Data Preparation', 'Text Simplification']",,"Current research in text simplification has been hampered by two central problems: i the small amount of high-quality parallel simplification data available, and ii the lack of explicit annotations of simplification operations, such as deletions or substitutions, on existing data. While the recently introduced Newsela corpus has alleviated the first problem, simplifications still need to be learned directly from parallel text using black-box, end-to-end approaches rather than from explicit annotations. These complex-simple parallel sentence pairs often differ to such a high degree that generalization becomes difficult. End-to-end models also make it hard to interpret what is actually learned from data. We propose a method that decomposes the task of TS into its sub-problems. We devise a way to automatically identify operations in a parallel corpus and introduce a sequence-labeling approach based on these annotations. Finally, we provide insights on the types of transformations that different approaches can model.",https://aclanthology.org/I17-1030,Asian Federation of Natural Language Processing,2017,November,Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 1: Long Papers),"Alva-Manchego, Fernando  and
Bingel, Joachim  and
Paetzold, Gustavo  and
Scarton, Carolina  and
Specia, Lucia",Learning How to Simplify From Explicit Labeling of Complex-Simplified Text Pairs,,I17,781
2020.finnlp-1.10,"['Domain-specific NLP', 'Data Management and Generation', 'Classification Applications', 'Low-resource Languages', 'Text Preprocessing']","['NLP for Finance', 'Data Augmentation', 'Text Segmentation']",,"This paper describes the method that we submitted to the FinSBD2-shared task in IJCAI-2020 to detect the sentence, list, and item boundaries and classify the items from noisy unstructured English and French financial texts. We used the spatial and semantic information of text to augment each tokenized word of text as a fixed-length sentence, and we labeled each word sentence as different boundary types. Then, we proposed the deep attention model based on word embedding to detect the sentence, list, and items boundaries in noisy English and French texts extracted from the financial documents and classified the item sentences into different item types. The experiment shows that the proposed method could be an effective solution to deal with the FinSBD2-shared task.",https://aclanthology.org/2020.finnlp-1.10,-,2020,05-Jan,Proceedings of the Second Workshop on Financial Technology and Natural Language Processing,"Tian, Ke  and
Chen, Hua  and
Yang, Jie","aiai at the FinSBD-2 Task: Sentence, list and Item Boundary Detection and Items classification of Financial Texts Using Data Augmentation and Attention",,finnlp,1079
2020.lrec-1.118,['Low-resource Languages'],,,"In order to access indigenous, regional knowledge contained in language corpora, semantic tools and network methods are most typically employed. In this paper we present an approach for the identification of dialectal variations of words, or words that do not pertain to High German, on the example of non-standard language legacy collection questionnaires of the Bavarian Dialects in Austria DB. Based on selected cultural categories relevant to the wider project context, common words from each of these cultural categories and their lemmas using GermaLemma were identified. Through word embedding models the semantic vicinity of each word was explored, followed by the use of German Wordnet Germanet and the Hunspell tool. Whilst none of these tools have a comprehensive coverage of standard German words, they serve as an indication of dialects in specific semantic hierarchies. Methods and tools applied in this study may serve as an example for other similar projects dealing with non-standard or endangered language collections, aiming to access, analyze and ultimately preserve native regional language heritage.",https://aclanthology.org/2020.lrec-1.118,European Language Resources Association,2020,May,Proceedings of the 12th Language Resources and Evaluation Conference,"Rocha Souza, Renato  and
Dorn, Amelie  and
Piringer, Barbara  and
Wandl-Vogt, Eveline","Identification of Indigenous Knowledge Concepts through Semantic Networks, Spelling Tools and Word Embeddings",,lrec,59
2020.crac-1.14,"['Data Management and Generation', 'Domain-specific NLP', 'Information Extraction']","['NLP for News and Media', 'Data Preparation', 'Coreference Resolution']","['NLP for Social Media', 'Annotation Processes']","Many people live-tweet televised events like Presidential debates and popular TV-shows and discuss people or characters in the event. Naturally, many tweets make pronominal reference to these people/characters. We propose an algorithm for resolving personal pronouns that make reference to people involved in an event, in tweet streams collected during the event.",https://aclanthology.org/2020.crac-1.14,Association for Computational Linguistics,2020,December,"Proceedings of the Third Workshop on Computational Models of Reference, Anaphora and Coreference","Andy, Anietie  and
Callison-Burch, Chris  and
Wijaya, Derry Tanti",Resolving Pronouns in Twitter Streams: Context can Help!,,crac,983
P18-2011,['Information Extraction'],,,"Identification of distinct and independent participants entities of interest in a narrative is an important task for many NLP applications. This task becomes challenging because these participants are often referred to using multiple aliases. In this paper, we propose an approach based on linguistic knowledge for identification of aliases mentioned using proper nouns, pronouns or noun phrases with common noun headword. We use Markov Logic Network MLN to encode the linguistic knowledge for identification of aliases. We evaluate on four diverse history narratives of varying complexity as well as newswire subset of ACE 2005 dataset. Our approach performs better than the state-of-the-art.",https://aclanthology.org/P18-2011,Association for Computational Linguistics,2018,July,Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),"Patil, Sangameshwar  and
Pawar, Sachin  and
Hingmire, Swapnil  and
Palshikar, Girish  and
Varma, Vasudeva  and
Bhattacharyya, Pushpak",Identification of Alias Links among Participants in Narratives,10.18653/v1/P18-2011,P18,805
2022.naacl-main.321,['Knowledge Representation and Reasoning'],,,"Spurious correlations are a threat to the trustworthiness of natural language processing systems, motivating research into methods for identifying and eliminating them. However, addressing the problem of spurious correlations requires more clarity on what they are and how they arise in language data. Gardner et al.  2021  argue that the compositional nature of language implies that all correlations between labels and individual ""input features"" are spurious. This paper analyzes this proposal in the context of a toy example, demonstrating three distinct conditions that can give rise to feature-label correlations in a simple PCFG. Linking the toy example to a structured causal model shows that 1 feature-label correlations can arise even when the label is invariant to interventions on the feature, and 2 feature-label correlations may be absent even when the label is sensitive to interventions on the feature. Because input features will be individually correlated with labels in all but very rare circumstances, domain knowledge must be applied to identify spurious correlations that pose genuine robustness threats.",https://aclanthology.org/2022.naacl-main.321,Association for Computational Linguistics,2022,July,Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,"Eisenstein, Jacob",Informativeness and Invariance: Two Perspectives on Spurious Correlations in Natural Language,10.18653/v1/2022.naacl-main.321,naacl,205
2022.udfestbr-1.4,"['Parsing', 'Learning Paradigms', 'Low-resource Languages', 'Classification Applications', 'Model Architectures']","['Supervised Learning', 'Syntactic Parsing', 'Recurrent Neural Networks (RNNs)']",['Long Short-Term Memory (LSTM) Models'],"Shallow parsing is an intermediate step to many natural language processing tasks, such as information retrieval, question answering, and information extraction. An alternative to full-sentence parsing consists of segmentation and identifying phrases in sentences. Building such a parser for the Portuguese language is challenging considering the proposed formalism for grammar annotation, the Universal Dependency UD. This paper addresses preliminary studies to overcome these barriers by annotating noun phrases tagged in UD.",https://aclanthology.org/2022.udfestbr-1.4,Association for Computational Linguistics,2022,March,Proceedings of the {U}niversal {D}ependencies {B}razilian Festival,"Oliveira, Guilherme Martiniano  and
Neto, Paulo Berlanga  and
Ruiz, Evandro Eduardo Seron",Shallow parsing of Portuguese texts annotated under Universal Dependencies,,udfestbr,1135
2021.americasnlp-1.18,"['Data Management and Generation', 'Low-resource Languages', 'Image and Video Processing']","['Optical Character Recognition (OCR)', 'Data Preparation']",,The Quechua linguistic family has a limited,https://aclanthology.org/2021.americasnlp-1.18,Association for Computational Linguistics,2021,June,Proceedings of the First Workshop on Natural Language Processing for Indigenous Languages of the Americas,"Cordova, Johanna  and
Nouvel, Damien",Toward Creation of Ancash Lexical Resources from OCR,10.18653/v1/2021.americasnlp-1.18,americasnlp,440
2020.ecomnlp-1.7,"['Model Architectures', 'Classification Applications', 'Learning Paradigms']",['Transformer Models'],,"Product matching, i.e., being able to infer the product being sold for a merchant-created offer, is crucial for any e-commerce marketplace, enabling product-based navigation, price comparisons, product reviews, etc. This problem proves a challenging task, mostly due to the extent of product catalog, data heterogeneity, missing product representants, and varying levels of data quality. Moreover, new products are being introduced every day, making it difficult to cast the problem as a classification task. In this work, we apply BERT-based models in a similarity learning setup to solve the product matching problem. We provide a thorough ablation study, showing the impact of architecture and training objective choices. Application of transformer-based architectures and proper sampling techniques significantly boosts performance for a range of e-commerce domains, allowing for production deployment.",https://aclanthology.org/2020.ecomnlp-1.7,Association for Computational Linguistics,2020,December,Proceedings of Workshop on Natural Language Processing in E-Commerce,"Tracz, Janusz  and
W{\'o}jcik, Piotr Iwo  and
Jasinska-Kobus, Kalina  and
Belluzzo, Riccardo  and
Mroczkowski, Robert  and
Gawlik, Ireneusz",BERT-based similarity learning for product matching,,ecomnlp,1109
L16-1010,"['Audio Generation and Processing', 'Dialogue Systems', 'Multilingual NLP', 'Low-resource Languages', 'Classification Applications']","['Automatic Speech Recognition (ASR)', 'Emotion Detection']",,"Emotion Recognition ER is an important part of dialogue analysis which can be used in order to improve the quality of Spoken Dialogue Systems SDSs. The emotional hypothesis of the current response of an end-user might be utilised by the dialogue manager component in order to change the SDS strategy which could result in a quality enhancement. In this study additional speaker-related information is used to improve the performance of the speech-based ER process. The analysed information is the speaker identity, gender and age of a user. Two schemes are described here, namely, using additional information as an independent variable within the feature vector and creating separate emotional models for each speaker, gender or age-cluster independently. The performances of the proposed approaches were compared against the baseline ER system, where no additional information has been used, on a number of emotional speech corpora of German, English, Japanese and Russian. The study revealed that for some of the corpora the proposed approach significantly outperforms the baseline methods with a relative difference of up to 11.9%.",https://aclanthology.org/L16-1010,European Language Resources Association (ELRA),2016,May,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),"Sidorov, Maxim  and
Schmitt, Alexander  and
Semenkin, Eugene  and
Minker, Wolfgang","Could Speaker, Gender or Age Awareness be beneficial in Speech-based Emotion Recognition?",,L16,590
2021.woah-1.22,"['Image and Video Processing', 'Learning Paradigms', 'Classification Applications', 'Data Management and Generation']","['Multimodal Learning', 'Hate and Offensive Speech Detection', 'Data Augmentation']",,"This paper describes our submission winning solution for Task A to the Shared Task on Hateful Meme Detection at WOAH 2021. We build our system on top of a state-of-the-art system for binary hateful meme classification that already uses image tags such as race, gender, and web entities. We add further metadata such as emotions and experiment with data augmentation techniques, as hateful instances are underrepresented in the data set.",https://aclanthology.org/2021.woah-1.22,Association for Computational Linguistics,2021,August,Proceedings of the 5th Workshop on Online Abuse and Harms (WOAH 2021),"Aggarwal, Piush  and
Liman, Michelle Espranita  and
Gold, Darina  and
Zesch, Torsten",VL-BERT+: Detecting Protected Groups in Hateful Multimodal Memes,10.18653/v1/2021.woah-1.22,woah,356
O17-1007,"['Domain-specific NLP', 'Low-resource Languages', 'Classification Applications']",['NLP for News and Media'],['NLP for Social Media'],"Under the issue of gender and Natural Language Processing NLP, most papers aim at gendernorm language that spoken by biologically males and females with opposite-sex desires. However, from the point of view of sexual orientation, this study presents the first work in the task of Chinese homosexual identification. Firstly, we collect homosexual texts from social media, and secondly examine linguistic behavior found in gay and lesbian texts. In addition, we also provide sets of linguistic features to automatically predict homosexual language with the adoption of 5-fold cross-validation Support Vector Machine SVM and Naive Bayes NB models. Training procedure in the study resulted in promising f-score around 70% with the use 68",https://aclanthology.org/O17-1007,The Association for Computational Linguistics and Chinese Language Processing (ACLCLP),2017,November,Proceedings of the 29th Conference on Computational Linguistics and Speech Processing ({ROCLING} 2017),"Wu, Hsiao-Han  and
Hsieh, Shu-Kai",Exploring Lavender Tongue from Social Media TextsIn Chinese,,O17,1068
2021.newsum-1.5,"['Automatic Text Summarization', 'Domain-specific NLP', 'Data Management and Generation', 'Low-resource Languages', 'Cross-lingual Application']","['NLP for Bibliometrics and Scientometrics', 'Data Preparation']",,"Cross-lingual summarization is a challenging task for which there are no cross-lingual scientific resources currently available. To overcome the lack of a high-quality resource, we present a new dataset for monolingual and cross-lingual summarization considering the English-German pair. We collect high-quality, real-world cross-lingual data from Spektrum der Wissenschaft, which publishes humanwritten German scientific summaries of English science articles on various subjects. The generated Spektrum dataset is small; therefore, we harvest a similar dataset from the Wikipedia Science Portal to complement it. The Wikipedia dataset consists of English and German articles, which can be used for monolingual and cross-lingual summarization. Furthermore, we present a quantitative analysis of the datasets and results of empirical experiments with several existing extractive and abstractive summarization models. The results suggest the viability and usefulness of the proposed dataset for monolingual and crosslingual summarization.",https://aclanthology.org/2021.newsum-1.5,Association for Computational Linguistics,2021,November,Proceedings of the Third Workshop on New Frontiers in Summarization,"Fatima, Mehwish  and
Strube, Michael",A Novel Wikipedia based Dataset for Monolingual and Cross-Lingual Summarization,10.18653/v1/2021.newsum-1.5,newsum,224
S19-2201,"['Question Answering (QA)', 'Data Management and Generation', 'Classification Applications']","['Misinformation Detection', 'Data Preparation', 'Community QA']",,"In the following, we describe our system developed for the Semeval2019 Task 8. We finetuned a BERT checkpoint on the qatar living forum dump and used this checkpoint to train a number of models. Our hand-in for subtask A consists of a fine-tuned classifier from this BERT checkpoint. For subtask B, we first have a classifier deciding whether a comment is factual or non-factual. If it is factual, we retrieve intra-forum evidence and using this evidence, have a classifier deciding the comment's veracity. We trained this classifier on ratings which we crawled from qatarliving.com.",https://aclanthology.org/S19-2201,Association for Computational Linguistics,2019,June,Proceedings of the 13th International Workshop on Semantic Evaluation,"Stammbach, Dominik  and
Varanasi, Stalin  and
Neumann, Guenter",DOMLIN at SemEval-2019 Task 8: Automated Fact Checking exploiting Ratings in Community Question Answering Forums,10.18653/v1/S19-2201,S19,171
2020.wanlp-1.12,"['Information Extraction', 'Learning Paradigms', 'Data Management and Generation', 'Low-resource Languages']","['Supervised Learning', 'Data Preparation']",['Annotation Processes'],"We present our work on automatically detecting isnads, the chains of authorities for a report that serve as citations in hadith and other classical Arabic texts. We experiment with both sequence labeling methods for identifying isnads in a single pass and a hybrid ""retrieve-and-tag"" approach, in which a retrieval model first identifies portions of the text that are likely to contain start points for isnads, then a sequence labeling model identifies the exact starting locations within these much smaller retrieved text chunks. We find that the usefulness of full-document sequence to sequence models is limited due to memory limitations and the ineffectiveness of such models at modeling very long documents. We conclude by sketching future improvements on the tagging task and more in-depth analysis of the people and relationships involved in the social network that influenced the evolution of the written tradition over time.",https://aclanthology.org/2020.wanlp-1.12,Association for Computational Linguistics,2020,December,Proceedings of the Fifth Arabic Natural Language Processing Workshop,"Muther, Ryan  and
Smith, David",Tracing Traditions: Automatic Extraction of Isnads from Classical Arabic Texts,,wanlp,191
2020.readi-1.8,"['Data Management and Generation', 'Low-resource Languages']",['Data Analysis'],,"In this paper, we propose visualizing results of a corpus-based study on text complexity using radar charts. We argue that the added value of this type of visualisation is the polygonal shape that provides an intuitive grasp of text complexity similarities across the registers of a corpus. The results that we visualize come from a study where we explored whether it is possible to automatically single out different facets of text complexity across the registers of a Swedish corpus. To this end, we used factor analysis as applied in Biber's Multi-Dimensional Analysis framework. The visualization of text complexity facets with radar charts indicates that there is correspondence between linguistic similarity and similarity of shape across registers.",https://aclanthology.org/2020.readi-1.8,European Language Resources Association,2020,May,Proceedings of the 1st Workshop on Tools and Resources to Empower People with REAding DIfficulties (READI),"Santini, Marina  and
Jonsson, Arne  and
Rennes, Evelina",Visualizing Facets of Text Complexity across Registers,,readi,344
2021.acl-long.451,"['Information Extraction', 'Model Architectures']","['Transformer Models', 'Named Entity Recognition (NER)']",,"Named Entity Recognition NER is the task of identifying spans that represent entities in sentences. Whether the entity spans are nested or discontinuous, the NER task can be categorized into the flat NER, nested NER, and discontinuous NER subtasks. These subtasks have been mainly solved by the token-level sequence labelling or span-level classification. However, these solutions can hardly tackle the three kinds of NER subtasks concurrently. To that end, we propose to formulate the NER subtasks as an entity span sequence generation task, which can be solved by a unified sequence-to-sequence Seq2Seq framework. Based on our unified framework, we can leverage the pre-trained Seq2Seq model to solve all three kinds of NER subtasks without the special design of the tagging schema or ways to enumerate spans. We exploit three types of entity representations to linearize entities into a sequence. Our proposed framework is easy-to-implement and achieves state-of-theart SoTA or near SoTA performance on eight English NER datasets, including two flat NER datasets, three nested NER datasets, and three discontinuous NER datasets 1 .",https://aclanthology.org/2021.acl-long.451,Association for Computational Linguistics,2021,August,Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),"Yan, Hang  and
Gui, Tao  and
Dai, Junqi  and
Guo, Qipeng  and
Zhang, Zheng  and
Qiu, Xipeng",A Unified Generative Framework for Various NER Subtasks,10.18653/v1/2021.acl-long.451,acl,1257
W16-2118,['Data Management and Generation'],"['Data Preparation', 'Data Analysis']",,"This paper investigates whether sentence structure analysis-examining who appears in subject versus object positioncan illuminate who academic articles portray as having agency in labor relations. We extract subjects and objects from a corpus of 3,800 academic articles, and compare both the relative occurrence of different groups workers, women, employers in each position and the verbs that most commonly attach to each group. We conclude that agency, while elusive, can potentially be modeled by sentence structure analysis.",https://aclanthology.org/W16-2118,Association for Computational Linguistics,2016,August,"Proceedings of the 10th {SIGHUM} Workshop on Language Technology for Cultural Heritage, Social Sciences, and Humanities","Hulden, Vilja","Whodunit... and to Whom? Subjects, Objects, and Actions in Research Articles on American Labor Unions",10.18653/v1/W16-2118,W16,1055
W18-5019,"['Text Generation', 'Data Management and Generation', 'Dialogue Systems', 'Model Architectures']","['Text Style Transfer', 'Data Preparation']",,"Natural language generators for taskoriented dialogue must effectively realize system dialogue actions and their associated semantics. In many applications, it is also desirable for generators to control the style of an utterance. To date, work on task-oriented neural generation has primarily focused on semantic fidelity rather than achieving stylistic goals, while work on style has been done in contexts where it is difficult to measure content preservation. Here we present three different sequence-to-sequence models and carefully test how well they disentangle content and style. We use a statistical generator, PERSONAGE, to synthesize a new corpus of over 88,000 restaurant domain utterances whose style varies according to models of personality, giving us total control over both the semantic content and the stylistic variation in the training data. We then vary the amount of explicit stylistic supervision given to the three models. We show that our most explicit model can simultaneously achieve high fidelity to both semantic and stylistic goals: this model adds a context vector of 36 stylistic parameters as input to the hidden state of the encoder at each time step, showing the benefits of explicit stylistic supervision, even when the amount of training data is large.",https://aclanthology.org/W18-5019,Association for Computational Linguistics,2018,July,Proceedings of the 19th Annual {SIG}dial Meeting on Discourse and Dialogue,"Oraby, Shereen  and
Reed, Lena  and
Tandon, Shubhangi  and
T.S., Sharath  and
Lukin, Stephanie  and
Walker, Marilyn",Controlling Personality-Based Stylistic Variation with Neural Natural Language Generators,10.18653/v1/W18-5019,W18,562
2021.humeval-1.1,"['Evaluation Techniques', 'Data Management and Generation', 'Commonsense Reasoning']","['Data Preparation', 'Data Analysis']",['Annotation Processes'],"Common sense is an integral part of human cognition which allows us to make sound decisions, communicate effectively with others and interpret situations and utterances. Endowing AI systems with commonsense knowledge capabilities will help us get closer to creating systems that exhibit human intelligence. Recent efforts in Natural Language Generation NLG have focused on incorporating commonsense knowledge through large-scale pretrained language models or by incorporating external knowledge bases. Such systems exhibit reasoning capabilities without common sense being explicitly encoded in the training set. These systems require careful evaluation, as they incorporate additional resources during training which adds additional sources of errors. Additionally, human evaluation of such systems can have significant variation, making it impossible to compare different systems and define baselines. This paper aims to demystify human evaluations of commonsenseenhanced NLG systems by proposing the Commonsense Evaluation Card CEC, a set of recommendations for evaluation reporting of commonsense-enhanced NLG systems, underpinned by an extensive analysis of human evaluations reported in the recent literature.",https://aclanthology.org/2021.humeval-1.1,Association for Computational Linguistics,2021,April,Proceedings of the Workshop on Human Evaluation of NLP Systems (HumEval),"Clinciu, Miruna-Adriana  and
Gkatzia, Dimitra  and
Mahamood, Saad","It's Commonsense, isn't it? Demystifying Human Evaluations in Commonsense-Enhanced NLG Systems",,humeval,500
J18-2005,"['Text Preprocessing', 'Text Clustering', 'Learning Paradigms', 'Low-resource Languages', 'Model Architectures']","['Unsupervised Learning', 'Text Segmentation']",['Word Segmentation'],"This article presents a probabilistic hierarchical clustering model for morphological segmentation. In contrast to existing approaches to morphology learning, our method allows learning hierarchical organization of word morphology as a collection of tree structured paradigms. The model is fully unsupervised and based on the hierarchical Dirichlet process. Tree hierarchies are learned along with the corresponding morphological paradigms simultaneously. Our model is evaluated on Morpho Challenge and shows competitive performance when compared to stateof-the-art unsupervised morphological segmentation systems. Although we apply this model for morphological segmentation, the model itself can also be used for hierarchical clustering of other types of data.",https://aclanthology.org/J18-2005,MIT Press,2018,June,,"Can, Burcu  and
Manandhar, Suresh",Tree Structured Dirichlet Processes for Hierarchical Morphological Segmentation,10.1162/COLI_a_00318,J18,372
2021.mrl-1.15,"['Text Generation', 'Multilingual NLP', 'Learning Paradigms', 'Low-resource Languages', 'Model Architectures']","['Transformer Models', 'Paraphrase and Rephrase Generation']",,"We present a novel technique for zero-shot paraphrase generation. The key contribution is an end-to-end multilingual paraphrasing model that is trained using translated parallel corpora to generate paraphrases into ""meaning spaces"" -replacing the final softmax layer with word embeddings. This architectural modification, plus a training procedure that incorporates an autoencoding objective, enables effective parameter sharing across languages for more fluent monolingual rewriting, and facilitates fluency and diversity in generation. Our continuous-output paraphrase generation models outperform zero-shot paraphrasing baselines, when evaluated on two languages using a battery of computational metrics as well as in human assessment. 1",https://aclanthology.org/2021.mrl-1.15,Association for Computational Linguistics,2021,November,Proceedings of the 1st Workshop on Multilingual Representation Learning,"Jegadeesan, Monisha  and
Kumar, Sachin  and
Wieting, John  and
Tsvetkov, Yulia",Improving the Diversity of Unsupervised Paraphrasing with Embedding Outputs,10.18653/v1/2021.mrl-1.15,mrl,317
J16-2006,['Embeddings'],,,"Distributional semantic models, deriving vector-based word representations from patterns of word usage in corpora, have many useful applications (Turney and Pantel 2010). Recently, there has been interest in compositional distributional models, which derive vectors for phrases from representations of their constituent words (Mitchell and Lapata 2010). Often, the values of distributional vectors are pointwise mutual information (PMI) scores obtained from raw co-occurrence counts. In this article we study the relation between the PMI dimensions of a phrase vector and its components in order to gain insights into which operations an adequate composition model should perform. We show mathematically that the difference between the PMI dimension of a phrase vector and the sum of PMIs in the corresponding dimensions of the phrase's parts is an independently interpretable value, namely, a quantification of the impact of the context associated with the relevant dimension on the phrase's internal cohesion, as also measured by PMI. We then explore this quantity empirically, through an analysis of adjective-noun composition.",https://aclanthology.org/J16-2006,MIT Press,2016,June,,"Paperno, Denis  and
Baroni, Marco",{S}quibs: When the Whole Is Less Than the Sum of Its Parts: How Composition Affects {PMI} Values in Distributional Semantic Vectors,10.1162/COLI_a_00250,J16,862
I17-1033,"['Text Generation', 'Image and Video Processing', 'Low-resource Languages', 'Model Architectures']",['Video Captioning'],,"In recent years, there has been a surge of interest in automatically describing images or videos in a natural language. These descriptions are useful for image/video search, etc. In this paper, we focus on procedure execution videos, in which a human makes or repairs something and propose a method for generating procedural texts from them. Since available video/text pairs are limited in size, the direct application of end-to-end deep learning is not feasible. Thus we propose to train Faster R-CNN network for object recognition and LSTM for text generation and combine them at run time. We took pairs of recipe and cooking video as an example, generated a recipe from a video, and compared it with the original recipe. The experimental results showed that our method can produce a recipe as accurate as the state-of-the-art scene descriptions.",https://aclanthology.org/I17-1033,Asian Federation of Natural Language Processing,2017,November,Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 1: Long Papers),"Ushiku, Atsushi  and
Hashimoto, Hayato  and
Hashimoto, Atsushi  and
Mori, Shinsuke",Procedural Text Generation from an Execution Video,,I17,368
2021.sigmorphon-1.18,"['Learning Paradigms', 'Multilingual NLP', 'Cross-lingual Application', 'Low-resource Languages']",['Transfer Learning'],,"This paper investigates how abstract processes like suffixation can be learned from morphological inflection task data using an analogical memory-based framework. In this framework, the inflection target form is specified by providing an example inflection of another word in the language. This model is capable of near-baseline performance on the Sig-Morphon 2020 inflection challenge. Such a model can make predictions for unseen languages, allowing one-shot inflection for natural languages and the investigation of morphological transfer with synthetic probes. Accuracy for one-shot transfer can be unexpectedly high for some target languages 88% in Shona and language families 53% across Romance. Probe experiments show that the model learns partially generalizable representations of prefixation, suffixation and reduplication, aiding its ability to transfer. The paper argues that the degree of generality of these process representations also helps to explain transfer results from previous research.",https://aclanthology.org/2021.sigmorphon-1.18,Association for Computational Linguistics,2021,August,"Proceedings of the 18th SIGMORPHON Workshop on Computational Research in Phonetics, Phonology, and Morphology","Elsner, Micha",What transfers in morphological inflection? Experiments with analogical models,10.18653/v1/2021.sigmorphon-1.18,sigmorphon,95
L16-1433,"['Learning Paradigms', 'Data Management and Generation', 'Dialogue Systems']","['Open Domain Dialogue Systems', 'Data Preparation', 'Unsupervised Learning', 'Response Generation']",,"This paper presents an automatic corpus-based process to author an open-domain conversational strategy usable both in chatterbot systems and as a fallback strategy for out-of-domain human utterances. Our approach is implemented on a corpus of television drama subtitles. This system is used as a chatterbot system to collect a corpus of 41 open-domain textual dialogues with 27 human participants. The general capabilities of the system are studied through objective measures and subjective self-reports in terms of understandability, repetition and coherence of the system responses selected in reaction to human utterances. Subjective evaluations of the collected dialogues are presented with respect to amusement, engagement and enjoyability. The main factors influencing those dimensions in our chatterbot experiment are discussed.",https://aclanthology.org/L16-1433,European Language Resources Association (ELRA),2016,May,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),"Dubuisson Duplessis, Guillaume  and
Letard, Vincent  and
Ligozat, Anne-Laure  and
Rosset, Sophie",Purely Corpus-based Automatic Conversation Authoring,,L16,1063
2020.ngt-1.6,"['Learning Paradigms', 'Data Management and Generation', 'Low-resource Languages', 'Text Generation']","['Supervised Learning', 'Data Preparation', 'Paraphrase and Rephrase Generation']",,"The article is focused on automatic development and ranking of a large corpus for Russian paraphrase generation which proves to be the first corpus of such type in Russian computational linguistics. Existing manually annotated paraphrase datasets for Russian are limited to small-sized ParaPhraser corpus and ParaPlag which are suitable for a set of NLP tasks, such as paraphrase and plagiarism detection, sentence similarity and relatedness estimation, etc. Due to size restrictions, these datasets can hardly be applied in end-to-end text generation solutions. Meanwhile, paraphrase generation requires a large amount of training data. In our study we propose a solution to the problem: we collect, rank and evaluate a new publicly available headline paraphrase corpus ParaPhraser Plus, and then perform text generation experiments with manual evaluation on automatically ranked corpora using the Universal Transformer architecture.",https://aclanthology.org/2020.ngt-1.6,Association for Computational Linguistics,2020,July,Proceedings of the Fourth Workshop on Neural Generation and Translation,"Gudkov, Vadim  and
Mitrofanova, Olga  and
Filippskikh, Elizaveta",Automatically Ranked Russian Paraphrase Corpus for Text Generation,10.18653/v1/2020.ngt-1.6,ngt,1103
2021.bppf-1.1,['Evaluation Techniques'],,,"Where have we been, and where are we going? It is easier to talk about the past than the future. These days, benchmarks evolve more bottom up such as papers with code. 1 There used to be more top-down leadership from government and industry, in the case of systems, with benchmarks such as SPEC. 2 Going forward, there may be more top-down leadership from organizations like MLPerf 3 and/or influencers like David Ferrucci 4 . Tasks such as reading comprehension become even more interesting as we move beyond English. Multilinguality introduces many challenges, and even more opportunities. Bio Sam Bowman has been on the faculty at NYU since 2016, when he completed PhD with Chris Manning and Chris Potts at Stanford. At NYU, he is a member of the Center for Data Science, the Department of Linguistics, and Courant Institute's Department of Computer Science. His research focuses on data, evaluation techniques, and modeling techniques for sentence and paragraph 9 https://github.com/kwchurch/ Benchmarking_past_present_future",https://aclanthology.org/2021.bppf-1.1,Association for Computational Linguistics,2021,August,"Proceedings of the 1st Workshop on Benchmarking: Past, Present and Future","Church, Kenneth  and
Liberman, Mark  and
Kordoni, Valia","Benchmarking: Past, Present and Future",10.18653/v1/2021.bppf-1.1,bppf,1197
2021.louhi-1.3,"['Domain-specific NLP', 'Data Management and Generation']","['Medical and Clinical NLP', 'Data Preparation', 'NLP for News and Media']",['NLP for Social Media'],"In online forums focused on health and wellbeing, individuals tend to seek and give the following social support: emotional and informational support. Understanding the expressions of these social supports in an online COVID-19 forum is important for: a the forum and its members to provide the right type of support to individuals and b determining the long term effects of the COVID-19 pandemic on the well-being of the public, thereby informing interventions. In this work, we build four machine learning models to measure the extent of the following social supports expressed in each post in a COVID-19 online forum: a emotional support given b emotional support sought c informational support given, and d informational support sought. Using these models, we aim to: i determine if there is a correlation between the different social supports expressed in posts e.g. when members of the forum give emotional support in posts, do they also tend to give or seek informational support in the same post? ii determine how these social supports sought and given changes over time in published posts. We find that i there is a positive correlation between the informational support given in posts and the emotional support given and emotional support sought, respectively, in these posts and ii over time, users tended to seek more emotional support and give less emotional support.",https://aclanthology.org/2021.louhi-1.3,Association for Computational Linguistics,2021,April,Proceedings of the 12th International Workshop on Health Text Mining and Information Analysis,"Andy, Anietie  and
Chu, Brian  and
Fathy, Ramie  and
Bennett, Barrington  and
Stokes, Daniel  and
Guntuku, Sharath Chandra",Understanding Social Support Expressed in a COVID-19 Online Forum,,louhi,759
2021.isa-1.5,"['Text Preprocessing', 'Discourse Analysis', 'Argument Mining', 'Parsing', 'Data Management and Generation', 'Classification Applications']","['Text Segmentation', 'Discourse Parsing', 'Data Preparation']",,"The paper presents a discourse-based approach to the analysis of argumentative texts based on the assumption that the coherence of a text should capture argumentation structure. Therefore, existing discourse analysis tools can be successfully applied for argument segmentation and annotation tasks. We tested widely used Penn Discourse Tree Bank parser Lin et al., 2010 and the state-of-the-art neural network NeuralEDUSeg Wang et al., 2018 and XLNet Yang et al., 2019 models on discourse segmentation and discourse relation recognition tasks. The two-stage approach outperformed the PDTB parser by broad margin, i.e. the best achieved F1 scores of 21.2% for PDTB parser vs 66.37% for NeuralEDUSeg and XLNet models. Neural network models were fine-tuned and evaluated on the argumentative corpus showing a promising accuracy of 60.22%. The complete argument structures were reconstructed for further argumentation mining tasks. The reference Dagstuhl argumentative corpus containing 2,222 elementary discourse unit pairs annotated with the toplevel and fine-grained PDTB relations will be released to the research community.",https://aclanthology.org/2021.isa-1.5,Association for Computational Linguistics,2021,June,Proceedings of the 17th Joint ACL - ISO Workshop on Interoperable Semantic Annotation,"Saveleva, Ekaterina  and
Petukhova, Volha  and
Mosbach, Marius  and
Klakow, Dietrich",Discourse-based Argument Segmentation and Annotation,,isa,612
2021.findings-acl.74,"['Text Generation', 'Embeddings', 'Learning Paradigms', 'Low-resource Languages', 'Cross-lingual Application', 'Model Architectures']","['Transformer Models', 'Transfer Learning', 'Word Embeddings', 'Large Language Models (LLMs)']",,"Large generative language models have been very successful for English, but other languages lag behind, in part due to data and computational limitations. We propose a method that may overcome these problems by adapting existing pre-trained models to new languages. Specifically, we describe the adaptation of English GPT-2 to Italian and Dutch by retraining lexical embeddings without tuning the Transformer layers. As a result, we obtain lexical embeddings for Italian and Dutch that are aligned with the original English lexical embeddings. Additionally, we scale up complexity by transforming relearned lexical embeddings of GPT-2 small to the GPT-2 medium embedding space. This method minimises the amount of training and prevents losing information during adaptation that was learned by GPT-2. English GPT-2 models with relearned lexical embeddings can generate realistic sentences in Italian and Dutch. Though on average these sentences are still identifiable as artificial by humans, they are assessed on par with sentences generated by a GPT-2 model fully trained from scratch.",https://aclanthology.org/2021.findings-acl.74,Association for Computational Linguistics,2021,August,Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021,"de Vries, Wietse  and
Nissim, Malvina",As Good as New. How to Successfully Recycle English GPT-2 to Make Models for Other Languages,10.18653/v1/2021.findings-acl.74,findings,1366
2022.woah-1.6,"['Domain-specific NLP', 'Low-resource Languages', 'Classification Applications', 'Data Management and Generation']","['Data Preparation', 'Hate and Offensive Speech Detection', 'NLP for News and Media']","['Annotation Processes', 'NLP for Social Media']","This work describes the process of creating a corpus of Twitter conversations annotated for the presence of counterspeech in response to toxic speech related to axes of discrimination linked to sexism, racism and homophobia. The main novelty is an annotated dataset comprising relevant tweets in their context of occurrence. The corpus is made up of tweets and responses captured by different profiles replying to discriminatory content or objectionably couched news. An annotation scheme was created to illustrate the relevant dimensions of toxic speech and counterspeech. An analysis of the collected and annotated data and of the Inter-Annotator Agreement IAA that emerged during the annotation process is included. Moreover, we report about preliminary experiments on automatic counterspeech detection, based on supervised automatic learning models trained on the new dataset. The results highlight the fundamental role played by the context in this detection task, confirming our intuitions about the importance to collect tweets in their context of occurrence.",https://aclanthology.org/2022.woah-1.6,Association for Computational Linguistics,2022,July,Proceedings of the Sixth Workshop on Online Abuse and Harms (WOAH),"Goffredo, Pierpaolo  and
Basile, Valerio  and
Cepollaro, Biancamaria  and
Patti, Viviana",Counter-TWIT: An Italian Corpus for Online Counterspeech in Ecological Contexts,10.18653/v1/2022.woah-1.6,woah,583
S18-1041,"['Domain-specific NLP', 'Data Management and Generation', 'Learning Paradigms', 'Low-resource Languages', 'Classification Applications']","['Data Augmentation', 'Emotion Detection', 'NLP for News and Media']",['NLP for Social Media'],"The present study describes our submission to SemEval 2018 Task 1: Affect in Tweets. Our Spanish-only approach aimed to demonstrate that it is beneficial to automatically generate additional training data by i translating training data from other languages and ii applying a semi-supervised learning method. We find strong support for both approaches, with those models outperforming our regular models in all subtasks. However, creating a stepwise ensemble of different models as opposed to simply averaging did not result in an increase in performance. We placed second EI-Reg, second EI-Oc, fourth V-Reg and fifth V-Oc in the four Spanish subtasks we participated in.",https://aclanthology.org/S18-1041,Association for Computational Linguistics,2018,June,Proceedings of The 12th International Workshop on Semantic Evaluation,"Kuijper, Marloes  and
van Lenthe, Mike  and
van Noord, Rik",UG18 at SemEval-2018 Task 1: Generating Additional Training Data for Predicting Emotion Intensity in Spanish,10.18653/v1/S18-1041,S18,461
2021.eval4nlp-1.15,"['Embeddings', 'Machine Translation (MT)', 'Evaluation Techniques', 'Data Management and Generation', 'Learning Paradigms', 'Low-resource Languages', 'Model Architectures']","['Transformer Models', 'Unsupervised Learning', 'Data Preparation', 'Neural MT (NMT)']",,"Quality Estimation QE for Machine Translation has been shown to reach relatively high accuracy in predicting sentence-level scores, relying on pretrained contextual embeddings and human-produced quality scores. However, the lack of explanations along with decisions made by end-to-end neural models makes the results difficult to interpret. Furthermore, word-level annotated datasets are rare due to the prohibitive effort required to perform this task, while they could provide interpretable signals in addition to sentence-level QE outputs. In this paper, we propose a novel QE architecture which tackles both the wordlevel data scarcity and the interpretability limitations of recent approaches. Sentence-level and word-level components are jointly pretrained through an attention mechanism based on synthetic data and a set of MT metrics embedded in a common space. Our approach is evaluated on the Eval4NLP 2021 shared task and our submissions reach the first position in all language pairs. The extraction of metricto-input attention weights show that different metrics focus on different parts of the source and target text, providing strong rationales in the decision-making process of the QE model.",https://aclanthology.org/2021.eval4nlp-1.15,Association for Computational Linguistics,2021,November,Proceedings of the 2nd Workshop on Evaluation and Comparison of NLP Systems,"Rubino, Raphael  and
Fujita, Atsushi  and
Marie, Benjamin",Error Identification for Machine Translation with Metric Embedding and Attention,10.18653/v1/2021.eval4nlp-1.15,eval4nlp,548
2020.ecnlp-1.6,"['Data Management and Generation', 'Classification Applications', 'Model Architectures', 'Domain-specific NLP']","['Intent Detection', 'Data Preparation', 'Recurrent Neural Networks (RNNs)']",['Long Short-Term Memory (LSTM) Models'],"In this work, we improve the intent classification in an English based e-commerce voice assistant by using inter-utterance context. For increased user adaptation and hence being more profitable, an e-commerce voice assistant is desired to understand the context of a conversation and not have the users repeat it in every utterance. For example, let a user's first utterance be 'find apples'. Then, the user may say 'i want organic only' to filter out the results generated by an assistant with respect to the first query. So, it is important for the assistant to take into account the context from the user's first utterance to understand her intention in the second one. In this paper, we present our approach for contextual intent classification in Walmart's e-commerce voice assistant. It uses the intent of the previous user utterance to predict the intent of her current utterance. With the help of experiments performed on real user queries we show that our approach improves the intent classification in the assistant.",https://aclanthology.org/2020.ecnlp-1.6,Association for Computational Linguistics,2020,July,Proceedings of The 3rd Workshop on e-Commerce and NLP,"Sharma, Arpit",Improving Intent Classification in an E-commerce Voice Assistant by Using Inter-Utterance Context,10.18653/v1/2020.ecnlp-1.6,ecnlp,765
W18-6560,"['Multilingual NLP', 'Text Generation', 'Low-resource Languages']",,,"We present a readily available API that solves the morphology component for surface realizers in 10 languages e.g., English, German and Finnish for any topic and is available as REST API. This can be used to add morphology to any kind of NLG application e.g., a multi-language chatbot, without requiring computational linguistic knowledge by the integrator.",https://aclanthology.org/W18-6560,Association for Computational Linguistics,2018,November,Proceedings of the 11th International Conference on Natural Language Generation,"Madsack, Andreas  and
Heininger, Johanna  and
Davaasambuu, Nyamsuren  and
Voronik, Vitaliia  and
K{\""a}ufl, Michael  and
Wei{\ss}graeber, Robert",Multi-Language Surface Realisation as REST API based NLG Microservice,10.18653/v1/W18-6560,W18,621
2021.gem-1.11,['Data Management and Generation'],,,"Developing documentation guidelines and easy-to-use templates for datasets and models is a challenging task, especially given the variety of backgrounds, skills, and incentives of the people involved in the building of natural language processing NLP tools. Nevertheless, the adoption of standard documentation practices across the field of NLP promotes more accessible and detailed descriptions of NLP datasets and models, while supporting researchers and developers in reflecting on their work. To help with the standardization of documentation, we present two case studies of efforts that aim to develop reusable documentation templates -the HuggingFace data card, a general purpose card for datasets in NLP, and the GEM benchmark data and model cards with a focus on natural language generation. We describe our process for developing these templates, including the identification of relevant stakeholder groups, the definition of a set of guiding principles, the use of existing templates as our foundation, and iterative revisions based on feedback.",https://aclanthology.org/2021.gem-1.11,Association for Computational Linguistics,2021,August,"Proceedings of the 1st Workshop on Natural Language Generation, Evaluation, and Metrics (GEM 2021)","McMillan-Major, Angelina  and
Osei, Salomey  and
Rodriguez, Juan Diego  and
Ammanamanchi, Pawan Sasanka  and
Gehrmann, Sebastian  and
Jernite, Yacine",Reusable Templates and Guides For Documenting Datasets and Models for Natural Language Processing and Generation: A Case Study of the HuggingFace and GEM Data and Model Cards,10.18653/v1/2021.gem-1.11,gem,98
2021.americasnlp-1.11,"['Finite State Machines', 'Parsing', 'Low-resource Languages']",['Morphological Parsing'],,"We represent the complexity of Yine Arawak morphology with a finite state transducer FST based morphological analyzer. Yine is a low-resource indigenous polysynthetic Peruvian language spoken by approximately 3,000 people and is classified as 'definitely endangered' by UNESCO. We review Yine morphology focusing on morphophonology, possessive constructions and verbal predicates. Then we develop FSTs to model these components proposing techniques to solve challenging problems such as complex patterns of incorporating open and closed category arguments. This is a work in progress and we still have more to do in the development and verification of our analyzer. Our analyzer will serve both as a tool to better document the Yine language and as a component of natural language processing NLP applications such as spell checking and correction.",https://aclanthology.org/2021.americasnlp-1.11,Association for Computational Linguistics,2021,June,Proceedings of the First Workshop on Natural Language Processing for Indigenous Languages of the Americas,"Ingunza Torres, Adriano  and
Miller, John  and
Oncevay, Arturo  and
Zariquiey Biondi, Roberto",Representation of Yine Arawak Morphology by Finite State Transducer Formalism,10.18653/v1/2021.americasnlp-1.11,americasnlp,1088
2020.louhi-1.6,"['Domain-specific NLP', 'Classification Applications']","['NLP for News and Media', 'Medical and Clinical NLP']",['NLP for Social Media'],"The automatic mapping of Adverse Drug Reaction ADR reports from user-generated content to concepts in a controlled medical vocabulary provides valuable insights for monitoring public health. While state-of-the-art deep learning-based sequence classification techniques achieve impressive performance for medical concepts with large amounts of training data, they show their limit with long-tail concepts that have a low number of training samples. The above hinders their adaptability to the changes of layman's terminology and the constant emergence of new informal medical terms. Our objective in this paper is to tackle the problem of normalizing long-tail ADR mentions in user-generated content. In this paper, we exploit the implicit semantics of rare ADRs for which we have few training samples, in order to detect the most similar class for the given ADR. The evaluation results demonstrate that our proposed approach addresses the limitations of the existing techniques when the amount of training data is limited.",https://aclanthology.org/2020.louhi-1.6,Association for Computational Linguistics,2020,November,Proceedings of the 11th International Workshop on Health Text Mining and Information Analysis,"Manousogiannis, Emmanouil  and
Mesbah, Sepideh  and
Bozzon, Alessandro  and
Sips, Robert-Jan  and
Szlanik, Zoltan  and
Baez, Selene",Normalization of Long-tail Adverse Drug Reactions in Social Media,10.18653/v1/2020.louhi-1.6,louhi,1131
P19-1347,"['Question Answering (QA)', 'Learning Paradigms', 'Model Architectures']","['Multimodal Learning', 'Recurrent Neural Networks (RNNs)']",,"In this work, we introduce a novel algorithm for solving the textbook question answering TQA task which describes more realistic QA problems compared to other recent tasks. We mainly focus on two related issues with analysis of the TQA dataset. First, solving the TQA problems requires to comprehend multimodal contexts in complicated input data. To tackle this issue of extracting knowledge features from long text lessons and merging them with visual features, we establish a context graph from texts and images, and propose a new module f-GCN based on graph convolutional networks GCN. Second, scientific terms are not spread over the chapters and subjects are split in the TQA dataset. To overcome this so called 'out-of-domain' issue, before learning QA problems, we introduce a novel self-supervised open-set learning process without any annotations. The experimental results show that our model significantly outperforms prior state-of-the-art methods. Moreover, ablation studies validate that both methods of incorporating f-GCN for extracting knowledge from multi-modal contexts and our newly proposed self-supervised learning process are effective for TQA problems.",https://aclanthology.org/P19-1347,Association for Computational Linguistics,2019,July,Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,"Kim, Daesik  and
Kim, Seonhoon  and
Kwak, Nojun",Textbook Question Answering with Multi-modal Context Graph Understanding and Self-supervised Open-set Comprehension,10.18653/v1/P19-1347,P19,28
2020.vardial-1.18,"['Domain-specific NLP', 'Multilingual NLP', 'Classification Applications', 'Low-resource Languages']",['NLP for News and Media'],,"We study the ability of large fine-tuned transformer models to solve a binary classification task of dialect identification, with a special interest in comparing the performance of multilingual to monolingual ones. The corpus analyzed contains Romanian and Moldavian samples from the news domain, as well as tweets for assessing the performance. We find that the monolingual models are superior to the multilingual ones and the best results are obtained using an SVM ensemble of 5 different transformer-based models. We provide our experimental results and an analysis of the attention mechanisms of the best-performing individual classifiers to explain their decisions. The code we used was released under an open-source license. Introduction Dialect Identification is a Natural Language Processing NLP task that started receiving more interest in recent years, in part due to VarDial, the workshop on NLP for Similar Languages, Varieties and",https://aclanthology.org/2020.vardial-1.18,International Committee on Computational Linguistics (ICCL),2020,December,"Proceedings of the 7th Workshop on NLP for Similar Languages, Varieties and Dialects","Popa, Cristian  and
{\textcommabelow{S}}tef{\u{a}}nescu, Vlad",Applying Multilingual and Monolingual Transformer-Based Models for Dialect Identification,,vardial,789
2021.mmsr-1.1,['Learning Paradigms'],['Multimodal Learning'],,"The last years have shown rapid developments in the field of multimodal machine learning, combining e.g., vision, text or speech. In this position paper we explain how the field uses outdated definitions of multimodality that prove unfit for the machine learning era. We propose a new task-relative definition of multimodality in the context of multimodal machine learning that focuses on representations and information that are relevant for a given machine learning task. With our new definition of multimodality we aim to provide a missing foundation for multimodal research, an important component of language grounding and a crucial milestone towards NLU.",https://aclanthology.org/2021.mmsr-1.1,Association for Computational Linguistics,2021,June,Proceedings of the 1st Workshop on Multimodal Semantic Representations (MMSR),"Parcalabescu, Letitia  and
Trost, Nils  and
Frank, Anette",What is Multimodality?,10.48550/arxiv.2103.06304,mmsr,387
2021.adaptnlp-1.23,"['Evaluation Techniques', 'Domain-specific NLP']",['NLP for News and Media'],['NLP for Social Media'],"The robustness of pretrained language models PLMs is generally measured using performance drops on two or more domains. However, we do not yet understand the inherent robustness achieved by contributions from different layers of a PLM. We systematically analyze the robustness of these representations layer by layer from two perspectives. First, we measure the robustness of representations by using domain divergence between two domains. We find that i Domain variance increases from the lower to the upper layers for vanilla PLMs; ii Models continuously pretrained on domain-specific data DAPT Gururangan et al., 2020  exhibit more variance than their pretrained PLM counterparts; and that iii Distilled models e.g.,DistilBERT also show greater domain variance. Second, we investigate the robustness of representations by analyzing the encoded syntactic and semantic information using diagnostic probes. We find that similar layers have similar amounts of linguistic information for data from an unseen domain.",https://aclanthology.org/2021.adaptnlp-1.23,Association for Computational Linguistics,2021,April,Proceedings of the Second Workshop on Domain Adaptation for NLP,"Ramesh Kashyap, Abhinav  and
Mehnaz, Laiba  and
Malik, Bhavitvya  and
Waheed, Abdul  and
Hazarika, Devamanyu  and
Kan, Min-Yen  and
Shah, Rajiv Ratn","Analyzing the Domain Robustness of Pretrained Language Models, Layer by Layer",,adaptnlp,596
2022.clinicalnlp-1.5,"['Domain-specific NLP', 'Data Management and Generation', 'Error Detection and Correction', 'Learning Paradigms', 'Model Architectures']","['Transformer Models', 'Data Preparation', 'Supervised Learning', 'Medical and Clinical NLP']",['Biomedical NLP'],"Radiology report is an official record of radiologists' interpretation of patients' radiographs and it's a crucial component in the overall medical diagnostic process. However, it can contain various types of errors that can lead to inadequate treatment or delay in diagnosis. To address this problem, we propose a deep learning framework to detect errors in radiology reports. Specifically, our method detects errors between findings and conclusion of chest X-ray reports based on a supervised learning framework. To compensate for the lack of data availability of radiology reports with errors, we develop an error generator to systematically create artificial errors in existing reports. In addition, we introduce a Medical Knowledge-enhancing Pretraining to further utilize the knowledge of abbreviations and key phrases frequently used in the medical domain. We believe that this is the first work to propose a deep learning framework for detecting errors in radiology reports based on a rich contextual and medical understanding. Validation on our radiologistsynthesized dataset, based on MIMIC-CXR, shows 0.80 and 0.95 of the area under precisionrecall curve AUPRC and the area under the ROC curve AUROC respectively, indicating that our framework can effectively detect errors in the real-world radiology reports.",https://aclanthology.org/2022.clinicalnlp-1.5,Association for Computational Linguistics,2022,July,Proceedings of the 4th Clinical Natural Language Processing Workshop,"Min, Dabin  and
Kim, Kaeun  and
Lee, Jong Hyuk  and
Kim, Yisak  and
Park, Chang Min",RRED : A Radiology Report Error Detector based on Deep Learning Framework,10.18653/v1/2022.clinicalnlp-1.5,clinicalnlp,1149
2020.bucc-1.5,"['Embeddings', 'Domain-specific NLP', 'Information Extraction', 'Low-resource Languages']","['Word Embeddings', 'Relation Extraction']",,"We report an experiment aimed at extracting words expressing a specific semantic relation using intersections of word embeddings. In a multilingual frame-based domain model, specific features of a concept are typically described through a set of non-arbitrary semantic relations. In karstology, our domain of choice which we are exploring though a comparable corpus in English and Croatian, karst phenomena such as landforms are usually described through their FORM, LOCATION, CAUSE, FUNCTION and COMPOSITION. We propose an approach to mine words pertaining to each of these relations by using a small number of seed adjectives, for which we retrieve closest words using word embeddings and then use intersections of these neighbourhoods to refine our search. Such crosslanguage expansion of semantically-rich vocabulary is a valuable aid in improving the coverage of a multilingual knowledge base, but also in exploring differences between languages in their respective conceptualisations of the domain.",https://aclanthology.org/2020.bucc-1.5,European Language Resources Association,2020,May,Proceedings of the 13th Workshop on Building and Using Comparable Corpora,"Vintar, {\v{S}}pela  and
Gr{\v{c}}i{\'c} Simeunovi{\'c}, Larisa  and
Martinc, Matej  and
Pollak, Senja  and
Stepi{\v{s}}nik, Uro{\v{s}}",Mining Semantic Relations from Comparable Corpora through Intersections of Word Embeddings,,bucc,203
Y17-1032,"['Low-resource Languages', 'Knowledge Representation and Reasoning']",,,"We discuss a process of exploiting a large corpus manually annotated with discourse relations -the Prague Discourse Treebank 2.0to create a lexicon of Czech discourse connectives CzeDLex. The data format and the data structure of the lexicon are based on a study of similar existing resources and are adapted for a uniform representation of both primary such as in English because, therefore and secondary connectives e.g. for this reason, this is the reason why. The main principle adopted for nesting entries in the lexicon is a discourse-semantic type expressed by the given connective word, which enables us to deal with a broad formal variability of connectives. We present a technical solution based on the XML-based Prague Markup Language that allows for an efficient incorporation of the lexicon into the family of Prague treebanksit can be directly opened and edited in the tree editor TrEd, processed from the command line in btred, interlinked with its source corpus and queried in the PML-Tree Query engine -and also for interconnecting CzeDLex with existing lexicons in other languages.",https://aclanthology.org/Y17-1032,The National University (Phillippines),2017,November,"Proceedings of the 31st Pacific Asia Conference on Language, Information and Computation","Synkov{\'a}, Pavl{\'\i}na  and
Rysov{\'a}, Magdal{\'e}na  and
Pol{\'a}kov{\'a}, Lucie  and
M{\'\i}rovsk{\'y}, Ji{\v{r}}{\'\i}",Extracting a Lexicon of Discourse Connectives in Czech from an Annotated Corpus,,Y17,870
2021.maiworkshop-1.8,"['Data Management and Generation', 'Information Extraction', 'Image and Video Processing']","['Data Preparation', 'Video Captioning']",,"Live video comments, or ""danmu"", are an emerging feature on Asian online video platforms. Danmu are time-synchronous comments that are overlaid on a video playback. These comments uniquely enrich the experience and engagement of their users, and have become a determining factor in the popularity of videos on these platforms. Similar to the ""cold start problem"" in recommender systems, a video will only start to attract attention when sufficient danmu comments have been posted on it. We study this video cold start problem and examine how new comments can be generated automatically on less-commented videos. We propose to predict danmu comments to promote user engagement, by exploiting a multi-modal combination of the video visual content, subtitles, audio signals, and any surrounding comments when they exist. Our method fuses these multiple modalities in a transformer network which is then trained for different comment density scenarios. We evaluate our proposed system through both a retrieval based evaluation method, as well as human judgement. Results show that our proposed system improves significantly over stateof-the-art methods.",https://aclanthology.org/2021.maiworkshop-1.8,Association for Computational Linguistics,2021,June,Proceedings of the Third Workshop on Multimodal Artificial Intelligence,"Wu, Hao  and
Pitie, Fran{\c{c}}ois  and
Jones, Gareth",Cold Start Problem For Automated Live Video Comments,10.18653/v1/2021.maiworkshop-1.8,maiworkshop,1148
2020.emnlp-main.376,"['Biases in NLP', 'Data Management and Generation']",['Data Preparation'],,"Languages typically provide more than one grammatical construction to express certain types of messages. A speaker's choice of construction is known to depend on multiple factors, including the choice of main verb -a phenomenon known as verb bias. Here we introduce DAIS, a large benchmark dataset containing 50K human judgments for 5K distinct sentence pairs in the English dative alternation. This dataset includes 200 unique verbs and systematically varies the definiteness and length of arguments. We use this dataset, as well as an existing corpus of naturally occurring data, to evaluate how well recent neural language models capture human preferences. Results show that larger models perform better than smaller models, and transformer architectures e.g. GPT-2 tend to out-perform recurrent architectures e.g. LSTMs even under comparable parameter and training settings. Additional analyses of internal feature representations suggest that transformers may better integrate specific lexical information with grammatical constructions.",https://aclanthology.org/2020.emnlp-main.376,Association for Computational Linguistics,2020,November,Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),"Hawkins, Robert  and
Yamakoshi, Takateru  and
Griffiths, Thomas  and
Goldberg, Adele",Investigating representations of verb bias in neural language models,10.18653/v1/2020.emnlp-main.376,emnlp,1067
L18-1126,"['Learning Paradigms', 'Data Management and Generation']","['Multimodal Learning', 'Data Preparation']",['Annotation Processes'],"In this paper, we present an approach to endow an Embodied Conversational Agent with engagement capabilities. We relied on a corpus of expert-novice interactions. Two types of manual annotation were conducted: non-verbal signals such as gestures, head movements and smiles; engagement level of both expert and novice during the interaction. Then, we used a temporal sequence mining algorithm to extract non-verbal sequences eliciting variation of engagement perception. Our aim is to apply these findings in human-agent interaction to analyze user's engagement level and to control agent's behavior. The novelty of this study is to consider explicitly engagement as sequence of multimodal behaviors.",https://aclanthology.org/L18-1126,European Language Resources Association (ELRA),2018,May,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),"Dermouche, Soumia  and
Pelachaud, Catherine",From analysis to modeling of engagement as sequences of multimodal behaviors,,L18,1278
2020.iwpt-1.22,"['Learning Paradigms', 'Parsing', 'Data Management and Generation', 'Low-resource Languages']","['Supervised Learning', 'Data Augmentation', 'Syntactic Parsing']",['Dependency Parsing'],"This paper presents the system used in our submission to the IWPT 2020 Shared Task. Our system is a graph-based parser with secondorder inference. For the low-resource Tamil corpus, we specially mixed the training data of Tamil with other languages and significantly improved the performance of Tamil. Due to our misunderstanding of the submission requirements, we submitted graphs that are not connected, which makes our system only rank 6th over 10 teams. However, after we fixed this problem, our system is 0.6 ELAS higher than the team that ranked 1st in the official results.",https://aclanthology.org/2020.iwpt-1.22,Association for Computational Linguistics,2020,July,Proceedings of the 16th International Conference on Parsing Technologies and the IWPT 2020 Shared Task on Parsing into Enhanced Universal Dependencies,"Wang, Xinyu  and
Jiang, Yong  and
Tu, Kewei",Enhanced Universal Dependency Parsing with Second-Order Inference and Mixture of Training Data,10.18653/v1/2020.iwpt-1.22,iwpt,1238
2021.dialdoc-1.9,['Question Answering (QA)'],,,"Retrieving relevant answers from heterogeneous data formats, for given for questions, is a challenging problem. The process of pinpointing relevant information suitable to answer a question is further compounded in large document collections containing documents of substantial length. This paper presents the models designed as part of our submission to the DialDoc21 Shared Task Documentgrounded Dialogue and Conversational Question Answering for span prediction in question answering. The proposed models leverage the superior predictive power of pretrained transformer models like RoBERTa, ALBERT and ELECTRA, to identify the most relevant information in an associated passage for the next agent turn. To further enhance the performance, the models were fine-tuned on different span selection based question answering datasets like SQuAD2.0 and Natural Questions NQ corpus. We also explored ensemble techniques for combining multiple models to achieve enhanced performance for the task. Our team SB NITK ranked 6 th on the leaderboard for the Knowledge Identification task, and our best ensemble model achieved an Exact score of 58.58 and an F1 score of 73.39.",https://aclanthology.org/2021.dialdoc-1.9,Association for Computational Linguistics,2021,August,Proceedings of the 1st Workshop on Document-grounded Dialogue and Conversational Question Answering (DialDoc 2021),"Bachina, Sony  and
Balumuri, Spandana  and
Kamath S, Sowmya",Ensemble ALBERT and RoBERTa for Span Prediction in Question Answering,10.18653/v1/2021.dialdoc-1.9,dialdoc,708
W17-6915,['Data Management and Generation'],['Data Analysis'],,"This paper describes a manual investigation of the SICK corpus, which is the proposed testing set for a new system for natural language inference. The system provides conceptual semantics for sentences, so that entailment-contradiction-neutrality relations between sentences can be identified. The investigation of the SICK corpus was a necessary task to check the quality of the testing data which is to be used as a golden standard for the new system. This checking also provides crucial insights for the implementation of the components of the system. The investigation showed that the human judgements used in the building of the SICK corpus can be erroneous, in this way deteriorating the quality of an otherwise useful resource. We also show that detecting the relationship between some pairs of the SICK corpus requires more than just lexical semantics, which provides us with guidelines and intuitions for our further implementation.",https://aclanthology.org/W17-6915,,2017,,{IWCS} 2017 {---} 12th International Conference on Computational Semantics {---} Short papers,"Kalouli, Aikaterini-Lida  and
Real, Livy  and
de Paiva, Valeria",Textual Inference: getting logic from humans,,W17,557
2020.insights-1.5,"['Topic Modeling', 'Learning Paradigms', 'Information Extraction', 'Model Architectures']","['Unsupervised Learning', 'Entity Linking']",,"Topic models have been widely used to discover hidden topics in a collection of documents. In this paper, we propose to investigate the role of two different types of relational information, i.e. document relationships and concept relationships. While exploiting the document network significantly improves topic coherence, the introduction of concepts and their relationships does not influence the results both quantitatively and qualitatively.",https://aclanthology.org/2020.insights-1.5,Association for Computational Linguistics,2020,November,Proceedings of the First Workshop on Insights from Negative Results in NLP,"Terragni, Silvia  and
Nozza, Debora  and
Fersini, Elisabetta  and
Enza, Messina",Which Matters Most? Comparing the Impact of Concept and Document Relationships in Topic Models,10.18653/v1/2020.insights-1.5,insights,1383
W16-6406,"['Machine Translation (MT)', 'Low-resource Languages']",['Rule-based MT (RBMT)'],,"In this paper, we focus on the incorporation of a valency lexicon into TectoMT system for Czech-Russian language pair. We demonstrate valency errors in MT output and describe how the introduction of a lexicon influenced the translation results. Though there was no impact on BLEU score, the manual inspection of concrete cases showed some improvement.",https://aclanthology.org/W16-6406,{\'U}FAL MFF UK,2016,October,Proceedings of the 2nd Deep Machine Translation Workshop,"Klyueva, Natalia  and
Kubo{\v{n}}, Vladislav",Incorporation of a valency lexicon into a TectoMT pipeline,,W16,630
2020.cogalex-1.6,"['Multilingual NLP', 'Cross-lingual Application', 'Low-resource Languages', 'Classification Applications']",,,"The HSemID system, submitted to the CogALex VI Shared Task is a hybrid system relying mainly on metric clusters measured in large web corpora, complemented by a vector space model using cosine similarity to detect semantic associations. Although the system reached rather weak results for the subcategories of synonyms, antonyms and hypernyms, with some differences from one language to another, it is able to measure general semantic associations as being random or not-random with an F1 score close to 0.80. The results strongly suggest that idiomatic constructions play a fundamental role in semantic associations. Further experiments are necessary in order to fine-tune the model to the subcategories of synonyms, antonyms, hypernyms and to explain surprising differences across languages.",https://aclanthology.org/2020.cogalex-1.6,Association for Computational Linguistics,2020,December,Proceedings of the Workshop on the Cognitive Aspects of the Lexicon,"Colson, Jean-Pierre",Extracting meaning by idiomaticity: Description of the HSemID system at CogALex VI 2020,,cogalex,502
2022.suki-1.2,"['Dialogue Systems', 'Model Architectures']",['Transformer Models'],,"Because of the compositionality of natural language, syntactic structure which contains the information about the relationship between words is a key factor for semantic understanding. However, the widely adopted Transformer is hard to learn the syntactic structure effectively in dialogue generation tasks. To explicitly model the compositionaity of language in Transformer Block, we restrict the information flow between words by constructing directed dependency graph and propose Dependency Relation Attention DRA. Experimental results demonstrate that DRA can further improve the performance of state-of-the-art models for dialogue generation.",https://aclanthology.org/2022.suki-1.2,Association for Computational Linguistics,2022,July,Proceedings of the Workshop on Structured and Unstructured Knowledge Integration (SUKI),"Chen, Xiaofeng  and
Chen, Yirong  and
Xing, Xiaofen  and
Xu, Xiangmin  and
Han, Wenjing  and
Tie, Qianfeng",Modeling Compositionality with Dependency Graph for Dialogue Generation,10.18653/v1/2022.suki-1.2,suki,123
2021.lchange-1.3,"['Domain-specific NLP', 'Low-resource Languages', 'Language Change Analysis']",['NLP for News and Media'],,"The use of automatic methods for the study of lexical semantic change LSC has led to the creation of evaluation benchmarks. Benchmark datasets, however, are intimately tied to the corpus used for their creation questioning their reliability as well as the robustness of automatic methods. This contribution investigates these aspects showing the impact of unforeseen social and cultural dimensions. We also identify a set of additional issues OCR quality, named entities that impact the performance of the automatic methods, especially when used to discover LSC.",https://aclanthology.org/2021.lchange-1.3,Association for Computational Linguistics,2021,August,Proceedings of the 2nd International Workshop on Computational Approaches to Historical Language Change 2021,"Basile, Pierpaolo  and
Caputo, Annalina  and
Caselli, Tommaso  and
Cassotti, Pierluigi  and
Varvara, Rossella",The Corpora They Are a-Changing: a Case Study in Italian Newspapers,10.18653/v1/2021.lchange-1.3,lchange,998
2021.metanlp-1.2,"['Learning Paradigms', 'Low-resource Languages', 'Cross-lingual Application', 'Model Architectures']","['Few-shot Learning', 'Transfer Learning', 'Transformer Models']",,"Multilingual pre-trained contextual embedding models Devlin et al., 2019 have achieved impressive performance on zero-shot cross-lingual transfer tasks. Finding the most effective strategy to fine-tune these models on high-resource languages so that it transfers well to the zero-shot languages is a nontrivial task. In this paper, we propose a novel meta-optimizer to soft-select which layers of the pre-trained model to freeze during fine-tuning. We train the meta-optimizer by simulating the zero-shot transfer scenario. Results on cross-lingual natural language inference show that our approach improves over the simple fine-tuning baseline and X-MAML Nooralahzadeh et al., 2020 .",https://aclanthology.org/2021.metanlp-1.2,Association for Computational Linguistics,2021,August,Proceedings of the 1st Workshop on Meta Learning and Its Applications to Natural Language Processing,"Xu, Weijia  and
Haider, Batool  and
Krone, Jason  and
Mansour, Saab",Soft Layer Selection with Meta-Learning for Zero-Shot Cross-Lingual Transfer,10.18653/v1/2021.metanlp-1.2,metanlp,1447
2020.tlt-1.1,"['Parsing', 'Data Management and Generation', 'Low-resource Languages']","['Morphological Parsing', 'Data Preparation', 'Syntactic Parsing']",['Dependency Parsing'],"We present a language-independent clausizer clause splitter based on Universal Dependencies Nivre et al., 2016 , and a clause-level tagger for grammatical tense, mood, voice and modality in German. The paper recapitulates verbal inflection in German-always juxtaposed with its close relative English-and transforms the linguistic theory into a rule-based algorithm. We achieve state-of-the-art accuracies of 92.6% for tense, 79.0% for mood, 93.8% for voice and 79.8% for modality in the literary domain. Our implementation is available at https://gitlab.gwdg. de/tillmann.doenicke/tense-tagger.",https://aclanthology.org/2020.tlt-1.1,Association for Computational Linguistics,2020,October,Proceedings of the 19th International Workshop on Treebanks and Linguistic Theories,"D{\""o}nicke, Tillmann","Clause-Level Tense, Mood, Voice and Modality Tagging for German",10.18653/v1/2020.tlt-1.1,tlt,1386
2021.cinlp-1.2,"['Biases in NLP', 'Ethics']",['Gender Bias'],,"Using observed language to understand interpersonal interactions is important in highstakes decision making. We propose a causal research design for observational nonexperimental data to estimate the natural direct and indirect effects of social group signals e.g. race or gender on speakers' responses with separate aspects of language as causal mediators. We illustrate the promises and challenges of this framework via a theoretical case study of the effect of an advocate's gender on interruptions from justices during U.S. Supreme Court oral arguments. We also discuss challenges conceptualizing and operationalizing causal variables such as gender and language that comprise of many components, and we articulate technical open challenges such as temporal dependence between language mediators in conversational settings.",https://aclanthology.org/2021.cinlp-1.2,Association for Computational Linguistics,2021,November,Proceedings of the First Workshop on Causal Inference and NLP,"Keith, Katherine  and
Rice, Douglas  and
O{'}Connor, Brendan",Text as Causal Mediators: Research Design for Causal Estimates of Differential Treatment of Social Groups via Language Aspects,10.18653/v1/2021.cinlp-1.2,cinlp,339
2021.mrl-1.13,"['Data Management and Generation', 'Knowledge Representation and Reasoning', 'Information Retrieval', 'Learning Paradigms', 'Low-resource Languages']","['Multimodal Learning', 'Data Preparation', 'Knowledge Graphs']",,"An exciting frontier in natural language understanding NLU and generation NLG calls for vision-and-  language models that can efficiently access external structured knowledge repositories. However, many existing knowledge bases only cover limited domains, or suffer from noisy data, and most of all are typically hard to integrate into neural language pipelines. To fill this gap, we release VisualSem: a high-quality knowledge graph KG which includes nodes with multilingual glosses, multiple illustrative images, and visually relevant relations. We also release a neural multi-modal retrieval model that can use images or sentences as inputs and retrieves entities in the KG. This multi-modal retrieval model can be integrated into any neural network model pipeline. We encourage the research community to use VisualSem for data augmentation and/or as a source of grounding, among other possible uses. Vi-sualSem as well as the multi-modal retrieval models are publicly available and can be downloaded in this URL: https://github. com/iacercalixto/visualsem. * Work initiated while in the University of Amsterdam during her MSc. research. 1 See for instance https://en.wikipedia.org/ wiki/Wikipedia:Images and https://commons. wikimedia.org/wiki/Main_Page.",https://aclanthology.org/2021.mrl-1.13,Association for Computational Linguistics,2021,November,Proceedings of the 1st Workshop on Multilingual Representation Learning,"Alberts, Houda  and
Huang, Ningyuan  and
Deshpande, Yash  and
Liu, Yibo  and
Cho, Kyunghyun  and
Vania, Clara  and
Calixto, Iacer",VisualSem: a high-quality knowledge graph for vision and language,10.18653/v1/2021.mrl-1.13,mrl,149
2019.rocling-1.28,"['Domain-specific NLP', 'Data Management and Generation', 'Audio Generation and Processing']",['Data Preparation'],,"Human perception on the singing voice differs with the factors of the singing voice and the subjects. On one hand, the background knowledge influences the understanding of voice for each subject. On the other hand, the difference of the voices presented to the subjects also affects the perception. In this paper, we discuss two factors reflecting on the similarity before and after singing voice conversion: prosodic features and subjects' familiarity to the singers. Three experiments were conducted. The first experiment tested the subjects' ability to identify the singer. The second experiment synthesized the singing voice with different singers' prosodic features, and let the subjects score the similarity. The third experiment presented timbre-converted singing voice with different combinations of prosodic features from two singers to the subjects for them to judge the similarity to the target singer. The results show that, first, the number of prosodic features contained in the synthesized voice is positively correlated with the scores in identification and similarity. Also, subjects who are more familiar personally with the target singers have better identification scores than target-unfamiliar subjects on the timbre-converted singing voices.",https://aclanthology.org/2019.rocling-1.28,The Association for Computational Linguistics and Chinese Language Processing (ACLCLP),2019,October,Proceedings of the 31st Conference on Computational Linguistics and Speech Processing (ROCLING 2019),"Kang, Kuan-Yi  and
Liu, Yi-Wen  and
Wang, Hsin-Min",Influences of Prosodic Feature Replacement on the Perceived Singing Voice Identity,,rocling,1071
2022.dadc-1.6,"['Question Answering (QA)', 'Learning Paradigms', 'Data Management and Generation']","['Data Preparation', 'Adversarial Learning']",['Annotation Processes'],"We present our experience as annotators in the creation of high-quality, adversarial machinereading-comprehension data for extractive QA for Task 1 of the First Workshop on Dynamic Adversarial Data Collection DADC. DADC is an emergent data collection paradigm with both models and humans in the loop. We set up a quasi-experimental annotation design and perform quantitative analyses across groups with different numbers of annotators focusing on successful adversarial attacks, cost analysis, and annotator confidence correlation. We further perform a qualitative analysis of our perceived difficulty of the task given the different topics of the passages in our dataset and conclude with recommendations and suggestions that might be of value to people working on future DADC tasks and related annotation interfaces.",https://aclanthology.org/2022.dadc-1.6,Association for Computational Linguistics,2022,July,Proceedings of the First Workshop on Dynamic Adversarial Data Collection,"Romero Diaz, Damian Y.  and
Anio{\l}, Magdalena  and
Culnan, John",Collecting high-quality adversarial data for machine reading comprehension tasks with humans and models in the loop,10.18653/v1/2022.dadc-1.6,dadc,1479
2021.metanlp-1.5,"['Cross-lingual Application', 'Domain-specific NLP', 'Multilingual NLP', 'Learning Paradigms', 'Low-resource Languages', 'Classification Applications']","['Few-shot Learning', 'Intent Detection', 'Transfer Learning']",,"Supervised deep learning-based approaches have been applied to task-oriented dialog and have proven to be effective for limited domain and language applications when a sufficient number of training examples are available. In practice, these approaches suffer from the drawbacks of domain-driven design and under-resourced languages. Domain and language models are supposed to grow and change as the problem space evolves. On one hand, research on transfer learning has demonstrated the cross-lingual ability of multilingual Transformers-based models to learn semantically rich representations. On the other, in addition to the above approaches, meta-learning have enabled the development of task and language learning algorithms capable of far generalization. Through this context, this article proposes to investigate the cross-lingual transferability of using synergistically few-shot learning with prototypical neural networks and multilingual Transformers-based models. Experiments in natural language understanding tasks on MultiATIS++ corpus shows that our approach substantially improves the observed transfer learning performances between the low and the high resource languages. More generally our approach confirms that the meaningful latent space learned in a given language can be can be generalized to unseen and underresourced ones using meta-learning.",https://aclanthology.org/2021.metanlp-1.5,Association for Computational Linguistics,2021,August,Proceedings of the 1st Workshop on Meta Learning and Its Applications to Natural Language Processing,"Cattan, Oralie  and
Rosset, Sophie  and
Servan, Christophe",On the cross-lingual transferability of multilingual prototypical models across NLU tasks,10.18653/v1/2021.metanlp-1.5,metanlp,688
2020.msr-1.2,"['Parsing', 'Model Architectures']",['Syntactic Parsing'],,"We present a system for mapping Universal Dependency structures to raw text which learns to restore word order by training an Interpreted Regular Tree Grammar IRTG that establishes a mapping between string and graph operations. The reinflection step is handled by a standard sequence-to-sequence architecture with a biLSTM encoder and an LSTM decoder with attention. We modify our 2019 system Kovcs et al., 2019 with a new grammar induction mechanism that allows IRTG rules to operate on lemmata in addition to part-of-speech tags and ensures that each word and its dependents are reordered using the most specific set of learned patterns. We also introduce a hierarchical approach to word order restoration that independently determines the word order of each clause in a sentence before arranging them with respect to the main clause, thereby improving overall readability and also making the IRTG parsing task tractable. We participated in the 2020 Surface Realization Shared task, subtrack T1a shallow, closed. Human evaluation shows we achieve significant improvements on two of the three out-of-domain datasets compared to the 2019 system we modified. Both components of our system are available on GitHub under an MIT license.",https://aclanthology.org/2020.msr-1.2,Association for Computational Linguistics,2020,December,Proceedings of the Third Workshop on Multilingual Surface Realisation,"Recski, G{\'a}bor  and
Kov{\'a}cs, {\'A}d{\'a}m  and
G{\'e}mes, Kinga  and
{\'A}cs, Judit  and
Kornai, Andras",BME-TUW at SR'20: Lexical grammar induction for surface realization,,msr,301
2020.trac-1.10,"['Classification Applications', 'Model Architectures']","['Transformer Models', 'Hate and Offensive Speech Detection']",,"This paper presents a system developed during our participation team name: scmhl5 in the TRAC-2 Shared Task on aggression identification. In particular, we participated in English Sub-task A on three-class classification 'Overtly Aggressive', 'Covertly Aggressive' and 'Non-aggressive' and English Sub-task B on binary classification for Misogynistic Aggression 'gendered' or 'non-gendered'. For both sub-tasks, our method involves using the pre-trained Bert model for extracting the text of each instance into a 768-dimensional vector of embeddings, and then training an ensemble of classifiers on the embedding features. Our method obtained accuracy of 0.703 and weighted F-measure of 0.664 for Sub-task A, whereas for Sub-task B the accuracy was 0.869 and weighted F-measure was 0.851. In terms of the rankings, the weighted F-measure obtained using our method for Sub-task A is ranked in the 10th out of 16 teams, whereas for Sub-task B the weighted F-measure is ranked in the 8th out of 15 teams.",https://aclanthology.org/2020.trac-1.10,European Language Resources Association (ELRA),2020,May,"Proceedings of the Second Workshop on Trolling, Aggression and Cyberbullying","Liu, Han  and
Burnap, Pete  and
Alorainy, Wafa  and
Williams, Matthew",Scmhl5 at TRAC-2 Shared Task on Aggression Identification: Bert Based Ensemble Learning Approach,,trac,974
2021.eacl-main.324,['Evaluation Techniques'],,,"Performance prediction, the task of estimating a system's performance without performing experiments, allows us to reduce the experimental burden caused by the combinatorial explosion of different datasets, languages, tasks, and models. In this paper, we make two contributions to improving performance prediction for NLP tasks. First, we examine performance predictors not only for holistic measures of accuracy like F1 or BLEU, but also fine-grained performance measures such as accuracy over individual classes of examples. Second, we propose methods to understand the reliability of a performance prediction model from two angles: confidence intervals and calibration. We perform an analysis of four types of NLP tasks, and both demonstrate the feasibility of fine-grained performance prediction and the necessity to perform reliability analysis for performance prediction methods in the future.",https://aclanthology.org/2021.eacl-main.324,Association for Computational Linguistics,2021,April,Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume,"Ye, Zihuiwen  and
Liu, Pengfei  and
Fu, Jinlan  and
Neubig, Graham",Towards More Fine-grained and Reliable NLP Performance Prediction,10.18653/v1/2021.eacl-main.324,eacl,408
2022.wnu-1.4,"['Text Generation', 'Domain-specific NLP', 'Model Architectures']","['Story Generation', 'Transformer Models', 'Large Language Models (LLMs)']",,"We experiment with adapting generative language models for the generation of long coherent narratives in the form of theatre plays. Since fully automatic generation of whole plays is not currently feasible, we created an interactive tool that allows a human user to steer the generation somewhat while minimizing intervention. We pursue two approaches to long-text generation: a flat generation with summarization of context, and a hierarchical text-to-text two-stage approach, where a synopsis is generated first and then used to condition generation of the final script. Our preliminary results and discussions with theatre professionals show improvements over vanilla language model generation, but also identify important limitations of our approach.",https://aclanthology.org/2022.wnu-1.4,Association for Computational Linguistics,2022,July,Proceedings of the 4th Workshop of Narrative Understanding (WNU2022),"Rosa, Rudolf  and
Schmidtov{\'a}, Patr{\'\i}cia  and
Du{\v{s}}ek, Ond{\v{r}}ej  and
Musil, Tom{\'a}{\v{s}}  and
Mare{\v{c}}ek, David  and
Obaid, Saad  and
Nov{\'a}kov{\'a}, Marie  and
Voseck{\'a}, Kl{\'a}ra  and
Dole{\v{z}}al, Josef",GPT-2-based Human-in-the-loop Theatre Play Script Generation,10.18653/v1/2022.wnu-1.4,wnu,77
Q18-1011,"['Machine Translation (MT)', 'Low-resource Languages', 'Model Architectures']","['Neural MT (NMT)', 'Recurrent Neural Networks (RNNs)']",,"Existing neural machine translation systems do not explicitly model what has been translated and what has not during the decoding phase. To address this problem, we propose a novel mechanism that separates the source information into two parts: translated PAST contents and untranslated FUTURE contents, which are modeled by two additional recurrent layers. The PAST and FUTURE contents are fed to both the attention model and the decoder states, which provides Neural Machine Translation NMT systems with the knowledge of translated and untranslated contents. Experimental results show that the proposed approach significantly improves the performance in Chinese-English, German-English, and English-German translation tasks. Specifically, the proposed model outperforms the conventional coverage model in terms of both the translation quality and the alignment error rate.  * Equal contributions.  Our code can be downloaded from https://github. com/zhengzx-nlp/past-and-future-nmt.",https://aclanthology.org/Q18-1011,MIT Press,2018,,,"Zheng, Zaixiang  and
Zhou, Hao  and
Huang, Shujian  and
Mou, Lili  and
Dai, Xinyu  and
Chen, Jiajun  and
Tu, Zhaopeng",Modeling Past and Future for Neural Machine Translation,10.1162/tacl_a_00011,Q18,231
2020.nlptea-1.17,"['Evaluation Techniques', 'Data Management and Generation', 'Low-resource Languages', 'Text Generation']","['Data Preparation', 'Text Simplification']",,"Text simplification is an important branch of natural language processing. At present, methods used to evaluate the semantic retention of text simplification are mostly based on string matching. We propose the SEMA text Simplification Evaluation Measure through Semantic Alignment, which is based on semantic alignment. Semantic alignments include complete alignment, partial alignment and hyponymy alignment. Our experiments show that the evaluation results of SEMA have a high consistency with human evaluation for the simplified corpus of Chinese and English news texts.",https://aclanthology.org/2020.nlptea-1.17,Association for Computational Linguistics,2020,December,Proceedings of the 6th Workshop on Natural Language Processing Techniques for Educational Applications,"Zhang, Xuan  and
Zhao, Huizhou  and
Zhang, KeXin  and
Zhang, Yiyang",SEMA: Text Simplification Evaluation through Semantic Alignment,10.18653/v1/2020.nlptea-1.17,nlptea,651
O16-1016,"['Audio Generation and Processing', 'Domain-specific NLP', 'Low-resource Languages', 'Data Management and Generation']","['Data Preparation', 'Data Analysis', 'Medical and Clinical NLP']",,"Literatures pertaining to English and Mandarin fricative/affricate productions by adults with cerebral palsy CP showed that acoustic measurements such as rise time contrast, initial burst rate contrast and friction noise duration contrast associated with fricative/affricate productions were highly correlated with overall speech intelligibility. However, the phonetic features of fricatives/affricates produced by Mandarin-learning children with CP were not fully explored. Therefore, this study targets on fricatives/affricates produced by ten Mandarin-learning CP children Mean: 6;10, Range: 4;6 -8;11 and ten Mandarin-learning typically developing children Mean: 5;7, Range: 5;2 -6;1. The current results from a speech repetition task showed that: 1 The fricative/affricate accurate rates and error patterns were similar between the two",https://aclanthology.org/O16-1016,The Association for Computational Linguistics and Chinese Language Processing (ACLCLP),2016,October,Proceedings of the 28th Conference on Computational Linguistics and Speech Processing ({ROCLING} 2016),"Liu, Chin-Ting  and
Chen, Li-mei  and
Lin, Yu-Ching  and
Cheng, Chia-Fang  and
Chang, Hui-chen",Speech Intelligibility and the Production of Fricative and Affricate among Mandarin-speaking Children with Cerebral Palsy,,O16,892
2020.iwltp-1.12,['Data Management and Generation'],,,"We present a workflow manager for the flexible creation and customisation of NLP processing pipelines. The workflow manager addresses challenges in interoperability across various different NLP tasks and hardware-based resource usage. Based on the four key principles of generality, flexibility, scalability and efficiency, we present the first version of the workflow manager by providing details on its custom definition language, explaining the communication components and the general system architecture and setup. We currently implement the system, which is grounded and motivated by real-world industry use cases in several innovation and transfer projects.",https://aclanthology.org/2020.iwltp-1.12,European Language Resources Association,2020,May,Proceedings of the 1st International Workshop on Language Technology Platforms,"Moreno-Schneider, Julian  and
Bourgonje, Peter  and
Kintzel, Florian  and
Rehm, Georg",A Workflow Manager for Complex NLP and Content Curation Workflows,,iwltp,76
L16-1627,"['Learning Paradigms', 'Model Architectures']",['Multimodal Learning'],,"Despite the recent success of distributional semantic models DSMs in various semantic tasks they remain disconnected with real-world perceptual cues since they typically rely on linguistic features. Text data constitute the dominant source of features for the majority of such models, although there is evidence from cognitive science that cues from other modalities contribute to the acquisition and representation of semantic knowledge. In this work, we propose the crossmodal extension of a two-tier text-based model, where semantic representations are encoded in the first layer, while the second layer is used for computing similarity between words. We exploit text-and image-derived features for performing computations at each layer, as well as various approaches for their crossmodal fusion. It is shown that the crossmodal model performs better from 0.68 to 0.71 correlation coefficient than the unimodal one for the task of similarity computation between words.",https://aclanthology.org/L16-1627,European Language Resources Association (ELRA),2016,May,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),"Iosif, Elias  and
Potamianos, Alexandros",Crossmodal Network-Based Distributional Semantic Models,,L16,559
L18-1725,"['Parsing', 'Data Management and Generation', 'Information Extraction', 'Knowledge Representation and Reasoning']","['Event Extraction', 'Data Preparation', 'Semantic Parsing', 'Ontologies']",['Semantic Role Labeling'],"In this paper, we describe the Circumstantial Event Ontology CEO, a newly developed ontology for calamity events that models semantic circumstantial relations between event classes, where we define circumstantial as inferred implicit causal relations. The circumstantial relations are inferred from the assertions of the event classes that involve a change to the same property of a participant. Our model captures that the change yielded by one event, explains to people the happening of the next event when observed. We describe the meta model and the contents of the ontology, the creation of a manually annotated corpus for circumstantial relations based on ECB+ and the first results on the evaluation of the ontology.",https://aclanthology.org/L18-1725,European Language Resources Association (ELRA),2018,May,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),"Segers, Roxane  and
Caselli, Tommaso  and
Vossen, Piek",The Circumstantial Event Ontology CEO and ECB+/CEO: an Ontology and Corpus for Implicit Causal Relations between Events,,L18,1237
2021.calcs-1.18,"['Multilingual NLP', 'Low-resource Languages', 'Classification Applications']",,,"We introduce a Cyrillic-to-Latin transliterator for the Tatar language based on subword-level language identification. The transliteration is a challenging task due to the following two reasons. First, because modern Tatar texts often contain intra-word code-switching to Russian, a different transliteration set of rules needs to be applied to each morpheme depending on the language, which necessitates morpheme-level language identification. Second, the fact that Tatar is a low-resource language, with most of the texts in Cyrillic, makes it difficult to prepare a sufficient dataset. Given this situation, we proposed a transliteration method based on subword-level language identification. We trained a language classifier with monolingual Tatar and Russian texts, and applied different transliteration rules in accord with the identified language. The results demonstrate that our proposed method outscores other Tatar transliteration tools, and imply that it correctly transcribes Russian loanwords to some extent.",https://aclanthology.org/2021.calcs-1.18,Association for Computational Linguistics,2021,June,Proceedings of the Fifth Workshop on Computational Approaches to Linguistic Code-Switching,"Taguchi, Chihiro  and
Sakai, Yusuke  and
Watanabe, Taro",Transliteration for Low-Resource Code-Switching Texts: Building an Automatic Cyrillic-to-Latin Converter for Tatar,10.18653/v1/2021.calcs-1.18,calcs,720
2021.sdp-1.7,"['Data Management and Generation', 'Argument Mining', 'Domain-specific NLP', 'Discourse Analysis']","['NLP for Bibliometrics and Scientometrics', 'Data Analysis']",,"Argument mining targets structures in natural language related to interpretation and persuasion. Most scholarly discourse involves interpreting experimental evidence and attempting to persuade other scientists to adopt the same conclusions, which could benefit from argument mining techniques. However, While various argument mining studies have addressed student essays and news articles, those that target scientific discourse are still scarce. This paper surveys existing work in argument mining of scholarly discourse, and provides an overview of current models, data, tasks, and applications. We identify a number of key challenges confronting argument mining in the scientific domain, and suggest some possible solutions and future directions.",https://aclanthology.org/2021.sdp-1.7,Association for Computational Linguistics,2021,June,Proceedings of the Second Workshop on Scholarly Document Processing,"Al Khatib, Khalid  and
Ghosal, Tirthankar  and
Hou, Yufang  and
de Waard, Anita  and
Freitag, Dayne",Argument Mining for Scholarly Document Processing: Taking Stock and Looking Ahead,10.18653/v1/2021.sdp-1.7,sdp,1491
2021.icnlsp-1.18,"['Topic Modeling', 'Data Management and Generation', 'Low-resource Languages', 'Domain-specific NLP']","['Data Preparation', 'Data Analysis', 'NLP for News and Media']","['NLP for Social Media', 'Annotation Processes']","Nowadays, online social media hugely influences individuals' daily lives, companies, institutions, and governments. Analyzing the online social content related to the productivity of any company becomes crucial to manage and supervise its activities and future trends. We investigate the quality of social signals and content related to Algerian products and services to enhance their exploitation and deployment. Our investigation relies on the statistical analysis of social signals and the textual analysis of User-Generated Contents Posts and Comments. The current work has been done on a sample of more than 50 brands gathering products and services on Facebook with 10K posts and their related comments totaling around 100K. We measure Users/Brand Engagement Rates ER considering reactions and content. We adopted a statistical analysis for the reactionbased measurement. We leveraged an LDAbased Topic Modeling Approach for contentbased measurement. Our findings emphasize the significance of the existing social signals and user-generated content in the Algerian context.",https://aclanthology.org/2021.icnlsp-1.18,Association for Computational Linguistics,2021,12--13 November,Proceedings of The Fourth International Conference on Natural Language and Speech Processing (ICNLSP 2021),"Chorana, Aicha  and
Cherroun, Hadda",User Generated Content and Engagement Analysis in Social Media case of Algerian Brands,,icnlsp,1113
S17-2155,"['Embeddings', 'Domain-specific NLP', 'Classification Applications']","['NLP for Finance', 'Word Embeddings', 'Sentiment Analysis (SA)', 'NLP for News and Media']",['NLP for Social Media'],"This paper presents the approach developed at the Faculty of Engineering of University of Porto, to participate in SemEval 2017, Task 5: Fine-grained Sentiment Analysis on Financial Microblogs and News. The task consisted in predicting a real continuous variable from -1.0 to +1.0 representing the polarity and intensity of sentiment concerning companies/stocks mentioned in short texts. We modeled the task as a regression analysis problem and combined traditional techniques such as pre-processing short texts, bag-of-words representations and lexical-based features with enhanced financial specific bag-ofembeddings. We used an external collection of tweets and news headlines mentioning companies/stocks from S&P 500 to create financial word embeddings which are able to capture domain-specific syntactic and semantic similarities. The resulting approach obtained a cosine similarity score of 0.69 in sub-task 5.1 -Microblogs and 0.68 in sub-task 5.2 -News Headlines.",https://aclanthology.org/S17-2155,Association for Computational Linguistics,2017,August,Proceedings of the 11th International Workshop on Semantic Evaluation ({S}em{E}val-2017),"Saleiro, Pedro  and
Mendes Rodrigues, Eduarda  and
Soares, Carlos  and
Oliveira, Eug{\'e}nio",FEUP at SemEval-2017 Task 5: Predicting Sentiment Polarity and Intensity with Financial Word Embeddings,10.18653/v1/S17-2155,S17,448
2019.ccnlg-1.4,"['Model Architectures', 'Text Generation']",['Recurrent Neural Networks (RNNs)'],['Long Short-Term Memory (LSTM) Models'],"Text embellishment is a natural language generation problem that aims to enhance the lexical and syntactic complexity of a text. i.e., for a given sentence, the goal is to generate a sentence that is lexically and syntactically complex while retaining the same semantic information and meaning. In contrast to text simplification Wang et al., 2016 , text embellishment is considered to be a more complex problem as it requires linguistic expertise, and therefore are difficult to be shared across different platforms and domain. In this paper, we have explored this problem through the light of neural machine translation and text simplification. Instead of using a standard sequential encoder-decoder network, we propose to improve text embellishment with the Transformer model. The proposed model yields superior performance in terms of lexical and syntactic embellishment and demonstrates broad applicability and effectiveness. We also introduce a language and domain agnostic evaluation set up specifically for the task of embellishment that can be used to test different embellishment algorithms.",https://aclanthology.org/2019.ccnlg-1.4,Association for Computational Linguistics,2019,29 October--3 November,Proceedings of the 4th Workshop on Computational Creativity in Language Generation,"Naskar, Subhajit  and
Saha, Soumya  and
Mukherjee, Sreeparna",Text Embellishment using Attention Based Encoder-Decoder Model,,ccnlg,331
D19-1355,"['Learning Paradigms', 'Information Extraction', 'Model Architectures']","['Transformer Models', 'Supervised Learning', 'Word Sense Disambiguation (WSD)']",,"Word Sense Disambiguation WSD aims to find the exact sense of an ambiguous word in a particular context. Traditional supervised methods rarely take into consideration the lexical resources like WordNet, which are widely utilized in knowledge-based methods. Recent studies have shown the effectiveness of incorporating gloss sense definition into neural networks for WSD. However, compared with traditional word expert supervised methods, they have not achieved much improvement. In this paper, we focus on how to better leverage gloss knowledge in a supervised neural WSD system. We construct context-gloss pairs and propose three BERT-based models for WSD. We fine-tune the pre-trained BERT model and achieve new state-of-the-art results on WSD task.",https://aclanthology.org/D19-1355,Association for Computational Linguistics,2019,November,Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"Huang, Luyao  and
Sun, Chi  and
Qiu, Xipeng  and
Huang, Xuanjing",GlossBERT: BERT for Word Sense Disambiguation with Gloss Knowledge,10.18653/v1/D19-1355,D19,6
2021.emnlp-main.508,['Knowledge Representation and Reasoning'],,,"Defeasible reasoning is the mode of reasoning where conclusions can be overturned by taking into account new evidence. Existing cognitive science literature on defeasible reasoning suggests that a person forms a mental model of the problem scenario before answering questions. Our research goal asks whether neural models can similarly benefit from envisioning the question scenario before answering a defeasible query. Our approach is, given a question, to have a model first create a graph of relevant influences, and then leverage that graph as an additional input when answering the question. Our system, CURIOUS, achieves a new stateof-the-art on three different defeasible reasoning datasets. This result is significant as it illustrates that performance can be improved by guiding a system to ""think about"" a question and explicitly model the scenario, rather than answering reflexively. 1",https://aclanthology.org/2021.emnlp-main.508,Association for Computational Linguistics,2021,November,Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,"Madaan, Aman  and
Tandon, Niket  and
Rajagopal, Dheeraj  and
Clark, Peter  and
Yang, Yiming  and
Hovy, Eduard",Think about it! Improving defeasible reasoning by first modeling the question scenario.,10.18653/v1/2021.emnlp-main.508,emnlp,940
2022.insights-1.16,"['Machine Translation (MT)', 'Learning Paradigms', 'Classification Applications']","['Misinformation Detection', 'Neural MT (NMT)', 'Sentiment Analysis (SA)']",['Fake News Detection'],"Current state-of-the-art NLP systems use large neural networks that require extensive computational resources for training. Inspired by human knowledge acquisition, researchers have proposed curriculum learning -sequencing tasks task-based curricula or ordering and sampling the datasets data-based curricula that facilitate training. This work investigates the benefits of data-based curriculum learning for large language models such as BERT and T5. We experiment with various curricula based on complexity measures and different sampling strategies. Extensive experiments on several NLP tasks show that curricula based on various complexity measures rarely have any benefits, while random sampling performs either as well or better than curricula.",https://aclanthology.org/2022.insights-1.16,Association for Computational Linguistics,2022,May,Proceedings of the Third Workshop on Insights from Negative Results in NLP,"Surkov, Maxim  and
Mosin, Vladislav  and
Yamshchikov, Ivan",Do Data-based Curricula Work?,10.18653/v1/2022.insights-1.16,insights,1180
2020.wnut-1.75,"['Model Architectures', 'Classification Applications']","['Recurrent Neural Networks (RNNs)', 'Transformer Models']",['Long Short-Term Memory (LSTM) Models'],"In the scope of WNUT-2020 Task 2, we developed various text classification systems, using deep learning models and one using linguistically informed rules. While both of the deep learning systems outperformed the system using the linguistically informed rules, we found that through the integration of the output of the three systems a better performance could be achieved than the standalone performance of each approach in a cross-validation setting. However, on the test data the performance of the integration was slightly lower than our best performing deep learning model. These results hardly indicate any progress in line of integrating machine learning and expert rules driven systems. We expect that the release of the annotation manuals and gold labels of the test data after this workshop will shed light on these perplexing results.",https://aclanthology.org/2020.wnut-1.75,Association for Computational Linguistics,2020,November,Proceedings of the Sixth Workshop on Noisy User-generated Text (W-NUT 2020),"H{\""u}rriyeto{\u{g}}lu, Ali  and
Safaya, Ali  and
Mutlu, Osman  and
Oostdijk, Nelleke  and
Y{\""o}r{\""u}k, Erdem",COVCOR20 at WNUT-2020 Task 2: An Attempt to Combine Deep Learning and Expert rules,10.18653/v1/2020.wnut-1.75,wnut,454
2021.iwclul-1.5,"['Audio Generation and Processing', 'Data Management and Generation', 'Low-resource Languages']","['Data Augmentation', 'Automatic Speech Recognition (ASR)']",,"It is widely known that a good language model LM can dramatically improve the quality of automatic speech recognition ASR. However, when dealing with a lowresource language, it is often the case that not only aligned audio data is scarce, but there are also not enough texts to train a good LM. This is the case of Beserman, an unwritten dialect of Udmurt Uralic > Permic. With about 10 hours of aligned audio and about 164K words of texts available for training, the word error rate of a Deepspeech model with the best set of parameters equals 56.4%. However, there are other linguistic resources available for Beserman, namely a bilingual Beserman-Russian dictionary and a rule-based morphological analyzer. The goal of this paper is to explore whether and how these additional resources can be exploited to improve the ASR quality. Specifically, I attempt to use them in order to expand the existing LM by generating a large number of fake sentences that in some way look like genuine Beserman text. It turns out that a sophisticated enough augmented LM generator can indeed improve the ASR quality. Nevertheless, the improvement is far from dramatic, with about 5% decrease in word error rate WER and 2% decrease in character error rate CER.",https://aclanthology.org/2021.iwclul-1.5,Association for Computational Linguistics,2021,September,Proceedings of the Seventh International Workshop on Computational Linguistics of Uralic Languages,"Arkhangelskiy, Timofey",Low-Resource ASR with an Augmented Language Model,,iwclul,919
2022.deeplo-1.13,"['Question Answering (QA)', 'Learning Paradigms', 'Data Management and Generation']",['Data Preparation'],,"Curriculum learning strategies in prior multitask learning approaches arrange datasets in a difficulty hierarchy either based on human perception or by exhaustively searching the optimal arrangement. However, human perception of difficulty may not always correlate well with machine interpretation leading to poor performance and exhaustive search is computationally expensive. Addressing these concerns, we propose two classes of techniques to arrange training instances into a learning curriculum based on difficulty scores computed via model-based approaches. The two classes i.e Dataset-level and Instance-level differ in granularity of arrangement. Through comprehensive experiments with 12 datasets, we show that instance-level and dataset-level techniques result in strong representations as they lead to an average performance improvement of 4.17% and 3.15% over their respective baselines. Furthermore, we find that most of this improvement comes from correctly answering the difficult instances, implying a greater efficacy of our techniques on difficult tasks.",https://aclanthology.org/2022.deeplo-1.13,Association for Computational Linguistics,2022,July,Proceedings of the Third Workshop on Deep Learning for Low-Resource Natural Language Processing,"Varshney, Neeraj  and
Mishra, Swaroop  and
Baral, Chitta",Let the Model Decide its Curriculum for Multitask Learning,10.18653/v1/2022.deeplo-1.13,deeplo,87
2022.iwslt-1.32,"['Text Generation', 'Machine Translation (MT)', 'Data Management and Generation', 'Learning Paradigms', 'Low-resource Languages', 'Model Architectures']","['Text Style Transfer', 'Data Augmentation', 'Neural MT (NMT)', 'Supervised Learning']",,"This paper describes Amazon Alexa AI's implementation for the IWSLT 2022 shared task on formality control. We focus on the unconstrained and supervised task for enhi Hindi and enja Japanese pairs where very limited formality annotated data is available. We propose three simple yet effective post editing strategies namely, T-V conversion, utilizing a verb conjugator and seq2seq models in order to rewrite the translated phrases into formal or informal language. Considering nuances for formality and informality in different languages, our analysis shows that a language-specific post editing strategy achieves the best performance. To address the unique challenge of limited formality annotations, we further develop a formality classifier to perform weaklylabelled data augmentation which automatically generates synthetic formality labels from large parallel corpus. Empirical results on the IWSLT formality testset have shown that proposed system achieved significant improvements in terms of formality accuracy while retaining BLEU score on-par with baseline.",https://aclanthology.org/2022.iwslt-1.32,Association for Computational Linguistics,2022,May,Proceedings of the 19th International Conference on Spoken Language Translation (IWSLT 2022),"Zhang, Daniel  and
Yu, Jiang  and
Verma, Pragati  and
Ganesan, Ashwinkumar  and
Campbell, Sarah",Improving Machine Translation Formality Control with Weakly-Labelled Data Augmentation and Post Editing Strategies,10.18653/v1/2022.iwslt-1.32,iwslt,1415
2021.findings-acl.374,"['Question Answering (QA)', 'Information Retrieval']",,,"Recent advancements in transformer-based models have greatly improved the ability of Question Answering QA systems to provide correct answers; in particular, answer sentence selection AS2 models, core components of retrieval-based systems, have achieved impressive results. While generally effective, these models fail to provide a satisfying answer when all retrieved candidates are of poor quality, even if they contain correct information. In AS2, models are trained to select the best answer sentence among a set of candidates retrieved for a given question. In this work, we propose to generate answers from a set of AS2 top candidates. Rather than selecting the best candidate, we train a sequence to sequence transformer model to generate an answer from a candidate set. Our tests on three English AS2 datasets show improvement up to 32 absolute points in accuracy over the state of the art. * This work was completed while the author was an intern at Amazon Alexa. Q: How a water pump works? c1: A small, electrically powered pump. c2: A large, electrically driven pump electropump for waterworks near the Hengsteysee, Germany. c3: A pump is a device that moves fluids liquids or gases, or sometimes slurries, by mechanical action. c4: Pumps can be classified into three major groups according to the method they use to move the fluid: direct lift, displacement, and gravity pumps. c5: Pumps operate by some mechanism typically reciprocating or rotary, and consume energy to perform mechanical work by moving the fluid. G: A water pump is a device that moves fluids by mechanical action.",https://aclanthology.org/2021.findings-acl.374,Association for Computational Linguistics,2021,August,Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021,"Hsu, Chao-Chun  and
Lind, Eric  and
Soldaini, Luca  and
Moschitti, Alessandro",Answer Generation for Retrieval-based Question Answering Systems,10.18653/v1/2021.findings-acl.374,findings,780
2020.framenet-1.10,"['Data Management and Generation', 'Knowledge Representation and Reasoning', 'Low-resource Languages']",['Data Preparation'],['Annotation Processes'],"The Emirati Arabic FrameNet EAFN project aims to initiate a FrameNet for Emirati Arabic, utilizing the Emirati Arabic Corpus. The goal is to create a resource comparable to the initial stages of the Berkeley FrameNet. The project is divided into manual and automatic tracks, based on the predominant techniques being used to collect frames in each track. Work on the EAFN is progressing, and we here report on initial results for annotations and evaluation. The EAFN project aims to provide a general semantic resource for the Arabic language, sure to be of interest to researchers from general linguistics to natural language processing. As we report here, the EAFN is well on target for the first release of data in the coming year.",https://aclanthology.org/2020.framenet-1.10,European Language Resources Association,2020,May,"Proceedings of the International FrameNet Workshop 2020: Towards a Global, Multilingual FrameNet","Gargett, Andrew  and
Leung, Tommi",Building the Emirati Arabic FrameNet,,framenet,818
S16-1207,"['Information Extraction', 'Knowledge Representation and Reasoning']",['Word Sense Disambiguation (WSD)'],,"This paper describes the Duluth systems that participated in Task 14 of SemEval 2016, Semantic Taxonomy Enrichment. There were three related systems in the formal evaluation which are discussed here, along with numerous post-evaluation runs. All of these systems identified synonyms between Word-Net and other dictionaries by measuring the gloss overlaps between them. These systems perform better than the random baseline and one post-evaluation variation was within a respectable margin of the median result attained by all participating systems.",https://aclanthology.org/S16-1207,Association for Computational Linguistics,2016,June,Proceedings of the 10th International Workshop on Semantic Evaluation ({S}em{E}val-2016),"Pedersen, Ted",Duluth at SemEval 2016 Task 14: Extending Gloss Overlaps to Enrich Semantic Taxonomies,10.18653/v1/S16-1207,S16,683
2021.newsum-1.7,"['Machine Translation (MT)', 'Low-resource Languages', 'Automatic Text Summarization']",['Abstractive Text Summarization'],,"We present work on summarising deliberative processes for non-English languages. Unlike commonly studied datasets, such as news articles, this deliberation dataset reflects difficulties of combining multiple narratives, mostly of poor grammatical quality, in a single text. We report an extensive evaluation of a wide range of abstractive summarisation models in combination with an off-the-shelf machine translation model. Texts are translated into English, summarised, and translated back to the original language. We obtain promising results regarding the fluency, consistency and relevance of the summaries produced. Our approach is easy to implement for many languages for production purposes by simply changing the translation model.",https://aclanthology.org/2021.newsum-1.7,Association for Computational Linguistics,2021,November,Proceedings of the Third Workshop on New Frontiers in Summarization,"Arana-Catania, Miguel  and
Procter, Rob  and
He, Yulan  and
Liakata, Maria",Evaluation of Abstractive Summarisation Models with Machine Translation in Deliberative Processes,10.18653/v1/2021.newsum-1.7,newsum,1201
2020.nli-1.1,"['Question Answering (QA)', 'Model Architectures', 'Learning Paradigms']","['Knowledge Base QA', 'Supervised Learning']",,"Knowledge-based question answering KB-QA has long focused on simple questions that can be answered from a single knowledge source, a manually curated or an automatically extracted KB. In this work, we look at answering complex questions which often require combining information from multiple sources. We present a novel KB-QA system, MULTIQUE, which can map a complex question to a complex query pattern using a sequence of simple queries each targeted at a specific KB. It finds simple queries using a neural-network based model capable of collective inference over textual relations in extracted KB and ontological relations in curated KB. Experiments show that our proposed system outperforms previous KB-QA systems on benchmark datasets, ComplexWebQuestions and WebQuestionsSP.",https://aclanthology.org/2020.nli-1.1,Association for Computational Linguistics,2020,July,Proceedings of the First Workshop on Natural Language Interfaces,"Bhutani, Nikita  and
Zheng, Xinyi  and
Qian, Kun  and
Li, Yunyao  and
Jagadish, H.",Answering Complex Questions by Combining Information from Curated and Extracted Knowledge Bases,10.18653/v1/2020.nli-1.1,nli,26
D19-6130,"['Data Management and Generation', 'Multilingual NLP', 'Information Extraction', 'Knowledge Representation and Reasoning', 'Learning Paradigms', 'Low-resource Languages', 'Cross-lingual Application']","['Data Preparation', 'Relation Extraction']",,"Although the vast majority of knowledge bases KBs are heavily biased towards English, Wikipedias do cover very different topics in different languages. Exploiting this, we introduce a new multilingual dataset X-WikiRE, framing relation extraction as a multilingual machine reading problem. We show that by leveraging this resource it is possible to robustly transfer models cross-lingually and that multilingual support significantly improves zero-shot relation extraction, enabling the population of low-resourced KBs from their well-populated counterparts.",https://aclanthology.org/D19-6130,Association for Computational Linguistics,2019,November,Proceedings of the 2nd Workshop on Deep Learning Approaches for Low-Resource NLP (DeepLo 2019),"Abdou, Mostafa  and
Sas, Cezar  and
Aralikatte, Rahul  and
Augenstein, Isabelle  and
S{\o}gaard, Anders","X-WikiRE: A Large, Multilingual Resource for Relation Extraction as Machine Comprehension",10.18653/v1/D19-6130,D19,472
Q16-1005,"['Discourse Analysis', 'Domain-specific NLP', 'Classification Applications', 'Data Management and Generation']","['Data Preparation', 'NLP for News and Media']",['NLP for Social Media'],"This paper presents an empirical study of linguistic formality. We perform an analysis of humans' perceptions of formality in four different genres. These findings are used to develop a statistical model for predicting formality, which is evaluated under different feature settings and genres. We apply our model to an investigation of formality in online discussion forums, and present findings consistent with theories of formality and linguistic coordination.",https://aclanthology.org/Q16-1005,MIT Press,2016,,,"Pavlick, Ellie  and
Tetreault, Joel",An Empirical Analysis of Formality in Online Communication,10.1162/tacl_a_00083,Q16,443
2021.naacl-main.163,"['Text Preprocessing', 'Low-resource Languages', 'Model Architectures']",['Text Segmentation'],,"Conditional Random Field CRF based neural models are among the most performant methods for solving sequence labeling problems. Despite its great success, CRF has the shortcoming of occasionally generating illegal sequences of tags, e.g. sequences containing an ""I-"" tag immediately after an ""O"" tag, which is forbidden by the underlying BIO tagging scheme. In this work, we propose Masked Conditional Random Field MCRF, an easy to implement variant of CRF that impose restrictions on candidate paths during both training and decoding phases. We show that the proposed method thoroughly resolves this issue and brings consistent improvement over existing CRF-based models with near zero additional cost.",https://aclanthology.org/2021.naacl-main.163,Association for Computational Linguistics,2021,June,Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,"Wei, Tianwen  and
Qi, Jianwei  and
He, Shenghuan  and
Sun, Songtao",Masked Conditional Random Fields for Sequence Labeling,10.18653/v1/2021.naacl-main.163,naacl,403
2020.stoc-1.6,['Classification Applications'],['Misinformation Detection'],['Fake Review Detection'],"In this paper, we present a web service platform for disinformation detection in hotel reviews written in English. The platform relies on a hybrid approach of computational stylometry techniques, machine learning and linguistic rules written using COGITO c , Expert System Corp.'s semantic intelligence software thanks to which it is possible to analyze texts and extract all their characteristics. We carried out a research experiment on the Deceptive Opinion Spam corpus, a balanced corpus composed of 1,600 hotel reviews of 20 Chicago hotels split into four datasets: positive truthful, negative truthful, positive deceptive and negative deceptive reviews. We investigated four different classifiers and we detected that Simple Logistic is the most performing algorithm for this type of classification.",https://aclanthology.org/2020.stoc-1.6,European Language Resources Association,2020,May,Proceedings for the First International Workshop on Social Threats in Online Conversations: Understanding and Management,"Pascucci, Antonio  and
Manna, Raffaele  and
Caterino, Ciro  and
Masucci, Vincenzo  and
Monti, Johanna",Is this hotel review truthful or deceptive? A platform for disinformation detection through computational stylometry,,stoc,483
J16-3001,"['Parsing', 'Low-resource Languages', 'Knowledge Representation and Reasoning']",['Syntactic Parsing'],['Dependency Parsing'],"Derivations under different grammar formalisms allow extraction of various dependency structures. Particularly, bilexical deep dependency structures beyond surface tree representation can be derived from linguistic analysis grounded by CCG, LFG, and HPSG. Traditionally, these dependency structures are obtained as a by-product of grammar-guided parsers. In this article, we study the alternative data-driven, transition-based approach, which has achieved great success for tree parsing, to build general dependency graphs. We integrate existing tree parsing techniques and present two new transition systems that can generate arbitrary directed graphs in an incremental manner. Statistical parsers that are competitive in both accuracy and efficiency can be built upon these transition systems. Furthermore, the heterogeneous design of transition systems yields diversity of the corresponding parsing models and thus greatly benefits parser ensemble. Concerning the disambiguation problem, we introduce two new techniques, namely, transition combination and tree approximation, to improve parsing quality. Transition combination makes every action performed by a parser significantly change configurations. Therefore, more distinct features can be extracted for statistical disambiguation. With the same goal of extracting informative features, tree approximation induces tree backbones from dependency graphs and re-uses tree parsing techniques to produce tree-related features. We conduct experiments on CCG-grounded functor-argument analysis, LFG-grounded grammatical relation analysis, and HPSG-grounded semantic dependency analysis for English and Chinese. Experiments demonstrate that data-driven models with appropriate transition systems can produce high-quality deep dependency analysis, comparable to more complex grammar-driven",https://aclanthology.org/J16-3001,MIT Press,2016,September,,"Zhang, Xun  and
Du, Yantao  and
Sun, Weiwei  and
Wan, Xiaojun",Transition-Based Parsing for Deep Dependency Structures,10.1162/COLI_a_00252,J16,977
2020.lr4sshoc-1.4,"['Domain-specific NLP', 'Data Management and Generation', 'Classification Applications', 'Low-resource Languages', 'Ethics']","['NLP for News and Media', 'Data Preparation', 'Hate and Offensive Speech Detection', 'Data Analysis']",['NLP for Social Media'],"We present a replication of a data-driven and linguistically inspired Verbal Aggression analysis framework that was designed to examine Twitter verbal attacks against predefined target groups of interest as an indicator of xenophobic attitudes during the financial crisis in Greece, in particular during the period 2013-2016. The research goal in this paper is to re-examine Verbal Aggression as an indicator of xenophobic attitudes in Greek Twitter three years later, in order to trace possible changes regarding the main t argets, the types and the content of the verbal attacks against the same targets in the post crisis era, given also the ongoing refugee crisis and the political landscape in Greece as it was shaped after the elections in 2019. The results indicate an interesting rearrangement of the main targets of the verbal attacks, while the content and the types of the attacks provide valuable insights about the way these targets are being framed as compared to the respective dominant perceptions and stereotypes about them during the period 2013-2016.",https://aclanthology.org/2020.lr4sshoc-1.4,European Language Resources Association,2020,May,Proceedings of the Workshop about Language Resources for the SSH Cloud,"Pontiki, Maria  and
Gavriilidou, Maria  and
Gkoumas, Dimitris  and
Piperidis, Stelios",Verbal Aggression as an Indicator of Xenophobic Attitudes in Greek Twitter during and after the Financial Crisis,,lr4sshoc,159
2020.mwe-1.11,"['Information Extraction', 'Classification Applications', 'Learning Paradigms', 'Embeddings', 'Multilingual NLP']","['Unsupervised Learning', 'Supervised Learning', 'Word Sense Disambiguation (WSD)']",,"The majority of multiword expressions can be interpreted as figuratively or literally in different contexts which pose challenges in a number of downstream tasks. Most previous work deals with this ambiguity following the observation that MWEs with different usages occur in distinctly different contexts. Following this insight, we explore the usefulness of contextual embeddings by means of both supervised and unsupervised classification. The results show that in the supervised setting, the state-of-the-art can be substantially improved for all expressions in the experiments. The unsupervised classification, similarly, yields very impressive results, comparing favorably to the supervised classifier for the majority of the expressions. We also show that multilingual contextual embeddings can also be employed for this task without leading to any significant loss in performance; hence, the proposed methodology has the potential to be extended to a number of languages.",https://aclanthology.org/2020.mwe-1.11,Association for Computational Linguistics,2020,December,Proceedings of the Joint Workshop on Multiword Expressions and Electronic Lexicons,"Kurfal{\i}, Murathan  and
{\""O}stling, Robert",Disambiguation of Potentially Idiomatic Expressions with Contextual Embeddings,,mwe,1241
2021.humeval-1.2,"['Evaluation Techniques', 'Text Generation']",,,"Human ratings are one of the most prevalent methods to evaluate the performance of natural language processing algorithms. Similarly, it is common to measure the quality of sentences generated by a natural language generation model using human raters. In this paper, we argue for exploring the use of subjective evaluations within the process of training language generation models in a multi-task learning setting. As a case study, we use a crowdauthored dialogue corpus to fine-tune six different language generation models. Two of these models incorporate multi-task learning and use subjective ratings of lines as part of an explicit learning goal. A human evaluation of the generated dialogue lines reveals that utterances generated by the multi-tasking models were subjectively rated as the most typical, most moving the conversation forward, and least offensive. Based on these promising first results, we discuss future research directions for incorporating subjective human evaluations into language model training and to hence keep the human user in the loop during the development process.",https://aclanthology.org/2021.humeval-1.2,Association for Computational Linguistics,2021,April,Proceedings of the Workshop on Human Evaluation of NLP Systems (HumEval),"Nyberg, Jakob  and
Paetzel, Maike  and
Manuvinakurike, Ramesh",Estimating Subjective Crowd-Evaluations as an Additional Objective to Improve Natural Language Generation,10.48550/arxiv.2104.05224,humeval,364
2021.maiworkshop-1.12,"['Question Answering (QA)', 'Model Architectures']","['Visual QA (VQA)', 'Graph Neural Networks (GNNs)']",,"Images are more than a collection of objects or attributes -they represent a web of relationships among interconnected objects. Scene Graph has emerged as a new modality as a structured graphical representation of images. Scene Graph encodes objects as nodes connected via pairwise relations as edges. To support question answering on scene graphs, we propose GraphVQA, a language-guided graph neural network framework that translates and executes a natural language question as multiple iterations of message passing among graph nodes. We explore the design space of GraphVQA framework, and discuss the trade-off of different design choices. Our experiments on GQA dataset show that GraphVQA outperforms the state-of-the-art model by a large margin 88.43% vs. 94.78%",https://aclanthology.org/2021.maiworkshop-1.12,Association for Computational Linguistics,2021,June,Proceedings of the Third Workshop on Multimodal Artificial Intelligence,"Liang, Weixin  and
Jiang, Yanhao  and
Liu, Zixuan",GraghVQA: Language-Guided Graph Neural Networks for Graph-based Visual Question Answering,10.18653/v1/2021.maiworkshop-1.12,maiworkshop,592
2021.konvens-1.4,"['Domain-specific NLP', 'Data Management and Generation', 'Learning Paradigms', 'Classification Applications', 'Model Architectures']","['Transfer Learning', 'Data Preparation', 'Hate and Offensive Speech Detection', 'NLP for News and Media']",,"In this research, we investigate techniques to detect hate speech in movies. We introduce a new dataset collected from the subtitles of six movies, where each utterance is annotated either as hate, offensive or normal. We apply transfer learning techniques of domain adaptation and fine-tuning on existing social media datasets, namely from Twitter and Fox News. We evaluate different representations, i.e., Bag of Words BoW, Bi-directional Long shortterm memory Bi-LSTM, and Bidirectional Encoder Representations from Transformers BERT on 11k movie subtitles. The BERT model obtained the best macro-averaged F1score of 77%. Hence, we show that transfer learning from the social media domain is efficacious in classifying hate and offensive speech in movies through subtitles.",https://aclanthology.org/2021.konvens-1.4,KONVENS 2021 Organizers,2021,6--9 September,Proceedings of the 17th Conference on Natural Language Processing (KONVENS 2021),"von Boguszewski, Niklas  and
Moin, Sana  and
Bhowmick, Anirban  and
Yimam, Seid Muhie  and
Biemann, Chris",How Hateful are Movies? A Study and Prediction on Movie Subtitles,10.48550/arxiv.2108.10724,konvens,506
2021.motra-1.8,"['Evaluation Techniques', 'Data Management and Generation', 'Low-resource Languages']",['Data Preparation'],,"Starting from the assumption that different word alignments of translations represent differing conceptualizations of crosslingual equivalence, we assess the variation of six different alignment methods for English-to-Spanish translated and postedited texts. We develop a word alignment dissimilarity indicator WADI and compare it to traditional segment-based alignment error rate AER. We average the WADI scores over the possible 15 different pairings of the six alignment methods for each source token and correlate the averaged WADI scores with translation process and product measures, including production duration, number of insertions, and word translation entropy. Results reveal modest correlations between WADI and production duration and insertions, as well as a moderate correlation between WADI and word translation entropy. This shows that differences in alignment decisions reflect on variation in translation decisions and demonstrates that aggregate WADI score could be used as a word-level feature to estimate post-editing difficulty.",https://aclanthology.org/2021.motra-1.8,Association for Computational Linguistics,2021,May,Proceedings for the First Workshop on Modelling Translation: Translatology in the Digital Age,"Gilbert, Devin  and
Carl, Michael",Word Alignment Dissimilarity Indicator: Alignment Links as Conceptualizations of a Focused Bilingual Lexicon,,motra,696
2021.bsnlp-1.3,"['Domain-specific NLP', 'Data Management and Generation', 'Classification Applications', 'Low-resource Languages']","['NLP for News and Media', 'Data Preparation', 'Hate and Offensive Speech Detection']",['NLP for Social Media'],"Abusive phenomena are commonplace in language on the web. The scope of recognizing abusive language is broad, covering many behaviours and forms of expression. This work addresses automatic detection of abusive language in Russian. The lexical, grammatical and morphological diversity of Russian language present potential difficulties for this task, which is addressed using a variety of machine learning approaches. We present a dataset and baselines for this task.",https://aclanthology.org/2021.bsnlp-1.3,Association for Computational Linguistics,2021,April,Proceedings of the 8th Workshop on Balto-Slavic Natural Language Processing,"Saitov, Kamil  and
Derczynski, Leon",Abusive Language Recognition in Russian,,bsnlp,17
2021.iwpt-1.15,"['Multilingual NLP', 'Parsing', 'Low-resource Languages']",['Syntactic Parsing'],['Dependency Parsing'],"We describe the second IWPT task on end-toend parsing from raw text to Enhanced Universal Dependencies. We provide details about the evaluation metrics and the datasets used for training and evaluation. We compare the approaches taken by participating teams and discuss the results of the shared task, also in comparison with the first edition of this task.",https://aclanthology.org/2021.iwpt-1.15,Association for Computational Linguistics,2021,August,Proceedings of the 17th International Conference on Parsing Technologies and the IWPT 2021 Shared Task on Parsing into Enhanced Universal Dependencies (IWPT 2021),"Bouma, Gosse  and
Seddah, Djam{\'e}  and
Zeman, Daniel",From Raw Text to Enhanced Universal Dependencies: The Parsing Shared Task at IWPT 2021,10.18653/v1/2021.iwpt-1.15,iwpt,1365
K16-1030,"['Discourse Analysis', 'Classification Applications', 'Model Architectures']",,,"Discourse relations can either be implicit or explicitly expressed by markers, such as 'therefore' and 'but'. How a speaker makes this choice is a question that is not well understood. We propose a psycholinguistic model that predicts whether a speaker will produce an explicit marker given the discourse relation s/he wishes to express. Based on the framework of the Rational Speech Acts model, we quantify the utility of producing a marker based on the information-theoretic measure of surprisal, the cost of production, and a bias to maintain uniform information density throughout the utterance. Experiments based on the Penn Discourse Treebank show that our approach outperforms stateof-the-art approaches, while giving an explanatory account of the speaker's choice.",https://aclanthology.org/K16-1030,Association for Computational Linguistics,2016,August,Proceedings of The 20th {SIGNLL} Conference on Computational Natural Language Learning,"Yung, Frances  and
Duh, Kevin  and
Komura, Taku  and
Matsumoto, Yuji",Modelling the Usage of Discourse Connectives as Rational Speech Acts,10.18653/v1/K16-1030,K16,964
2020.cllrd-1.4,"['Information Extraction', 'Data Management and Generation']","['Data Preparation', 'Anaphora Resolution']",['Annotation Processes'],"Crowdsourcing approaches provide a difficult design challenge for developers. There is a trade-off between the efficiency of the task to be done and the reward given to the user for participating, whether it be altruism, social enhancement, entertainment or money. This paper explores how crowdsourcing and citizen science systems collect data and complete tasks, illustrated by a case study from the online language game-with-a-purpose Phrase Detectives. The game was originally developed to be a constrained interface to prevent player collusion, but subsequently benefited from posthoc analysis of over 76k unconstrained inputs from users. Understanding the interface design and task deconstruction are critical for enabling users to participate in such systems and the paper concludes with a discussion of the idea that social networks can be viewed as form of citizen science platform with both constrained and unconstrained inputs making for a highly complex dataset.",https://aclanthology.org/2020.cllrd-1.4,European Language Resources Association,2020,May,Proceedings of the LREC 2020 Workshop on ``Citizen Linguistics in Language Resource Development'',"Chamberlain, Jon  and
Kruschwitz, Udo  and
Poesio, Massimo",Speaking Outside the Box: Exploring the Benefits of Unconstrained Input in Crowdsourcing and Citizen Science Platforms,,cllrd,729
2021.sigtyp-1.6,"['Multilingual NLP', 'Data Management and Generation', 'Low-resource Languages', 'Cross-lingual Application']",['Data Preparation'],['Annotation Processes'],"FrameNet and the Multilingual FrameNet project have produced multilingual semantic annotations of parallel texts that yield extremely fine-grained typological insights. Moreover, frame semantic annotation of a wide cross-section of languages can provide information on the limits of Frame Semantics Fillmore, 1982 Fillmore, , 1985 . Multilingual semantic annotation offers critical input for research on linguistic diversity and recurrent patterns in computational typology. Drawing on results from FrameNet annotation of parallel texts, this paper proposes frame semantic annotation as a new component to complement the state of the art in computational semantic typology. 1",https://aclanthology.org/2021.sigtyp-1.6,Association for Computational Linguistics,2021,June,Proceedings of the Third Workshop on Computational Typology and Multilingual NLP,"Ellsworth, Michael  and
Baker, Collin  and
Petruck, Miriam R. L.",FrameNet and Typology,10.18653/v1/2021.sigtyp-1.6,sigtyp,1014
2022.spnlp-1.7,['Model Architectures'],['Transformer Models'],,"Transformers' quadratic complexity with respect to the input sequence length has motivated a body of work on efficient sparse approximations to softmax. An alternative path, used by entmax transformers, consists of having built-in exact sparse attention; however this approach still requires quadratic computation. In this paper, we propose Sparsefinder, a simple model trained to identify the sparsity pattern of entmax attention before computing it. We experiment with three variants of our method, based on distances, quantization, and clustering, on two tasks: machine translation attention in the decoder and masked language modeling encoder-only. Our work provides a new angle to study model efficiency by doing extensive analysis of the tradeoff between the sparsity and recall of the predicted attention graph. This allows for detailed comparison between different models along their Pareto curves, important to guide future benchmarks for sparse attention models.",https://aclanthology.org/2022.spnlp-1.7,Association for Computational Linguistics,2022,May,Proceedings of the Sixth Workshop on Structured Prediction for NLP,"Treviso, Marcos  and
G{\'o}is, Ant{\'o}nio  and
Fernandes, Patrick  and
Fonseca, Erick  and
Martins, Andre",Predicting Attention Sparsity in Transformers,10.18653/v1/2022.spnlp-1.7,spnlp,501
2020.osact-1.17,"['Domain-specific NLP', 'Data Management and Generation', 'Classification Applications', 'Low-resource Languages']","['NLP for News and Media', 'Hate and Offensive Speech Detection', 'Data Augmentation']",['NLP for Social Media'],"Social media are pervasive in our life, making it necessary to ensure safe online experiences by detecting and removing offensive and hate speech. In this work, we report our submission to the Offensive Language and hate-speech Detection shared task organized with the 4 th Workshop on Open-Source Arabic Corpora and Processing Tools Arabic OSACT4. We focus on developing purely deep learning systems, without a need for feature engineering. For that purpose, we develop an effective method for automatic data augmentation and show the utility of training both offensive and hate speech models off i.e., by fine-tuning previously trained affective models i.e., sentiment and emotion. Our best models are significantly better than a vanilla BERT model, with 89.60% acc 82.31% macro F1 for hate speech and 95.20% acc 70.51% macro F1 on official TEST data.",https://aclanthology.org/2020.osact-1.17,European Language Resource Association,2020,May,"Proceedings of the 4th Workshop on Open-Source Arabic Corpora and Processing Tools, with a Shared Task on Offensive Language Detection","Elmadany, AbdelRahim  and
Zhang, Chiyu  and
Abdul-Mageed, Muhammad  and
Hashemi, Azadeh",Leveraging Affective Bidirectional Transformers for Offensive Language Detection,10.48550/arxiv.2006.01266,osact,1120
2020.alw-1.12,"['Domain-specific NLP', 'Data Management and Generation', 'Classification Applications', 'Learning Paradigms', 'Model Architectures']","['NLP for News and Media', 'Data Preparation', 'Supervised Learning', 'Hate and Offensive Speech Detection']","['NLP for Social Media', 'Annotation Processes']","Incivility is not only prevalent on online social media platforms, but also has concrete effects on individual users, online groups, the platforms themselves, and the society at large. Given the prevalence and effects of online incivility, and the challenges involved in humanbased incivility detection, it is urgent to develop validated and versatile automatic approaches to identifying uncivil posts and comments. This project advances both a neural, BERT-based classifier as well as a logistic regression classifier to identify uncivil comments. The classifier is trained on a dataset of Reddit posts, which are annotated for incivility, and further expanded using a combination of labeled data from Reddit and Twitter. Our best performing model achieves an F 1 of 0.802 on our Reddit test set. The final model is not only applicable across social media platforms and their distinct data structures, but also computationally versatile, and -as such -ready to be used on vast volumes of online data. All trained models and annotated data are made available to the research community.",https://aclanthology.org/2020.alw-1.12,Association for Computational Linguistics,2020,November,Proceedings of the Fourth Workshop on Online Abuse and Harms,"Davidson, Sam  and
Sun, Qiusi  and
Wojcieszak, Magdalena",Developing a New Classifier for Automated Identification of Incivility in Social Media,10.18653/v1/2020.alw-1.12,alw,46
2020.mmw-1.5,"['Data Management and Generation', 'Knowledge Representation and Reasoning']",,,"Information systems gathering big amounts of resources growing with time containing distinct modalities text, audio, video, images,  GIS  and aggregating content in various ways modular e-learning modules, Web systems presenting cultural artefacts require tools supporting content description. The subject of the description may be the topic and the characteristics of the content expressed by sets of attributes. To describe such resources one can just use some of existing indexing languages like thesauri, classification systems, domain and upper ontologies, terminologies or dictionaries. When appropriate language does not exist, it is necessary to build a new system, which will have to serve both experts who describe resources and non-experts who search through them. The solution presented in this paper used to resource description, allows experts to freely select words and expressions, which are organized in hierarchies of various nature, including that of domain and application character. This is based on the wordnet structure, which introduces a clear order for each of these groups due to its lexical nature. The paper presents two systems where such approach was applied: the Earchaeology.org e-learning content repository in which domain knowledge was integrated to describe content topics and the Hatch system gathering multimodal information about the archaeological site targeted at a wide audience, where application conceptualization was applied to describe the content by a set of attributes.",https://aclanthology.org/2020.mmw-1.5,The European Language Resources Association (ELRA),2020,May,Proceedings of the LREC 2020 Workshop on Multimodal Wordnets (MMW2020),"Marciniak, Jacek",Wordnet As a Backbone of Domain and Application Conceptualizations in Systems with Multimodal Data,,mmw,1075
2020.cogalex-1.8,"['Model Architectures', 'Classification Applications', 'Low-resource Languages']",['Recurrent Neural Networks (RNNs)'],['Long Short-Term Memory (LSTM) Models'],This paper 1 presents a bidirectional transformer based approach for recognising semantic relationships between a pair of words as proposed by CogALex VI 2 shared task in 2020. The system presented here works by employing BERT embeddings of the words and passing the same over tuned neural network to produce a learning model for the pair of words and their relationships. Afterwards the very same model is used for the relationship between unknown words from the test set. CogALex VI 2 provided Subtask 1 as the identification of relationship of three specific categories amongst English pair of words and the presented system opts to work on that. The resulted relationships of the unknown words are analysed here which shows a balanced performance in overall characteristics with some scope for improvement.,https://aclanthology.org/2020.cogalex-1.8,Association for Computational Linguistics,2020,December,Proceedings of the Workshop on the Cognitive Aspects of the Lexicon,"Karmakar, Saurav  and
McCrae, John P.",CogALex-VI Shared Task: Bidirectional Transformer based Identification of Semantic Relations,,cogalex,690
2022.woah-1.5,"['Domain-specific NLP', 'Data Management and Generation', 'Low-resource Languages', 'Classification Applications', 'Model Architectures']","['Data Preparation', 'Hate and Offensive Speech Detection', 'NLP for News and Media']","['NLP for Social Media', 'Annotation Processes']","This paper presents a comprehensive corpus for the study of socially unacceptable language in Dutch. The corpus extends and revise an existing resource with more data and introduces a new annotation dimension for offensive language, making it a unique resource in the Dutch language panorama. Each language phenomenon abusive and offensive language in the corpus has been annotated with a multilayer annotation scheme modelling the explicitness and the targets of the abuse/offence in the message. We have conducted a new set of experiments with different classification algorithms on all annotation dimensions. Monolingual Pre-Trained Language Models prove as the best systems, obtaining a macro-average F1 of 0.828 for binary classification of offensive language, and 0.579 for the targets of offensive messages. Furthermore, the best system obtains a macro-average F1 of 0.637 for distinguishing between abusive and offensive messages.",https://aclanthology.org/2022.woah-1.5,Association for Computational Linguistics,2022,July,Proceedings of the Sixth Workshop on Online Abuse and Harms (WOAH),"Ruitenbeek, Ward  and
Zwart, Victor  and
Van Der Noord, Robin  and
Gnezdilov, Zhenja  and
Caselli, Tommaso",``Zo Grof !'': A Comprehensive Corpus for Offensive and Abusive Language in Dutch,10.18653/v1/2022.woah-1.5,woah,734
O17-1016,"['Low-resource Languages', 'Classification Applications']",['Sentiment Analysis (SA)'],,"Valence shifters are complex linguistic structures that can modify the sentiment orientations of texts. In this paper, the authors concentrate on the study of shifters in Vietnamese texts and a discussion on the distribution of different types of shifters in the hotel reviews is presented. Finally, an approach for extracting the contextual valance shifters is proposed.",https://aclanthology.org/O17-1016,The Association for Computational Linguistics and Chinese Language Processing (ACLCLP),2017,November,Proceedings of the 29th Conference on Computational Linguistics and Speech Processing ({ROCLING} 2017),"Tran, Thien Khai  and
Phan, Tuoi Thi",Toward Contextual Valence Shifters in Vietnamese Reviews,,O17,1049
2020.ccl-1.89,"['Information Extraction', 'Model Architectures']","['Relation Extraction', 'Named Entity Recognition (NER)']",,"Joint entity and relation extraction has received increasing interests recently, due to the capability of utilizing the interactions between both steps. Among existing studies, the Multi-Head Selection MHS framework is efficient in extracting entities and relations simultaneously. However, the method is weak for its limited performance. In this paper, we propose several effective insights to address this problem. First, we propose an entity-specific Relative Position Representation eRPR to allow the model to fully leverage the distance information between entities and context tokens. Second, we introduce an auxiliary Global Relation Classification GRC to enhance the learning of local contextual features. Moreover, we improve the semantic representation by adopting a pre-trained language model BERT as the feature encoder. Finally, these new keypoints are closely integrated with the multi-head selection framework and optimized jointly. Extensive experiments on two benchmark datasets demonstrate that our approach overwhelmingly outperforms previous works in terms of all evaluation metrics, achieving significant improvements for relation F1 by +2.40% on CoNLL04 and +1.90% on ACE05, respectively.",https://aclanthology.org/2020.ccl-1.89,Chinese Information Processing Society of China,2020,October,Proceedings of the 19th Chinese National Conference on Computational Linguistics,"Zhao, Tianyang  and
Yan, Zhao  and
Cao, Yunbo  and
Li, Zhoujun",Entity Relative Position Representation based Multi-head Selection for Joint Entity and Relation Extraction,10.1007/978-3-030-63031-7_14,ccl,480
2022.dlg4nlp-1.2,"['Machine Translation (MT)', 'Low-resource Languages', 'Knowledge Representation and Reasoning', 'Model Architectures']","['Transformer Models', 'Neural MT (NMT)', 'Abstract Meaning Representation (AMR)']",,"Previous studies have shown that the Abstract Meaning Representation AMR can improve Neural Machine Translation NMT. However, there has been little work investigating incorporating AMR graphs into Transformer models. In this work, we propose a novel encoder-decoder architecture which augments the Transformer model with a Heterogeneous Graph Transformer Yao et al., 2020 which encodes source sentence AMR graphs. Experimental results demonstrate the proposed model outperforms the Transformer model and previous non-Transformer based models on two different language pairs in both the high resource setting and low resource setting. Our source code, training corpus and released models are available at https://github.com/ jlab-nlp/amr-nmt.",https://aclanthology.org/2022.dlg4nlp-1.2,Association for Computational Linguistics,2022,July,Proceedings of the 2nd Workshop on Deep Learning on Graphs for Natural Language Processing (DLG4NLP 2022),"Li, Changmao  and
Flanigan, Jeffrey",Improving Neural Machine Translation with the Abstract Meaning Representation by Combining Graph and Sequence Transformers,10.18653/v1/2022.dlg4nlp-1.2,dlg4nlp,1121
2021.reinact-1.5,"['Question Answering (QA)', 'Domain-specific NLP', 'Classification Applications', 'Image and Video Processing']","['Intent Detection', 'Visual QA (VQA)', 'Medical and Clinical NLP']",,"Visual Question Answering VQA systems are increasingly adept at a variety of tasks, and this technology can be used to assist blind and partially sighted people. To do this, the system's responses must not only be accurate, but usable. It is also vital for assistive technologies to be designed with a focus on: 1 privacy, as the camera may capture a user's mail, medication bottles, or other sensitive information; 2 transparency, so that the system's behaviour can be explained and trusted by users; and 3 controllability, to tailor the system for a particular domain or user group. We have therefore extended a conversational VQA framework, called Aye-saac, with these objectives in mind. Specifically, we gave Aye-saac the ability to answer visual questions in the kitchen, a particularly challenging area for visually impaired people. Our system 1 can now answer questions about quantity, positioning, and system confidence in regards to 299 kitchen objects. Questions about the spatial relations between these objects are particularly helpful to visually impaired people, and our system output more usable answers than other state of the art end-to-end VQA systems.",https://aclanthology.org/2021.reinact-1.5,Association for Computational Linguistics,2021,October,Proceedings of the Reasoning and Interaction Conference (ReInAct 2021),"Baker, Katie  and
Parekh, Amit  and
Fabre, Adrien  and
Addlesee, Angus  and
Kruiper, Ruben  and
Lemon, Oliver",The Spoon Is in the Sink: Assisting Visually Impaired People in the Kitchen,,reinact,471
D16-1156,"['Parsing', 'Data Management and Generation', 'Image and Video Processing', 'Learning Paradigms', 'Model Architectures']","['Multimodal Learning', 'Syntactic Parsing', 'Data Preparation']",,"We present an approach to simultaneously perform semantic segmentation and prepositional phrase attachment resolution for captioned images. Some ambiguities in language cannot be resolved without simultaneously reasoning about an associated image. If we consider the sentence ""I shot an elephant in my pajamas"", looking at language alone and not using common sense, it is unclear if it is the person or the elephant wearing the pajamas or both. Our approach produces a diverse set of plausible hypotheses for both semantic segmentation and prepositional phrase attachment resolution that are then jointly reranked to select the most consistent pair. We show that our semantic segmentation and prepositional phrase attachment resolution modules have complementary strengths, and that joint reasoning produces more accurate results than any module operating in isolation. Multiple hypotheses are also shown to be crucial to improved multiple-module reasoning. Our vision and language approach significantly outperforms the Stanford Parser De Marneffe et al., 2006 by 17.91% 28.69% relative and 12.83% 25.28% relative in two different experiments. We also make small improvements over DeepLab-CRF Chen et al., 2015 .",https://aclanthology.org/D16-1156,Association for Computational Linguistics,2016,November,Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing,"Christie, Gordon  and
Laddha, Ankit  and
Agrawal, Aishwarya  and
Antol, Stanislaw  and
Goyal, Yash  and
Kochersberger, Kevin  and
Batra, Dhruv",Resolving Language and Vision Ambiguities Together: Joint Segmentation \& Prepositional Attachment Resolution in Captioned Scenes,10.18653/v1/D16-1156,D16,479
D16-1001,"['Parsing', 'Model Architectures']","['Syntactic Parsing', 'Recurrent Neural Networks (RNNs)']","['Constituency Parsing', 'Long Short-Term Memory (LSTM) Models']","Parsing accuracy using efficient greedy transition systems has improved dramatically in recent years thanks to neural networks. Despite striking results in dependency parsing, however, neural models have not surpassed stateof-the-art approaches in constituency parsing. To remedy this, we introduce a new shiftreduce system whose stack contains merely sentence spans, represented by a bare minimum of LSTM features. We also design the first provably optimal dynamic oracle for constituency parsing, which runs in amortized O1 time, compared to On 3  oracles for standard dependency parsing. Training with this oracle, we achieve the best F 1 scores on both English and French of any parser that does not use reranking or external data.",https://aclanthology.org/D16-1001,Association for Computational Linguistics,2016,November,Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing,"Cross, James  and
Huang, Liang",Span-Based Constituency Parsing with a Structure-Label System and Provably Optimal Dynamic Oracles,10.18653/v1/D16-1001,D16,1324
2022.bionlp-1.1,"['Question Answering (QA)', 'Domain-specific NLP', 'Evaluation Techniques']",['Medical and Clinical NLP'],,"The healthcare domain suffers from the spread of poor quality articles on the Internet. While manual efforts exist to check the quality of online healthcare articles, they are not sufficient to assess all those in circulation. Such quality assessment can be automated as a text classification task, however, explanations for the labels are necessary for the users to trust the model predictions. While current explainable systems tackle explanation generation as summarization, we propose a new approach based on question answering QA that allows us to generate explanations for multiple criteria using a single model. We show that this QA-based approach is competitive with the current state-of-the-art, and complements summarization-based models for explainable quality assessment. We also introduce a human evaluation protocol more appropriate than automatic metrics for the evaluation of explanation generation models. Story #1511 Criterion 1: Does the article adequately discuss the costs of the intervention? Answer: Not Satisfactory Explanation: There was no discussion of cost as there was in the competing AP story. Criterion 2: Does the article adequately quantify the benefits of the treatment/test/product/procedure? Answer: Satisfactory Explanation: The story adequately quantified the benefits seen in the study that led to FDA approval.",https://aclanthology.org/2022.bionlp-1.1,Association for Computational Linguistics,2022,May,Proceedings of the 21st Workshop on Biomedical Language Processing,"Boissonnet, Alodie  and
Saeidi, Marzieh  and
Plachouras, Vassilis  and
Vlachos, Andreas",Explainable Assessment of Healthcare Articles with QA,10.18653/v1/2022.bionlp-1.1,bionlp,1376
D17-2001,['Data Management and Generation'],,,"A new Python API, integrated within the NLTK suite, offers access to the FrameNet 1.7 lexical database. The lexicon structured in terms of frames as well as annotated sentences can be processed programatically, or browsed with human-readable displays via the interactive Python prompt.",https://aclanthology.org/D17-2001,Association for Computational Linguistics,2017,September,Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing: System Demonstrations,"Schneider, Nathan  and
Wooters, Chuck",The NLTK FrameNet API: Designing for Discoverability with a Rich Linguistic Resource,10.18653/v1/D17-2001,D17,94
W16-6624,['Text Generation'],['Data-to-Text Generation'],['Table-to-Text Generation'],"Most of the existing natural language generation NLG techniques employing statistical methods are typically resource and time intensive. On the other hand, handcrafted rulebased and template-based NLG systems typically require significant human/designer efforts. In this paper, we proposed a statistical NLG technique which does not require any semantic relational knowledge and takes much less time to generate output text. The system can be used in those cases where source non-textual data are in the form of tuple in some tabular dataset. We carried out our experiments on the Prodigy-METEO wind forecasting dataset. For the evaluation purpose, we used both human evaluation and automatic evaluation. From the evaluation results we found that the linguistic quality and correctness of the texts generated by the system are better than many existing NLG systems.",https://aclanthology.org/W16-6624,Association for Computational Linguistics,2016,September 5-8,Proceedings of the 9th International Natural Language Generation conference,"Mahapatra, Joy  and
Naskar, Sudip Kumar  and
Bandyopadhyay, Sivaji",Statistical Natural Language Generation from Tabular Non-textual Data,10.18653/v1/W16-6624,W16,114
2022.bea-1.28,"['Ethics', 'Text Generation', 'Domain-specific NLP', 'Dialogue Systems']",['Chatbots'],,"State-of-the-art chatbots for English are now able to hold conversations on virtually any topic e.g. Adiwardana et al., 2020; Roller et al., 2021 . However, existing dialogue systems in the language learning domain still use handcrafted rules and pattern matching, and are much more limited in scope. In this paper, we make an initial foray into adapting opendomain dialogue generation for second language learning. We propose and implement decoding strategies that can adjust the difficulty level of the chatbot according to the learner's needs, without requiring further training of the chatbot. These strategies are then evaluated using judgements from human examiners trained in language education. Our results show that re-ranking candidate outputs is a particularly effective strategy, and performance can be further improved by adding sub-token penalties and filtering.",https://aclanthology.org/2022.bea-1.28,Association for Computational Linguistics,2022,July,Proceedings of the 17th Workshop on Innovative Use of NLP for Building Educational Applications (BEA 2022),"Tyen, Gladys  and
Brenchley, Mark  and
Caines, Andrew  and
Buttery, Paula",Towards an open-domain chatbot for language practice,10.18653/v1/2022.bea-1.28,bea,826
2019.icon-1.11,"['Data Management and Generation', 'Model Architectures', 'Knowledge Representation and Reasoning']","['Recurrent Neural Networks (RNNs)', 'Ontologies', 'Data Analysis']",['Long Short-Term Memory (LSTM) Models'],"Increased internet bandwidth at low cost is leading to the creation of large volumes of unstructured data. This data explosion opens up opportunities for the creation of a variety of data-driven intelligent systems, such as the Semantic Web. Ontologies form one of the most crucial layers of semantic web, and the extraction and enrichment of ontologies given this data explosion becomes an inevitable research problem. In this paper, we survey the literature on semi-automatic and automatic ontology extraction and enrichment and classify them into four broad categories based on the approach. Then, we proceed to narrow down four algorithms from each of these categories, implement and analytically compare them based on parameters like context relevance, efficiency and precision. Lastly, we propose a Long Short Term Memory Networks LSTM based deep learning approach to try and overcome the gaps identified in these approaches.",https://aclanthology.org/2019.icon-1.11,NLP Association of India,2019,December,Proceedings of the 16th International Conference on Natural Language Processing,"Iyer, Vivek  and
Mohan, Lalit  and
Bhatia, Mehar  and
Reddy, Y. Raghu",A Survey on Ontology Enrichment from Text,,icon,1141
S17-2065,"['Domain-specific NLP', 'Classification Applications', 'Model Architectures']","['Humor Detection', 'Recurrent Neural Networks (RNNs)', 'NLP for News and Media']","['NLP for Social Media', 'Long Short-Term Memory (LSTM) Models']","In this paper we present a deep-learning system that competed at SemEval-2017 Task 6 ""#HashtagWars: Learning a Sense of Humor"". We participated in Subtask A, in which the goal was, given two Twitter messages, to identify which one is funnier. We propose a Siamese architecture with bidirectional Long Short-Term Memory LSTM networks, augmented with an attention mechanism. Our system works on the token-level, leveraging word embeddings trained on a big collection of unlabeled Twitter messages. We ranked 2 nd in 7 teams. A post-completion improvement of our model, achieves state-of-theart results on #HashtagWars dataset.",https://aclanthology.org/S17-2065,Association for Computational Linguistics,2017,August,Proceedings of the 11th International Workshop on Semantic Evaluation ({S}em{E}val-2017),"Baziotis, Christos  and
Pelekis, Nikos  and
Doulkeridis, Christos",DataStories at SemEval-2017 Task 6: Siamese LSTM with Attention for Humorous Text Comparison,10.18653/v1/S17-2065,S17,1093
2019.ccnlg-1.5,"['Domain-specific NLP', 'Model Architectures', 'Low-resource Languages', 'Text Generation']","['NLP for News and Media', 'Story Generation']",['Narrative Plot in Storytelling'],"An advertisement video is a narrative based on a short story, and it is among the various narrative techniques used for constructing a plot. Although research into advertisement copies and explanations has been recently conducted, only a few researchers, including us, have discussed the automated generation of advertising stories and plots in detail. This study proposes the prototype of an advertising plot generation system based on a comprehensive analysis of advertisement videos.",https://aclanthology.org/2019.ccnlg-1.5,Association for Computational Linguistics,2019,29 October--3 November,Proceedings of the 4th Workshop on Computational Creativity in Language Generation,"Ono, Juumpei  and
Sasaki, Atsushi  and
Ogata, Takashi",Advertising Plot Generation System Based on Comprehensive Narrative Analysis of Advertisement Videos,,ccnlg,43
2020.wmt-1.82,"['Data Management and Generation', 'Error Detection and Correction', 'Learning Paradigms', 'Low-resource Languages', 'Cross-lingual Application', 'Model Architectures']","['Transformer Models', 'Data Augmentation']",,"This paper describes POSTECH-ETRI's submission to WMT2020 for the shared task on automatic post-editing APE for 2 language pairs: English-German En-De and English-Chinese En-Zh. We propose APE systems based on a cross-lingual language model, which jointly adopts translation language modeling TLM and masked language modeling MLM training objectives in the pre-training stage; the APE models then utilize jointly learned language representations between the source language and the target language. In addition, we created 19 million new sythetic triplets as additional training data for our final ensemble model. According to experimental results on the WMT2020 APE development data set, our models showed an improvement over the baseline by TER of 3.58 and a BLEU score of +5.3 for the En-De subtask; and TER of 5.29 and a BLEU score of +7.32 for the En-Zh subtask.",https://aclanthology.org/2020.wmt-1.82,Association for Computational Linguistics,2020,November,Proceedings of the Fifth Conference on Machine Translation,"Lee, Jihyung  and
Lee, WonKee  and
Shin, Jaehun  and
Jung, Baikjin  and
Kim, Young-Kil  and
Lee, Jong-Hyeok",POSTECH-ETRI's Submission to the WMT2020 APE Shared Task: Automatic Post-Editing with Cross-lingual Language Model,,wmt,219
2021.gem-1.12,"['Text Generation', 'Dialogue Systems', 'Learning Paradigms', 'Classification Applications', 'Model Architectures']",['Data-to-Text Generation'],,"We explore the use of self-training and acceptability classifiers with pre-trained models for natural language generation in structure-totext settings using three GEM datasets E2E, WebNLG-en, Schema-Guided Dialog. With the Schema-Guided Dialog dataset, we also experiment with including multiple turns of context in the input. We find that self-training with reconstruction matching along with acceptability classifier filtering can improve semantic correctness, though gains are limited in the full-data setting. With context-conditioning, we find that including multiple turns in the context encourages the model to align with the user's word and phrasing choices as well as to generate more self-consistent responses. In future versions of the GEM challenge, we encourage the inclusion of few-shot tracks to encourage research on data efficiency.",https://aclanthology.org/2021.gem-1.12,Association for Computational Linguistics,2021,August,"Proceedings of the 1st Workshop on Natural Language Generation, Evaluation, and Metrics (GEM 2021)","Bakshi, Shreyan  and
Batra, Soumya  and
Heidari, Peyman  and
Arun, Ankit  and
Jain, Shashank  and
White, Michael","Structure-to-Text Generation with Self-Training, Acceptability Classifiers and Context-Conditioning for the GEM Shared Task",10.18653/v1/2021.gem-1.12,gem,362
2021.clpsych-1.10,"['Topic Modeling', 'Domain-specific NLP', 'Classification Applications', 'Model Architectures']","['Latent Dirichlet Allocation (LDA)', 'NLP for News and Media', 'Recurrent Neural Networks (RNNs)', 'Medical and Clinical NLP']","['NLP for Mental Health', 'NLP for Social Media', 'Long Short-Term Memory (LSTM) Models']","We describe our system for identifying users at-risk for suicide based on their tweets developed for the CLPsych 2021 Shared Task. Based on research in mental health studies linking self-harm tendencies with suicide, in our system, we attempt to characterize selfharm aspects expressed in user tweets over a period of time. To this end, we design SHT M , a Self-Harm Topic Model that combines Latent Dirichlet Allocation with a selfharm dictionary for modeling daily tweets of users. Next, differences in moods and topics over time are captured as features to train a deep learning model for suicide prediction.",https://aclanthology.org/2021.clpsych-1.10,Association for Computational Linguistics,2021,June,Proceedings of the Seventh Workshop on Computational Linguistics and Clinical Psychology: Improving Access,"Gollapalli, Sujatha Das  and
Zagatti, Guilherme Augusto  and
Ng, See-Kiong",Suicide Risk Prediction by Tracking Self-Harm Aspects in Tweets: NUS-IDS at the CLPsych 2021 Shared Task,10.18653/v1/2021.clpsych-1.10,clpsych,625
2019.lilt-18.2,"['Data Management and Generation', 'Low-resource Languages', 'Knowledge Representation and Reasoning']","['Data Preparation', 'Abstract Meaning Representation (AMR)']",['Annotation Processes'],"Meaning Representation AMR is a meaning representation framework in which the meaning of a full sentence is represented as a single-rooted, acyclic, directed graph. In this article, we describe an on-going project to build a Chinese AMR CAMR corpus, which currently includes 10,149 sentences from the newsgroup and weblog portion of the Chinese TreeBank CTB. We describe the annotation specifications for the CAMR corpus, which follow the annotation principles of English AMR but make adaptations where needed to accommodate the linguistic facts of Chinese. The CAMR specifications also include a systematic treatment of sentence-internal discourse relations. One significant change we have made to the AMR annotation methodology is the inclusion of the alignment between word tokens in the sentence and the concepts/relations in the CAMR annotation to make it easier for automatic parsers to model the correspondence between a sentence and its meaning representation. We develop an annotation tool for CAMR, and the inter-agreement as measured by the Smatch score between the two annotators is 0.83, indicating reliable annotation. We also present some quantitative analysis of the CAMR corpus. 46.71% of the AMRs of the sentences are non-tree graphs. Moreover, the AMR of 88.95% of the sentences has concepts inferred from the context of the sentence but do not correspond to a specific word 2 / LiLT volume 18, issue 1 June 2019 or phrase in a sentence, and the average number of such inferred concepts per sentence is 2.88. These statistics will have to be taken into account when developing automatic Chinese AMR parsers.",https://aclanthology.org/2019.lilt-18.2,CSLI Publications,2019,July,"Linguistic Issues in Language Technology, Volume 18, 2019 - Exploiting Parsed Corpora: Applications in Research, Pedagogy, and Processing","Li, Bin  and
Wen, Yuan  and
Song, Li  and
Qu, Weiguang  and
Xue, Nianwen",Building a Chinese AMR Bank with Concept and Relation Alignments,10.33011/lilt.v18i.1429,lilt,1261
2020.sltu-1.31,"['Data Management and Generation', 'Low-resource Languages']",['Data Analysis'],,"The present paper aims at providing a first study of lenition-and fortition-type phenomena in coda position in Romanian, a language that can be considered as less-resourced. Our data show that there are two contexts for devoicing in Romanian: before a voiceless obstruent, which means that there is regressive voicelessness assimilation in the language, and before pause, which means that there is a tendency towards final devoicing proper. The data also show that non-canonical voicing is an instance of voicing assimilation, as it is observed mainly before voiced consonants voiced obstruents and sonorants alike. Two conclusions can be drawn from our analyses. First, from a phonetic point of view, the two devoicing phenomena exhibit the same behavior regarding place of articulation of the coda, while voicing assimilation displays the reverse tendency. In particular, alveolars, which tend to devoice the most, also voice the least. Second, the two assimilation processes have similarities that could distinguish them from final devoicing as such. Final devoicing seems to be sensitive to speech style and gender of the speaker, while assimilation processes do not. This may indicate that the two kinds of processes are phonologized at two different degrees in the language, final devoicing being more sociolinguistically stigmatized than assimilation.",https://aclanthology.org/2020.sltu-1.31,European Language Resources association,2020,May,Proceedings of the 1st Joint Workshop on Spoken Language Technologies for Under-resourced languages (SLTU) and Collaboration and Computing for Under-Resourced Languages (CCURL),"Hutin, Mathilde  and
Niculescu, Oana  and
Vasilescu, Ioana  and
Lamel, Lori  and
Adda-Decker, Martine",Lenition and Fortition of Stop Codas in Romanian,,sltu,600
2022.wassa-1.14,"['Domain-specific NLP', 'Multilingual NLP', 'Learning Paradigms', 'Low-resource Languages', 'Classification Applications', 'Model Architectures']","['Transformer Models', 'NLP for News and Media']",['NLP for Social Media'],"Masked language models MLMs are pretrained with a denoising objective that is in a mismatch with the objective of downstream fine-tuning. We propose pragmatic masking and surrogate fine-tuning as two complementing strategies that exploit social cues to drive pre-trained representations toward a broad set of concepts useful for a wide class of social meaning tasks. We test our models on 15 different Twitter datasets for social meaning detection. Our methods achieve 2.34% F 1 over a competitive baseline, while outperforming domain-specific language models pre-trained on large datasets. Our methods also excel in few-shot learning: with only 5% of training data severely few-shot, our methods enable an impressive 68.54% average F 1 . The methods are also language agnostic, as we show in a zero-shot setting involving six datasets from three different languages. 1",https://aclanthology.org/2022.wassa-1.14,Association for Computational Linguistics,2022,May,"Proceedings of the 12th Workshop on Computational Approaches to Subjectivity, Sentiment {\&} Social Media Analysis","Zhang, Chiyu  and
Abdul-Mageed, Muhammad",Improving Social Meaning Detection with Pragmatic Masking and Surrogate Fine-Tuning,10.18653/v1/2022.wassa-1.14,wassa,739
2021.semspace-1.3,"['Data Management and Generation', 'Knowledge Representation and Reasoning']",['Data Preparation'],,"While the DisCoCat model Coecke et al., 2010 has been proved a valuable tool for studying compositional aspects of language at the level of semantics, its strong dependency on pregroup grammars poses important restrictions: first, it prevents large-scale experimentation due to the absence of a pregroup parser; and second, it limits the expressibility of the model to context-free grammars. In this paper we solve these problems by reformulating DisCoCat as a passage from Combinatory Categorial Grammar CCG to a category of semantics. We start by showing that standard categorial grammars can be expressed as a biclosed category, where all rules emerge as currying/uncurrying the identity; we then proceed to model permutation-inducing rules by exploiting the symmetry of the compact closed category encoding the word meaning. We provide a proof of concept for our method, converting ""Alice in Wonderland"" into DisCoCat form, a corpus that we make available to the community. man, 1987, 1996. We achieve this by encoding CCG as a biclosed category, where all standard order-preserving rules of the grammar find a natural translation into biclosed diagrams. CCG rules whose purpose is to relax word ordering and allow cross-serial dependencies are encoded as special morphisms. We then define a closed monoidal functor from the biclosed category freely generated over a set of atomic types, a set of words, and the set of arrows encoding the special rules of the grammar to a compact-closed category. We show that since the category of the semantics is symmetric, the special rules that allow word permutation can be encoded efficiently using the mechanism of ""swapping the wires"". As we will see in Section 3, while in the past there were other attempts to represent CCG in DisCoCat using similar methods Grefenstette, 2013, this is the first time that a complete and theoretically sound treatment is provided and implemented in practice. By presenting a version of DisCoCat which is no longer bound to pregroups, we achieve two important outcomes. First, since CCG is shown to be a mildly context-sensitive grammar Vijay-Shanker and Weir, 1994, we increase the generative power of DisCoCat accordingly; and second, due to the availability of many robust CCG parsers that can be used for obtaining the derivations of sentences in large datasets -see, for example Clark and Curran, 2007 -we make large-scale DisCoCat experiments on sentences of arbitrary grammatical structures possible for the first time. In fact, we demonstrate the applicability of the proposed method by using a standard CCG parser Yoshikawa et al., 2017 to obtain derivations for all sentences in the book ""Alice's Adventures in Wonderland"", which we then convert to DisCoCat diagrams based on the theory described in this paper. This resource -the first in its kind -is now available to the DisCoCat community for facilitating research and experiments. Furthermore, a web-based tool that allows the conversion of any sentence into a DisCoCat diagram is available at CQC's QNLP website. 2",https://aclanthology.org/2021.semspace-1.3,Association for Computational Linguistics,2021,June,"Proceedings of the 2021 Workshop on Semantic Spaces at the Intersection of NLP, Physics, and Cognitive Science (SemSpace)","Yeung, Richie  and
Kartsaklis, Dimitri",A CCG-Based Version of the DisCoCat Framework,10.48550/arxiv.2105.07720,semspace,1251
2021.emnlp-main.566,"['Information Retrieval', 'Learning Paradigms', 'Data Management and Generation', 'Question Answering (QA)']","['Unsupervised Learning', 'Data Preparation', 'Open-Domain QA', 'Data Augmentation']",,"In this work, we introduce back-training, an alternative to self-training for unsupervised domain adaptation UDA from source to target domain. While self-training generates synthetic training data where natural inputs are aligned with noisy outputs, back-training results in natural outputs aligned with noisy inputs. This significantly reduces the gap between the target domain and synthetic data distribution, and reduces model overfitting to the source domain. We run UDA experiments on question generation and passage retrieval from the Natural Questions domain to machine learning and biomedical domains. We find that back-training vastly outperforms selftraining by a mean improvement of 7.8 BLEU-4 points on generation, and 17.6% top-20 retrieval accuracy across both domains. We further propose consistency filters to remove low-quality synthetic data before training. We also release a new domain-adaptation dataset-MLQuestions containing 35K unaligned questions, 50K unaligned passages, and 3K aligned question-passage pairs.",https://aclanthology.org/2021.emnlp-main.566,Association for Computational Linguistics,2021,November,Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,"Kulshreshtha, Devang  and
Belfer, Robert  and
Serban, Iulian Vlad  and
Reddy, Siva",Back-Training excels Self-Training at Unsupervised Domain Adaptation of Question Generation and Passage Retrieval,10.18653/v1/2021.emnlp-main.566,emnlp,901
2021.wanlp-1.35,"['Domain-specific NLP', 'Low-resource Languages', 'Classification Applications']",['NLP for News and Media'],['NLP for Social Media'],"This paper presents our approach to address the EACL WANLP-2021 Shared Task 1: Nuanced Arabic Dialect Identification NADI. The task is aimed at developing a system that identifies the geographical locationcountry/province from where an Arabic tweet in the form of modern standard Arabic or dialect comes from. We solve the task in two parts. The first part involves pre-processing the provided dataset by cleaning, adding and segmenting various parts of the text. This is followed by carrying out experiments with different versions of two Transformer based models, AraBERT and AraELECTRA. Our final approach achieved macro F1-scores of 0.216, 0.235, 0.054, and 0.043 in the four subtasks, and we were ranked second in MSA identification subtasks and fourth in DA identification subtasks.",https://aclanthology.org/2021.wanlp-1.35,Association for Computational Linguistics,2021,April,Proceedings of the Sixth Arabic Natural Language Processing Workshop,"Wadhawan, Anshul",Dialect Identification in Nuanced Arabic Tweets Using Farasa Segmentation and AraBERT,10.48550/arxiv.2102.09749,wanlp,1188
2020.signlang-1.4,"['Data Management and Generation', 'Low-resource Languages']",['Data Analysis'],,"Lexicostatistics is the main method used in previous work measuring linguistic distances between sign languages. As a method, it disregards any possible structural/grammatical similarity, instead focusing exclusively on lexical items, but it is time consuming as it requires some comparable phonological coding i.e. form description as well as concept matching i.e. meaning description of signs across the sign languages to be compared. In this paper, we present a novel approach for measuring lexical similarity across any two sign languages using the Global Signbank platform, a lexical database of uniformly coded signs. The method involves a feature-by-feature comparison of all matched phonological features. This method can be used in two distinct ways: 1 automatically comparing the amount of lexical overlap between two sign languages with a more detailed feature-description than previous lexicostatistical methods; 2 finding exact form-matches across languages that are either matched or mismatched in meaning i.e. true or false friends. We show the feasability of this method by comparing three languages datasets in Global Signbank, and are currently expanding both the size of these three as well as the total number of datasets.",https://aclanthology.org/2020.signlang-1.4,European Language Resources Association (ELRA),2020,May,"Proceedings of the LREC2020 9th Workshop on the Representation and Processing of Sign Languages: Sign Language Resources in the Service of the Language Community, Technological Challenges and Application Perspectives","B{\""o}rstell, Carl  and
Crasborn, Onno  and
Whynot, Lori",Measuring Lexical Similarity across Sign Languages in Global Signbank,,signlang,1331
2021.nllp-1.16,"['Domain-specific NLP', 'Information Extraction', 'Data Management and Generation']","['NLP for the Legal Domain', 'Data Preparation']",,"Domain-specific terminology is ubiquitous in legal documents. Despite potential utility in populating glossaries and ontologies or as arguments in information extraction and document classification tasks, there has been limited work done for legal terminology extraction. This paper describes some work to remedy this omission. In the described research, we make some modifications to the Termolator, a high-performing, open-source terminology extractor which has been tuned to scientific articles. Our changes are designed to improve the Termolator's results when applied to United States Supreme Court decisions. Unaltered and using the recommended settings, the original Termolator provides a list of terminology with a precision of 23% and 25% for the categories of economic activity development set and criminal procedures test set respectively. These were the most frequently occurring broad issues in Washington University in St. Louis Database corpus, a database of Supreme Court decisions that have been manually classified by topic. Our contribution includes the introduction of several legal domain-specific filtration steps and changes to the web search relevance score; each incrementally improved precision culminating in a combined precision of 63% and 65%. We also evaluated the baseline version of the Termolator on more specific subcategories and on broad issues with fewer cases. Our results show that a narrowed scope as well as smaller document numbers significantly lower the precision. In both cases, the modifications to the Termolator improve precision.",https://aclanthology.org/2021.nllp-1.16,Association for Computational Linguistics,2021,November,Proceedings of the Natural Legal Language Processing Workshop 2021,"Pham, Nhi  and
Pham, Lachlan  and
Meyers, Adam L.",Legal Terminology Extraction with the Termolator,10.18653/v1/2021.nllp-1.16,nllp,1432
2020.aacl-srw.12,"['Domain-specific NLP', 'Data Management and Generation', 'Classification Applications', 'Low-resource Languages']","['Data Preparation', 'Sentiment Analysis (SA)']","['Annotation Processes', 'Aspect-Based SA (ABSA)']","Along with the rise of people generated content on social sites, sentiment analysis has gained more importance. Aspect Based Sentiment Analysis ABSA is a task of identifying the sentiment at aspect level. It has more importance than sentiment analysis from commercial point of view. To the best of our knowledge, there is very few work on ABSA in Urdu language. Recent work on ABSA has limitations. Only predefined aspects are identified in a specific domain. So our focus is on the creation and evaluation of dataset for ABSA in Urdu language which will support multiple aspects. This dataset will provide a baseline evaluation for ABSA systems.",https://aclanthology.org/2020.aacl-srw.12,Association for Computational Linguistics,2020,December,Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing: Student Research Workshop,"Rani, Sadaf  and
Anwar, Muhammad Waqas",Resource Creation and Evaluation of Aspect Based Sentiment Analysis in Urdu,10.18653/v1/2020.aacl-srw.12,aacl,230
D17-1179,"['Multilingual NLP', 'Learning Paradigms', 'Low-resource Languages', 'Model Architectures']",['Supervised Learning'],,"In this paper we propose an end-toend neural CRF autoencoder NCRF-AE model for semi-supervised learning of sequential structured prediction problems. Our NCRF-AE consists of two parts: an encoder which is a CRF model enhanced by deep neural networks, and a decoder which is a generative model trying to reconstruct the input. Our model has a unified structure with different loss functions for labeled and unlabeled data with shared parameters. We developed a variation of the EM algorithm for optimizing both the encoder and the decoder simultaneously by decoupling their parameters. Our experimental results over the Part-of-Speech POS tagging task on eight different languages, show that the NCRF-AE model can outperform competitive systems in both supervised and semi-supervised scenarios.",https://aclanthology.org/D17-1179,Association for Computational Linguistics,2017,September,Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing,"Zhang, Xiao  and
Jiang, Yong  and
Peng, Hao  and
Tu, Kewei  and
Goldwasser, Dan",Semi-supervised Structured Prediction with Neural CRF Autoencoder,10.18653/v1/D17-1179,D17,684
2020.sigdial-1.25,"['Data Management and Generation', 'Model Architectures', 'Discourse Analysis']","['Data Preparation', 'Data Analysis']",['Annotation Processes'],"During an interaction the tendency of speakers to change their speech production to make it more similar to their interlocutor's speech is called convergence. Convergence had been studied due to its relevance for cognitive models of communication as well as for dialogue system adaptation to the user. Convergence effects have been established on controlled data sets while tracking its dynamics on generic corpora has provided positive but more contrasted outcomes. We propose to enrich large conversational corpora with dialogue acts information and to use these acts as filters to create subsets of homogeneous conversational activity. Those subsets allow a more precise comparison between speakers' speech variables. We compare convergence on acoustic variables Energy, Pitch and Speech Rate measured on raw data sets, with human and automatically data sets labelled with dialog acts type. We found that such filtering helps in observing convergence suggesting that future studies should consider such high level dialogue activity types and the related NLP techniques as important tools for analyzing conversational interpersonal dynamics.",https://aclanthology.org/2020.sigdial-1.25,Association for Computational Linguistics,2020,July,Proceedings of the 21th Annual Meeting of the Special Interest Group on Discourse and Dialogue,"Fuscone, Simone  and
Favre, Benoit  and
Pr{\'e}vot, Laurent",Filtering conversations through dialogue acts labels for improving corpus-based convergence studies,10.18653/v1/2020.sigdial-1.25,sigdial,192
2022.suki-1.7,"['Question Answering (QA)', 'Learning Paradigms', 'Knowledge Representation and Reasoning']","['Knowledge Base QA', 'Transfer Learning', 'Multihop Reasoning']",,"Multi-hop question answering QA combines multiple pieces of evidence to search for the correct answer. Reasoning over a text corpus TextQA and/or a knowledge base KBQA has been extensively studied and led to distinct system architectures. However, knowledge transfer between such two QA systems has been under-explored. Research questions like what knowledge is transferred or whether the transferred knowledge can help answer over one source using another one, are yet to be answered. In this paper, therefore, we study the knowledge transfer of multi-hop reasoning between structured and unstructured sources. We first propose a unified QA framework named SIMULTQA to enable knowledge transfer and bridge the distinct supervisions from KB and text sources. Then, we conduct extensive analyses to explore how knowledge is transferred by leveraging the pre-training and fine-tuning paradigm. We focus on the low-resource finetuning to show that pre-training SIMULTQA on one source can substantially improve its performance on the other source. More fine-grained analyses on transfer behaviors reveal the types of transferred knowledge and transfer patterns. We conclude with insights into how to construct better QA datasets and systems to exploit knowledge transfer for future work. 1",https://aclanthology.org/2022.suki-1.7,Association for Computational Linguistics,2022,July,Proceedings of the Workshop on Structured and Unstructured Knowledge Integration (SUKI),"Mo, Lingbo  and
Wang, Zhen  and
Zhao, Jie  and
Sun, Huan",Knowledge Transfer between Structured and Unstructured Sources for Complex Question Answering,10.18653/v1/2022.suki-1.7,suki,546
2021.teachingnlp-1.10,['Parsing'],['Syntactic Parsing'],['Dependency Parsing'],"Dependency parsing is increasingly the popular parsing formalism in practice. This assignment provides a practice exercise in implementing the shift-reduce dependency parser of Chen and Manning 2014. This parser is a two-layer feed-forward neural network, which students implement in PyTorch, providing practice in developing deep learning models and exposure to developing parser models.",https://aclanthology.org/2021.teachingnlp-1.10,Association for Computational Linguistics,2021,June,Proceedings of the Fifth Workshop on Teaching NLP,"Jurgens, David",Learning PyTorch Through A Neural Dependency Parsing Exercise,10.18653/v1/2021.teachingnlp-1.10,teachingnlp,1472
2021.ecnlp-1.20,"['Automatic Text Summarization', 'Domain-specific NLP', 'Data Management and Generation', 'Information Extraction', 'Model Architectures']","['Abstractive Text Summarization', 'Data Preparation']",['Annotation Processes'],"We improve customer experience and gain their trust when their issues are resolved rapidly with less friction. Existing work has focused on reducing the overall case resolution time by binning a case into predefined categories and routing it to the desired support engineer. However, the actions taken by the engineer during case analysis and resolution are altogether ignored, even though it forms the bulk of the case resolution time. In this work, we propose two systems that enable support engineers to resolve cases faster. The first, a guidance extraction model, that mines historical cases and provides technical guidance phrases to the support engineers. These phrases can then be used to educate the customer or to obtain critical information needed to resolve the case and thus minimize the number of correspondences between the engineer and customer. The second, a summarization model that creates an abstractive summary of a case to provide better context to the support engineer. Through quantitative evaluation we obtain an F1 score of 0.64 on the guidance extraction model and a BertScore F1 of 0.55 on the summarization model.",https://aclanthology.org/2021.ecnlp-1.20,Association for Computational Linguistics,2021,August,Proceedings of The 4th Workshop on e-Commerce and NLP,"Bannihatti Kumar, Vinayshekhar  and
Yarramsetty, Mohan  and
Sun, Sharon  and
Goel, Anukul",SupportNet: Neural Networks for Summary Generation and Key Segment Extraction from Technical Support Tickets,10.18653/v1/2021.ecnlp-1.20,ecnlp,1378
2020.rail-1.3,"['Domain-specific NLP', 'Data Management and Generation', 'Classification Applications', 'Low-resource Languages']","['NLP for News and Media', 'Data Preparation', 'Data Augmentation']",,"The recent advances in Natural Language Processing have only been a boon for well represented languages, negating research in lesser known global languages. This is in part due to the availability of curated data and research resources. One of the current challenges concerning low-resourced languages are clear guidelines on the collection, curation and preparation of datasets for different use-cases. In this work, we take on the task of creating two datasets that are focused on news headlines i.e short text for Setswana and Sepedi and the creation of a news topic classification task from these datasets. In this study, we document our work, propose baselines for classification, and investigate an approach on data augmentation better suited to low-resourced languages in order to improve the performance of the classifiers.",https://aclanthology.org/2020.rail-1.3,European Language Resources Association (ELRA),2020,May,Proceedings of the first workshop on Resources for African Indigenous Languages,"Marivate, Vukosi  and
Sefara, Tshephisho  and
Chabalala, Vongani  and
Makhaya, Keamogetswe  and
Mokgonyane, Tumisho  and
Mokoena, Rethabile  and
Modupe, Abiodun","Investigating an Approach for Low Resource Language Dataset Creation, Curation and Classification: Setswana and Sepedi",,rail,1
W19-3213,"['Domain-specific NLP', 'Classification Applications', 'Model Architectures']","['Medical and Clinical NLP', 'Recurrent Neural Networks (RNNs)', 'NLP for News and Media']","['NLP for Social Media', 'Long Short-Term Memory (LSTM) Models']",This paper describes a system for automatically classifying adverse effects mentions in tweets developed for the task 1 at Social Media Mining for Health Applications SMM4H Shared Task 2019. We have developed a system based on LSTM neural networks inspired by the excellent results obtained by deep learning classifiers in the last edition of this task. The network is trained along with Twitter GloVe pre-trained word embeddings.,https://aclanthology.org/W19-3213,Association for Computational Linguistics,2019,August,Proceedings of the Fourth Social Media Mining for Health Applications ({\#}SMM4H) Workshop {\&} Shared Task,"Cortes-Tejada, Javier  and
Martinez-Romo, Juan  and
Araujo, Lourdes",NLP@UNED at SMM4H 2019: Neural Networks Applied to Automatic Classifications of Adverse Effects Mentions in Tweets,10.18653/v1/W19-3213,W19,280
2022.bigscience-1.2,"['Audio Generation and Processing', 'Low-resource Languages']",['Automatic Speech Recognition (ASR)'],,"This papers aims at improving spoken language modeling LM using very large amount of automatically transcribed speech. We leverage the INA French National Audiovisual Institute 1  collection and obtain 19GB of text after applying ASR on 350,000 hours of diverse TV shows. From this, spoken language models are trained either by fine-tuning an existing LM FlauBERT 2  or through training a LM from scratch. The new models FlauBERT-Oral are shared with the community 3 and are evaluated not only in terms of word prediction accuracy but also for two downstream tasks: classification of TV shows and syntactic parsing of speech. Experimental results show that FlauBERT-Oral is better than its initial FlauBERT version demonstrating that, despite its inherent noisy nature, ASR-Generated text can be useful to improve spoken language modeling.",https://aclanthology.org/2022.bigscience-1.2,Association for Computational Linguistics,2022,May,Proceedings of BigScience Episode {\#}5 -- Workshop on Challenges {\&} Perspectives in Creating Large Language Models,"Herv{\'e}, Nicolas  and
Pelloin, Valentin  and
Favre, Benoit  and
Dary, Franck  and
Laurent, Antoine  and
Meignier, Sylvain  and
Besacier, Laurent",Using ASR-Generated Text for Spoken Language Modeling,10.18653/v1/2022.bigscience-1.2,bigscience,169
2021.udw-1.8,"['Biases in NLP', 'Parsing', 'Low-resource Languages', 'Data Management and Generation']","['Data Augmentation', 'Syntactic Parsing']",['Dependency Parsing'],"Many downstream applications are using dependency trees, and are thus relying on dependency parsers producing correct, or at least consistent, output. However, dependency parsers are trained using machine learning, and are therefore susceptible to unwanted inconsistencies due to biases in the training data. This paper explores the effects of such biases in four languages -English, Swedish, Russian, and Ukrainian -though an experiment where we study the effect of replacing numerals in sentences. We show that such seemingly insignificant changes in the input can cause large differences in the output, and suggest that data augmentation can remedy the problems.",https://aclanthology.org/2021.udw-1.8,Association for Computational Linguistics,2021,December,"Proceedings of the Fifth Workshop on Universal Dependencies (UDW, SyntaxFest 2021)","Kalpakchi, Dmytro  and
Boye, Johan",Minor changes make a difference: a case study on the consistency of UD-based dependency parsers,,udw,202
W17-1207,"['Machine Translation (MT)', 'Low-resource Languages']","['Statistical MT (SMT)', 'Neural MT (NMT)', 'Rule-based MT (RBMT)']",,"Catalan and Spanish are two related languages given that both derive from Latin. They share similarities in several linguistic levels including morphology, syntax and semantics. This makes them particularly interesting for the MT task. Given the recent appearance and popularity of neural MT, this paper analyzes the performance of this new approach compared to the well-established rule-based and phrase-based MT systems. Experiments are reported on a large database of 180 million words. Results, in terms of standard automatic measures, show that neural MT clearly outperforms the rule-based and phrase-based MT system on in-domain test set, but it is worst in the out-of-domain test set. A naive system combination specially works for the latter. In-domain manual analysis shows that neural MT tends to improve both adequacy and fluency, for example, by being able to generate more natural translations instead of literal ones, choosing to the adequate target word when the source word has several translations and improving gender agreement. However, out-of-domain manual analysis shows how neural MT is more affected by unknown words or contexts.",https://aclanthology.org/W17-1207,Association for Computational Linguistics,2017,April,"Proceedings of the Fourth Workshop on {NLP} for Similar Languages, Varieties and Dialects ({V}ar{D}ial)","Costa-juss{\`a}, Marta R.","Why Catalan-Spanish Neural Machine Translation? Analysis, comparison and combination with standard Rule and Phrase-based technologies",10.18653/v1/W17-1207,W17,250
D17-1015,"['Model Architectures', 'Data Management and Generation', 'Image and Video Processing']",['Data Preparation'],,"We present a model for locating regions in space based on natural language descriptions. Starting with a 3D scene and a sentence, our model is able to associate words in the sentence with regions in the scene, interpret relations such as on top of or next to, and finally locate the region described in the sentence. All components form a single neural network that is trained end-to-end without prior knowledge of object segmentation. To evaluate our model, we construct and release a new dataset consisting of Minecraft scenes with crowdsourced natural language descriptions. We achieve a 32% relative error reduction compared to a strong neural baseline.",https://aclanthology.org/D17-1015,Association for Computational Linguistics,2017,September,Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing,"Kitaev, Nikita  and
Klein, Dan",Where is Misty? Interpreting Spatial Descriptors by Modeling Regions in Space,10.18653/v1/D17-1015,D17,730
2020.bucc-1.7,"['Domain-specific NLP', 'Data Management and Generation', 'Low-resource Languages']","['Medical and Clinical NLP', 'Data Analysis']",['Biomedical NLP'],"This paper describes and evaluates three methods for reducing the research space for parallel sentences in monolingual comparable corpora. Basically, when searching for parallel sentences between two comparable documents, all the possible sentence pairs between the documents have to be considered, which introduces a great degree of imbalance between parallel pairs and non-parallel pairs. This is a problem because, even with a highly performing algorithm, a lot of noise will be present in the extracted results, thus introducing a need for an extensive and costly manual check phase. We propose to study how we can drastically reduce the number of sentence pairs that have to be fed to a classifier so that the results can be manually handled. We work on a manually annotated subset obtained from a French comparable corpus.",https://aclanthology.org/2020.bucc-1.7,European Language Resources Association,2020,May,Proceedings of the 13th Workshop on Building and Using Comparable Corpora,"Cardon, R{\'e}mi  and
Grabar, Natalia",Reducing the Search Space for Parallel Sentences in Comparable Corpora,,bucc,1489
J18-4012,"['Embeddings', 'Data Management and Generation', 'Learning Paradigms', 'Classification Applications', 'Model Architectures']","['Data Preparation', 'Adversarial Learning', 'Unsupervised Learning', 'Word Embeddings', 'Supervised Learning']",['Annotation Processes'],"Participants in an asynchronous conversation e.g., forum, e-mail interact with each other at different times, performing certain communicative acts, called speech acts e.g., question, request. In this article, we propose a hybrid approach to speech act recognition in asynchronous conversations. Our approach works in two main steps: a long short-term memory recurrent neural network LSTM-RNN first encodes each sentence separately into a task-specific distributed representation, and this is then used in a conditional random field CRF model to capture the conversational dependencies between sentences. The LSTM-RNN model uses pretrained word embeddings learned from a large conversational corpus and is trained to classify sentences into speech act types. The CRF model can consider arbitrary graph structures to model conversational dependencies in an asynchronous conversation. In addition, to mitigate the problem of limited annotated data in the asynchronous domains, we adapt the LSTM-RNN model to learn from synchronous conversations e.g., meetings, using domain adversarial training of neural networks. Empirical evaluation shows the effectiveness of our approach over existing ones: i LSTM-RNNs provide better task-specific representations, ii conversational word embeddings benefit the LSTM-RNNs more than the off-the-shelf ones, iii adversarial training gives better domain-invariant representations, and iv the global CRF model improves over local models.",https://aclanthology.org/J18-4012,MIT Press,2018,December,,"Joty, Shafiq  and
Mohiuddin, Tasnim",Modeling Speech Acts in Asynchronous Conversations: A Neural-CRF Approach,10.1162/coli_a_00339,J18,994
2020.textgraphs-1.3,"['Data Management and Generation', 'Information Extraction', 'Model Architectures']","['Data Preparation', 'Named Entity Recognition (NER)', 'Graph Neural Networks (GNNs)', 'Recurrent Neural Networks (RNNs)']",['Long Short-Term Memory (LSTM) Models'],"Named entity recognition NER from visual documents, such as invoices, receipts or business cards, is a critical task for visual document understanding. Most classical approaches use a sequence-based model typically BiLSTM-CRF framework without considering document structure. Recent work on graph-based model using graph convolutional networks to encode visual and textual features have achieved promising performance on the task. However, few attempts take geometry information of text segments text in bounding box in visual documents into account. Meanwhile, existing methods do not consider that related text segments which need to be merged to form a complete entity in many real-world situations. In this paper, we present GraphNEMR, a graph-based model that uses graph convolutional networks to jointly merge text segments and recognize named entities. By incorporating geometry information from visual documents into our model, richer 2D context information is generated to improve document representations. To merge text segments, we introduce a novel mechanism that captures both geometry information as well as semantic information based on pre-trained language model. Experimental results show that the proposed GraphNEMR model outperforms both sequence-based and graph-based SOTA methods significantly.",https://aclanthology.org/2020.textgraphs-1.3,Association for Computational Linguistics,2020,December,Proceedings of the Graph-based Methods for Natural Language Processing (TextGraphs),"Luo, Chuwei  and
Wang, Yongpan  and
Zheng, Qi  and
Li, Liangchen  and
Gao, Feiyu  and
Zhang, Shiyu",Merge and Recognize: A Geometry and 2D Context Aware Graph Model for Named Entity Recognition from Visual Documents,10.18653/v1/2020.textgraphs-1.3,textgraphs,1469
2021.acl-long.460,"['Embeddings', 'Classification Applications']",['Hate and Offensive Speech Detection'],,"There is content such as hate speech, offensive, toxic or aggressive documents, which are perceived differently by their consumers. They are commonly identified using classifiers solely based on textual content that generalize pre-agreed meanings of difficult problems. Such models provide the same results for each user, which leads to high misclassification rate observable especially for contentious, aggressive documents. Both document controversy and user nonconformity require new solutions. Therefore, we propose novel personalized approaches that respect individual beliefs expressed by either user conformity-based measures or various embeddings of their previous text annotations. We found that only a few annotations of most controversial documents are enough for all our personalization methods to significantly outperform classic, generalized solutions. The more controversial the content, the greater the gain. The personalized solutions may be used to efficiently filter unwanted aggressive content in the way adjusted to a given person.",https://aclanthology.org/2021.acl-long.460,Association for Computational Linguistics,2021,August,Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),"Kanclerz, Kamil  and
Figas, Alicja  and
Gruza, Marcin  and
Kajdanowicz, Tomasz  and
Kocon, Jan  and
Puchalska, Daria  and
Kazienko, Przemyslaw",Controversy and Conformity: from Generalized to Personalized Aggressiveness Detection,10.18653/v1/2021.acl-long.460,acl,799
2021.starsem-1.10,"['Question Answering (QA)', 'Learning Paradigms', 'Data Management and Generation']","['Transfer Learning', 'Data Preparation']",['Annotation Processes'],"Recent question answering and machine reading benchmarks frequently reduce the task to one of pinpointing spans within a certain text passage that answers the given question. Typically, these systems are not required to actually understand the text on a deeper level that allows for more complex reasoning on the information contained. We introduce a new dataset called BiQuAD that requires deeper comprehension in order to answer questions in both extractive and deductive fashion. The dataset consist of 4, 190 closed-domain texts and a total of 99, 149 question-answer pairs. The texts are synthetically generated soccer match reports that verbalize the main events of each match. All texts are accompanied by a structured Datalog program that represents a logical model of its information. We show that state-of-the-art QA models do not perform well on the challenging long form contexts and reasoning requirements posed by the dataset. In particular, transformer based state-of-theart models achieve F 1 -scores of only 39.0. We demonstrate how these synthetic datasets align structured knowledge with natural text and aid model introspection when approaching complex text understanding.",https://aclanthology.org/2021.starsem-1.10,Association for Computational Linguistics,2021,August,Proceedings of *SEM 2021: The Tenth Joint Conference on Lexical and Computational Semantics,"Grimm, Frank  and
Cimiano, Philipp",BiQuAD: Towards QA based on deeper text understanding,10.18653/v1/2021.starsem-1.10,starsem,875
K19-1077,"['Automatic Text Summarization', 'Model Architectures']",['Abstractive Text Summarization'],,"Various Seq2Seq learning models designed for machine translation were applied for abstractive summarization task recently. Despite these models provide high ROUGE scores, they are limited to generate comprehensive summaries with a high level of abstraction due to its degenerated attention distribution. We introduce Diverse Convolutional Seq2Seq ModelDivCNN Seq2Seq using Determinantal Point Processes methodsMicro DPPs and Macro DPPs to produce attention distribution considering both quality and diversity. Without breaking the end to end architecture, Di-vCNN Seq2Seq achieves a higher level of comprehensiveness compared to vanilla models and strong baselines. All the reproducible codes and datasets are available online 1 .",https://aclanthology.org/K19-1077,Association for Computational Linguistics,2019,November,Proceedings of the 23rd Conference on Computational Natural Language Learning (CoNLL),"Li, Lei  and
Liu, Wei  and
Litvak, Marina  and
Vanetik, Natalia  and
Huang, Zuying",In Conclusion Not Repetition: Comprehensive Abstractive Summarization with Diversified Attention Based on Determinantal Point Processes,10.18653/v1/K19-1077,K19,1367
C16-1083,"['Ethics', 'Biases in NLP', 'Domain-specific NLP', 'Data Management and Generation', 'Classification Applications']","['Gender Bias', 'Data Preparation']",['Annotation Processes'],"University students in the United States are routinely asked to provide feedback on the quality of the instruction they have received. Such feedback is widely used by university administrators to evaluate teaching ability, despite growing evidence that students assign lower numerical scores to women and people of color, regardless of the actual quality of instruction. In this paper, we analyze students' written comments on faculty evaluation forms spanning eight years and five STEM disciplines in order to determine whether open-ended comments reflect these same biases. First, we apply sentiment analysis techniques to the corpus of comments to determine the overall affect of each comment. We then use this information, in combination with other features, to explore whether there is bias in how students describe their instructors. We show that while the gender of the evaluated instructor does not seem to affect students' expressed level of overall satisfaction with their instruction, it does strongly influence the language that they use to describe their instructors and their experience in class.",https://aclanthology.org/C16-1083,The COLING 2016 Organizing Committee,2016,December,"Proceedings of {COLING} 2016, the 26th International Conference on Computational Linguistics: Technical Papers","Terkik, Andamlak  and
Prud{'}hommeaux, Emily  and
Ovesdotter Alm, Cecilia  and
Homan, Christopher  and
Franklin, Scott",Analyzing Gender Bias in Student Evaluations,,C16,845
2020.lrec-1.606,"['Domain-specific NLP', 'Data Management and Generation', 'Classification Applications', 'Low-resource Languages']","['NLP for News and Media', 'Data Preparation', 'Stance Detection']",['NLP for Social Media'],"Entity framing is the selection of aspects of an entity to promote a particular viewpoint towards that entity. We investigate entity framing of political figures through the use of names and titles in German online discourse, enhancing current research in entity framing through titling and naming that concentrates on English only. We collect tweets that mention prominent German politicians and annotate them for stance. We find that the formality of naming in these tweets correlates positively with their stance. This confirms sociolinguistic observations that naming and titling can have a status-indicating function and suggests that this function is dominant in German tweets mentioning political figures. We also find that this status-indicating function is much weaker in tweets from users that are politically left-leaning than in tweets by right-leaning users. This is in line with observations from moral psychology that left-leaning and right-leaning users assign different importance to maintaining social hierarchies.",https://aclanthology.org/2020.lrec-1.606,European Language Resources Association,2020,May,Proceedings of the 12th Language Resources and Evaluation Conference,"van den Berg, Esther  and
Korfhage, Katharina  and
Ruppenhofer, Josef  and
Wiegand, Michael  and
Markert, Katja",Doctor Who? Framing Through Names and Titles in German,,lrec,79
2021.nodalida-main.44,"['Data Management and Generation', 'Error Detection and Correction', 'Low-resource Languages']",['Data Augmentation'],,"We perform neural machine translation of sentence fragments in order to create large amounts of training data for English grammatical error correction. Our method aims at simulating mistakes made by second language learners, and produces a wider range of non-native style language in comparison to state-of-the-art synthetic data creation methods. In addition to purely grammatical errors, our approach generates other types of errors, such as lexical errors. We perform grammatical error correction experiments using neural sequence-to-sequence models, and carry out quantitative and qualitative evaluation. A model trained on data created using our proposed method is shown to outperform a baseline model on test data with a high proportion of errors.",https://aclanthology.org/2021.nodalida-main.44,"Link{\""o}ping University Electronic Press, Sweden",2021,May 31--2 June,Proceedings of the 23rd Nordic Conference on Computational Linguistics (NoDaLiDa),"Sj{\""o}blom, Eetu  and
Creutz, Mathias  and
Vahtola, Teemu",Grammatical Error Generation Based on Translated Fragments,10.48550/arxiv.2104.09933,nodalida,670
2020.coling-main.288,"['Model Architectures', 'Classification Applications']","['Emotion Detection', 'Multilabel Text Classification']",,"Emotion lexicons have been shown effective for emotion classification Baziotis et al., 2018 . Previous studies handle emotion lexicon construction and emotion classification separately. In this paper, we propose an emotional network EmNet to jointly learn sentence emotions and construct emotion lexicons which are dynamically adapted to a given context. The dynamic emotion lexicons are useful for handling words with multiple emotions based on different context, which can effectively improve the classification accuracy. We validate the approach on two representative architectures -LSTM and BERT, demonstrating its superiority on identifying emotions in English tweets. Our model outperforms several approaches proposed in previous studies and achieves new state-of-the-art on the benchmark Twitter dataset.",https://aclanthology.org/2020.coling-main.288,International Committee on Computational Linguistics,2020,December,Proceedings of the 28th International Conference on Computational Linguistics,"Zhou, Deyu  and
Wu, Shuangzhi  and
Wang, Qing  and
Xie, Jun  and
Tu, Zhaopeng  and
Li, Mu",Emotion Classification by Jointly Learning to Lexiconize and Classify,10.18653/v1/2020.coling-main.288,coling,1214
K18-1024,"['Text Generation', 'Low-resource Languages', 'Model Architectures']",['Poetry Generation'],,"As a precious part of the human cultural heritage, Chinese poetry has influenced people for generations. Automatic poetry composition is a challenge for AI. In recent years, significant progress has been made in this area benefiting from the development of neural networks. However, the coherence in meaning, theme or even artistic conception for a generated poem as a whole still remains a big problem. In this paper, we propose a novel Salient-Clue mechanism for Chinese poetry generation. Different from previous work which tried to exploit all the context information, our model selects the most salient characters automatically from each so-far generated line to gradually form a salient clue, which is utilized to guide successive poem generation process so as to eliminate interruptions and improve coherence. Besides, our model can be flexibly extended to control the generated poem in different aspects, for example, poetry style, which further enhances the coherence. Experimental results show that our model is very effective, outperforming three strong baselines.",https://aclanthology.org/K18-1024,Association for Computational Linguistics,2018,October,Proceedings of the 22nd Conference on Computational Natural Language Learning,"Yi, Xiaoyuan  and
Li, Ruoyu  and
Sun, Maosong",Chinese Poetry Generation with a Salient-Clue Mechanism,10.18653/v1/K18-1024,K18,381
U18-1006,"['Domain-specific NLP', 'Information Extraction', 'Data Management and Generation']","['NLP for Finance', 'Data Preparation']",['Annotation Processes'],"Business documents encode a wealth of information in a format tailored to human consumption -i.e. aesthetically disbursed natural language text, graphics and tables. We address the task of extracting key fields e.g. the amount due on an invoice from a wide-variety of potentially unseen document formats. In contrast to traditional template driven extraction systems, we introduce a content-driven machinelearning approach which is both robust to noise and generalises to unseen document formats. In a comparison of our approach with alternative invoice extraction systems, we observe an absolute accuracy gain of 20% across compared fields, and a 25%-94% reduction in extraction latency.",https://aclanthology.org/U18-1006,,2018,December,Proceedings of the Australasian Language Technology Association Workshop 2018,"Holt, Xavier  and
Chisholm, Andrew",Extracting structured data from invoices,,U18,496
Q18-1029,"['Machine Translation (MT)', 'Model Architectures']","['Neural MT (NMT)', 'Recurrent Neural Networks (RNNs)']",,"Existing neural machine translation NMT models generally translate sentences in isolation, missing the opportunity to take advantage of document-level information. In this work, we propose to augment NMT models with a very light-weight cache-like memory network, which stores recent hidden representations as translation history. The probability distribution over generated words is updated online depending on the translation history retrieved from the memory, endowing NMT models with the capability to dynamically adapt over time. Experiments on multiple domains with different topics and styles show the effectiveness of the proposed approach with negligible impact on the computational cost.",https://aclanthology.org/Q18-1029,MIT Press,2018,,,"Tu, Zhaopeng  and
Liu, Yang  and
Shi, Shuming  and
Zhang, Tong",Learning to Remember Translation History with a Continuous Cache,10.1162/tacl_a_00029,Q18,710
2022.bionlp-1.26,"['Evaluation Techniques', 'Classification Applications']",,,"It is commonly claimed that inter-annotator agreement IAA is the ceiling of machine learning ML performance, i.e., that the agreement between an ML system's predictions and an annotator can not be higher than the agreement between two annotators. Although Boguslav and Cohen 2017 showed that this claim is falsified by many real-world ML systems, the claim has persisted. As a complement to this real-world evidence, we conducted a comprehensive set of simulations, and show that an ML model can outperform IAA even if and especially if annotators are noisy and differ in their underlying classification functions, as long as the ML model is reasonably well-specified. Although the latter condition has long been elusive, leading ML models to underperform IAA, we anticipate that this condition will be increasingly met in the era of big data and deep learning. Our work has implications for 1 maximizing the value of machine learning, 2 adherence to ethical standards in computing, and 3 economical use of annotated resources, which is paramount in settings where annotation is especially expensive, like biomedical natural language processing.",https://aclanthology.org/2022.bionlp-1.26,Association for Computational Linguistics,2022,May,Proceedings of the 21st Workshop on Biomedical Language Processing,"Richie, Russell  and
Grover, Sachin  and
Tsui, Fuchiang (Rich)",Inter-annotator agreement is not the ceiling of machine learning performance: Evidence from a comprehensive set of simulations,10.18653/v1/2022.bionlp-1.26,bionlp,1377
2021.inlg-1.2,"['Text Generation', 'Data Management and Generation', 'Knowledge Representation and Reasoning', 'Model Architectures']",['Data Preparation'],,"Recent developments in natural language generation NLG have bolstered arguments in favor of re-introducing explicit coding of discourse relations in the input to neural models. In the Methodius corpus, a meaning representation MR is hierarchically structured and includes discourse relations. Meanwhile pre-trained language models have been shown to implicitly encode rich linguistic knowledge which provides an excellent resource for NLG. By virtue of synthesizing these lines of research, we conduct extensive experiments on the benefits of using pre-trained models and discourse relation information in MRs, focusing on the improvement of discourse coherence and correctness. We redesign the Methodius corpus; we also construct another Methodius corpus in which MRs are not hierarchically structured but flat. We report experiments on different versions of the corpora, which probe when, where, and how pre-trained models benefit from MRs with discourse relation information in them. We conclude that discourse relations significantly improve NLG when data is limited.",https://aclanthology.org/2021.inlg-1.2,Association for Computational Linguistics,2021,August,Proceedings of the 14th International Conference on Natural Language Generation,"Maskharashvili, Aleksandre  and
Stevens-Guille, Symon  and
Li, Xintong  and
White, Michael",Neural Methodius Revisited: Do Discourse Relations Help with Pre-Trained Models Too?,10.18653/v1/2021.inlg-1.2,inlg,1273
W16-5202,['Data Management and Generation'],,,"The US National Science Foundation NSF SI 2 -funded LAPPS Grid project has developed an open-source platform for enabling complex analyses while hiding complexities associated with underlying infrastructure, that can be accessed through a web interface, deployed on any Unix system, or run from the cloud. It provides sophisticated tool integration and history capabilities, a workflow system for building automated multi-step analyses, state-of-the-art evaluation capabilities, and facilities for sharing and publishing analyses. This paper describes the current facilities available in the LAPPS Grid and outlines the project's ongoing activities to enhance the framework.",https://aclanthology.org/W16-5202,The COLING 2016 Organizing Committee,2016,December,Proceedings of the Third International Workshop on Worldwide Language Service Infrastructure and Second Workshop on Open Infrastructures and Analysis Frameworks for Human Language Technologies ({WLSI}/{OIAF}4{HLT}2016),"Ide, Nancy  and
Suderman, Keith  and
Nyberg, Eric  and
Pustejovsky, James  and
Verhagen, Marc",LAPPS/Galaxy: Current State and Next Steps,,W16,1347
2021.alvr-1.4,"['Biases in NLP', 'Evaluation Techniques', 'Learning Paradigms', 'Image and Video Processing']","['Multimodal Learning', 'Unsupervised Learning']",,"Phrase grounding PG is a multimodal task that grounds language in images. PG systems are evaluated on well-known benchmarks, using Intersection over Union IoU as evaluation metric. This work highlights a disconcerting bias in the evaluation of grounded plural phrases, which arises from representing sets of objects as a union box covering all component bounding boxes, in conjunction with the IoU metric. We detect, analyze and quantify an evaluation bias in the grounding of plural phrases and define a novel metric, c-IoU, based on a union box's component boxes. We experimentally show that our new metric greatly alleviates this bias and recommend using it for fairer evaluation of plural phrases in PG tasks.",https://aclanthology.org/2021.alvr-1.4,Association for Computational Linguistics,2021,June,Proceedings of the Second Workshop on Advances in Language and Vision Research,"Suter, Julia  and
Parcalabescu, Letitia  and
Frank, Anette",Grounding Plural Phrases: Countering Evaluation Biases by Individuation,10.18653/v1/2021.alvr-1.4,alvr,151
2021.nlp4if-1.7,"['Domain-specific NLP', 'Classification Applications', 'Data Management and Generation']","['Misinformation Detection', 'Data Preparation', 'NLP for News and Media']",['Fake News Detection'],"In this paper, we explore the construction of natural language explanations for news claims, with the goal of assisting fact-checking and news evaluation applications. We experiment with two methods: 1 an extractive method based on Biased TextRank -a resource-effective unsupervised graph-based algorithm for content extraction; and 2 an abstractive method based on the GPT-2 language model. We perform comparative evaluations on two misinformation datasets in the political and health news domains, and find that the extractive method shows the most promise.",https://aclanthology.org/2021.nlp4if-1.7,Association for Computational Linguistics,2021,June,"Proceedings of the Fourth Workshop on NLP for Internet Freedom: Censorship, Disinformation, and Propaganda","Kazemi, Ashkan  and
Li, Zehua  and
P{\'e}rez-Rosas, Ver{\'o}nica  and
Mihalcea, Rada",Extractive and Abstractive Explanations for Fact-Checking and Evaluation of News,10.18653/v1/2021.nlp4if-1.7,nlp4if,1402
2022.lchange-1.14,"['Language Change Analysis', 'Domain-specific NLP', 'Topic Modeling', 'Data Management and Generation', 'Learning Paradigms']","['Medical and Clinical NLP', 'Data Preparation', 'Semantic Change Analysis', 'Unsupervised Learning', 'NLP for News and Media']",['NLP for Social Media'],"This paper explores lexical meaning changes in a new dataset, which includes tweets from before and after the COVID-related lockdown in April 2020. We use this dataset to evaluate traditional and more recent unsupervised approaches to lexical semantic change that make use of contextualized word representations based on the BERT neural language model to obtain representations of word usages. We argue that previous models that encode local representations of words cannot capture global context shifts such as the context shift of face masks since the pandemic outbreak. We experiment with neural topic models to track context shifts of words. We show that this approach can reveal textual associations of words that go beyond their lexical meaning representation. We discuss future work and how to proceed capturing the pragmatic aspect of meaning change as opposed to lexical semantic change.",https://aclanthology.org/2022.lchange-1.14,Association for Computational Linguistics,2022,May,Proceedings of the 3rd Workshop on Computational Approaches to Historical Language Change,"Kellert, Olga  and
Mahmud Uz Zaman, Md",Using neural topic models to track context shifts of words: a case study of COVID-related terms before and after the lockdown in April 2020,10.18653/v1/2022.lchange-1.14,lchange,1182
2021.naacl-main.430,"['Evaluation Techniques', 'Text Generation', 'Dialogue Systems', 'Classification Applications']",,,"Side effects during neural network tuning are typically measured by overall accuracy changes. However, we find that even with similar overall accuracy, existing tuning methods result in non-negligible instance-wise side effects. Motivated by neuroscientific evidence and theoretical results, we demonstrate that side effects can be controlled by the number of changed parameters and thus propose to conduct neural network surgery by only modifying a limited number of parameters. Neural network surgery can be realized using diverse techniques, and we investigate three lines of methods. Experimental results on representative tuning problems validate the effectiveness of the surgery approach. The dynamic selecting method achieves the best overall performance that not only satisfies the tuning goal but also induces fewer instance-wise side effects by changing only 10 5 of the parameters.",https://aclanthology.org/2021.naacl-main.430,Association for Computational Linguistics,2021,June,Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,"Zhang, Zhiyuan  and
Ren, Xuancheng  and
Su, Qi  and
Sun, Xu  and
He, Bin",Neural Network Surgery: Injecting Data Patterns into Pre-trained Models with Minimal Instance-wise Side Effects,10.18653/v1/2021.naacl-main.430,naacl,268
2020.calcs-1.1,"['Domain-specific NLP', 'Data Management and Generation', 'Information Extraction', 'Classification Applications', 'Low-resource Languages']","['NLP for News and Media', 'Data Preparation']",['Annotation Processes'],"The extraction of anglicisms lexical borrowings from English is relevant both for lexicographic purposes and for NLP downstream tasks. We introduce a corpus of European Spanish newspaper headlines annotated with anglicisms and a baseline model for anglicism extraction. In this paper we present: 1 a corpus of 21,570 newspaper headlines written in European Spanish annotated with emergent anglicisms and 2 a conditional random field baseline model with handcrafted features for anglicism extraction. We present the newspaper headlines corpus, describe the annotation tagset and guidelines and introduce a CRF model that can serve as baseline for the task of detecting anglicisms. The presented work is a first step towards the creation of an anglicism extractor for Spanish newswire.",https://aclanthology.org/2020.calcs-1.1,European Language Resources Association,2020,May,Proceedings of the The 4th Workshop on Computational Approaches to Code Switching,"Alvarez-Mellado, Elena",An Annotated Corpus of Emerging Anglicisms in Spanish Newspaper Headlines,10.48550/arxiv.2004.02929,calcs,1417
2020.findings-emnlp.295,"['Data Management and Generation', 'Error Detection and Correction', 'Audio Generation and Processing']","['Data Preparation', 'Automatic Speech Recognition (ASR)']",,"Natural language processing of conversational speech requires the availability of high-quality transcripts. In this paper, we express our skepticism towards the recent reports of very low Word Error Rates WERs achieved by modern Automatic Speech Recognition ASR systems on benchmark datasets. We outline several problems with popular benchmarks and compare three state-of-the-art commercial ASR systems on an internal dataset of real-life spontaneous human conversations and HUB'05 public benchmark. We show that WERs are significantly higher than the best reported results. We formulate a set of guidelines which may aid in the creation of real-life, multi-domain datasets with high quality annotations for training and testing of robust ASR systems.",https://aclanthology.org/2020.findings-emnlp.295,Association for Computational Linguistics,2020,November,Findings of the Association for Computational Linguistics: EMNLP 2020,"Szyma{\'n}ski, Piotr  and
{\.Z}elasko, Piotr  and
Morzy, Mikolaj  and
Szymczak, Adrian  and
{\.Z}y{\l}a-Hoppe, Marzena  and
Banaszczak, Joanna  and
Augustyniak, Lukasz  and
Mizgajski, Jan  and
Carmiel, Yishay",WER we are and WER we think we are,10.18653/v1/2020.findings-emnlp.295,findings,170
2020.socialnlp-1.7,"['Domain-specific NLP', 'Information Extraction', 'Low-resource Languages', 'Model Architectures', 'Text Preprocessing']","['NLP for News and Media', 'Named Entity Recognition (NER)', 'Text Segmentation']","['Word Segmentation', 'NLP for Social Media']","Chinese word segmentation is necessary to provide word-level information for Chinese named entity recognition NER systems. However, segmentation error propagation is a challenge for Chinese NER while processing colloquial data like social media text. In this paper, we propose a model UIcwsNN that specializes in identifying entities from Chinese social media text, especially by leveraging uncertain information of word segmentation. Such ambiguous information contains all the potential segmentation states of a sentence that provides a channel for the model to infer deep word-level characteristics. We propose a trilogy i.e., Candidate Position Embedding  Position Selective Attention  Adaptive Word Convolution to encode uncertain word segmentation information and acquire appropriate word-level representation. Experimental results on the social media corpus show that our model alleviates the segmentation error cascading trouble effectively, and achieves a significant performance improvement of 2% over previous state-of-the-art methods.",https://aclanthology.org/2020.socialnlp-1.7,Association for Computational Linguistics,2020,July,Proceedings of the Eighth International Workshop on Natural Language Processing for Social Media,"Jia, Shengbin  and
Ding, Ling  and
Chen, Xiaojun  and
E, Shijia  and
Xiang, Yang",Incorporating Uncertain Segmentation Information into Chinese NER for Social Media Text,10.18653/v1/2020.socialnlp-1.7,socialnlp,1341
D18-1162,['Parsing'],['Syntactic Parsing'],['Constituency Parsing'],"We introduce a method to reduce constituent parsing to sequence labeling. For each word w t , it generates a label that encodes: 1 the number of ancestors in the tree that the words w t and w t+1 have in common, and 2 the nonterminal symbol at the lowest common ancestor. We first prove that the proposed encoding function is injective for any tree without unary branches. In practice, the approach is made extensible to all constituency trees by collapsing unary branches. We then use the PTB and CTB treebanks as testbeds and propose a set of fast baselines. We achieve 90.7% F-score on the PTB test set, outperforming the Vinyals et al.  2015  sequence-to-sequence parser. In addition, sacrificing some accuracy, our approach achieves the fastest constituent parsing speeds reported to date on PTB by a wide margin. 1",https://aclanthology.org/D18-1162,Association for Computational Linguistics,2018,October-November,Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing,"G{\'o}mez-Rodr{\'\i}guez, Carlos  and
Vilares, David",Constituent Parsing as Sequence Labeling,10.18653/v1/D18-1162,D18,541
2021.emnlp-demo.33,['Automatic Text Summarization'],['Abstractive Text Summarization'],,"We introduce iFACETSUM, 1 a web application for exploring topical document sets. iFACETSUM integrates interactive summarization together with faceted search, by providing a novel faceted navigation scheme that yields abstractive summaries for the user's selections. This approach offers both a comprehensive overview as well as concise details regarding subtopics of choice. Fine-grained facets are automatically produced based on cross-document coreference pipelines, rendering generic concepts, entities and statements surfacing in the source texts. We analyze the effectiveness of our application through smallscale user studies, which suggest the usefulness of our approach. * Equal contribution. 1 Demo at https://biu-nlp.github.io/iFAC ETSUM/WebApp/client/, and code at https://gith ub.com/BIU-NLP/iFACETSUM.",https://aclanthology.org/2021.emnlp-demo.33,Association for Computational Linguistics,2021,November,Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing: System Demonstrations,"Hirsch, Eran  and
Eirew, Alon  and
Shapira, Ori  and
Caciularu, Avi  and
Cattan, Arie  and
Ernst, Ori  and
Pasunuru, Ramakanth  and
Ronen, Hadar  and
Bansal, Mohit  and
Dagan, Ido",iFacetSum: Coreference-based Interactive Faceted Summarization for Multi-Document Exploration,10.18653/v1/2021.emnlp-demo.33,emnlp,297
2021.adaptnlp-1.6,"['Parsing', 'Multilingual NLP', 'Data Management and Generation', 'Low-resource Languages', 'Model Architectures']","['Latent Dirichlet Allocation (LDA)', 'Syntactic Parsing', 'Data Preparation']",['Dependency Parsing'],"While high performance has been obtained for dependency parsing of high-resource languages, performance for low-resource languages lags behind. In this paper we focus on the parsing of the low-resource language Frisian. We use a sample of code-switched, spontaneously spoken data, which proves to be a challenging setup. We propose to train a parser specifically tailored towards the target domain, by selecting instances from multiple treebanks. Specifically, we use Latent Dirichlet Allocation LDA, with word and character N-gram features. The best single source treebank NL ALPINO resulted in an LAS of 54.7 whereas our data selection outperformed the single best transfer treebank and led to 55.6 LAS on the test data. Additional experiments consisted of removing diacritics from our Frisian data, creating more similar training data by cropping sentences and running our best model using XLM-R. These experiments did not lead to a better performance.",https://aclanthology.org/2021.adaptnlp-1.6,Association for Computational Linguistics,2021,April,Proceedings of the Second Workshop on Domain Adaptation for NLP,"Braggaar, Anouck  and
van der Goot, Rob","Challenges in Annotating and Parsing Spoken, Code-switched, Frisian-Dutch Data",,adaptnlp,761
S18-1145,"['Domain-specific NLP', 'Classification Applications', 'Data Management and Generation']",['Data Preparation'],['Annotation Processes'],"The four sub-tasks of SecureNLP build towards a capability for quickly highlighting critical information from malware reports, such as the specific actions taken by a malware sample. Digital Operatives DO submitted to sub-tasks 1 and 2, using standard text analysis technology text classification for sub-task 1, and a CRF for sub-task 2. Performance is broadly competitive with other submitted systems on sub-task 1 and weak on subtask 2. The annotation guidelines for the intermediate sub-tasks create a linkage to the final task, which is both an annotation challenge and a potentially useful feature of the task. The methods DO chose do not attempt to make use of this linkage, which may be a missed opportunity. This motivates a post-hoc error analysis. It appears that the annotation task is very hard, and that in some cases both deep conceptual knowledge and substantial surrounding context are needed in order to correctly classify sentences.",https://aclanthology.org/S18-1145,Association for Computational Linguistics,2018,June,Proceedings of The 12th International Workshop on Semantic Evaluation,"Brew, Chris",Digital Operatives at SemEval-2018 Task 8: Using dependency features for malware NLP,10.18653/v1/S18-1145,S18,377
2021.law-1.5,"['Parsing', 'Information Extraction', 'Data Management and Generation']","['Syntactic Parsing', 'Data Preparation', 'Relation Extraction']","['Dependency Parsing', 'Annotation Processes']","In this paper we investigate the possibility of extracting predicate-argument relations from UD trees and enhanced UD graphs. Concretely, we apply UD parsers on an English question answering/semantic-role labeling data set FitzGerald et al., 2018 and check if the annotations reflect the relations in the resulting parse trees, using a small number of rules to extract this information. We find that 79.1% of the argument-predicate pairs can be found in this way, on the basis of Udify Kondratyuk and Straka, 2019 . Error analysis reveals that half of the error cases are attributable to shortcomings in the dataset. The remaining errors are mostly due to predicateargument relations not being extractible algorithmically from the UD trees requiring semantic reasoning to be resolved. The parser itself is only responsible for a small portion of errors. Our analysis suggests a number of improvements to the UD annotation schema: we propose to enhance the schema in four ways, in order to capture argument-predicate relations. Additionally, we propose improvements regarding data collection for question answering/semantic-role labeling data.",https://aclanthology.org/2021.law-1.5,Association for Computational Linguistics,2021,November,Proceedings of The Joint 15th Linguistic Annotation Workshop (LAW) and 3rd Designing Meaning Representations (DMR) Workshop,"Ek, Adam  and
Bernardy, Jean-Philippe  and
Chatzikyriakidis, Stergios",Can predicate-argument relationships be extracted from UD trees?,10.18653/v1/2021.law-1.5,law,1268
2021.mrqa-1.9,"['Ethics', 'Biases in NLP', 'Evaluation Techniques', 'Data Management and Generation', 'Question Answering (QA)']",['Data Preparation'],,"Deep learning models have shown great success in question answering QA, however, biases in the training data may lead to them amplifying or reflecting inequity. To probe for bias in QA systems, we create two benchmarks for closed and open domain question answering, consisting of ambiguous questions and bias metrics. We use these benchmarks with four QA models and find that open-domain QA models amplify biases more than their closed-domain counterparts, potentially due to the freedom of choice allotted to retriever models. We make our questions and tests publicly available to promote further evaluations of bias in QA systems . 1",https://aclanthology.org/2021.mrqa-1.9,Association for Computational Linguistics,2021,November,Proceedings of the 3rd Workshop on Machine Reading for Question Answering,"Mao, Andrew  and
Raman, Naveen  and
Shu, Matthew  and
Li, Eric  and
Yang, Franklin  and
Boyd-Graber, Jordan",Eliciting Bias in Question Answering Models through Ambiguity,10.18653/v1/2021.mrqa-1.9,mrqa,952
U17-1008,"['Domain-specific NLP', 'Classification Applications', 'Data Management and Generation']","['Data Preparation', 'Medical and Clinical NLP']",,"The automatic detection of negation and speculation in clinical notes is vital when searching for genuine instances of a given phenomenon. This paper describes a new corpus of negation and speculation data, in the veterinary clinical note domain, and describes a series of experiments whereby we port a CRF-based method across from the BioScope corpus to this novel domain.",https://aclanthology.org/U17-1008,,2017,December,Proceedings of the Australasian Language Technology Association Workshop 2017,"Cheng, Katherine  and
Baldwin, Timothy  and
Verspoor, Karin",Automatic Negation and Speculation Detection in Veterinary Clinical Text,,U17,879
J17-2005,"['Question Answering (QA)', 'Domain-specific NLP', 'Data Management and Generation']","['Data Preparation', 'Multiple Choice QA (MCQA)']",,"We propose a question answering QA approach for standardized science exams that both identifies correct answers and produces compelling human-readable justifications for why those answers are correct. Our method first identifies the actual information needed in a question using psycholinguistic concreteness norms, then uses this information need to construct answer justifications by aggregating multiple sentences from different knowledge bases using syntactic and lexical information. We then jointly rank answers and their justifications using a reranking perceptron that treats justification quality as a latent variable. We evaluate our method on 1,000 multiple-choice questions from elementary school science exams, and empirically demonstrate that it performs better than several strong baselines, including neural network approaches. Our best configuration answers 44% of the questions correctly, where the top justifications for 57% of these correct answers contain a compelling human-readable justification that explains the inference required to arrive at the correct answer. We include a detailed characterization of the justification quality for both our method and a strong baseline, and show that information aggregation is key to addressing the information need in complex questions.",https://aclanthology.org/J17-2005,MIT Press,2017,June,,"Jansen, Peter  and
Sharp, Rebecca  and
Surdeanu, Mihai  and
Clark, Peter",Framing QA as Building and Ranking Intersentence Answer Justifications,10.1162/COLI_a_00287,J17,678
2021.cinlp-1.3,"['Biases in NLP', 'Ethics', 'Model Architectures', 'Commonsense Reasoning']",,,"Recent work has raised concerns on the risk of spurious correlations and unintended biases in statistical machine learning models that threaten model robustness and fairness. In this paper, we propose a simple and intuitive regularization approach to integrate causal knowledge during model training and build a robust and fair model by emphasizing causal features and de-emphasizing spurious features. Specifically, we first manually identify causal and spurious features with principles inspired from the counterfactual framework of causal inference. Then, we propose a regularization approach to penalize causal and spurious features separately. By adjusting the strength of the penalty for each type of feature, we build a predictive model that relies more on causal features and less on non-causal features. We conduct experiments to evaluate model robustness and fairness on three datasets with multiple metrics. Empirical results show that the new models built with causal awareness significantly improve model robustness with respect to counterfactual texts and model fairness with respect to sensitive attributes.",https://aclanthology.org/2021.cinlp-1.3,Association for Computational Linguistics,2021,November,Proceedings of the First Workshop on Causal Inference and NLP,"Wang, Zhao  and
Shu, Kai  and
Culotta, Aron",Enhancing Model Robustness and Fairness with Causality: A Regularization Approach,10.18653/v1/2021.cinlp-1.3,cinlp,1165
2020.coling-main.197,"['Model Architectures', 'Learning Paradigms', 'Text Generation']","['Adversarial Learning', 'Text Style Transfer']",,"Textual style transfer involves modifying the style of a text while preserving its content. This assumes that it is possible to separate style from content. This paper investigates whether this separation is possible. We use sentiment transfer as our case study for style transfer analysis. Our experimental methodology frames style transfer as a multi-objective problem, balancing style shift with content preservation and fluency. Due to the lack of parallel data for style transfer we employ a variety of adversarial encoder-decoder networks in our experiments. Also, we use a probing methodology to analyse how these models encode style-related features in their latent spaces. The results of our experiments which are further confirmed by a human evaluation reveal an inherent trade-off between the multiple style transfer objectives and indicate that style cannot be usefully separated from content within these style-transfer systems.",https://aclanthology.org/2020.coling-main.197,International Committee on Computational Linguistics,2020,December,Proceedings of the 28th International Conference on Computational Linguistics,"Jafaritazehjani, Somayeh  and
Lecorv{\'e}, Gw{\'e}nol{\'e}  and
Lolive, Damien  and
Kelleher, John",Style versus Content: A distinction without a learnable difference?,10.18653/v1/2020.coling-main.197,coling,756
2020.ccl-1.91,"['Learning Paradigms', 'Low-resource Languages', 'Machine Translation (MT)']","['Neural MT (NMT)', 'Reinforcement Learning']",,"Reinforcement learning (RL) has made remarkable progress in neural machine translation (N-MT). However, it exists the problems with uneven sampling distribution, sparse rewards and high variance in training phase. Therefore, we propose a multi-reward reinforcement learning training strategy to decouple action selection and value estimation. Meanwhile, our method combines with language model rewards to jointly optimize model parameters. In addition, we add Gumbel noise in sampling to obtain more effective semantic information. To verify the robustness of our method, we not only conducted experiments on large corpora, but also performed on low-resource languages. Experimental results show that our work is superior to the baselines in WMT14 English-German, LDC2014 Chinese-English and CWMT2018 Mongolian-Chinese tasks, which fully certificates the effectiveness of our method.",https://aclanthology.org/2020.ccl-1.91,Chinese Information Processing Society of China,2020,October,Proceedings of the 19th Chinese National Conference on Computational Linguistics,"Sun, Shuo  and
Hou, Hongxu  and
Wu, Nier  and
Guo, Ziyue  and
Zhang, Chaowei",Multi-Reward based Reinforcement Learning for Neural Machine Translation,10.1007/978-3-030-63031-7_16,ccl,1230
2021.disrpt-1.1,"['Text Preprocessing', 'Parsing', 'Multilingual NLP', 'Information Extraction', 'Data Management and Generation', 'Low-resource Languages', 'Model Architectures']","['Text Segmentation', 'Discourse Parsing', 'Data Analysis', 'Relation Extraction']",,"In 2021, we organized the second iteration of a shared task dedicated to the underlying units used in discourse parsing across formalisms: the DISRPT Shared Task Discourse Relation Parsing and Treebanking. Adding to the 2019 tasks on Elementary Discourse Unit Segmentation and Connective Detection, this iteration of the Shared Task included for the first time a track on discourse relation classification across three formalisms: RST, SDRT, and PDTB. In this paper we review the data included in the Shared Task, which covers nearly 3 million manually annotated tokens from 16 datasets in 11 languages, survey and compare submitted systems and report on system performance on each task for both annotated and plain-tokenized versions of the data.",https://aclanthology.org/2021.disrpt-1.1,Association for Computational Linguistics,2021,November,Proceedings of the 2nd Shared Task on Discourse Relation Parsing and Treebanking (DISRPT 2021),"Zeldes, Amir  and
Liu, Yang Janet  and
Iruskieta, Mikel  and
Muller, Philippe  and
Braud, Chlo{\'e}  and
Badene, Sonia","The DISRPT 2021 Shared Task on Elementary Discourse Unit Segmentation, Connective Detection, and Relation Classification",10.18653/v1/2021.disrpt-1.1,disrpt,1323
Q19-1012,"['Question Answering (QA)', 'Knowledge Representation and Reasoning', 'Model Architectures']",['Knowledge Base QA'],,"Recent years have seen increasingly complex question-answering on knowledge bases KBQA involving logical, quantitative, and comparative reasoning over KB subgraphs. Neural Program Induction NPI is a pragmatic approach toward modularizing the reasoning process by translating a complex natural language query into a multi-step executable program. While NPI has been commonly trained with the ''gold'' program or its sketch, for realistic KBQA applications such gold programs are expensive to obtain. There, practically only natural language queries and the corresponding answers can be provided for training. The resulting combinatorial explosion in program space, along with extremely sparse rewards, makes NPI for KBQA ambitious and challenging. We present Complex Imperative Program Induction from Terminal Rewards CIPITR, an advanced neural programmer that mitigates reward sparsity with auxiliary rewards, and restricts the program space to semantically correct programs using high-level constraints, KB schema, and inferred answer type. CIPITR solves complex KBQA considerably more accurately than key-value memory networks and neural symbolic machines NSM. For moderately complex queries requiring 2-to 5-step programs, CIPITR scores at least 3 higher F1 than the competing systems. On one of the hardest class of programs comparative reasoning with 5-10 steps, CIPITR outperforms NSM by a factor of 89 and memory networks by 9 times. 1 * Now at Hike Messenger 1 The NSM baseline in this work is a re-implemented version, as the original code was not available.",https://aclanthology.org/Q19-1012,MIT Press,2019,,,"Saha, Amrita  and
Ansari, Ghulam Ahmed  and
Laddha, Abhishek  and
Sankaranarayanan, Karthik  and
Chakrabarti, Soumen",Complex Program Induction for Querying Knowledge Bases in the Absence of Gold Programs,10.1162/tacl_a_00262,Q19,194
2021.nlp4posimpact-1.6,"['Ethics', 'Information Retrieval', 'Domain-specific NLP']","['Information Filtering', 'NLP for News and Media']",['Recommender Systems'],"In this position paper, we present a research agenda and ideas for facilitating exposure to diverse viewpoints in news recommendation. Recommending news from diverse viewpoints is important to prevent potential filter bubble effects in news consumption, and stimulate a healthy democratic debate. To account for the complexity that is inherent to humans as citizens in a democracy, we anticipate among others individual-level differences in acceptance of diversity. We connect this idea to techniques in Natural Language Processing, where distributional language models would allow us to place different users and news articles in a multidimensional space based on semantic content, where diversity is operationalized as distance and variance. In this way, we can model individual ""latitudes of diversity"" for different users, and thus personalize viewpoint diversity in support of a healthy public debate. In addition, we identify technical, ethical and conceptual issues related to our presented ideas. Our investigation describes how NLP can play a central role in diversifying news recommendations.",https://aclanthology.org/2021.nlp4posimpact-1.6,Association for Computational Linguistics,2021,August,Proceedings of the 1st Workshop on NLP for Positive Impact,"Reuver, Myrthe  and
Mattis, Nicolas  and
Sax, Marijn  and
Verberne, Suzan  and
Tintarev, Nava  and
Helberger, Natali  and
Moeller, Judith  and
Vrijenhoek, Sanne  and
Fokkens, Antske  and
van Atteveldt, Wouter","Are we human, or are we users? The role of natural language processing in human-centric news recommenders that nudge users to diverse content",10.18653/v1/2021.nlp4posimpact-1.6,nlp4posimpact,64
2021.sigdial-1.4,['Discourse Analysis'],,,"There is increasing interest in modeling style choices in dialog, for example for enabling dialog systems to adapt to their users. It is commonly assumed that each user has his or her own stable characteristics, but for interaction style the truth of this assumption has not been well examined. I investigated using a vectorspace model of interaction styles, derived from the Switchboard corpus of telephone conversations and a broad set of prosodic-behavior features. While most individuals exhibited interaction style tendencies, these were generally far from stable, with a predictive model based on individual tendencies outperforming a speaker-independent model by only 3.6%. The tendencies were somewhat stronger for some speakers, including generally males, and for some dimensions of variation.",https://aclanthology.org/2021.sigdial-1.4,Association for Computational Linguistics,2021,July,Proceedings of the 22nd Annual Meeting of the Special Interest Group on Discourse and Dialogue,"Ward, Nigel",Individual Interaction Styles: Evidence from a Spoken Chat Corpus,10.18653/v1/2021.sigdial-1.4,sigdial,1304
2020.sdp-1.15,"['Domain-specific NLP', 'Data Management and Generation', 'Information Extraction', 'Knowledge Representation and Reasoning', 'Model Architectures']","['Medical and Clinical NLP', 'Data Preparation', 'Named Entity Recognition (NER)', 'Knowledge Graphs', 'Graph Neural Networks (GNNs)', 'Relation Extraction']",['Biomedical NLP'],"We introduce a generic, human-out-of-theloop pipeline, ERLKG, to perform rapid association analysis of any biomedical entity with other existing entities from a corpora of the same domain. Our pipeline consists of a Knowledge Graph KG created from the Open Source CORD-19 dataset by fully automating the procedure of information extraction using SciBERT. The best latent entity representations are then found by benchnmarking different KG embedding techniques on the task of link prediction using a Graph Convolution Network Auto Encoder GCN-AE. We demonstrate the utility of ERLKG with respect to COVID-19 through multiple qualitative evaluations. Due to the lack of a gold standard, we propose a relatively large intrinsic evaluation dataset for COVID-19 and use it for validating the top two performing KG embedding techniques. We find TransD to be the best performing KG embedding technique with Pearson and Spearman correlation scores of 0.4348 and 0.4570 respectively. We demonstrate that a considerable number of ERLKG's top protein, chemical and disease predictions are currently in consideration for COVID-19 related research.",https://aclanthology.org/2020.sdp-1.15,Association for Computational Linguistics,2020,November,Proceedings of the First Workshop on Scholarly Document Processing,"Basu, Sayantan  and
Chakraborty, Sinchani  and
Hassan, Atif  and
Siddique, Sana  and
Anand, Ashish",ERLKG: Entity Representation Learning and Knowledge Graph based association analysis of COVID-19 through mining of unstructured biomedical corpora,10.18653/v1/2020.sdp-1.15,sdp,1100
2016.gwc-1.44,"['Knowledge Representation and Reasoning', 'Low-resource Languages']","['Semantic Web', 'Ontologies']",,This paper presents our first attempt at verifying integrity constraints of our openWordnet-PT against the ontology for Wordnets encoding. Our wordnet is distributed in Resource Description Format RDF and we want to guarantee not only the syntax correctness but also its semantics soundness.,https://aclanthology.org/2016.gwc-1.44,Global Wordnet Association,2016,27--30 January,Proceedings of the 8th Global WordNet Conference (GWC),"Rademaker, Alexandre  and
Chalub, Fabricio",Verifying Integrity Constraints of a RDF-based WordNet,,gwc,1077
W18-4109,"['Domain-specific NLP', 'Low-resource Languages', 'Data Management and Generation']","['Data Analysis', 'Medical and Clinical NLP']",,"ssssss Older adults tend to suffer a decline in some of their cognitive capabilities, being language one of least affected processes. Word association norms WAN also known as free word associations reflect word-word relations, the participant reads or hears a word and is asked to write or say the first word that comes to mind. Free word associations show how the organization of semantic memory remains almost unchanged with age. We have performed a WAN task with very small samples of older adults with Alzheimer's disease AD, vascular dementia VaD and mixed dementia MxD, and also with a control group of typical aging adults, matched by age, sex and education. All of them are native speakers of Mexican Spanish. The results show, as expected, that Alzheimer disease has a very important impact in lexical retrieval, unlike vascular and mixed dementia. This suggests that linguistic tests elaborated from WAN can be also used for detecting AD at early stages.",https://aclanthology.org/W18-4109,Association for Computational Linguistics,2018,August,Proceedings of the First International Workshop on Language Cognition and Computational Models,"Arias-Trejo, Natalia  and
Minto-Garc{\'\i}a, Aline  and
Luna-Umanzor, Diana I.  and
R{\'\i}os-Ponce, Alma E.  and
Mariana, Balderas-Pliego  and
Bel-Enguix, Gemma",Word-word Relations in Dementia and Typical Aging,,W18,980
U19-1011,"['Learning Paradigms', 'Classification Applications', 'Model Architectures']","['Transfer Learning', 'Sentiment Analysis (SA)', 'Recurrent Neural Networks (RNNs)']",['Long Short-Term Memory (LSTM) Models'],"Catastrophic forgetting -whereby a model trained on one task is fine-tuned on a second, and in doing so, suffers a ""catastrophic"" drop in performance over the first task -is a hurdle in the development of better transfer learning techniques. Despite impressive progress in reducing catastrophic forgetting, we have limited understanding of how different architectures and hyper-parameters affect forgetting in a network. In this paper, we aim to understand factors which cause forgetting during sequential training. Our primary finding is that CNNs forget less than LSTMs. We show that max-pooling is the underlying operation which helps CNNs alleviate forgetting compared to LSTMs. We also found that curriculum learning Bengio et al., 2009 , placing a hard task towards the end of task sequence, reduces forgetting. We analysed the effect of fine-tuning contextual embeddings on catastrophic forgetting, and found that using fixed word embeddings is preferable to fine-tuning. 1",https://aclanthology.org/U19-1011,Australasian Language Technology Association,2019,4--6 December,Proceedings of the The 17th Annual Workshop of the Australasian Language Technology Association,"Arora, Gaurav  and
Rahimi, Afshin  and
Baldwin, Timothy",Does an LSTM forget more than a CNN? An empirical study of catastrophic forgetting in NLP,,U19,431
2020.intellang-1.2,['Dialogue Systems'],['Response Generation'],,"The user experience of an asynchronous video interview system is often deemed noninteractive and one-sided. Interview candidates anticipate them to be natural and coherent like a traditional face-to-face interview. One aspect of improving the interaction is by asking relevant follow-up questions based on the previously asked questions, and its answers. We propose a follow-up question generation model capable of generating relevant and diverse follow-up questions. We develop a 3D virtual interviewing system, Maya, equipped with follow-up question generator. Many existing asynchronous interviewing systems pose questions that are fixed and scripted. Maya, on the contrary, reacts with relevant followup questions, a relatively unexplored dimension in virtual interviewing systems. We leverage the implicit knowledge from deep pretrained language models along with a small corpus of interview questions to generate rich and diverse follow-up questions in natural language. The generated questions achieve 77% relevance with human evaluation. We compare our follow-up question generation model with strong baselines of neural network and rulebased systems and show that it produces better quality questions.",https://aclanthology.org/2020.intellang-1.2,Association for Computational Lingustics,2020,September,Proceedings of the Workshop on Intelligent Information Processing and Natural Language Generation,"B, Pooja Rao S  and
Agnihotri, Manish  and
Jayagopi, Dinesh Babu",Automatic Follow-up Question Generation for Asynchronous Interviews,,intellang,1405
2021.nlp4musa-1.2,"['Evaluation Techniques', 'Domain-specific NLP', 'Classification Applications', 'Model Architectures']",['Transformer Models'],,"Since their conception for NLP tasks in 2017, Transformer neural networks have been increasingly used with compelling results for a variety of symbolic MIR tasks including music analysis, classification and generation. Although the concept of self-attention between words in text can intuitively be transposed as a relation between musical objects such as notes or chords in a score, it remains relatively unknown what kind of musical relations precisely tend to be captured by self attention mechanisms when applied to musical data. Moreover, the principle of self-attention has been elaborated in NLP to help model the ""meaning"" of a sentence while in the musical domain this concept appears to be more subjective. In this explorative work, we open the music transformer black box looking to identify which aspects of music are actually learnt by the self-attention mechanism. We apply this approach to two MIR probing tasks : composer classification and cadence identification.",https://aclanthology.org/2021.nlp4musa-1.2,Association for Computational Linguistics,2021,November,Proceedings of the 2nd Workshop on NLP for Music and Spoken Audio (NLP4MusA),"Loiseau, Gabriel  and
Keller, Mikaela  and
Bigo, Louis",What Musical Knowledge Does Self-Attention Learn ?,,nlp4musa,1390
2021.findings-acl.176,"['Parsing', 'Data Management and Generation', 'Information Extraction', 'Model Architectures']","['Data Preparation', 'Coreference Resolution']",['Annotation Processes'],"Screenplays refer to characters using different names, pronouns, and nominal expressions. We need to resolve these mentions to the correct referent character for better story understanding and holistic research in computational narratology. Coreference resolution of character mentions in screenplays becomes challenging because of the large document lengths, unique structural features like scene headers, interleaving of action and speech passages, and reliance on the accompanying video. In this work, we first adapt widelyused annotation guidelines to address domainspecific issues in screenplays. We develop an automatic screenplay parser to extract the structural information and design coreference rules based upon the structure. Our model exploits these structural features and outperforms a benchmark coreference model on the screenplay coreference resolution task.",https://aclanthology.org/2021.findings-acl.176,Association for Computational Linguistics,2021,August,Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021,"Baruah, Sabyasachee  and
Nallan Chakravarthula, Sandeep  and
Narayanan, Shrikanth",Annotation and Evaluation of Coreference Resolution in Screenplays,10.18653/v1/2021.findings-acl.176,findings,771
2021.triton-1.9,"['Learning Paradigms', 'Domain-specific NLP', 'Information Extraction']","['Transfer Learning', 'Named Entity Recognition (NER)', 'Medical and Clinical NLP']",['Biomedical NLP'],"The domain-specialised application of Named Entity Recognition NER is known as Biomedical NER BioNER, which aims to identify and classify biomedical concepts that are of interest to researchers, such as genes, proteins, chemical compounds, drugs, mutations, diseases, and so on. The BioNER task is very similar to general NER but recognising Biomedical Named Entities BNEs is more challenging than recognising proper names from newspapers due to the characteristics of biomedical nomenclature. In order to address the challenges posed by BioNER, seven machine learning models were implemented comparing a transfer learning approach based on fine-tuned BERT with Bi-LSTM based neural models and a CRF model used as baseline. Precision, Recall and F1-score were used as performance scores evaluating the models on two well-known biomedical corpora: JNLPBA and BIOCREATIVE IV BC-IV. Strict and partial matching were considered as evaluation criteria. The reported results show that a transfer learning approach based on fine-tuned BERT outperforms all others methods achieving the highest scores for all metrics on both corpora.",https://aclanthology.org/2021.triton-1.9,INCOMA Ltd.,2021,July,Proceedings of the Translation and Interpreting Technology Online Conference,"Cariello, Maria Carmela  and
Lenci, Alessandro  and
Mitkov, Ruslan",A Comparison between Named Entity Recognition Models in the Biomedical Domain,10.26615/978-954-452-071-7_009,triton,508
2020.sigmorphon-1.25,"['Information Retrieval', 'Audio Generation and Processing', 'Low-resource Languages', 'Text Generation']",,,"We investigate the problem of searching for a lexeme-set in speech by searching for its inflectional variants. Experimental results indicate how lexeme-set search performance changes with the number of hypothesized inflections, while ablation experiments highlight the relative importance of different components in the lexeme-set search pipeline and the value of using curated inflectional paradigms. We provide a recipe and evaluation set for the community to use as an extrinsic measure of the performance of inflection generation approaches.",https://aclanthology.org/2020.sigmorphon-1.25,Association for Computational Linguistics,2020,July,"Proceedings of the 17th SIGMORPHON Workshop on Computational Research in Phonetics, Phonology, and Morphology","Adams, Oliver  and
Wiesner, Matthew  and
Trmal, Jan  and
Nicolai, Garrett  and
Yarowsky, David",Induced Inflection-Set Keyword Search in Speech,10.18653/v1/2020.sigmorphon-1.25,sigmorphon,1059
2021.wanlp-1.38,"['Data Management and Generation', 'Low-resource Languages', 'Classification Applications', 'Model Architectures']","['Transformer Models', 'Data Augmentation', 'Sarcasm Detection', 'Sentiment Analysis (SA)']",,"In this paper, we describe our efforts on the shared task of sarcasm and sentiment detection in Arabic Abu Farha et al., 2021 . The shared task consists of two subtasks: Sarcasm Detection Subtask 1 and Sentiment Analysis Subtask 2. Our experiments were based on fine-tuning seven BERT-based models with data augmentation to solve the imbalanced data problem. For both tasks, the MARBERT BERT-based model with data augmentation outperformed other models with an increase of the F-score by 15% for both tasks which shows the effectiveness of our approach.",https://aclanthology.org/2021.wanlp-1.38,Association for Computational Linguistics,2021,April,Proceedings of the Sixth Arabic Natural Language Processing Workshop,"Abuzayed, Abeer  and
Al-Khalifa, Hend",Sarcasm and Sentiment Detection In Arabic Tweets Using BERT-based Models and Data Augmentation,,wanlp,599
D19-5823,"['Question Answering (QA)', 'Data Management and Generation', 'Information Retrieval', 'Classification Applications']",['Data Preparation'],,"Although advances in neural architectures for NLP problems and unsupervised pre-training led to impressive improvements on question answering and natural language inference, reasoning over long texts still poses a great challenge. Here, we consider the task of question answering from full narratives e.g., books or movie scripts, or their summaries, tackling the NarrativeQA challenge NQA; Kocisky et al.  2018 . We introduce a heuristic extractive version of the data set, which allows us to approach the more feasible problem of answer extraction rather than generation. We develop models for passage retrieval and answer span prediction using this data set. We use pre-trained BERT embeddings for injecting prior knowledge into our system. We show that our setup leads to state of the art performance on summary-level QA. On narrativelevel QA, our model performs competitively on the METEOR metric. We analyze the relative contributions of BERT embeddings and the extractive model setup, and provide a detailed error analysis.",https://aclanthology.org/D19-5823,Association for Computational Linguistics,2019,November,Proceedings of the 2nd Workshop on Machine Reading for Question Answering,"Frermann, Lea",Extractive NarrativeQA with Heuristic Pre-Training,10.18653/v1/D19-5823,D19,1020
2021.inlg-1.45,"['Text Generation', 'Model Architectures']","['Transformer Models', 'Data-to-Text Generation']",,"Ever since neural models were adopted in datato-text language generation, they have invariably been reliant on extrinsic components to improve their semantic accuracy, because the models normally do not exhibit the ability to generate text that reliably mentions all of the information provided in the input. In this paper, we propose a novel decoding method that extracts interpretable information from encoder-decoder models' cross-attention, and uses it to infer which attributes are mentioned in the generated text, which is subsequently used to rescore beam hypotheses. Using this decoding method with T5 and BART, we show on three datasets its ability to dramatically reduce semantic errors in the generated outputs, while maintaining their state-of-the-art quality.",https://aclanthology.org/2021.inlg-1.45,Association for Computational Linguistics,2021,August,Proceedings of the 14th International Conference on Natural Language Generation,"Juraska, Juraj  and
Walker, Marilyn",Attention Is Indeed All You Need: Semantically Attention-Guided Decoding for Data-to-Text NLG,10.18653/v1/2021.inlg-1.45,inlg,908
P18-1086,"['Data Management and Generation', 'Knowledge Representation and Reasoning', 'Image and Video Processing', 'Classification Applications', 'Model Architectures']","['Data Preparation', 'Multilabel Text Classification']",,"Despite recent advances in knowledge representation, automated reasoning, and machine learning, artificial agents still lack the ability to understand basic actioneffect relations regarding the physical world, for example, the action of cutting a cucumber most likely leads to the state where the cucumber is broken apart into smaller pieces. If artificial agents e.g., robots ever become our partners in joint tasks, it is critical to empower them with such action-effect understanding so that they can reason about the state of the world and plan for actions. Towards this goal, this paper introduces a new task on naive physical action-effect prediction, which addresses the relations between concrete actions expressed in the form of verbnoun pairs and their effects on the state of the physical world as depicted by images. We collected a dataset for this task and developed an approach that harnesses web image data through distant supervision to facilitate learning for action-effect prediction. Our empirical results have shown that web data can be used to complement a small number of seed examples e.g., three examples for each action for model learning. This opens up possibilities for agents to learn physical action-effect relations for tasks at hand through communication with humans with a few examples.",https://aclanthology.org/P18-1086,Association for Computational Linguistics,2018,July,Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),"Gao, Qiaozi  and
Yang, Shaohua  and
Chai, Joyce  and
Vanderwende, Lucy",What Action Causes This? Towards Naive Physical Action-Effect Prediction,10.18653/v1/P18-1086,P18,1404
S19-2212,"['Domain-specific NLP', 'Information Extraction', 'Model Architectures']",['NLP for News and Media'],['NLP for Social Media'],"In this paper, we describe a suggestion mining system that participated in SemEval 2019 Task 9, SubTask A -Suggestion Mining from Online Reviews and Forums. Given some suggestions from online reviews and forums that can be classified into suggestion and nonsuggestion classes. In this task, we combine the attention mechanism with the LSTM model, which is the final system we submitted. The final submission achieves 14th place in Task 9, SubTask A with the accuracy of 0.6776. After the challenge, we train a series of neural network models such as convolutional neural networkCNN, TextCNN, long short-term memoryLSTM and C-LSTM. Finally, we make an ensemble on the predictions of these models and get a better result.",https://aclanthology.org/S19-2212,Association for Computational Linguistics,2019,June,Proceedings of the 13th International Workshop on Semantic Evaluation,"Li, Junyi",Lijunyi at SemEval-2019 Task 9: An attention-based LSTM and ensemble of different models for suggestion mining from online reviews and forums,10.18653/v1/S19-2212,S19,91
2021.depling-1.3,"['Parsing', 'Data Management and Generation', 'Multilingual NLP']","['Data Preparation', 'Semantic Parsing', 'Data Analysis', 'Syntactic Parsing']","['Annotation Processes', 'Dependency Parsing']","We discuss the role of enhanced Universal Dependencies E-UD in the task of deriving semantic predicate-argument structures from UD treebanks in a universal, non-language-specific way. We consider the usefulness of three kinds of E-UD annotation controllers of xcomps, propagation of outgoing dependencies in coordinations, and coreference in relative clauses and assess some heuristics for automatically adding such enhancements. We conclude that one large obstacle both for deriving predicate-argument structures from UD treebanks and for the automatic enhancement of basic UD treebanks is the fact that UD does not represent empty elements such as pro-dropped arguments, and we suggest that devoting effort to this would often provide a better return on investment than spending resources on improving or adding E-UD annotations. This work is licensed under a Creative Commons Attribution 4.0 International Licence. Licence details: http:// creativecommons.org/licenses/by/4.0/. 1 The relations should also be labelled, giving rise to a task of translating UD grammatical functions to appropriate semantic roles, but we do not consider this task here. Also, note that UD annotation does not allow us to identify eventualities introduced by non-verbal predicates e.g. action nouns, so we ignore those in this paper. 2  The TuDeT treebanks merely copy the basic dependencies over into the E-UD, while the Akkadian treebank only contains a single E-UD edge. 3 There are many existing systems for augmenting basic UD dependency trees, and several whose effectiveness has been reported in the literature",https://aclanthology.org/2021.depling-1.3,Association for Computational Linguistics,2021,December,"Proceedings of the Sixth International Conference on Dependency Linguistics (Depling, SyntaxFest 2021)","Findlay, Jamie Y.  and
Haug, Dag T. T.",How useful are Enhanced Universal Dependencies for semantic interpretation?,,depling,442
2022.humeval-1.5,"['Text Generation', 'Domain-specific NLP', 'Data Management and Generation']","['Data Analysis', 'Medical and Clinical NLP']",['NLP for Mental Health'],"Dieting is a behaviour change task that is difficult for many people to conduct successfully. This is due to many factors, including stress and cost. Mobile applications offer an alternative to traditional coaching. However, previous work on apps evaluation only focused on dietary outcomes, ignoring users' emotional state despite its influence on eating habits. In this work, we introduce a novel evaluation of the effects that tailored communication can have on the emotional load of dieting. We implement this by augmenting a traditional diet-app with affective NLG, text-tailoring and persuasive communication techniques. We then run a short 2-weeks experiment and check dietary outcomes, user feedback of produced text and, most importantly, its impact on emotional state, through PANAS questionnaire. Results show that tailored communication significantly improved users' emotional state, compared to an app-only control group.",https://aclanthology.org/2022.humeval-1.5,Association for Computational Linguistics,2022,May,Proceedings of the 2nd Workshop on Human Evaluation of NLP Systems (HumEval),"Balloccu, Simone  and
Reiter, Ehud",Beyond calories: evaluating how tailored communication reduces emotional load in diet-coaching,10.18653/v1/2022.humeval-1.5,humeval,613
2021.teachingnlp-1.9,"['Domain-specific NLP', 'Model Architectures']",,,"Deep neural networks have revolutionized many fields, including Natural Language Processing. This paper outlines teaching materials for an introductory lecture on deep learning in Natural Language Processing NLP. The main submitted material covers a summer school lecture on encoder-decoder models. Complementary to this is a set of jupyter notebook slides from earlier teaching, on which parts of the lecture were based on. The main goal of this teaching material is to provide an overview of neural network approaches to natural language processing, while linking modern concepts back to the roots showing traditional essential counterparts. The lecture departs from count-based statistical methods and spans up to gated recurrent networks and attention, which is ubiquitous in today's NLP.",https://aclanthology.org/2021.teachingnlp-1.9,Association for Computational Linguistics,2021,June,Proceedings of the Fifth Workshop on Teaching NLP,"Plank, Barbara",From back to the roots into the gated woods: Deep learning for NLP,10.18653/v1/2021.teachingnlp-1.9,teachingnlp,706
C18-1185,"['Learning Paradigms', 'Information Extraction', 'Model Architectures', 'Domain-specific NLP']","['Transfer Learning', 'Named Entity Recognition (NER)', 'Recurrent Neural Networks (RNNs)', 'NLP for News and Media']",['Long Short-Term Memory (LSTM) Models'],"In this paper, we propose to use a sequence to sequence model for Named Entity Recognition NER and we explore the effectiveness of such model in a progressive NER setting -a Transfer Learning TL setting. We train an initial model on source data and transfer it to a model that can recognize new NE categories in the target data during a subsequent step, when the source data is no longer available. Our solution consists in: i to reshape and re-parametrize the output layer of the first learned model to enable the recognition of new NEs; ii to leave the rest of the architecture unchanged, such that it is initialized with parameters transferred from the initial model; and iii to fine tune the network on the target data. Most importantly, we design a new NER approach based on sequence to sequence Seq2Seq models, which can intuitively work better in our progressive setting. We compare our approach with a Bidirectional LSTM, which is a strong neural NER model. Our experiments show that the Seq2Seq model performs very well on the standard NER setting and it is more robust in the progressive setting. Our approach can recognize previously unseen NE categories while preserving the knowledge of the seen data.",https://aclanthology.org/C18-1185,Association for Computational Linguistics,2018,August,Proceedings of the 27th International Conference on Computational Linguistics,"Chen, Lingzhen  and
Moschitti, Alessandro",Learning to Progressively Recognize New Named Entities with Sequence to Sequence Models,,C18,1463
2021.fnp-1.11,"['Data Management and Generation', 'Classification Applications', 'Domain-specific NLP']","['NLP for Finance', 'Data Preparation', 'Sentiment Analysis (SA)', 'Data Analysis']",['Annotation Processes'],"Specialized press and professional information channels influence beliefs on economic outlook or prospects for financial markets by drawing attention on particular events, and disseminating domain expert opinions. Analyzing this textual data allows for a better understanding of investors' beliefs and detecting key indicators for market dynamics. Though considerable efforts have been made to develop data-hungry algorithms on coarsegrained level sentiment analysis on financerelated social media messages, performing fine-grained level target-dependent opinion analysis on documents written by domain experts and journalists is still a relatively unexploited field.",https://aclanthology.org/2021.fnp-1.11,Association for Computational Linguistics,2021,15-16 September,Proceedings of the 3rd Financial Narrative Processing Workshop,"Hu, Jiahui  and
Paroubek, Patrick  and
Schumacher, Dirk",Annotation model and corpus for opinionated economy and finance narrative detection,,fnp,1474
2020.paclic-1.3,"['Data Management and Generation', 'Audio Generation and Processing', 'Low-resource Languages']","['Data Preparation', 'Data Analysis']",,"In this paper, we present a statistical and machine learning approach to the acoustic discrimination of a cross-linguistically unusual phonological contrast, initial geminates vs. singletons in Pattani Malay. We show that the only statistically significant difference between geminates and singletons is the duration of the consonant itself. No differences in F0 and intensity were observed on the following vowel, contra earlier reports. We further investigated the robustness of this contrast using linear discriminant analysis. Results show that discrimination is above chance, but poor ~62%. The large overlap between the two categories may be partly due to the naturalistic nature of our speech samples. However, we also found that the contrast is neutralized in some minimal pairs. This merger is surprising since initial geminates are often the sole realization of lexical and morphosyntactic contrasts. We suggest that the singleton/initial geminate contrast is now best characterized as a marginal contrast. We hypothesize that this marginally contrastive status may be the result of an on-going sound change, perhaps connected with the more modest role that initial geminates play in Pattani Malay morphophonological alternations.",https://aclanthology.org/2020.paclic-1.3,Association for Computational Linguistics,2020,October,"Proceedings of the 34th Pacific Asia Conference on Language, Information and Computation","Burroni, Francesco  and
Maspong, Sireemas  and
Pittayaporn, Pittayawat  and
Kochaiyaphum, Pimthip",A new look at Pattani Malay Initial Geminates: a statistical and machine learning approach,,paclic,1249
2022.acl-long.400,"['Dialogue Systems', 'Data Management and Generation', 'Information Extraction', 'Classification Applications', 'Model Architectures']","['Transformer Models', 'Data Preparation', 'Dialogue State Tracking (DST)']",,"We present a complete pipeline to extract characters in a novel and link them to their direct-speech utterances. Our model is divided into three independent components: extracting direct-speech, compiling a list of characters, and attributing those characters to their utterances. Although we find that existing systems can perform the first two tasks accurately, attributing characters to direct speech is a challenging problem due to the narrator's lack of explicit character mentions, and the frequent use of nominal and pronominal coreference when such explicit mentions are made. We adapt the progress made on Dialogue State Tracking to tackle a new problem: attributing speakers to dialogues. This is the first application of deep learning to speaker attribution, and it shows that is possible to overcome the need for the hand-crafted features and rules used in the past. Our full pipeline improves the performance of state-of-the-art models by a relative 50% in F1-score.",https://aclanthology.org/2022.acl-long.400,Association for Computational Linguistics,2022,May,Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),"Cuesta-Lazaro, Carolina  and
Prasad, Animesh  and
Wood, Trevor",What does the sea say to the shore? A BERT based DST style approach for speaker to dialogue attribution in novels,10.18653/v1/2022.acl-long.400,acl,685
2021.nlp4if-1.15,"['Domain-specific NLP', 'Low-resource Languages', 'Classification Applications', 'Model Architectures']","['Transformer Models', 'Misinformation Detection', 'Medical and Clinical NLP', 'NLP for News and Media']",['NLP for Social Media'],"This paper describes the winning model in the Arabic NLP4IF shared task for fighting the COVID-19 infodemic. The goal of the shared task is to check disinformation about COVID-19 in Arabic tweets. Our proposed model has been ranked 1 st with an F1-Score of 0.780 and an Accuracy score of 0.762. A variety of transformer-based pre-trained language models have been experimented with through this study. The best-scored model is an ensemble of AraBERT-Base, Asafya-BERT, and AR-BERT models. One of the study's key findings is showing the effect the pre-processing can have on every model's score. In addition to describing the winning model, the current study shows the error analysis.",https://aclanthology.org/2021.nlp4if-1.15,Association for Computational Linguistics,2021,June,"Proceedings of the Fourth Workshop on NLP for Internet Freedom: Censorship, Disinformation, and Propaganda","Qarqaz, Ahmed  and
Abujaber, Dia  and
Abdullah, Malak",R00 at NLP4IF-2021 Fighting COVID-19 Infodemic with Transformers and More Transformers,10.18653/v1/2021.nlp4if-1.15,nlp4if,314
U17-1009,"['Domain-specific NLP', 'Information Extraction']","['Named Entity Recognition (NER)', 'Medical and Clinical NLP']",['Biomedical NLP'],"We investigate the problem of extracting mentions of medications and adverse drug events using sequence labelling and nonsequence labelling methods. We experiment with three different methods on two different datasets, one from a patient forum with noisy text and one containing narrative patient records. An analysis of the output from these methods are reported to identify what types of named entities are best identified using these methods and, more specifically, how well the discontinuous and overlapping entities that are prevalent in our forum dataset are identified. Our findings can guide studies to choose different methods based on the complexity of the named entities involved, in particular in text mining for pharmacovigilance.",https://aclanthology.org/U17-1009,,2017,December,Proceedings of the Australasian Language Technology Association Workshop 2017,"Dai, Xiang  and
Karimi, Sarvnaz  and
Paris, Cecile",Medication and Adverse Event Extraction from Noisy Text,,U17,338
2020.wac-1.2,"['Information Extraction', 'Evaluation Techniques', 'Multilingual NLP', 'Low-resource Languages']",,,"This article examines extraction methods designed to retain the main text content of web pages and discusses how the extraction could be oriented and evaluated: can and should it be as generic as possible to ensure opportunistic corpus construction? The evaluation grounds on a comparative benchmark of open-source tools used on pages in five different languages Chinese, English, Greek, Polish and Russian, it features several metrics to obtain more fine-grained differentiations. Our experiments highlight the diversity of web page layouts across languages or publishing countries. These discrepancies are reflected by diverging performances so that the right tool has to be chosen accordingly.",https://aclanthology.org/2020.wac-1.2,European Language Resources Association,2020,May,Proceedings of the 12th Web as Corpus Workshop,"Barbaresi, Adrien  and
Lejeune, Ga{\""e}l",Out-of-the-Box and into the Ditch? Multilingual Evaluation of Generic Text Extraction Tools,,wac,778
2021.hackashop-1.11,"['Knowledge Representation and Reasoning', 'Model Architectures']",['Transformer Models'],,"Large pretrained language models using the transformer neural network architecture are becoming a dominant methodology for many natural language processing tasks, such as question answering, text classification, word sense disambiguation, text completion and machine translation. Commonly comprising hundreds of millions of parameters, these models offer state-of-the-art performance, but at the expense of interpretability. The attention mechanism is the main component of transformer networks. We present AttViz, a method for exploration of self-attention in transformer networks, which can help in explanation and debugging of the trained models by showing associations between text tokens in an input sequence. We show that existing deep learning pipelines can be explored with AttViz, which offers novel visualizations of the attention heads and their aggregations. We implemented the proposed methods in an online toolkit and an offline library. Using examples from news analysis, we demonstrate how AttViz can be used to inspect and potentially better understand what a model has learned.",https://aclanthology.org/2021.hackashop-1.11,Association for Computational Linguistics,2021,April,Proceedings of the EACL Hackashop on News Media Content Analysis and Automated Report Generation,"{\v{S}}krlj, Bla{\v{z}}  and
Sheehan, Shane  and
Er{\v{z}}en, Nika  and
Robnik-{\v{S}}ikonja, Marko  and
Luz, Saturnino  and
Pollak, Senja",Exploring Neural Language Models via Analysis of Local and Global Self-Attention Spaces,,hackashop,68
2021.econlp-1.1,"['Domain-specific NLP', 'Classification Applications', 'Data Management and Generation']","['NLP for Finance', 'Data Preparation', 'Sentiment Analysis (SA)']","['Aspect-Based SA (ABSA)', 'Annotation Processes']","In this paper about aspect-based sentiment analysis ABSA, we present the first version of a fine-grained annotated corpus for targetbased opinion analysis TBOA to analyze economic activities or financial markets. We have annotated, at an intra-sentential level, a corpus of sentences extracted from documents representative of financial analysts' most-read materials by considering how financial actors communicate about the evolution of event trends and analyze related publications news, official communications, etc.. Since we focus on identifying the expressions of opinions related to the economy and financial markets, we annotated the sentences that contain at least one subjective expression about a domain-specific term. Candidate sentences for annotations were randomly chosen from texts of specialized press and professional information channels over a period ranging from 1986 to 2021. Our annotation scheme relies on various linguistic markers like domain-specific vocabulary, syntactic structures, and rhetorical relations to explicitly describe the author's subjective stance. We investigated and evaluated the recourse to automatic pre-annotation with existing natural language processing technologies to alleviate the annotation workload. Our aim is to propose a corpus usable on the one hand as training material for the automatic detection of the opinions expressed on an extensive range of domain-specific aspects and on the other hand as a gold standard for evaluation TBOA. In this paper, we present our pre-annotation models and evaluations of their performance, introduce our annotation scheme and report on the main characteristics of our corpus.",https://aclanthology.org/2021.econlp-1.1,Association for Computational Linguistics,2021,November,Proceedings of the Third Workshop on Economics and Natural Language Processing,"Hu, Jiahui  and
Paroubek, Patrick",A Fine-Grained Annotated Corpus for Target-Based Opinion Analysis of Economic and Financial Narratives,10.18653/v1/2021.econlp-1.1,econlp,1139
2020.latechclfl-1.8,"['Information Extraction', 'Image and Video Processing', 'Low-resource Languages']",['Optical Character Recognition (OCR)'],,"We present Vital Records, a demonstrator based on deep-learning approaches to handwritten-text recognition, table processing and information extraction, which enables data from century-old documents to be parsed and analysed, making it possible to explore death records in space and time. This demonstrator provides a user interface for browsing and visualising data extracted from 80,000 handwritten pages of tabular data.",https://aclanthology.org/2020.latechclfl-1.8,International Committee on Computational Linguistics,2020,December,"Proceedings of the The 4th Joint SIGHUM Workshop on Computational Linguistics for Cultural Heritage, Social Sciences, Humanities and Literature","Dejean, Herve  and
Meunier, Jean-Luc",Vital Records: Uncover the past from historical handwritten records,,latechclfl,671
2020.semeval-1.61,"['Commonsense Reasoning', 'Model Architectures', 'Classification Applications']",['Transformer Models'],,"In this paper, we describe our system for Task 4 of SemEval 2020, which involves differentiating between natural language statements that confirm to common sense and those that do not. The organizers propose three subtasks -first, selecting between two sentences, the one which is against common sense. Second, identifying the most crucial reason why a statement does not make sense. Third, generating novel reasons for explaining the against common sense statement. Out of the three subtasks, this paper reports the system description of subtask A and subtask B. This paper proposes a model based on transformer neural network architecture for addressing the subtasks. The novelty in work lies in the architecture design, which handles the logical implication of contradicting statements and simultaneous information extraction from both sentences. We use a parallel instance of transformers, which is responsible for a boost in the performance. We achieved an accuracy of 94.8% in subtask A and 89% in subtask B on the test set.",https://aclanthology.org/2020.semeval-1.61,International Committee for Computational Linguistics,2020,December,Proceedings of the Fourteenth Workshop on Semantic Evaluation,"Dash, Soumya Ranjan  and
Routray, Sandeep  and
Varshney, Prateek  and
Modi, Ashutosh",CS-NET at SemEval-2020 Task 4: Siamese BERT for ComVE,10.18653/v1/2020.semeval-1.61,semeval,148
2021.newsum-1.4,"['Learning Paradigms', 'Automatic Text Summarization', 'Domain-specific NLP']","['Abstractive Text Summarization', 'Reinforcement Learning', 'NLP for News and Media']",,"We consider the problem of topic-focused abstractive summarization, where the goal is to generate an abstractive summary focused on a particular topic, a phrase of one or multiple words. We hypothesize that the task of generating topic-focused summaries can be improved by showing the model what it must not focus on. We introduce a deep reinforcement learning approach to topic-focused abstractive summarization, trained on rewards with a novel negative example baseline. We define the input in this problem as the source text preceded by the topic. We adapt the CNN-Daily Mail and New York Times summarization datasets for this task. We then show through experiments on existing rewards that the use of a negative example baseline can outperform the use of a self-critical baseline, in ROUGE, BERTSCORE, and human evaluation metrics.",https://aclanthology.org/2021.newsum-1.4,Association for Computational Linguistics,2021,November,Proceedings of the Third Workshop on New Frontiers in Summarization,"Mrini, Khalil  and
Liu, Can  and
Dreyer, Markus",Rewards with Negative Examples for Reinforced Topic-Focused Abstractive Summarization,10.18653/v1/2021.newsum-1.4,newsum,550
2020.lrec-1.643,"['Parsing', 'Multilingual NLP', 'Model Architectures', 'Low-resource Languages']",['Syntactic Parsing'],['Dependency Parsing'],"We propose a simple yet accurate method for dependency parsing that treats parsing as tagging PaT. That is, our approach addresses the parsing of dependency trees with a sequence model implemented with a bidirectional LSTM over BERT embeddings, where the ""tag"" to be predicted at each token position is the relative position of the corresponding head. For example, for the sentence John eats cake, the tag to be predicted for the token cake is -1 because its head eats occurs one token to the left. Despite its simplicity, our approach performs well. For example, our approach outperforms the state-of-the-art method of Fernndez-Gonzlez and Gmez-Rodrguez, 2019 on Universal Dependencies UD by 1.76% unlabeled attachment score UAS for English, 1.98% UAS for French, and 1.16% UAS for German. On average, on 15 UD languages, our method with minimal tuning performs comparably with this state-of-the-art approach, being only 0.16% UAS, and 0.82% LAS behind.",https://aclanthology.org/2020.lrec-1.643,European Language Resources Association,2020,May,Proceedings of the 12th Language Resources and Evaluation Conference,"Vacareanu, Robert  and
Gouveia Barbosa, George Caique  and
Valenzuela-Esc{\'a}rcega, Marco A.  and
Surdeanu, Mihai",Parsing as Tagging,,lrec,37
O18-1020,"['Data Management and Generation', 'Low-resource Languages']","['Data Preparation', 'Data Analysis']",,"The goal of this research was to determine the distribution of compounds in Mandarin Chinese from the aspect of semantics. In particular, the focus was on two types of compounds: compounds interpreted by semantic relations or by functional properties between constituents. We collected 880 compounds from a dictionary and categorized them into two types of noun-noun compounds in Mandarin, including relation-based compounds e.g.,  zhngguci ""Chinese food"" and functional property-based compounds e.g.,    liymi ""arched eyebrows"". Finally, the frequency of occurrence of the two types of compounds was determined. The results showed that relation-based compounds occurred much more frequently than property-based compounds in our data 96.1% vs. 3.8%. In addition, it was found that within the relation-based compounds, noun-noun compounds using the FOR relation e.g.,  xnzh ""letter paper"" had the highest rates of occurrence 37.6%, while the CAUSE relation e.g.,  doshng ""wounds by a knife"" had the lowest rates of occurrence 0.7%. On the other hand, the functional property-based compounds, almost always referring to objects in the NATURAL KIND domain, take metaphorical meanings from individual constituents. Our study suggests that the relation-based meanings for interpreting compounds are common in daily conversation, which could be the dominant strategy people use to interpret novel compounds. This research has practical implications for natural language processing in dealing with segmentation of compounds and multiword expressions in Mandarin and even recognition of novel word combinations.",https://aclanthology.org/O18-1020,The Association for Computational Linguistics and Chinese Language Processing (ACLCLP),2018,October,Proceedings of the 30th Conference on Computational Linguistics and Speech Processing ({ROCLING} 2018),"Gong, Shu-Ping  and
Liu, Chih-Hung",On the Semantic Relations and Functional Properties of Noun-Noun Compounds in Mandarin,,O18,679
2020.law-1.7,"['Information Extraction', 'Domain-specific NLP', 'Data Management and Generation']","['Data Preparation', 'Data Analysis']",['Annotation Processes'],"Generating expert ground truth annotations of documents can be a very expensive process. However, such annotations are essential for training domain-specific keyphrase extraction models, especially when utilizing data-intensive deep learning models in unique domains such as real-estate. Therefore, it is critical to optimize the manual annotation process to maximize the quality of the annotations while minimizing the cost of manual labor. To address this need, we explore multiple annotation strategies including self-review and peerreview as well as various methods of resolving annotator disagreements. We evaluate these annotation strategies with respect to their cost and on the task of learning keyphrase extraction models applied with an experimental dataset in the real-estate domain. The results demonstrate that different annotation strategies should be considered depending on specific metrics such as precision and recall. *",https://aclanthology.org/2020.law-1.7,Association for Computational Linguistics,2020,December,Proceedings of the 14th Linguistic Annotation Workshop,"Chau, Hung  and
Balaneshin, Saeid  and
Liu, Kai  and
Linda, Ondrej",Understanding the Tradeoff between Cost and Quality of Expert Annotations for Keyphrase Extraction,,law,1162
2021.adaptnlp-1.21,"['Parsing', 'Embeddings', 'Low-resource Languages', 'Learning Paradigms', 'Cross-lingual Application', 'Multilingual NLP']","['Word Embeddings', 'Unsupervised Learning', 'Syntactic Parsing']",['Dependency Parsing'],"Linear embedding transformation has been shown to be effective for zero-shot crosslingual transfer tasks and achieve surprisingly promising results. However, cross-lingual embedding space mapping is usually studied in static word-level embeddings, where a space transformation is derived by aligning representations of translation pairs that are referred from dictionaries. We move further from this line and investigate a contextual embedding alignment approach which is sense-level and dictionary-free. To enhance the quality of the mapping, we also provide a deep view of properties of contextual embeddings, i.e., the anisotropy problem and its solution. Experiments on zero-shot dependency parsing through the concept-shared space built by our embedding transformation substantially outperform state-of-the-art methods using multilingual embeddings.",https://aclanthology.org/2021.adaptnlp-1.21,Association for Computational Linguistics,2021,April,Proceedings of the Second Workshop on Domain Adaptation for NLP,"Xu, Haoran  and
Koehn, Philipp",Zero-Shot Cross-Lingual Dependency Parsing through Contextual Embedding Transformation,10.48550/arxiv.2103.02212,adaptnlp,239
L18-1094,"['Multilingual NLP', 'Domain-specific NLP', 'Low-resource Languages', 'Knowledge Representation and Reasoning']",['Link Prediction'],,"This paper presents our work dealing with a potential application in e-lexicography: the automatized creation of specialized multilingual dictionaries from structured data, which are available in the form of comparable multilingual classification schemes or taxonomies. As starting examples, we use comparable industry classification schemes, which frequently occur in the context of stock exchanges and business reports. Initially, we planned to follow an approach based on cross-taxonomies and cross-languages string mapping to automatically detect candidate multilingual dictionary entries for this specific domain. However, the need to first transform the comparable classification schemes into a shared formal representation language in order to be able to properly align their components before implementing the algorithms for the multilingual lexicon extraction soon became apparent. We opted for the SKOS-XL vocabulary for modelling the multilingual terminological part of the comparable taxonomies and for OntoLex-Lemon for modelling the multilingual lexical entries which can be extracted from the original data. In this paper, we present the suggested modelling architecture, which demonstrates how terminological elements and lexical items can be formally integrated and explicitly cross-linked in the context of the Linguistic Linked Open Data LLOD.",https://aclanthology.org/L18-1094,European Language Resources Association (ELRA),2018,May,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),"Declerck, Thierry  and
Egorova, Kseniya  and
Schnur, Eileen",An Integrated Formal Representation for Terminological and Lexical Data included in Classification Schemes,,L18,891
2021.nlpmc-1.9,"['Automatic Text Summarization', 'Domain-specific NLP', 'Data Management and Generation', 'Learning Paradigms', 'Model Architectures']","['Medical and Clinical NLP', 'Large Language Models (LLMs)', 'Data Augmentation', 'Data Preparation', 'Few-shot Learning']",,"In medical dialogue summarization, summaries must be coherent and must capture all the medically relevant information in the dialogue. However, learning effective models for summarization require large amounts of labeled data which is especially hard to obtain. We present an algorithm to create synthetic training data with an explicit focus on capturing medically relevant information. We utilize GPT-3 as the backbone of our algorithm and scale 210 human labeled examples to yield results comparable to using 6400 human labeled examples 30x leveraging low-shot learning and an ensemble method. In detailed experiments, we show that this approach produces high quality training data that can further be combined with human labeled data to get summaries that are strongly preferable to those produced by models trained on human data alone both in terms of medical accuracy and coherency.",https://aclanthology.org/2021.nlpmc-1.9,Association for Computational Linguistics,2021,June,Proceedings of the Second Workshop on Natural Language Processing for Medical Conversations,"Chintagunta, Bharath  and
Katariya, Namit  and
Amatriain, Xavier  and
Kannan, Anitha",Medically Aware GPT-3 as a Data Generator for Medical Dialogue Summarization,10.18653/v1/2021.nlpmc-1.9,nlpmc,164
P17-1153,"['Biases in NLP', 'Ethics', 'Data Management and Generation']","['Data Preparation', 'Data Analysis']",,"We examine differences in portrayal of characters in movies using psycholinguistic and graph theoretic measures computed directly from screenplays. Differences are examined with respect to characters' gender, race, age and other metadata. Psycholinguistic metrics are extrapolated to dialogues in movies using a linear regression model built on a set of manually annotated seed words. Interesting patterns are revealed about relationships between genders of production team and the gender ratio of characters. Several correlations are noted between gender, race, age of characters and the linguistic metrics.",https://aclanthology.org/P17-1153,Association for Computational Linguistics,2017,July,Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),"Ramakrishna, Anil  and
Mart{\'\i}nez, Victor R.  and
Malandrakis, Nikolaos  and
Singla, Karan  and
Narayanan, Shrikanth",Linguistic analysis of differences in portrayal of movie characters,10.18653/v1/P17-1153,P17,276
2021.adaptnlp-1.14,"['Learning Paradigms', 'Domain-specific NLP']","['Transfer Learning', 'NLP for News and Media']",['NLP for Social Media'],"Transfer Learning has been shown to be a powerful tool for Natural Language Processing NLP and has outperformed the standard supervised learning paradigm, as it takes benefit from the pre-learned knowledge. Nevertheless, when transfer is performed between less related domains, it brings a negative transfer, i.e. it hurts the transfer performance. In this research, we shed light on the hidden negative transfer occurring when transferring from the News domain to the Tweets domain, through quantitative and qualitative analysis. Our experiments on three NLP tasks: Part-Of-Speech tagging, Chunking and Named Entity recognition reveal interesting insights.",https://aclanthology.org/2021.adaptnlp-1.14,Association for Computational Linguistics,2021,April,Proceedings of the Second Workshop on Domain Adaptation for NLP,"Meftah, Sara  and
Semmar, Nasredine  and
Tamaazousti, Youssef  and
Essafi, Hassane  and
Sadat, Fatiha",On the Hidden Negative Transfer in Sequential Transfer Learning for Domain Adaptation from News to Tweets,,adaptnlp,1051
2020.nlp4musa-1.1,"['Data Management and Generation', 'Classification Applications', 'Model Architectures']","['Data Preparation', 'Recurrent Neural Networks (RNNs)']",['Long Short-Term Memory (LSTM) Models'],"The element-wise attention mechanism has been widely used in modern sequence models for text and music. The original attention mechanism focuses on token-level similarity to determine the attention weights. However, these models have difficulty capturing sequence-level relations in music, including repetition, retrograde, and sequences. In this paper, we introduce a new attention module called the sequential attention SeqAttn, which calculates attention weights based on the similarity between pairs of sub-sequences rather than individual tokens. We show that the module is more powerful at capturing sequence-level music relations than the original design. The module shows potential in both music relation discovery and music generation. 1",https://aclanthology.org/2020.nlp4musa-1.1,Association for Computational Linguistics,2020,16-Oct,Proceedings of the 1st Workshop on NLP for Music and Audio (NLP4MusA),"Jiang, Junyan  and
Xia, Gus  and
Berg-Kirkpatrick, Taylor",Discovering Music Relations with Sequential Attention,,nlp4musa,494
J17-2004,"['Domain-specific NLP', 'Dialogue Systems']",['Medical and Clinical NLP'],,"82% accuracy. We also learn dialogue strategies to avoid confusion in the first place, which is accomplished using a partially observable Markov decision process and which obtains accuracies up to 96.1% that are significantly higher than several baselines. This work represents a major step towards automated dialogue systems for individuals with dementia.",https://aclanthology.org/J17-2004,MIT Press,2017,June,,"Chinaei, Hamidreza  and
Currie, Leila Chan  and
Danks, Andrew  and
Lin, Hubert  and
Mehta, Tejas  and
Rudzicz, Frank",Identifying and Avoiding Confusion in Dialogue with People with Alzheimer's Disease,10.1162/COLI_a_00290,J17,258
W19-3315,"['Classification Applications', 'Model Architectures']",,,"This paper proposes using a Bidirectional LSTM-CRF model in order to identify the tense and aspect of verbs. The information that this classifier outputs can be useful for ordering events and can provide a pre-processing step to improve efficiency of annotating this type of information. This neural network architecture has been successfully employed for other sequential labeling tasks, and we show that it significantly outperforms the rule-based tool TMV-annotator on the Propbank I dataset.",https://aclanthology.org/W19-3315,Association for Computational Linguistics,2019,August,Proceedings of the First International Workshop on Designing Meaning Representations,"Myers, Skatje  and
Palmer, Martha","ClearTAC: Verb Tense, Aspect, and Form Classification Using Neural Nets",10.18653/v1/W19-3315,W19,1316
2021.vardial-1.12,"['Multilingual NLP', 'Data Management and Generation', 'Low-resource Languages', 'Classification Applications']",['Data Augmentation'],,"In this work we compare the performance of convolutional neural networks and shallow models on three out of the four language identification shared tasks proposed in the Var-Dial Evaluation Campaign 2021. In our experiments, convolutional neural networks and shallow models yielded comparable performance in the Romanian Dialect Identification RDI and the Dravidian Language Identification DLI shared tasks, after the training data was augmented, while an ensemble of support vector machines and Nave Bayes models was the best performing model in the Uralic Language Identification ULI task. While the deep learning models did not achieve state-ofthe-art performance at the tasks and tended to overfit the data, the ensemble method was one of two methods that beat the existing baseline for the first track of the ULI shared task. 1",https://aclanthology.org/2021.vardial-1.12,Association for Computational Linguistics,2021,April,"Proceedings of the Eighth Workshop on NLP for Similar Languages, Varieties and Dialects","Ceolin, Andrea",Comparing the Performance of CNNs and Shallow Models for Language Identification,,vardial,629
W19-7813,"['Low-resource Languages', 'Classification Applications']",,,"This paper describes the elaboration of a training corpus containing Hungarian sentences that are labelled according to a syntactic criterion, namely the syntactic role of a very common multifunctional word volt 'was/had'. The labels are assigned by a rule-based algorithm that specifies the function of the target word based on the English pairs of the sentences extracted from a parallel corpus. The reasoning of this idea is that the required syntactic information is easier to retrieve in English than in Hungarian. The accuracy achieved by the algorithm was fair but still needs improvement in order to use the output as reliable training data. The obtained training corpus was tested with FastText's text classifier, the results of which showed that the targeted disambiguation problem is resolvable using neural network based text classification.",https://aclanthology.org/W19-7813,Association for Computational Linguistics,2019,August,"Proceedings of the 18th International Workshop on Treebanks and Linguistic Theories (TLT, SyntaxFest 2019)","D{\""o}m{\""o}t{\""o}r, Andrea",Syntax is clearer on the other side - Using parallel corpus to extract monolingual data,10.18653/v1/W19-7813,W19,1179
Y18-1026,"['Data Management and Generation', 'Low-resource Languages', 'Domain-specific NLP']",['Data Preparation'],['Annotation Processes'],"Personality is the important internal framework that we use when we communicate with the others; thus, it is critical for vehicle-driver communication. Evaluating an individual's personality requires associating behavior and subjectivity. To develop the vehicle-driver communication system that will be most applicable to actual driving settings, a corpus is necessary that includes colloquial and daily experiential expressions along with their emotions, polarity, and sentiments. In addition, the corpus must include a wide range of subjective measures, such as human judgements, perceptions, and cognitions during car driving. Thus, we construct a driving experience corpus DEC that constitutes 253 blog articles 7,831 sentences with the following four manually annotated tags: 1 driving experience DE, 2 other's behavior OB, 3 self-behavior SB and  4  subjectivity SJ. In this paper, we describe the guidelines, corpus specification and agreement analysis between annotators. We identified three difficulties: the extended self, important information, and voice in mind. We conducted automatic annotation experiment on the corpus using Conditional Random Fields CRF. The results indicated F-Scores of .768, .478, .534, and .749 for DE, OB, SB, and SJ, respectively, on the test set. Our error analysis reveals difficulties in interpreting nominatives and recognizing facts.",https://aclanthology.org/Y18-1026,Association for Computational Linguistics,2018,1{--}3 December,"Proceedings of the 32nd Pacific Asia Conference on Language, Information and Computation","Iwai, Ritsuko  and
Kawahara, Daisuke  and
Kumada, Takatsune  and
Kurohashi, Sadao",Annotating a Driving Experience Corpus with Behavior and Subjectivity,10.5715/jnlp.26.329,Y18,178
2020.spnlp-1.4,"['Learning Paradigms', 'Data Management and Generation', 'Information Extraction', 'Model Architectures']",['Data Preparation'],,"Model-complete text comprehension aims at interpreting a natural language text with respect to a semantic domain model describing the classes and their properties relevant for the domain in question. Solving this task can be approached as a structured prediction problem, consisting in inferring the most probable instance of the semantic model given the text. In this work, we focus on the challenging subproblem of cardinality prediction that consists in predicting the number of distinct individuals of each class in the semantic model. We show that cardinality prediction can successfully be approached by modeling the overall task as a joint inference problem, predicting the number of individuals of certain classes while at the same time extracting their properties. We approach this task with probabilistic graphical models computing the maximum-aposteriori instance of the semantic model. Our main contribution lies on the empirical investigation and analysis of different approximative inference strategies based on Gibbs sampling. We present and evaluate our models on the task of extracting key parameters from scientific full text articles describing pre-clinical studies in the domain of spinal cord injury.",https://aclanthology.org/2020.spnlp-1.4,Association for Computational Linguistics,2020,November,Proceedings of the Fourth Workshop on Structured Prediction for NLP,"ter Horst, Hendrik  and
Cimiano, Philipp",Structured Prediction for Joint Class Cardinality and Entity Property Inference in Model-Complete Text Comprehension,10.18653/v1/2020.spnlp-1.4,spnlp,1280
2022.constraint-1.2,"['Parsing', 'Learning Paradigms', 'Classification Applications', 'Image and Video Processing']","['Semantic Parsing', 'Multimodal Learning', 'Hate and Offensive Speech Detection']",['Semantic Role Labeling'],"The memes serve as an important tool in online communication, whereas some hateful memes endanger cyberspace by attacking certain people or subjects. Recent studies address hateful memes detection while further understanding of relationships of entities in memes remains unexplored. This paper presents our work at the Constraint@ACL2022 Shared Task: Hero, Villain and Victim: Dissecting harmful memes for semantic role labelling of entities. In particular, we propose our approach utilizing transformerbased multimodal models through a visual commonsense reasoning VCR method with data augmentation, continual pretraining, loss reweighting, and ensemble learning. We describe the models used, the ways of preprocessing and experiments implementation. As a result, our best model achieves the Macro F1-score of 54.707 on the test set of this shared task 1 .",https://aclanthology.org/2022.constraint-1.2,Association for Computational Linguistics,2022,May,Proceedings of the Workshop on Combating Online Hostile Posts in Regional Languages during Emergency Situations,"Zhou, Ziming  and
Zhao, Han  and
Dong, Jingjing  and
Gao, Jun  and
Liu, Xiaolong",DD-TIG at Constraint@ACL2022: Multimodal Understanding and Reasoning for Role Labeling of Entities in Hateful Memes,10.18653/v1/2022.constraint-1.2,constraint,254
D19-1680,"['Domain-specific NLP', 'Data Management and Generation', 'Information Extraction', 'Knowledge Representation and Reasoning', 'Learning Paradigms', 'Model Architectures']","['Data Preparation', 'Supervised Learning', 'Ontologies']",['Annotation Processes'],"Solving long-lasting problems such as food insecurity requires a comprehensive understanding of interventions applied by governments and international humanitarian assistance organizations, and their results and consequences. Towards achieving this grand goal, a crucial first step is to extract past interventions and when and where they have been applied, from hundreds of thousands of reports automatically. In this paper, we developed a corpus annotated with interventions to foster research, and developed an information extraction system for extracting interventions and their location and time from text. We demonstrate early, very encouraging results on extracting interventions.",https://aclanthology.org/D19-1680,Association for Computational Linguistics,2019,November,Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"Min, Bonan  and
Chan, Yee Seng  and
Qiu, Haoling  and
Fasching, Joshua",Towards Machine Reading for Interventions from Humanitarian-Assistance Program Literature,10.18653/v1/D19-1680,D19,746
2020.ecomnlp-1.4,"['Information Extraction', 'Data Management and Generation', 'Discourse Analysis', 'Argument Mining', 'Information Retrieval', 'Model Architectures', 'Dialogue Systems']","['Information Filtering', 'Data Preparation']",['Recommender Systems'],"We propose a novel way of conversational recommendation, where instead of asking questions to the user to acquire their preferences; the recommender tracks their conversation with other people, including customer support agents CSA, and joins the conversation only when it is time to introduce a recommendation. Building a recommender that joins a human conversation RJC, we propose information extraction, discourse and argumentation analyses, as well as dialogue management techniques to compute a recommendation for a product and service that is needed by the customer, as inferred from the conversation. A special case of such conversations is considered where the customer raises his problem with CSA in an attempt to resolve it, along with receiving a recommendation for a product with features addressing this problem. We evaluate performance of RJC is in a number of human-human and human-chat bot dialogues, and demonstrate that RJC is an efficient and less intrusive way to provide high relevance and persuasive recommendations.",https://aclanthology.org/2020.ecomnlp-1.4,Association for Computational Linguistics,2020,December,Proceedings of Workshop on Natural Language Processing in E-Commerce,"Galitsky, Boris  and
Ilvovsky, Dmitry",Interrupt me Politely: Recommending Products and Services by Joining Human Conversation,,ecomnlp,921
D16-1221,"['Embeddings', 'Domain-specific NLP', 'Data Management and Generation', 'Classification Applications', 'Model Architectures']","['Data Preparation', 'Word Embeddings']",,"We develop a novel embedding-based model for predicting legislative roll-call votes from bill text. The model introduces multidimensional ideal vectors for legislators as an alternative to single dimensional ideal point models for quantitatively analyzing roll-call data. These vectors are learned to correspond with pre-trained word embeddings which allows us to analyze which features in a bill text are most predictive of political support. Our model is quite simple, while at the same time allowing us to successfully predict legislator votes on specific bills with higher accuracy than past methods.",https://aclanthology.org/D16-1221,Association for Computational Linguistics,2016,November,Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing,"Kraft, Peter  and
Jain, Hirsh  and
Rush, Alexander M.",An Embedding Model for Predicting Roll-Call Votes,10.18653/v1/D16-1221,D16,1379
2020.nlpcovid19-acl.12,"['Data Management and Generation', 'Domain-specific NLP', 'Classification Applications', 'Model Architectures']","['Data Preparation', 'NLP for News and Media', 'Recurrent Neural Networks (RNNs)', 'Medical and Clinical NLP']","['NLP for Mental Health', 'NLP for Social Media', 'Long Short-Term Memory (LSTM) Models']","This preliminary analysis uses a deep LSTM neural network with fastText embeddings to predict population rates of depression on Reddit in order to estimate the effect of COVID-19 on mental health. We find that year over year, depression rates on Reddit are up 50% , suggesting a 15-million person increase in the number of depressed Americans and a $7.5 billion increase in depression related spending. This finding comes at a time when uncertainty about the impact of COVID-19 on physical and economic health is still high, and suggests that in addition to those factors, mental health must be considered as well. As data becomes available, further research will be needed to validate the results of this preliminary investigation.",https://aclanthology.org/2020.nlpcovid19-acl.12,Association for Computational Linguistics,2020,July,Proceedings of the 1st Workshop on {NLP} for {COVID-19} at {ACL} 2020,"Wolohan, JT",Estimating the effect of COVID-19 on mental health: Linguistic indicators of depression during a global pandemic,,nlpcovid19,405
2021.blackboxnlp-1.2,"['Parsing', 'Text Preprocessing']","['Part-of-Speech (POS) Tagging', 'Syntactic Parsing']",['Dependency Parsing'],"Previous work on probing word representations for linguistic knowledge has focused on interpolation tasks. In this paper, we instead analyse probes in an extrapolation setting, where the inputs at test time are deliberately chosen to be 'harder' than the training examples. We argue that such an analysis can shed further light on the open question whether probes actually decode linguistic knowledge, or merely learn the diagnostic task from shallow features. To quantify the hardness of an example, we consider scoring functions based on linguistic, statistical, and learning-related criteria, all of which are applicable to a broad range of NLP tasks. We discuss the relative merits of these criteria in the context of two syntactic probing tasks, part-of-speech tagging and syntactic dependency labelling. From our theoretical and experimental analysis, we conclude that distance-based and hard statistical criteria show the clearest differences between interpolation and extrapolation settings, while at the same time being transparent, intuitive, and easy to control.",https://aclanthology.org/2021.blackboxnlp-1.2,Association for Computational Linguistics,2021,November,Proceedings of the Fourth BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP,"Kunz, Jenny  and
Kuhlmann, Marco",Test Harder than You Train: Probing with Extrapolation Splits,10.18653/v1/2021.blackboxnlp-1.2,blackboxnlp,332
D19-1287,"['Data Management and Generation', 'Parsing', 'Low-resource Languages', 'Model Architectures']","['Syntactic Parsing', 'Data Analysis', 'Recurrent Neural Networks (RNNs)']",['Constituency Parsing'],"Neural language models have achieved stateof-the-art performances on many NLP tasks, and recently have been shown to learn a number of hierarchically-sensitive syntactic dependencies between individual words. However, equally important for language processing is the ability to combine words into phrasal constituents, and use constituent-level features to drive downstream expectations. Here we investigate neural models' ability to represent constituent-level features, using coordinated noun phrases as a case study. We assess whether different neural language models trained on English and French represent phrase-level number and gender features, and use those features to drive downstream expectations. Our results suggest that models use a linear combination of NP constituent number to drive CoordNP/verb number agreement. This behavior is highly regular and even sensitive to local syntactic context, however it differs crucially from observed human behavior. Models have less success with gender agreement. Models trained on large corpora perform best, and there is no obvious advantage for models trained using explicit syntactic supervision.",https://aclanthology.org/D19-1287,Association for Computational Linguistics,2019,November,Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"An, Aixiu  and
Qian, Peng  and
Wilcox, Ethan  and
Levy, Roger",Representation of Constituents in Neural Language Models: Coordination Phrase as a Case Study,10.18653/v1/D19-1287,D19,681
2020.conll-1.37,"['Evaluation Techniques', 'Model Architectures']",['Transformer Models'],,"Transformers are being used extensively across several sequence modeling tasks. Significant research effort has been devoted to experimentally probe the inner workings of Transformers. However, our conceptual and theoretical understanding of their power and inherent limitations is still nascent. In particular, the roles of various components in Transformers such as positional encodings, attention heads, residual connections, and feedforward networks, are not clear. In this paper, we take a step towards answering these questions. We analyze the computational power as captured by Turing-completeness. We first provide an alternate and simpler proof to show that vanilla Transformers are Turing-complete and then we prove that Transformers with only positional masking and without any positional encoding are also Turing-complete. We further analyze the necessity of each component for the Turing-completeness of the network; interestingly, we find that a particular type of residual connection is necessary. We demonstrate the practical implications of our results via experiments on machine translation and synthetic tasks.",https://aclanthology.org/2020.conll-1.37,Association for Computational Linguistics,2020,November,Proceedings of the 24th Conference on Computational Natural Language Learning,"Bhattamishra, Satwik  and
Patel, Arkil  and
Goyal, Navin",On the Computational Power of Transformers and Its Implications in Sequence Modeling,10.18653/v1/2020.conll-1.37,conll,686
D18-1459,"['Machine Translation (MT)', 'Low-resource Languages', 'Model Architectures', 'Text Preprocessing']","['Text Segmentation', 'Neural MT (NMT)', 'Recurrent Neural Networks (RNNs)']",['Word Segmentation'],"In this paper, we propose an additionsubtraction twin-gated recurrent network ATR to simplify neural machine translation. The recurrent units of ATR are heavily simplified to have the smallest number of weight matrices among units of all existing gated RNNs. With the simple addition and subtraction operation, we introduce a twin-gated mechanism to build input and forget gates which are highly correlated. Despite this simplification, the essential non-linearities and capability of modeling long-distance dependencies are preserved. Additionally, the proposed ATR is more transparent than LSTM/GRU due to the simplification. Forward self-attention can be easily established in ATR, which makes the proposed network interpretable. Experiments on WMT14 translation tasks demonstrate that ATR-based neural machine translation can yield competitive performance on English-German and English-French language pairs in terms of both translation quality and speed. Further experiments on NIST Chinese-English translation, natural language inference and Chinese word segmentation verify the generality and applicability of ATR on different natural language processing tasks.",https://aclanthology.org/D18-1459,Association for Computational Linguistics,2018,October-November,Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing,"Zhang, Biao  and
Xiong, Deyi  and
Su, Jinsong  and
Lin, Qian  and
Zhang, Huiji",Simplifying Neural Machine Translation with Addition-Subtraction Twin-Gated Recurrent Networks,10.18653/v1/D18-1459,D18,497
2020.lr4sshoc-1.5,"['Domain-specific NLP', 'Information Extraction', 'Low-resource Languages']",['NLP for Finance'],,"For the analysis of historical wage development, no structured data is available. Job advertisements, as found in newspapers can provide insights into what different types of jobs paid, but require language technology to structure in a format conducive to quantitative analysis. In this paper, we report on our experiments to mine wages from 19th century newspaper advertisements and detail the challenges that need to be overcome to perform a socio-economic analysis of textual data sources.",https://aclanthology.org/2020.lr4sshoc-1.5,European Language Resources Association,2020,May,Proceedings of the Workshop about Language Resources for the SSH Cloud,"Ros, Ruben  and
van Erp, Marieke  and
Rijpma, Auke  and
Zijdeman, Richard",Mining Wages in Nineteenth-Century Job Advertisements. The Application of Language Resources and Language Technology to study Economic and Social Inequality,,lr4sshoc,262
2022.deeplo-1.20,"['Data Management and Generation', 'Low-resource Languages', 'Classification Applications', 'Model Architectures']","['Data Preparation', 'Emotion Detection', 'Recurrent Neural Networks (RNNs)']",['Long Short-Term Memory (LSTM) Models'],"Natural Language Processing NLP tasks in non-dominant and low-resource languages have not experienced significant progress. Although pre-trained BERT models are available, GPU-dependency, large memory requirement, and data scarcity often limit their applicability. As a solution, this paper proposes a fusion chain architecture comprised of one or more layers of CNN, LSTM, and BiLSTM and identifies precise configuration and chain length. The study shows that a simpler, CPU-trainable non-BERT fusion CNN + BiLSTM + CNN is sufficient to surpass the textual classification performance of the BERT-related models in resource-limited languages and environments. The fusion architecture competitively approaches the state-of-the-art accuracy in several Bengali NLP tasks and a six-class emotion detection task for a newly developed Bengali dataset. Interestingly, the performance of the identified fusion model, for instance, CNN + BiLSTM + CNN, also holds for other lowresource languages and environments. Efficacy study shows that the CNN + BiLSTM + CNN model outperforms BERT implementation for Vietnamese languages and performs almost equally in English NLP tasks experiencing artificial data scarcity. For the GLUE benchmark and other datasets such as Emotion, IMDB, and Intent classification, the CNN + BiLSTM + CNN model often surpasses or competes with BERT-base, TinyBERT, DistilBERT, and mBERT. Besides, a position-sensitive selfattention layer role further improves the fusion models' performance in the Bengali emotion classification. The models are also compressible to as low as  5 smaller through pruning and retraining, making them more viable for resource-constrained environments. Together, this study may help NLP practitioners and serve as a blueprint for NLP model choices in textual classification for low-resource languages and environments.",https://aclanthology.org/2022.deeplo-1.20,Association for Computational Linguistics,2022,July,Proceedings of the Third Workshop on Deep Learning for Low-Resource Natural Language Processing,"Mustavi Maheen, Syed  and
Rahman Faisal, Moshiur  and
Rafakat Rahman, Md.  and
Karim, Md. Shahriar",Alternative non-BERT model choices for the textual classification in low-resource languages and environments,10.18653/v1/2022.deeplo-1.20,deeplo,211
P19-1157,['Domain-specific NLP'],,,"Training neural sequence-to-sequence models with simple token-level log-likelihood is now a standard approach to historical text normalization, albeit often outperformed by phrasebased models. Policy gradient training enables direct optimization for exact matches, and while the small datasets in historical text normalization are prohibitive of from-scratch reinforcement learning, we show that policy gradient fine-tuning leads to significant improvements across the board. Policy gradient training, in particular, leads to more accurate normalizations for long or unseen words.",https://aclanthology.org/P19-1157,Association for Computational Linguistics,2019,July,Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,"Flachs, Simon  and
Bollmann, Marcel  and
S{\o}gaard, Anders",Historical Text Normalization with Delayed Rewards,10.18653/v1/P19-1157,P19,474
2022.in2writing-1.4,"['Text Generation', 'Error Detection and Correction', 'Low-resource Languages']","['Grammatical Error Correction (GEC)', 'Paraphrase and Rephrase Generation']",,"Low-literate users with intellectual or developmental disabilities IDD and/or complex communication needs CCN require specific writing support. We present a system that interactively supports fast and correct writing of a variant of Leichte Sprache LS; German term for easy-toread German, slightly extended within and beyond the inner-sentential syntactic level. The system provides simple and intuitive dialogues for selecting options from a natural-language paraphrase generator. Moreover, it reminds the user to add text elements enhancing understandability, audience design, and text coherence. In earlier development phases, the system was evaluated with different groups of substitute users. Here, we report a case study with seven low-literate users with IDD. 1 They may be supported by rule-based validation tools for LS, see, e.g., languagetool.org/de/leichtesprache/ or automatic text-simplification cf. Ebling et al., 2022; for English, see, e.g., paperswithcode.com/task/text-simplification",https://aclanthology.org/2022.in2writing-1.4,Association for Computational Linguistics,2022,May,Proceedings of the First Workshop on Intelligent and Interactive Writing Assistants (In2Writing 2022),"Steinmetz, Ina  and
Harbusch, Karin",A text-writing system for Easy-to-Read German evaluated with low-literate users with cognitive impairment,10.18653/v1/2022.in2writing-1.4,in2writing,1387
2020.louhi-1.5,"['Information Extraction', 'Domain-specific NLP', 'Data Management and Generation', 'Low-resource Languages']","['Data Preparation', 'Medical and Clinical NLP']",,"The lack of publicly accessible text corpora is a major obstacle for progress in natural language processing. For medical applications, unfortunately, all language communities other than English are low-resourced. In this work, we present GGPONC German Guideline Program in Oncology NLP Corpus, a freely distributable German language corpus based on clinical practice guidelines for oncology. This corpus is one of the largest ever built from German medical documents. Unlike clinical documents, clinical guidelines do not contain any patient-related information and can therefore be used without data protection restrictions. Moreover, GGPONC is the first corpus for the German language covering diverse conditions in a large medical subfield and provides a variety of metadata, such as literature references and evidence levels. By applying and evaluating existing medical information extraction pipelines for German text, we are able to draw comparisons for the use of medical language to other corpora, medical and non-medical ones.",https://aclanthology.org/2020.louhi-1.5,Association for Computational Linguistics,2020,November,Proceedings of the 11th International Workshop on Health Text Mining and Information Analysis,"Borchert, Florian  and
Lohr, Christina  and
Modersohn, Luise  and
Langer, Thomas  and
Follmann, Markus  and
Sachs, Jan Philipp  and
Hahn, Udo  and
Schapranow, Matthieu-P.",GGPONC: A Corpus of German Medical Text with Rich Metadata Based on Clinical Practice Guidelines,10.18653/v1/2020.louhi-1.5,louhi,303
2020.nlptea-1.9,"['Data Management and Generation', 'Error Detection and Correction', 'Learning Paradigms', 'Low-resource Languages', 'Classification Applications', 'Model Architectures']","['Grammatical Error Correction (GEC)', 'Data Preparation', 'Supervised Learning']",,"This paper introduces our system at NLPTEA2020 shared task for CGED, which is able to detect, locate, identify and correct grammatical errors in Chinese writings. The system consists of three components: GED, GEC, and post processing. GED is an ensemble of multiple BERT-based sequence labeling models for handling GED tasks. GEC performs error correction. We exploit a collection of heterogenous models, including Seq2Seq, GECToR and a candidate generation module to obtain correction candidates. Finally in the post processing stage, results from GED and GEC are fused to form the final outputs. We tune our models to lean towards optimizing precision, which we believe is more crucial in practice. As a result, among the six tracks in the shared task, our system performs well in the correction tracks: measured in F1 score, we rank first, with the highest precision, in the TOP3 correction track and third in the TOP1 correction track, also with the highest precision. Ours are among the top 4 to 6 in other tracks, except for FPR where we rank 12. And our system achieves the highest precisions among the top 10 submissions at IDENTIFICATION and POSITION tracks.",https://aclanthology.org/2020.nlptea-1.9,Association for Computational Linguistics,2020,December,Proceedings of the 6th Workshop on Natural Language Processing Techniques for Educational Applications,"Fang, Meiyuan  and
Fu, Kai  and
Wang, Jiping  and
Liu, Yang  and
Huang, Jin  and
Duan, Yitao",A Hybrid System for NLPTEA-2020 CGED Shared Task,10.18653/v1/2020.nlptea-1.9,nlptea,1212
2020.coling-main.583,"['Data Management and Generation', 'Information Extraction', 'Cross-lingual Application', 'Low-resource Languages']","['Data Preparation', 'Named Entity Recognition (NER)']","['NER for Nested Entities', 'Annotation Processes']","This paper introduces DAN+, a new multi-domain corpus and annotation guidelines for Danish nested named entities NEs and lexical normalization to support research on cross-lingual cross-domain learning for a less-resourced language. We empirically assess three strategies to model the two-layer Named Entity Recognition NER task. We compare transfer capabilities from German versus in-language annotation from scratch. We examine language-specific versus multilingual BERT, and study the effect of lexical normalization on NER. Our results show that 1 the most robust strategy is multi-task learning which is rivaled by multi-label decoding, 2 BERT-based NER models are sensitive to domain shifts, and 3 in-language BERT and lexical normalization are the most beneficial on the least canonical data. Our results also show that an out-of-domain setup remains challenging, while performance on news plateaus quickly. This highlights the importance of cross-domain evaluation of cross-lingual transfer.",https://aclanthology.org/2020.coling-main.583,International Committee on Computational Linguistics,2020,December,Proceedings of the 28th International Conference on Computational Linguistics,"Plank, Barbara  and
Jensen, Kristian N{\o}rgaard  and
van der Goot, Rob",DaN+: Danish Nested Named Entities and Lexical Normalization,10.18653/v1/2020.coling-main.583,coling,832
2020.acl-main.533,"['Model Architectures', 'Audio Generation and Processing', 'Embeddings', 'Machine Translation (MT)']","['Word Embeddings', 'Automatic Speech Recognition (ASR)']",,"Speech translation ST aims to learn transformations from speech in the source language to the text in the target language. Previous works show that multitask learning improves the ST performance, in which the recognition decoder generates the text of the source language, and the translation decoder obtains the final translations based on the output of the recognition decoder. Because whether the output of the recognition decoder has the correct semantics is more critical than its accuracy, we propose to improve the multitask ST model by utilizing word embedding as the intermediate.",https://aclanthology.org/2020.acl-main.533,Association for Computational Linguistics,2020,July,Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,"Chuang, Shun-Po  and
Sung, Tzu-Wei  and
Liu, Alexander H.  and
Lee, Hung-yi","Worse WER, but Better BLEU? Leveraging Word Embedding as Intermediate in Multitask End-to-End Speech Translation",10.18653/v1/2020.acl-main.533,acl,517
L16-1141,['Knowledge Representation and Reasoning'],"['Semantic Web', 'Ontologies']",['Ontology Extension'],"We introduce PreMOn predicate model for ontologies, a linguistic resource for exposing predicate models PropBank, NomBank,  VerbNet, and FrameNet  and mappings between them e.g, SemLink as Linked Open Data. It consists of two components: i the PreMOn Ontology, an extension of the lemon model by the W3C Ontology-Lexica Community Group, that enables to homogeneously represent data from the various predicate models; and, ii the PreMOn Dataset, a collection of RDF datasets integrating various versions of the aforementioned predicate models and mapping resources. PreMOn is freely available and accessible online in different ways, including through a dedicated SPARQL endpoint.",https://aclanthology.org/L16-1141,European Language Resources Association (ELRA),2016,May,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),"Corcoglioniti, Francesco  and
Rospocher, Marco  and
Aprosio, Alessio Palmero  and
Tonelli, Sara",PreMOn: a Lemon Extension for Exposing Predicate Models as Linked Data,,L16,830
2020.figlang-1.29,"['Data Management and Generation', 'Classification Applications', 'Low-resource Languages', 'Model Architectures', 'Figurative Language']","['Recurrent Neural Networks (RNNs)', 'Idiomatic Expressions', 'Data Preparation']","['Long Short-Term Memory (LSTM) Models', 'Annotation Processes']","Supervised disambiguation of verbal idioms VID poses special demands on the quality and quantity of the annotated data used for learning and evaluation. In this paper, we present a new VID corpus for German and perform a series of VID disambiguation experiments on it. Our best classifier, based on a neural architecture, yields an error reduction across VIDs of 57% in terms of accuracy compared to a simple majority baseline.",https://aclanthology.org/2020.figlang-1.29,Association for Computational Linguistics,2020,July,Proceedings of the Second Workshop on Figurative Language Processing,"Ehren, Rafael  and
Lichte, Timm  and
Kallmeyer, Laura  and
Waszczuk, Jakub",Supervised Disambiguation of German Verbal Idioms with a BiLSTM Architecture,10.18653/v1/2020.figlang-1.29,figlang,645
2020.nuse-1.12,"['Data Management and Generation', 'Domain-specific NLP']",['Data Analysis'],,"This paper studies emotion arcs in student narratives. We construct emotion arcs based on event affect and implied sentiments, which correspond to plot elements in the story. We show that student narratives can show elements of plot structure in their emotion arcs and that properties of these arcs can be useful indicators of narrative quality. We build a system and perform analysis to show that our arc-based features are complementary to previously studied sentiment features in this area.",https://aclanthology.org/2020.nuse-1.12,Association for Computational Linguistics,2020,July,"Proceedings of the First Joint Workshop on Narrative Understanding, Storylines, and Events","Somasundaran, Swapna  and
Chen, Xianyang  and
Flor, Michael",Emotion Arcs of Student Narratives,10.18653/v1/2020.nuse-1.12,nuse,847
P19-1398,"['Learning Paradigms', 'Classification Applications']",['Unsupervised Learning'],,"There exist few text-specific methods for unsupervised anomaly detection, and for those that do exist, none utilize pre-trained models for distributed vector representations of words. In this paper we introduce a new anomaly detection method-Context Vector Data Description CVDD-which builds upon word embedding models to learn multiple sentence representations that capture multiple semantic contexts via the self-attention mechanism. Modeling multiple contexts enables us to perform contextual anomaly detection of sentences and phrases with respect to the multiple themes and concepts present in an unlabeled text corpus. These contexts in combination with the self-attention weights make our method highly interpretable. We demonstrate the effectiveness of CVDD quantitatively as well as qualitatively on the wellknown Reuters, 20 Newsgroups, and IMDB Movie Reviews datasets.",https://aclanthology.org/P19-1398,Association for Computational Linguistics,2019,July,Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,"Ruff, Lukas  and
Zemlyanskiy, Yury  and
Vandermeulen, Robert  and
Schnake, Thomas  and
Kloft, Marius","Self-Attentive, Multi-Context One-Class Classification for Unsupervised Anomaly Detection on Text",10.18653/v1/P19-1398,P19,33
2019.gwc-1.25,"['Commonsense Reasoning', 'Data Management and Generation', 'Knowledge Representation and Reasoning']",['Data Analysis'],,"We describe a detailed analysis of a sample of large benchmark of commonsense reasoning problems that has been automatically obtained from WordNet, SUMO and their mapping. The objective is to provide a better assessment of the quality of both the benchmark and the involved knowledge resources for advanced commonsense reasoning tasks. By means of this analysis, we are able to detect some knowledge misalignments, mapping errors and lack of knowledge and resources. Our final objective is the extraction of some guidelines towards a better exploitation of this commonsense knowledge framework by the improvement of the included resources.",https://aclanthology.org/2019.gwc-1.25,Global Wordnet Association,2019,July,Proceedings of the 10th Global Wordnet Conference,"{\'A}lvez, Javier  and
Gonzalez-Dios, Itziar  and
Rigau, German",Commonsense Reasoning Using WordNet and SUMO: a Detailed Analysis,10.48550/arxiv.1909.02314,gwc,655
2022.tacl-1.26,"['Error Detection and Correction', 'Data Management and Generation', 'Low-resource Languages', 'Model Architectures']","['Data Preparation', 'Grammatical Error Correction (GEC)', 'Data Analysis']",['Annotation Processes'],"We introduce a large and diverse Czech corpus annotated for grammatical error correction GEC with the aim to contribute to the still scarce data resources in this domain for languages other than English. The Grammar Error Correction Corpus for Czech GECCC offers a variety of four domains, covering error distributions ranging from high error density essays written by non-native speakers, to website texts, where errors are expected to be much less common. We compare several Czech GEC systems, including several Transformer-based ones, setting a strong baseline to future research. Finally, we meta-evaluate common GEC metrics against human judgments on our data. We make the new Czech GEC corpus publicly available under the CC BY-SA 4.0 license at http://hdl.handle.net/11234 /1-4639.",https://aclanthology.org/2022.tacl-1.26,MIT Press,2022,,,"N{\'a}plava, Jakub  and
Straka, Milan  and
Strakov{\'a}, Jana  and
Rosen, Alexandr",Czech Grammar Error Correction with a Large and Diverse Corpus,10.1162/tacl_a_00470,tacl,1247
N19-1076,"['Multilingual NLP', 'Parsing', 'Low-resource Languages']",['Syntactic Parsing'],['Dependency Parsing'],"We propose a novel transition-based algorithm that straightforwardly parses sentences from left to right by building n attachments, with n being the length of the input sentence. Similarly to the recent stack-pointer parser by Ma et al. 2018 , we use the pointer network framework that, given a word, can directly point to a position from the sentence. However, our left-to-right approach is simpler than the original top-down stack-pointer parser not requiring a stack and reduces transition sequence length in half, from 2n  1 actions to n. This results in a quadratic non-projective parser that runs twice as fast as the original while achieving the best accuracy to date on the English PTB dataset 96.04% UAS, 94.43% LAS among fully-supervised singlemodel dependency parsers, and improves over the former top-down transition system in the majority of languages tested.",https://aclanthology.org/N19-1076,Association for Computational Linguistics,2019,June,"Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)","Fern{\'a}ndez-Gonz{\'a}lez, Daniel  and
G{\'o}mez-Rodr{\'\i}guez, Carlos",Left-to-Right Dependency Parsing with Pointer Networks,10.18653/v1/N19-1076,N19,259
P16-1099,"['Domain-specific NLP', 'Discourse Analysis', 'Data Management and Generation', 'Learning Paradigms', 'Classification Applications']","['Data Preparation', 'Supervised Learning', 'NLP for News and Media']","['Annotation Processes', 'NLP for Social Media']","We construct a humans-in-the-loop supervised learning framework that integrates crowdsourcing feedback and local knowledge to detect job-related tweets from individual and business accounts. Using data-driven ethnography, we examine discourse about work by fusing languagebased analysis with temporal, geospational, and labor statistics information.",https://aclanthology.org/P16-1099,Association for Computational Linguistics,2016,August,Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),"Liu, Tong  and
Homan, Christopher  and
Ovesdotter Alm, Cecilia  and
Lytle, Megan  and
Marie White, Ann  and
Kautz, Henry",Understanding Discourse on Work and Job-Related Well-Being in Public Social Media,10.18653/v1/P16-1099,P16,1406
2020.signlang-1.11,"['Information Retrieval', 'Data Management and Generation', 'Low-resource Languages', 'Image and Video Processing']","['Data Preparation', 'Sign Language and Fingerspelling Recognition']",,"This study presents a new method to search sign language lexica, using a full sign as input for a query. Thus, a dictionary user can look up information about a sign by signing the sign to a webcam. The recorded sign is then compared to potential matching signs in the lexicon. As such, it provides a new way of searching sign language dictionaries to complement existing methods based on spoken language glosses or phonological features, like handshape or location. The ""find the sign"" method analyzes the recorded sign using OpenPose to extract the body and finger joint positions. To compare the recorded sign with the signs in the database, the variation in trajectories of the dominant hand and of the fingers is quantified and compared, using Dynamic Time Warping DTW. The method was tested with ten people with various degrees of sign language proficiency. Each subject viewed a set of 20 out of 100 total signs from the newly compiled Ghanaian Sign Language lexicon and was asked to replicate the signs. The results show that our method can predict the matching sign with 87% and 74% accuracy at the Top-10 and Top-5 ranking level respectively by using only the trajectory of the dominant hand. Additionally, more proficient signers obtain 90% accuracy at the Top-10 ranking. The methodology has the potential to be used also as a variation measurement tool to quantify the difference in signing between different signers or sign languages in general.",https://aclanthology.org/2020.signlang-1.11,European Language Resources Association (ELRA),2020,May,"Proceedings of the LREC2020 9th Workshop on the Representation and Processing of Sign Languages: Sign Language Resources in the Service of the Language Community, Technological Challenges and Application Perspectives","Fragkiadakis, Manolis  and
Nyst, Victoria  and
van der Putten, Peter",Signing as Input for a Dictionary Query: Matching Signs Based on Joint Positions of the Dominant Hand,,signlang,850
N19-1245,"['Data Management and Generation', 'Domain-specific NLP', 'Model Architectures', 'Question Answering (QA)']","['Data Preparation', 'Mathematical QA', 'Recurrent Neural Networks (RNNs)']","['Annotation Processes', 'Long Short-Term Memory (LSTM) Models']","We introduce a large-scale dataset of math word problems and an interpretable neural math problem solver that learns to map problems to operation programs. Due to annotation challenges, current datasets in this domain have been either relatively small in scale or did not offer precise operational annotations over diverse problem types. We introduce a new representation language to model precise operation programs corresponding to each math problem that aim to improve both the performance and the interpretability of the learned models. Using this representation language, our new dataset, MathQA, significantly enhances the AQuA dataset with fully-specified operational programs. We additionally introduce a neural sequence-to-program model enhanced with automatic problem categorization. Our experiments show improvements over competitive baselines in our MathQA as well as the AQuA datasets. The results are still significantly lower than human performance indicating that the dataset poses new challenges for future research. Our dataset is available at: https: //math-qa.github.io/math-QA/.",https://aclanthology.org/N19-1245,Association for Computational Linguistics,2019,June,"Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)","Amini, Aida  and
Gabriel, Saadia  and
Lin, Shanchuan  and
Koncel-Kedziorski, Rik  and
Choi, Yejin  and
Hajishirzi, Hannaneh",MathQA: Towards Interpretable Math Word Problem Solving with Operation-Based Formalisms,10.18653/v1/N19-1245,N19,398
W19-1604,"['Data Management and Generation', 'Image and Video Processing']",['Data Preparation'],,"Human-robot interaction often occurs in the form of instructions given from a human to a robot. For a robot to successfully follow instructions, a common representation of the world and objects in it should be shared between humans and the robot so that the instructions can be grounded. Achieving this representation can be done via learning, where both the world representation and the language grounding are learned simultaneously. However, in robotics this can be a difficult task due to the cost and scarcity of data. In this paper, we tackle the problem by separately learning the world representation of the robot and the language grounding. While this approach can address the challenges in getting sufficient data, it may give rise to inconsistencies between both learned components. Therefore, we further propose Bayesian learning to resolve such inconsistencies between the natural language grounding and a robot's world representation by exploiting spatio-relational information that is implicitly present in instructions given by a human. Moreover, we demonstrate the feasibility of our approach on a scenario involving a robotic arm in the physical world.",https://aclanthology.org/W19-1604,Association for Computational Linguistics,2019,June,Proceedings of the Combined Workshop on Spatial Language Understanding ({S}p{LU}) and Grounded Communication for Robotics ({R}obo{NLP}),"Can, Ozan Arkan  and
Zuidberg Dos Martires, Pedro  and
Persson, Andreas  and
Gaal, Julian  and
Loutfi, Amy  and
De Raedt, Luc  and
Yuret, Deniz  and
Saffiotti, Alessandro",Learning from Implicit Information in Natural Language Instructions for Robotic Manipulations,10.18653/v1/W19-1604,W19,1264
C16-1131,"['Machine Translation (MT)', 'Embeddings', 'Model Architectures']","['Word Embeddings', 'Statistical MT (SMT)', 'Recurrent Neural Networks (RNNs)']",,"Neural network training has been shown to be advantageous in many natural language processing applications, such as language modelling or machine translation. In this paper, we describe in detail a novel domain adaptation mechanism in neural network training. Instead of learning and adapting the neural network on millions of training sentences -which can be very timeconsuming or even infeasible in some cases -we design a domain adaptation gating mechanism which can be used in recurrent neural networks and quickly learn the out-of-domain knowledge directly from the word vector representations with little speed overhead. In our experiments, we use the recurrent neural network language model LM as a case study. We show that the neural LM perplexity can be reduced by 7.395 and 12.011 using the proposed domain adaptation mechanism on the Penn Treebank and News data, respectively. Furthermore, we show that using the domain-adapted neural LM to re-rank the statistical machine translation n-best list on the French-to-English language pair can significantly improve translation quality.",https://aclanthology.org/C16-1131,The COLING 2016 Organizing Committee,2016,December,"Proceedings of {COLING} 2016, the 26th International Conference on Computational Linguistics: Technical Papers","Zhang, Jian  and
Wu, Xiaofeng  and
Way, Andy  and
Liu, Qun",Fast Gated Neural Domain Adaptation: Language Model as a Case Study,,C16,786
2020.alvr-1.4,"['Evaluation Techniques', 'Image and Video Processing', 'Dialogue Systems']",,,"Task success is the standard metric used to evaluate referential visual dialogue systems. In this paper we propose two new metrics that evaluate how each question contributes to the goal. First, we measure how effective each question is by evaluating whether the question discards objects that are not the referent. Second, we define referring questions as those that univocally identify one object in the image. We report the new metrics for human dialogues and for state of the art publicly available models on GuessWhat?!. Regarding our first metric, we find that successful dialogues do not have a higher percentage of effective questions for most models. With respect to the second metric, humans make questions at the end of the dialogue that are referring, confirming their guess before guessing. Human dialogues that use this strategy have a higher task success but models do not seem to learn it.",https://aclanthology.org/2020.alvr-1.4,Association for Computational Linguistics,2020,July,Proceedings of the First Workshop on Advances in Language and Vision Research,"Mazuecos, Mauricio  and
Testoni, Alberto  and
Bernardi, Raffaella  and
Benotti, Luciana",On the role of effective and referring questions in GuessWhat?!,10.18653/v1/2020.alvr-1.4,alvr,923
2021.triton-1.16,"['Domain-specific NLP', 'Low-resource Languages', 'Classification Applications']","['Misinformation Detection', 'NLP for News and Media']","['Fake News Detection', 'NLP for Social Media']","The exponential growth of the internet and social media in the past decade gave way to the increase in dissemination of false or misleading information. Since the 2016 US presidential election, the term ""fake news"" became increasingly popular and this phenomenon has received more attention. In the past years several fact-checking agencies were created, but due to the great number of daily posts on social media, manual checking is insufficient. Currently, there is a pressing need for automatic fake news detection tools, either to assist manual fact-checkers or to operate as standalone tools. There are several projects underway on this topic, but most of them focus on English. This research-in-progress paper discusses the employment of deep learning methods, and the development of a tool, for detecting false news in Portuguese. As a first step we shall compare well-established architectures that were tested in other languages and analyse their performance on our Portuguese data. Based on the preliminary results of these classifiers, we shall choose a deep learning model or combine several deep learning models which hold promise to enhance the performance of our fake news detection system.",https://aclanthology.org/2021.triton-1.16,INCOMA Ltd.,2021,July,Proceedings of the Translation and Interpreting Technology Online Conference,"Venturott, L{\'\i}gia  and
Mitkov, Ruslan",Fake News Detection for Portuguese with Deep Learning,10.26615/978-954-452-071-7_016,triton,945
2021.law-1.9,"['Parsing', 'Data Management and Generation', 'Low-resource Languages', 'Cross-lingual Application']","['Syntactic Parsing', 'Data Preparation', 'Data Analysis']","['Dependency Parsing', 'Annotation Processes']","In this paper, we present a first attempt at enriching German Universal Dependencies UD treebanks with enhanced dependencies. Similarly to the converter for English Schuster and Manning, 2016 , we develop a rule-based system for deriving enhanced dependencies from the basic layer, covering three linguistic phenomena: relative clauses, coordination, and raising/control. For quality control, we manually correct or validate a set of 196 sentences, finding that around 90% of added relations are correct. Our data analysis reveals that difficulties arise mainly due to inconsistencies in the basic layer annotations. We show that the English system is in general applicable to German data, but that adapting to the particularities of the German treebanks and language increases precision and recall by up to 10%. Comparing the application of our converter on gold standard dependencies vs. automatic parses, we find that F1 drops by around 10% in the latter setting due to error propagation. Finally, an enhanced UD parser trained on a converted treebank performs poorly when evaluated against our annotations, indicating that more work remains to be done to create gold standard enhanced German treebanks.",https://aclanthology.org/2021.law-1.9,Association for Computational Linguistics,2021,November,Proceedings of The Joint 15th Linguistic Annotation Workshop (LAW) and 3rd Designing Meaning Representations (DMR) Workshop,"B{\""u}rkle, Teresa  and
Gr{\""u}newald, Stefan  and
Friedrich, Annemarie",A Corpus Study of Creating Rule-Based Enhanced Universal Dependencies for German,10.18653/v1/2021.law-1.9,law,126
2019.iwslt-1.30,"['Low-resource Languages', 'Model Architectures', 'Text Preprocessing', 'Machine Translation (MT)']","['Neural MT (NMT)', 'Text Segmentation']",['Word Segmentation'],"One of the basic tasks of computational language documentation CLD is to identify word boundaries in an unsegmented phonemic stream. While several unsupervised monolingual word segmentation algorithms exist in the literature, they are challenged in real-world CLD settings by the small amount of available data. A possible remedy is to take advantage of glosses or translation in a foreign, wellresourced, language, which often exist for such data. In this paper, we explore and compare ways to exploit neural machine translation models to perform unsupervised boundary detection with bilingual information, notably introducing a new loss function for jointly learning alignment and segmentation. We experiment with an actual under-resourced language, Mboshi, and show that these techniques can effectively control the output segmentation length.",https://aclanthology.org/2019.iwslt-1.30,Association for Computational Linguistics,2019,November 2-3,Proceedings of the 16th International Conference on Spoken Language Translation,"Godard, Pierre  and
Besacier, Laurent  and
Yvon, Fran{\c{c}}ois",Controlling Utterance Length in NMT-based Word Segmentation with Attention,10.48550/arxiv.1910.08418,iwslt,1270
W18-3025,"['Embeddings', 'Machine Translation (MT)', 'Image and Video Processing', 'Low-resource Languages', 'Model Architectures']",['Neural MT (NMT)'],,"In this study, we compare token representations constructed from visual features i.e., pixels with standard lookup-based embeddings. Our goal is to gain insight about the challenges of encoding a text representation from low-level features, e.g. from characters or pixels. We focus on Chinese, which-as a logographic language-has properties that make a representation via visual features challenging and interesting. To train and evaluate different models for the token representation, we chose the task of character-based neural machine translation NMT from Chinese to English. We found that a token representation computed only from visual features can achieve competitive results to lookup embeddings. However, we also show different strengths and weaknesses in the models' performance in a part-ofspeech tagging task and also a semantic similarity task. In summary, we show that it is possible to achieve a text representation only from pixels. We hope that this is a useful stepping stone for future studies that exclusively rely on visual input, or aim at exploiting visual features of written language.",https://aclanthology.org/W18-3025,Association for Computational Linguistics,2018,July,Proceedings of The Third Workshop on Representation Learning for {NLP},"Broscheit, Samuel",Learning Distributional Token Representations from Visual Features,10.18653/v1/W18-3025,W18,1223
2020.alvr-1.3,"['Domain-specific NLP', 'Data Management and Generation', 'Learning Paradigms', 'Text Generation', 'Image and Video Processing', 'Model Architectures']","['Supervised Learning', 'Data Augmentation', 'Medical and Clinical NLP']",,"Visual Question Generation VQG, the task of generating a question based on image contents, is an increasingly important area that combines natural language processing and computer vision. Although there are some recent works that have attempted to generate questions from images in the open domain, the task of VQG in the medical domain has not been explored so far. In this paper, we introduce an approach to generation of visual questions about radiology images called VQGR, i.e. an algorithm that is able to ask a question when shown an image. VQGR first generates new training data from the existing examples, based on contextual word embeddings and image augmentation techniques. It then uses the variational auto-encoders model to encode images into a latent space and decode natural language questions. Experimental automatic evaluations performed on the VQA-RAD dataset of clinical visual questions show that VQGR achieves good performances compared with the baseline system. The source code is available at https://github.com/ sarrouti/vqgr.",https://aclanthology.org/2020.alvr-1.3,Association for Computational Linguistics,2020,July,Proceedings of the First Workshop on Advances in Language and Vision Research,"Sarrouti, Mourad  and
Ben Abacha, Asma  and
Demner-Fushman, Dina",Visual Question Generation from Radiology Images,10.18653/v1/2020.alvr-1.3,alvr,718
2020.mwe-1.9,"['Text Generation', 'Embeddings']",,,"In this talk I present Generationary, an approach that goes beyond the mainstream assumption that word senses can be represented as discrete items of a predefined inventory, and put forward a unified model which produces contextualized definitions for arbitrary lexical items, from words to phrases and even sentences. Generationary employs a novel span-based encoding scheme to fine-tune an English pre-trained Encoder-Decoder system and generate new definitions. Our model outperforms previous approaches in the generative task of Definition Modeling in many settings, but it also matches or surpasses the state of the art in discriminative tasks such as Word Sense Disambiguation and Word-in-Context. I also show that Generationary benefits from training on definitions from multiple inventories, with strong gains across benchmarks, including a novel dataset of definitions for free adjective-noun phrases, and discuss interesting examples of generated definitions.",https://aclanthology.org/2020.mwe-1.9,Association for Computational Linguistics,2020,December,Proceedings of the Joint Workshop on Multiword Expressions and Electronic Lexicons,"Navigli, Roberto",Invited Talk: Generationary or: ``How We Went beyond Sense Inventories and Learned to Gloss'',,mwe,569
2021.gem-1.1,"['Text Generation', 'Evaluation Techniques']",['Text Simplification'],,"Sentence-level text simplification is evaluated using both automated metrics and human evaluation. For automatic evaluation, a combination of metrics is usually employed to evaluate different aspects of the simplification. Flesch-Kincaid Grade Level FKGL is one metric that has been regularly used to measure the readability of system output. In this paper, we argue that FKGL should not be used to evaluate text simplification systems. We provide experimental analyses on recent system output showing that the FKGL score can easily be manipulated to improve the score dramatically with only minor impact on other automated metrics BLEU and SARI. Instead of using FKGL, we suggest that the component statistics, along with others, be used for posthoc analysis to understand system behavior.",https://aclanthology.org/2021.gem-1.1,Association for Computational Linguistics,2021,August,"Proceedings of the 1st Workshop on Natural Language Generation, Evaluation, and Metrics (GEM 2021)","Tanprasert, Teerapaun  and
Kauchak, David",Flesch-Kincaid is Not a Text Simplification Evaluation Metric,10.18653/v1/2021.gem-1.1,gem,766
2022.slpat-1.9,"['Dialogue Systems', 'Multi-agent Communication Systems']","['Intelligent Agents', 'Response Generation']",,"Conversational assistants are ubiquitous among the general population, however, these systems have not had an impact on people with disabilities, or speech and language disorders, for whom basic day-to-day communication and social interaction is a huge struggle. Language model technology can play a huge role in empowering these users and help them interact with others with less effort via interaction support. To enable this population, we build a system that can represent them in a social conversation and generate responses that can be controlled by the users using cues/keywords. For an ongoing conversation, this system can suggest responses that a user can choose. We also build models that can speed up this communication by suggesting relevant cues in the dialog response context. We introduce a keyword-loss to lexically constrain the model response output. We present automatic and human evaluation of our cue/keyword predictor and the controllable dialog system to show that our models perform significantly better than models without control. Our evaluation and user study shows that keyword-control on end-to-end response generation models is powerful and can enable and empower users with degenerative disorders to carry out their day-to-day communication.",https://aclanthology.org/2022.slpat-1.9,Association for Computational Linguistics,2022,May,Ninth Workshop on Speech and Language Processing for Assistive Technologies (SLPAT-2022),"H. Kumar, Shachi  and
Su, Hsuan  and
Manuvinakurike, Ramesh  and
Pinaroc, Max  and
Prasad, Sai  and
Sahay, Saurav  and
Nachman, Lama",CueBot: Cue-Controlled Response Generation for Assistive Interaction Usages,10.18653/v1/2022.slpat-1.9,slpat,142
2020.lrec-1.161,"['Data Management and Generation', 'Classification Applications', 'Low-resource Languages']","['Data Preparation', 'Misinformation Detection', 'Data Analysis']",,"Fact-checking information before publication has long been a core task for journalists, but recent times have seen the emergence of dedicated news items specifically aimed at fact-checking after publication. This relatively new form of fact-checking receives a fair amount of attention from academics, with current research focusing mostly on journalists' motivations for publishing post-hoc factchecks, the effects of fact-checking on the perceived accuracy of false claims, and the creation of computational tools for automatic factchecking. In this paper, we propose to study fact-checks from a corpus linguistic perspective. This will enable us to gain insight in the scope and contents of fact-checks, to investigate what fact-checks can teach us about the way in which science appears incorrectly in the news, and to see how fact-checks behave in the science communication landscape. We report on the creation of FactCorp, a 1,16 million-word corpus containing 1,974 fact-checks from three major Dutch newspapers. We also present results of several exploratory analyses, including a rhetorical moves analysis, a qualitative content elements analysis, and keyword analyses. Through these analyses, we aim to demonstrate the wealth of possible applications that FactCorp allows, thereby stressing the importance of creating such resources.",https://aclanthology.org/2020.lrec-1.161,European Language Resources Association,2020,May,Proceedings of the 12th Language Resources and Evaluation Conference,"van der Meulen, Marten  and
Reijnierse, W. Gudrun",FactCorp: A Corpus of Dutch Fact-checks and its Multiple Usages,,lrec,1070
2021.adaptnlp-1.24,"['Data Management and Generation', 'Learning Paradigms', 'Automatic Text Summarization', 'Model Architectures']","['Few-shot Learning', 'Recurrent Neural Networks (RNNs)']",['Long Short-Term Memory (LSTM) Models'],"Interleaved texts, where posts belonging to different threads occur in a sequence, commonly occur in online chat posts, so that it can be time-consuming to quickly obtain an overview of the discussions. Existing systems first disentangle the posts by threads and then extract summaries from those threads. A major issue with such systems is error propagation from the disentanglement component. While endto-end trainable summarization system could obviate explicit disentanglement, such systems require a large amount of labeled data. To address this, we propose to pretrain an endto-end trainable hierarchical encoder-decoder system using synthetic interleaved texts. We show that by fine-tuning on a real-world meeting dataset AMI, such a system out-performs a traditional two-step system by 22%. We also compare against transformer models and observed that pretraining with synthetic data both the encoder and decoder outperforms the BertSumExtAbs transformer model which pretrains only the encoder on a large dataset.",https://aclanthology.org/2021.adaptnlp-1.24,Association for Computational Linguistics,2021,April,Proceedings of the Second Workshop on Domain Adaptation for NLP,"Karn, Sanjeev Kumar  and
Chen, Francine  and
Chen, Yan-Ying  and
Waltinger, Ulli  and
Sch{\""u}tze, Hinrich",Few-Shot Learning of an Interleaved Text Summarization Model by Pretraining with Synthetic Data,10.48550/arxiv.2103.05131,adaptnlp,1295
E17-1106,"['Data Management and Generation', 'Classification Applications', 'Discourse Analysis', 'Domain-specific NLP']","['Data Preparation', 'Medical and Clinical NLP']","['NLP for Mental Health', 'Annotation Processes']","As the number of people receiving psychotherapeutic treatment increases, the automatic evaluation of counseling practice arises as an important challenge in the clinical domain. In this paper, we address the automatic evaluation of counseling performance by analyzing counselors' language during their interaction with clients. In particular, we present a model towards the automation of Motivational Interviewing MI coding, which is the current gold standard to evaluate MI counseling. First, we build a dataset of hand labeled MI encounters; second, we use text-based methods to extract and analyze linguistic patterns associated with counselor behaviors; and third, we develop an automatic system to predict these behaviors. We introduce a new set of features based on semantic information and syntactic patterns, and show that they lead to accuracy figures of up to 90%, which represent a significant improvement with respect to features used in the past.",https://aclanthology.org/E17-1106,Association for Computational Linguistics,2017,April,"Proceedings of the 15th Conference of the {E}uropean Chapter of the Association for Computational Linguistics: Volume 1, Long Papers","P{\'e}rez-Rosas, Ver{\'o}nica  and
Mihalcea, Rada  and
Resnicow, Kenneth  and
Singh, Satinder  and
An, Lawrence  and
Goggin, Kathy J.  and
Catley, Delwyn",Predicting Counselor Behaviors in Motivational Interviewing Encounters,10.18653/v1/e17-1106,E17,119
2020.ecnlp-1.12,"['Domain-specific NLP', 'Classification Applications', 'Data Management and Generation']","['NLP for the Legal Domain', 'Data Preparation', 'Sentiment Analysis (SA)']",,"In recent years, the focus of e-Commerce research has been on better understanding the relationship between the internet marketplace, customers, and goods and services. This has been done by examining information that can be gleaned from consumer information, recommender systems, click rates, or the way purchasers go about making buying decisions, for example. This paper takes a very different approach and examines the companies themselves. In the past ten years, e-Commerce giants such as Amazon, Skymall, Wayfair, and Groupon have been embroiled in class action security lawsuits promulgated under Rule 10b5, which, in short, is one of the Securities and Exchange Commission's main rules surrounding fraud. Lawsuits are extremely expensive to the company and can damage a company's brand extensively, with the shareholders left to suffer the consequences. We examined the Management Discussion and Analysis and the Market Risks for 96 companies using sentiment analysis on selected financial measures and found that we were able to predict the outcome of the lawsuits in our dataset using sentiment tone alone to a recall of 0.8207 using the Random Forest classifier. We believe that this is an important contribution as it has cross-domain implications and potential, and opens up new areas of research in e-Commerce, finance, and law, as the settlements from the class action lawsuits in our dataset alone are in excess of $1.6 billion dollars, in aggregate.",https://aclanthology.org/2020.ecnlp-1.12,Association for Computational Linguistics,2020,July,Proceedings of The 3rd Workshop on e-Commerce and NLP,"Taylor, Stacey  and
Keselj, Vlado",e-Commerce and Sentiment Analysis: Predicting Outcomes of Class Action Lawsuits,10.18653/v1/2020.ecnlp-1.12,ecnlp,1030
2021.dravidianlangtech-1.42,"['Domain-specific NLP', 'Multilingual NLP', 'Low-resource Languages', 'Classification Applications', 'Model Architectures']","['Hate and Offensive Speech Detection', 'NLP for News and Media']",['NLP for Social Media'],"With the advent of social media, we have seen a proliferation of data and public discourse. Unfortunately, this includes offensive content as well. The problem is exacerbated due to the sheer number of languages spoken in these platforms and the multiple other modalities used for sharing offensive content images, gifs, videos and more. In this paper, we propose a multilingual ensemble based model that can identify offensive content targeted against an individual or group in low resource Dravidian language. Our model is able to handle code-mixed data as well as instances where the script used is mixed for instance, Tamil and Latin. Our solution ranked number one for Malayalam dataset and ranked 4th and 5th for Tamil and Kannada, respectively. The code is available at github.com/Debapriya-Tula/ EACL2021-DravidianTask-Bitions.",https://aclanthology.org/2021.dravidianlangtech-1.42,Association for Computational Linguistics,2021,April,Proceedings of the First Workshop on Speech and Language Technologies for Dravidian Languages,"Tula, Debapriya  and
Potluri, Prathyush  and
Ms, Shreyas  and
Doddapaneni, Sumanth  and
Sahu, Pranjal  and
Sukumaran, Rohan  and
Patwa, Parth",Bitions@DravidianLangTech-EACL2021: Ensemble of Multilingual Language Models with Pseudo Labeling for offence Detection in Dravidian Languages,,dravidianlangtech,430
2021.iwpt-1.14,"['Parsing', 'Data Management and Generation', 'Multilingual NLP']",['Data Preparation'],,"Many neural end-to-end systems today do not rely on syntactic parse trees, as much of the information that parse trees provide is encoded in the parameters of pretrained models. Lessons learned from parsing technologies and from taking a multilingual perspective, however, are still relevant even for end-to-end models. This talk will describe work that relies on compositionality in semantic parsing and in reading comprehension requiring numerical reasoning. We'll then describe a new dataset that requires advances in multilingual modeling, and some approaches designed to better model morphology than off-the-shelf subword models that make some progress on these challenges.",https://aclanthology.org/2021.iwpt-1.14,Association for Computational Linguistics,2021,August,Proceedings of the 17th International Conference on Parsing Technologies and the IWPT 2021 Shared Task on Parsing into Enhanced Universal Dependencies (IWPT 2021),"Pitler, Emily",Incorporating Compositionality and Morphology into End-to-End Models,10.18653/v1/2021.iwpt-1.14,iwpt,726
2021.findings-acl.404,"['Evaluation Techniques', 'Data Management and Generation', 'Model Architectures', 'Commonsense Reasoning']","['Large Language Models (LLMs)', 'Data Preparation']",,"We present a new probing dataset named PROST: Physical Reasoning about Objects Through Space and Time. This dataset contains 18,736 multiple-choice questions made from 14 manually curated templates, covering 10 physical reasoning concepts. All questions are designed to probe both causal and masked language models in a zero-shot setting. We conduct an extensive analysis which demonstrates that state-of-the-art pretrained models are inadequate at physical reasoning: they are influenced by the order in which answer options are presented to them, they struggle when the superlative in a question is inverted e.g., most  least, and increasing the amount of pretraining data and parameters only yields minimal improvements. These results provide support for the hypothesis that current pretrained models' ability to reason about physical interactions is inherently limited by a lack of real world experience. By highlighting these limitations, we hope to motivate the development of models with a human-like understanding of the physical world. * *Email has no accent, but includes the hyphen A person drops a glass, a pillow, a coin, and a pen from a balcony. The MASK is most likely to break.",https://aclanthology.org/2021.findings-acl.404,Association for Computational Linguistics,2021,August,Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021,"Aroca-Ouellette, St{\'e}phane  and
Paik, Cory  and
Roncone, Alessandro  and
Kann, Katharina",PROST: Physical Reasoning about Objects through Space and Time,10.18653/v1/2021.findings-acl.404,findings,1300
O17-2003,"['Information Extraction', 'Domain-specific NLP', 'Low-resource Languages', 'Knowledge Representation and Reasoning']",,,"With the progress of e-commerce and web technology, a large volume of consumer reviews for products are generated from time to time, which contain rich information regarding consumer requirements and preferences. Although China has the largest e-commerce market in the world, but few of researchers investigated how to extract product feature from Chinese consumer reviews effectively, not to analyze the relations among product features which are very significant to implement comprehensive applications. In this research, a framework is proposed to extract product features from Chinese consumer reviews and construct product feature structure tree. Through three filtering algorithms and two-stage optimizing word segmantation process, phrases are identified from consumer reviews. And the expanded rule template, which consists of elements: phrase, POS, dependency relation, governing word, and opinion, is constructed to train the model of conditional random filed CRF. Then the product features are extracted based on CRF. Besides, two index are defined to describe product feature quantitatively such as frequency and sentiment score. Based on these, product feature structure tree is established through a potential parent node searching process. Furthermore, categories of extensive experiments are conducted based on 5,806 experimental corpuses from taobao.com, suning.com, and zhongguancun.com. The results from these experiments provide evidences to guide product feature extraction process. Finally, an application of analyzing the influences among product features is conducted based on product feature structure tree. It provides valuable management connotations for designer, manufacturer, or retailer.",https://aclanthology.org/O17-2003,,2017,June,"International Journal of Computational Linguistics {\&} {C}hinese Language Processing, Volume 22, Number 1, June 2017","Xu, Xinsheng  and
Lin, Jing  and
Xiao, Ying  and
Yu, Jianzhe",An Approach to Extract Product Features from Chinese Consumer Reviews and Establish Product Feature Structure Tree,,O17,871
2021.woah-1.7,"['Domain-specific NLP', 'Low-resource Languages', 'Classification Applications', 'Data Management and Generation']","['Data Preparation', 'Hate and Offensive Speech Detection', 'NLP for News and Media']",['NLP for Social Media'],"Social media texts such as blog posts, comments, and tweets often contain offensive languages including racial hate speech comments, personal attacks, and sexual harassments. Detecting inappropriate use of language is, therefore, of utmost importance for the safety of the users as well as for suppressing hateful conduct and aggression. Existing approaches to this problem are mostly available for resource-rich languages such as English and German. In this paper, we characterize the offensive language in Nepali, a low-resource language, highlighting the challenges that need to be addressed for processing Nepali social media text. We also present experiments for detecting offensive language using supervised machine learning. Besides contributing the first baseline approaches of detecting offensive language in Nepali, we also release human annotated data sets to encourage future research on this crucial topic.",https://aclanthology.org/2021.woah-1.7,Association for Computational Linguistics,2021,August,Proceedings of the 5th Workshop on Online Abuse and Harms (WOAH 2021),"Niraula, Nobal B.  and
Dulal, Saurab  and
Koirala, Diwa",Offensive Language Detection in Nepali Social Media,10.18653/v1/2021.woah-1.7,woah,1359
2020.cllrd-1.5,"['Data Management and Generation', 'Knowledge Representation and Reasoning']","['Data Preparation', 'Abstract Meaning Representation (AMR)']",['Annotation Processes'],"Meaning Representations AMRs, a syntax-free representation of phrase semantics Banarescu et al., 2013 , are useful for capturing the meaning of a phrase and reflecting the relationship between concepts that are referred to. However, annotating AMRs is time consuming and expensive. The existing annotation process requires expertly trained workers who have knowledge of an extensive set of guidelines for parsing phrases. In this paper, we propose a cost-saving two-step process for the creation of a corpus of AMR-phrase pairs for spatial referring expressions. The first step uses non-specialists to perform simple annotations that can be leveraged in the second step to accelerate the annotation performed by the experts. We hypothesize that our process will decrease the cost per annotation and improve consistency across annotators. Few corpora of spatial referring expressions exist and the resulting language resource will be valuable for referring expression comprehension and generation modeling.",https://aclanthology.org/2020.cllrd-1.5,European Language Resources Association,2020,May,Proceedings of the LREC 2020 Workshop on ``Citizen Linguistics in Language Resource Development'',"Martin, Mary  and
Mauceri, Cecilia  and
Palmer, Martha  and
Heckman, Christoffer",Leveraging Non-Specialists for Accurate and Time Efficient AMR Annotation,,cllrd,1294
J16-3005,"['Low-resource Languages', 'Classification Applications']",,,"To gain additional insights about the string kernels approach, the features selected by the classifier as being more discriminating are analyzed in this work. The analysis also offers information about localized language transfer effects, since the features used by the proposed model are p-grams of various lengths. The features captured by the model typically include stems, function words, and word prefixes and suffixes, which have the potential to generalize over purely word-based features. By analyzing the discriminating features, this article offers insights into two kinds of language transfer effects, namely, word choice lexical transfer and morphological differences. The goal of the current study is to give a full view of the string kernels approach and shed some light on why this approach works so well.",https://aclanthology.org/J16-3005,MIT Press,2016,September,,"Ionescu, Radu Tudor  and
Popescu, Marius  and
Cahill, Aoife",String Kernels for Native Language Identification: Insights from Behind the Curtains,10.1162/COLI_a_00256,J16,1310
2022.wordplay-1.1,"['Data Management and Generation', 'Multi-agent Communication Systems', 'Knowledge Representation and Reasoning']","['Intelligent Agents', 'Data Analysis']",,"Text Worlds are virtual environments for embodied agents that, unlike 2D or 3D environments, are rendered exclusively using textual descriptions. These environments offer an alternative to higher-fidelity 3D environments due to their low barrier to entry, providing the ability to study semantics, compositional inference, and other high-level tasks with rich action spaces while controlling for perceptual input. This systematic survey outlines recent developments in tooling, environments, and agent modeling for Text Worlds, while examining recent trends in knowledge graphs, common sense reasoning, transfer learning of Text World performance to higher-fidelity environments, as well as near-term development targets that, once achieved, make Text Worlds an attractive general research paradigm for natural language processing.",https://aclanthology.org/2022.wordplay-1.1,Association for Computational Linguistics,2022,July,Proceedings of the 3rd Wordplay: When Language Meets Games Workshop (Wordplay 2022),"Jansen, Peter",A Systematic Survey of Text Worlds as Embodied Natural Language Environments,10.18653/v1/2022.wordplay-1.1,wordplay,416
E17-1098,['Finite State Machines'],,,"Weighted finite automata and transducers including hidden Markov models and conditional random fields are widely used in natural language processing NLP to perform tasks such as morphological analysis, part-of-speech tagging, chunking, named entity recognition, speech recognition, and others. Parallelizing finite state algorithms on graphics processing units GPUs would benefit many areas of NLP. Although researchers have implemented GPU versions of basic graph algorithms, limited previous work, to our knowledge, has been done on GPU algorithms for weighted finite automata. We introduce a GPU implementation of the Viterbi and forward-backward algorithm, achieving decoding speedups of up to 5.2x over our serial implementation running on different computer architectures and 6093x over OpenFST.",https://aclanthology.org/E17-1098,Association for Computational Linguistics,2017,April,"Proceedings of the 15th Conference of the {E}uropean Chapter of the Association for Computational Linguistics: Volume 1, Long Papers","Argueta, Arturo  and
Chiang, David",Decoding with Finite-State Transducers on GPUs,10.18653/v1/e17-1098,E17,1303
2021.fever-1.3,"['Data Management and Generation', 'Information Extraction', 'Classification Applications', 'Model Architectures']","['Claim Verification', 'Graph Neural Networks (GNNs)', 'Data Analysis']",,"This paper presents an end-to-end system for fact extraction and verification using textual and tabular evidence, the performance of which we demonstrate on the FEVEROUS dataset. We experiment with both a multi-task learning paradigm to jointly train a graph attention network for both the task of evidence extraction and veracity prediction, as well as a single objective graph model for solely learning veracity prediction and separate evidence extraction. In both instances, we employ a framework for per-cell linearization of tabular evidence, thus allowing us to treat evidence from tables as sequences. The templates we employ for linearizing tables capture the context as well as the content of table data. We furthermore provide a case study to show the interpretability our approach. Our best performing system achieves a FEVEROUS score of 0.23 and 53% label accuracy on the blind test data. 1 * Work done while the author was an intern at J.P. Morgan AI Research.",https://aclanthology.org/2021.fever-1.3,Association for Computational Linguistics,2021,November,Proceedings of the Fourth Workshop on Fact Extraction and VERification (FEVER),"Kotonya, Neema  and
Spooner, Thomas  and
Magazzeni, Daniele  and
Toni, Francesca",Graph Reasoning with Context-Aware Linearization for Interpretable Fact Extraction and Verification,10.18653/v1/2021.fever-1.3,fever,188
W19-3110,"['Text Preprocessing', 'Parsing', 'Data Management and Generation', 'Low-resource Languages', 'Finite State Machines']","['Morphological Parsing', 'Part-of-Speech (POS) Tagging', 'Data Preparation']",,"We present a broad coverage model of Turkish morphology and an open-source morphological analyzer that implements it. The model captures intricacies of Turkish morphologysyntax interface, thus could be used as a baseline that guides language model development. It introduces a novel fine part-of-speech tagset, a fine-grained affix inventory and represents morphotactics without zero-derivations. The morphological analyzer is freely available. It consists of modular reusable components of human-annotated gold standard lexicons, implements Turkish morphotactics as finite-state transducers using OpenFst and morphophonemic processes as Thrax grammars. 1",https://aclanthology.org/W19-3110,Association for Computational Linguistics,2019,September,Proceedings of the 14th International Conference on Finite-State Methods and Natural Language Processing,"Ozturel, Adnan  and
Kayadelen, Tolga  and
Demirsahin, Isin",A Syntactically Expressive Morphological Analyzer for Turkish,10.18653/v1/W19-3110,W19,652
2022.wordplay-1.2,"['Domain-specific NLP', 'Model Architectures']",,,"We describe our system for playing a minimal improvisational game in a group. In Chain Reaction, players collectively build a chain of word pairs or solid compounds. The game emphasizes memory and rapid improvisation, while absurdity and humor increases during play. Our approach is unique in that we have grounded our work in the principles of oral culture according to Walter Ong, an early scholar of orature. We show how a simple computer model can be designed to embody many aspects of oral poetics, suggesting design directions for other work in oral improvisation and poetics. The opportunities for our system's further development include creating culturally specific automated players; situating play in different temporal, physical, and social contexts; and constructing a more elaborate improvisor.",https://aclanthology.org/2022.wordplay-1.2,Association for Computational Linguistics,2022,July,Proceedings of the 3rd Wordplay: When Language Meets Games Workshop (Wordplay 2022),"Montfort, Nick  and
Bartlett Fernandez, Sebastian",A Minimal Computational Improviser Based on Oral Thought,10.18653/v1/2022.wordplay-1.2,wordplay,926
2022.iwslt-1.31,"['Machine Translation (MT)', 'Multilingual NLP', 'Low-resource Languages']",['Neural MT (NMT)'],,"This paper describes the SLT-CDT-UoS group's submission to the first Special Task on Formality Control for Spoken Language Translation, part of the IWSLT 2022 Evaluation Campaign. Our efforts were split between two fronts: data engineering and altering the objective function for best hypothesis selection. We used language-independent methods to extract formal and informal sentence pairs from the provided corpora; using English as a pivot language, we propagated formality annotations to languages treated as zero-shot in the task; we also further improved formality controlling with a hypothesis re-ranking approach. On the test sets for English-to-German and Englishto-Spanish, we achieved an average accuracy of .935 within the constrained setting and .995 within unconstrained setting. In a zero-shot setting for English-to-Russian and English-to-Italian, we scored average accuracy of .590 for constrained setting and .659 for unconstrained.",https://aclanthology.org/2022.iwslt-1.31,Association for Computational Linguistics,2022,May,Proceedings of the 19th International Conference on Spoken Language Translation (IWSLT 2022),"Vincent, Sebastian  and
Barrault, Lo{\""\i}c  and
Scarton, Carolina",Controlling Formality in Low-Resource NMT with Domain Adaptation and Re-Ranking: SLT-CDT-UoS at IWSLT2022,10.18653/v1/2022.iwslt-1.31,iwslt,1080
2022.naacl-main.246,"['Information Retrieval', 'Data Management and Generation', 'Low-resource Languages', 'Classification Applications']","['Misinformation Detection', 'Data Preparation']",['Annotation Processes'],"The explosion of misinformation spreading in the media ecosystem urges for automated factchecking. While misinformation spans both geographic and linguistic boundaries, most work in the field has focused on English. Datasets and tools available in other languages, such as Chinese, are limited. In order to bridge this gap, we construct CHEF, the first CHinese Evidence-based Fact-checking dataset of 10K real-world claims. The dataset covers multiple domains, ranging from politics to public health, and provides annotated evidence retrieved from the Internet. Further, we develop established baselines and a novel approach that is able to model the evidence retrieval as a latent variable, allowing jointly training with the veracity prediction model in an end-to-end fashion. Extensive experiments show that CHEF will provide a challenging testbed for the development of fact-checking systems designed to retrieve and reason over non-English claims. Source code and data are available 1 .",https://aclanthology.org/2022.naacl-main.246,Association for Computational Linguistics,2022,July,Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,"Hu, Xuming  and
Guo, Zhijiang  and
Wu, GuanYu  and
Liu, Aiwei  and
Wen, Lijie  and
Yu, Philip",CHEF: A Pilot Chinese Dataset for Evidence-Based Fact-Checking,10.18653/v1/2022.naacl-main.246,naacl,1016
P18-1230,"['Information Extraction', 'Model Architectures']","['Recurrent Neural Networks (RNNs)', 'Word Sense Disambiguation (WSD)']",['Long Short-Term Memory (LSTM) Models'],"Word Sense Disambiguation WSD aims to identify the correct meaning of polysemous words in the particular context. Lexical resources like WordNet which are proved to be of great help for WSD in the knowledge-based methods. However, previous neural networks for WSD always rely on massive labeled data context, ignoring lexical resources like glosses sense definitions. In this paper, we integrate the context and glosses of the target word into a unified framework in order to make full use of both labeled data and lexical knowledge. Therefore, we propose GAS: a gloss-augmented WSD neural network which jointly encodes the context and glosses of the target word. GAS models the semantic relationship between the context and the gloss in an improved memory network framework, which breaks the barriers of the previous supervised methods and knowledge-based methods. We further extend the original gloss of word sense via its semantic relations in WordNet to enrich the gloss information. The experimental results show that our model outperforms the state-of-theart systems on several English all-words WSD datasets.",https://aclanthology.org/P18-1230,Association for Computational Linguistics,2018,July,Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),"Luo, Fuli  and
Liu, Tianyu  and
Xia, Qiaolin  and
Chang, Baobao  and
Sui, Zhifang",Incorporating Glosses into Neural Word Sense Disambiguation,10.18653/v1/P18-1230,P18,1013
I17-1039,"['Machine Translation (MT)', 'Data Management and Generation', 'Low-resource Languages', 'Model Architectures']","['Data Preparation', 'Neural MT (NMT)']",,"While neural machine translation NMT has become the new paradigm, the parameter optimization requires large-scale parallel data which is scarce in many domains and language pairs. In this paper, we address a new translation scenario in which there only exists monolingual corpora and phrase pairs. We propose a new method towards translation with partially aligned sentence pairs which are derived from the phrase pairs and monolingual corpora. To make full use of the partially aligned corpora, we adapt the conventional NMT training method in two aspects. On one hand, different generation strategies are designed for aligned and unaligned target words. On the other hand, a different objective function is designed to model the partially aligned parts. The experiments demonstrate that our method can achieve a relatively good result in such a translation scenario, and tiny bitexts can boost translation quality to a large extent.",https://aclanthology.org/I17-1039,Asian Federation of Natural Language Processing,2017,November,Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 1: Long Papers),"Wang, Yining  and
Zhao, Yang  and
Zhang, Jiajun  and
Zong, Chengqing  and
Xue, Zhengshan",Towards Neural Machine Translation with Partially Aligned Corpora,10.48550/arxiv.1711.01006,I17,1443
2021.cl-3.20,"['Embeddings', 'Knowledge Representation and Reasoning']",['Word Embeddings'],,"Word embeddings are vectorial semantic representations built with either counting or predicting techniques aimed at capturing shades of meaning from word co-occurrences. Since their introduction, these representations have been criticized for lacking interpretable dimensions. This property of word embeddings limits our understanding of the semantic features they actually encode. Moreover, it contributes to the ""black box"" nature of the tasks in which they are used, since the reasons for word embedding performance often remain opaque to humans. In this contribution, we explore the semantic properties encoded in word embeddings by mapping them onto interpretable vectors, consisting of explicit and neurobiologically motivated semantic features Binder et al. 2016 . Our exploration takes into account different types of embeddings, including factorized count vectors and predict models Skip-Gram, GloVe, etc., as well as the most recent contextualized representations i.e., ELMo and BERT.",https://aclanthology.org/2021.cl-3.20,MIT Press,2021,November,,"Chersoni, Emmanuele  and
Santus, Enrico  and
Huang, Chu-Ren  and
Lenci, Alessandro",Decoding Word Embeddings with Brain-Based Semantic Features,10.1162/coli_a_00412,cl,1275
D19-5507,"['Ethics', 'Domain-specific NLP', 'Data Management and Generation', 'Information Extraction', 'Model Architectures']","['Data Augmentation', 'Data Preparation']",,"Illicit activity on the Web often uses noisy text to obscure information between client and seller, such as the seller's phone number. This presents an interesting challenge to language understanding systems; how do we model adversarial noise in a text extraction system? This paper addresses the sex trafficking domain, and proposes some of the first neural network architectures to learn and extract phone numbers from noisy text. We create a new adversarial advertisement dataset, propose several RNN-based models to solve the problem, and most notably propose a visual character language model to interpret unseen unicode characters. We train a CRF jointly with a CNN to improve number recognition by 89% over just a CRF. Through data augmentation in this unique model, we present the first results on characters never seen in training.",https://aclanthology.org/D19-5507,Association for Computational Linguistics,2019,November,Proceedings of the 5th Workshop on Noisy User-generated Text (W-NUT 2019),"Chambers, Nathanael  and
Forman, Timothy  and
Griswold, Catherine  and
Lu, Kevin  and
Khastgir, Yogaish  and
Steckler, Stephen",Character-Based Models for Adversarial Phone Extraction: Preventing Human Sex Trafficking,10.18653/v1/D19-5507,D19,173
2020.emnlp-main.190,['Question Answering (QA)'],,,"While models have reached superhuman performance on popular question answering QA datasets such as SQuAD, they have yet to outperform humans on the task of question answering itself. In this paper, we investigate if models are learning reading comprehension from QA datasets by evaluating BERT-based models across five datasets. We evaluate models on their generalizability to out-of-domain examples, responses to missing or incorrect data, and ability to handle question variations. We find that no single dataset is robust to all of our experiments and identify shortcomings in both datasets and evaluation methods. Following our analysis, we make recommendations for building future QA datasets that better evaluate the task of question answering through reading comprehension. We also release code to convert QA datasets to a shared format for easier experimentation at https: //github.com/amazon-research/ qa-dataset-converter.",https://aclanthology.org/2020.emnlp-main.190,Association for Computational Linguistics,2020,November,Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),"Sen, Priyanka  and
Saffari, Amir",What do Models Learn from Question Answering Datasets?,10.18653/v1/2020.emnlp-main.190,emnlp,335
2020.conll-1.13,['Model Architectures'],['Recurrent Neural Networks (RNNs)'],['Long Short-Term Memory (LSTM) Models'],"Recurrent Neural Networks RNNs have been shown to capture various aspects of syntax from raw linguistic input. In most previous experiments, however, learning happens over unrealistic corpora, which do not reflect the type and amount of data a child would be exposed to. This paper remedies this state of affairs by training a Long Short-Term Memory network LSTM over a realistically sized subset of child-directed input. The behaviour of the network is analysed over time using a novel methodology which consists in quantifying the level of grammatical abstraction in the model's generated output its 'babbling', compared to the language it has been exposed to. We show that the LSTM indeed abstracts new structures as learning proceeds.",https://aclanthology.org/2020.conll-1.13,Association for Computational Linguistics,2020,November,Proceedings of the 24th Conference on Computational Natural Language Learning,"Pannitto, Ludovica  and
Herbelot, Aur{\'e}lie",Recurrent babbling: evaluating the acquisition of grammar from limited input data,10.18653/v1/2020.conll-1.13,conll,740
2021.nlp4convai-1.17,"['Audio Generation and Processing', 'Information Retrieval', 'Dialogue Systems']","['Information Filtering', 'Automatic Speech Recognition (ASR)']",,"Query rewrite QR is an emerging component in conversational AI systems, reducing user defect. User defect is caused by various reasons, such as errors in the spoken dialogue system, users' slips of the tongue or their abridged language. Many of the user defects stem from personalized factors, such as user's speech pattern, dialect, or preferences. In this work, we propose a personalized search-based QR framework, which focuses on automatic reduction of user defect. We build a personalized index for each user, which encompasses diverse affinity layers to reflect personal preferences for each user in the conversational AI. Our personalized QR system contains retrieval and ranking layers. Supported by user feedback based learning, training our models does not require hand-annotated data. Experiments on personalized test set showed that our personalized QR system is able to correct systematic and user errors by utilizing phonetic and semantic inputs.",https://aclanthology.org/2021.nlp4convai-1.17,Association for Computational Linguistics,2021,November,Proceedings of the 3rd Workshop on Natural Language Processing for Conversational AI,"Cho, Eunah  and
Jiang, Ziyan  and
Hao, Jie  and
Chen, Zheng  and
Gupta, Saurabh  and
Fan, Xing  and
Guo, Chenlei",Personalized Search-based Query Rewrite System for Conversational AI,10.18653/v1/2021.nlp4convai-1.17,nlp4convai,1155
2020.pam-1.18,"['Domain-specific NLP', 'Low-resource Languages', 'Language Change Analysis', 'Data Management and Generation']","['Semantic Change Analysis', 'Data Analysis', 'NLP for News and Media']",,"We present ongoing research on the relationship between short-term semantic shifts and frequency change patterns by examining the case of the refugee crisis in Austria from 2015 to 2016. Our experiments are carried out on a diachronic corpus of Austrian German, namely a corpus of newspaper articles. We trace the evolution of the usage of words that represent concepts in the context of the refugee crisis by analyzing cosine similarities of word vectors over time as well as similarities based on the words' nearest neighbourhood sets. In order to investigate how exactly the contextual meanings have changed, we measure cosine similarity between the following pairs of words: words describing the refugee crisis, on the one hand, and words indicating the process of mediatization and politicization of the refugee crisis in Austria proposed by a domain expert, on the other hand. We evaluate our approach against expert knowledge. The paper presents the current findings and outlines the directions of the future work.",https://aclanthology.org/2020.pam-1.18,Association for Computational Linguistics,2020,June,Proceedings of the Probability and Meaning Conference (PaM 2020),"Marakasova, Anna  and
Neidhardt, Julia",Short-term Semantic Shifts and their Relation to Frequency Change,,pam,707
Y17-1029,"['Data Management and Generation', 'Low-resource Languages']",['Data Analysis'],,"The present study investigated the following: i how NPs bearing differing GRs behave with respect to two proposed subject diagnostics -Honorific Agreement HA and Plural Copying on adverbs PC and ii whether scrambling allows non-Subject GRs to control these properties. An experimental investigation using Magnitude Estimation ME was conducted. The result revealed that the sentences with Subject NP controller got higher acceptability scores compared to non-Subject NP controllers for both diagnostics and that scrambling did not have an effect on acceptability. While both HA and PC showed a similar pattern of preference for Subject controllers, the contrast between Subject and non-Subject controllers was more pronounced with HA.",https://aclanthology.org/Y17-1029,The National University (Phillippines),2017,November,"Proceedings of the 31st Pacific Asia Conference on Language, Information and Computation","Kim, Ji-Hye  and
Lee, Yong-Hun  and
Yoon, James Hye-Suk",Subjecthood and Grammatical Relations in Korean: An Experimental Study with Honorific Agreement and Plural Copying,,Y17,277
S16-2026,"['Embeddings', 'Discourse Analysis', 'Low-resource Languages', 'Classification Applications']",['Word Embeddings'],,"Givenness Schwarzschild, 1999 is one of the central notions in the formal pragmatic literature discussing the organization of discourse. In this paper, we explore where distributional semantics can help address the gap between the linguistic insights into the formal pragmatic notion of Givenness and its implementation in computational linguistics. As experimental testbed, we focus on short answer assessment, in which the goal is to assess whether a student response correctly answers the provided reading comprehension question or not. Current approaches only implement a very basic, surface-based perspective on Givenness: A word of the answer that appears as such in the question counts as GIVEN. We show that an approach approximating Givenness using distributional semantics to check whether a word in a sentence is similar enough to a word in the context to count as GIVEN is more successful quantitatively and supports interesting qualitative insights into the data and the limitations of a basic distributional semantic approach identifying Givenness at the lexical level.",https://aclanthology.org/S16-2026,Association for Computational Linguistics,2016,August,Proceedings of the Fifth Joint Conference on Lexical and Computational Semantics,"Ziai, Ramon  and
De Kuthy, Kordula  and
Meurers, Detmar",Approximating Givenness in Content Assessment through Distributional Semantics,10.18653/v1/S16-2026,S16,1123
2022.fever-1.5,"['Model Architectures', 'Data Management and Generation', 'Classification Applications', 'Adversarial Attacks and Robustness']","['Transformer Models', 'Claim Verification', 'Data Analysis']",,"The influence of fake news in the perception of reality has become a mainstream topic in the last years due to the fast propagation of misleading information. In order to help in the fight against misinformation, automated solutions to fact-checking are being actively developed within the research community. In this context, the task of Automated Claim Verification is defined as assessing the truthfulness of a claim by finding evidence about its veracity. In this work we empirically demonstrate that enriching a BERT model with explicit semantic information such as Semantic Role Labelling helps to improve results in claim verification as proposed by the FEVER benchmark. Furthermore, we perform a number of explainability tests that suggest that the semantically-enriched model is better at handling complex cases, such as those including passive forms or multiple propositions.",https://aclanthology.org/2022.fever-1.5,Association for Computational Linguistics,2022,May,Proceedings of the Fifth Fact Extraction and VERification Workshop (FEVER),"Calvo Figueras, Blanca  and
Oller, Montse  and
Agerri, Rodrigo",A Semantics-Aware Approach to Automated Claim Verification,10.18653/v1/2022.fever-1.5,fever,902
2021.argmining-1.9,"['Domain-specific NLP', 'Data Management and Generation', 'Classification Applications', 'Argument Mining', 'Low-resource Languages']",['Data Preparation'],['Annotation Processes'],"Public participation processes allow citizens to engage in municipal decision-making processes by expressing their opinions on specific issues. Municipalities often only have limited resources to analyze a possibly large amount of textual contributions that need to be evaluated in a timely and detailed manner. Automated support for the evaluation is therefore essential, e.g. to analyze arguments. In this paper, we address A the identification of argumentative discourse units and B their classification as major position or premise in German public participation processes. The objective of our work is to make argument mining viable for use in municipalities. We compare different argument mining approaches and develop a generic model that can successfully detect argument structures in different datasets of mobility-related urban planning. We introduce a new data corpus comprising five public participation processes. In our evaluation, we achieve high macro F 1 scores 0.76 -0.80 for the identification of argumentative units; 0.86 -0.93 for their classification on all datasets. Additionally, we improve previous results for the classification of argumentative units on a similar German online participation dataset.",https://aclanthology.org/2021.argmining-1.9,Association for Computational Linguistics,2021,November,Proceedings of the 8th Workshop on Argument Mining,"Romberg, Julia  and
Conrad, Stefan",Citizen Involvement in Urban Planning - How Can Municipalities Be Supported in Evaluating Public Participation Processes for Mobility Transitions?,10.18653/v1/2021.argmining-1.9,argmining,814
2022.computel-1.21,"['Audio Generation and Processing', 'Multilingual NLP', 'Low-resource Languages', 'Model Architectures']","['Transformer Models', 'Automatic Speech Recognition (ASR)']",,"This is a report on results obtained in the development of speech recognition tools intended to support linguistic documentation efforts. The test case is an extensive fieldwork corpus of Japhug, an endangered language of the Trans-Himalayan Sino-Tibetan family. The goal is to reduce the transcription workload of field linguists. The method used is a deep learning approach based on the language-specific tuning of a generic pre-trained representation model, XLS-R, using a Transformer architecture. We note difficulties in implementation, in terms of learning stability. But this approach brings significant improvements nonetheless. The quality of phonemic transcription is improved over earlier experiments; and most significantly, the new approach allows for reaching the stage of automatic word recognition. Subjective evaluation of the tool by the author of the training data confirms the usefulness of this approach.",https://aclanthology.org/2022.computel-1.21,Association for Computational Linguistics,2022,May,Proceedings of the Fifth Workshop on the Use of Computational Methods in the Study of Endangered Languages,"Guillaume, S{\'e}verine  and
Wisniewski, Guillaume  and
Macaire, C{\'e}cile  and
Jacques, Guillaume  and
Michaud, Alexis  and
Galliot, Benjamin  and
Coavoux, Maximin  and
Rossato, Solange  and
Nguy{\^e}n, Minh-Ch{\^a}u  and
Fily, Maxime","Fine-tuning pre-trained models for Automatic Speech Recognition, experiments on a fieldwork corpus of Japhug Trans-Himalayan family",10.18653/v1/2022.computel-1.21,computel,1309
L18-1291,"['Learning Paradigms', 'Low-resource Languages']",['Unsupervised Learning'],,"The paper presents a semi-automatic method for the construction of derivational networks. The proposed approach applies a sequential pattern mining technique in order to construct useful morphological features in an unsupervised manner. The features take the form of regular expressions and later are used to feed a machine-learned ranking model. The network is constructed by applying resulting model to sort the lists of possible base words and selecting the most probable ones. This approach, besides relatively small training set and a lexicon, does not require any additional language resources such as a list of alternations groups, POS tags etc. The proposed approach is applied to the lexeme sets of two languages, namely Polish and Spanish, which results in the establishment of two novel word-formation networks. Finally, the network constructed for Polish is merged with the derivational connections extracted from the Polish WordNet and those resulting from the derivational rules developed by a linguist, resulting in the biggest word-formation network for that language. The presented approach is general enough to be adopted for other languages.",https://aclanthology.org/L18-1291,European Language Resources Association (ELRA),2018,May,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),"Lango, Mateusz  and
{\v{S}}ev{\v{c}}{\'\i}kov{\'a}, Magda  and
{\v{Z}}abokrtsk{\'y}, Zden{\v{e}}k",Semi-Automatic Construction of Word-Formation Networks for Polish and Spanish,,L18,370
2021.cnl-1.2,"['Domain-specific NLP', 'Low-resource Languages', 'Machine Translation (MT)']",['Rule-based MT (RBMT)'],,"Grammar-based domain-specific MT systems are a common use case for CNLs. High-quality translation lexica are a crucial part of such systems, but involve time consuming work and significant linguistic knowledge. With parallel example sentences available, statistical alignment tools can help automate part of the process, but they are not suitable for small datasets and do not always perform well with complex multiword expressions. In addition, the correspondences between word forms obtained in this way cannot be used directly. Addressing these problems, we propose a grammar-based approach to this task and put it to test in a simple translation pipeline.",https://aclanthology.org/2021.cnl-1.2,Special Interest Group on Controlled Natural Language,2021,September,Proceedings of the Seventh International Workshop on Controlled Natural Language (CNL 2020/21),"Masciolini, Arianna  and
Ranta, Aarne",Grammar-Based Concept Alignment for Domain-Specific Machine Translation,,cnl,139
2022.eamt-1.23,"['Domain-specific NLP', 'Machine Translation (MT)', 'Evaluation Techniques', 'Data Management and Generation', 'Low-resource Languages']",['Data Preparation'],['Annotation Processes'],"This paper illustrates a new evaluation framework developed at Unbabel for measuring the quality of source language text and its effect on both Machine Translation MT and Human Post-Edition PE performed by non-professional post-editors. We examine both agent and user-generated content from the Customer Support domain and propose that differentiating the two is crucial to obtaining high quality translation output. Furthermore, we present results of initial experimentation with a new evaluation typology based on the Multidimensional Quality Metrics MQM Framework Lommel et al., 2014 , specifically tailored toward the evaluation of source language text. We show how the MQM Framework Lommel et al., 2014 can be adapted to assess errors of monolingual source texts and demonstrate how very specific source errors propagate to the MT and PE targets. Finally, we illustrate how MT systems are not robust enough to handle specific types of source noise in the context of Customer Support data.",https://aclanthology.org/2022.eamt-1.23,European Association for Machine Translation,2022,June,Proceedings of the 23rd Annual Conference of the European Association for Machine Translation,"Gon{\c{c}}alves, Madalena  and
Buchicchio, Marianna  and
Stewart, Craig  and
Moniz, Helena  and
Lavie, Alon",Agent and User-Generated Content and its Impact on Customer Support MT,,eamt,1095
2021.ranlp-1.132,"['Authorship Verification', 'Data Management and Generation']",,,"Recent research has documented that results reported in frequently-cited authorship attribution papers are difficult to reproduce. Inaccessible code and data are often proposed as factors which block successful reproductions. Even when original materials are available, problems remain which prevent researchers from comparing the effectiveness of different methods. To solve the remaining problems-the lack of fixed test sets and the use of inappropriately homogeneous corpora-our paper contributes materials for five closed-set authorship identification experiments. The five experiments feature texts from 106 distinct authors. Experiments involve a range of contemporary non-fiction American English prose. These experiments provide the foundation for comparable and reproducible authorship attribution research involving contemporary writing.",https://aclanthology.org/2021.ranlp-1.132,INCOMA Ltd.,2021,September,Proceedings of the International Conference on Recent Advances in Natural Language Processing (RANLP 2021),"Riddell, Allen  and
Wang, Haining  and
Juola, Patrick",A Call for Clarity in Contemporary Authorship Attribution Evaluation,10.26615/978-954-452-072-4_132,ranlp,861
K18-1035,"['Learning Paradigms', 'Parsing', 'Model Architectures']","['Semantic Parsing', 'Recurrent Neural Networks (RNNs)']",['Long Short-Term Memory (LSTM) Models'],"Weakly-supervised semantic parsers are trained on utterance-denotation pairs, treating logical forms as latent. The task is challenging due to the large search space and spuriousness of logical forms. In this paper we introduce a neural parser-ranker system for weakly-supervised semantic parsing. The parser generates candidate tree-structured logical forms from utterances using clues of denotations. These candidates are then ranked based on two criterion: their likelihood of executing to the correct denotation, and their agreement with the utterance semantics. We present a scheduled training procedure to balance the contribution of the two objectives. Furthermore, we propose to use a neurally encoded lexicon to inject prior domain knowledge to the model. Experiments on three Freebase datasets demonstrate the effectiveness of our semantic parser, achieving results within the state-of-the-art range.",https://aclanthology.org/K18-1035,Association for Computational Linguistics,2018,October,Proceedings of the 22nd Conference on Computational Natural Language Learning,"Cheng, Jianpeng  and
Lapata, Mirella",Weakly-Supervised Neural Semantic Parsing with a Generative Ranker,10.18653/v1/K18-1035,K18,1311
2020.starsem-1.10,"['Evaluation Techniques', 'Embeddings', 'Data Management and Generation', 'Model Architectures']","['Transformer Models', 'Data Preparation', 'Word Embeddings']",,"Contextualized word representations have become a driving force in NLP, motivating widespread interest in understanding their capabilities and the mechanisms by which they operate. Particularly intriguing is their ability to identify and encode conceptual abstractions. Past work has probed BERT representations Devlin et al., 2019 for this competence, finding that BERT can correctly retrieve noun hypernyms in cloze tasks. In this work, we ask the question: do probing studies shed light on systematic knowledge in BERT representations? As a case study, we examine hypernymy knowledge encoded in BERT representations. In particular, we demonstrate through a simple consistency probe that the ability to correctly retrieve hypernyms in cloze tasks, as used in prior work, does not correspond to systematic knowledge in BERT. Our main conclusion is cautionary: even if BERT demonstrates high probing accuracy for a particular competence, it does not necessarily follow that BERT 'understands' a concept, and it cannot be expected to systematically generalize across applicable contexts. 1",https://aclanthology.org/2020.starsem-1.10,Association for Computational Linguistics,2020,December,Proceedings of the Ninth Joint Conference on Lexical and Computational Semantics,"Ravichander, Abhilasha  and
Hovy, Eduard  and
Suleman, Kaheer  and
Trischler, Adam  and
Cheung, Jackie Chi Kit",On the Systematicity of Probing Contextualized Word Representations: The Case of Hypernymy in BERT,,starsem,1286
2022.distcurate-1.4,"['Low-resource Languages', 'Cross-lingual Application']",,,"Despite advances in statistical approaches to the modeling of meaning, many questions about the ideal way of exploiting both knowledge-based e.g., FrameNet, WordNet and data-based methods e.g., BERT remain unresolved. This workshop focuses on these questions with three session papers that run the gamut from highly distributional methods Lekkas et al., 2022 , to highly curated methods Gamonal, 2022, and techniques with statistical methods producing structured semantics Lawley and Schubert, 2022. In addition, we begin the workshop with a small comparison of cross-lingual techniques for frame semantic alignment for one language pair Spanish and English. None of the distributional techniques consistently aligns the 1-best frame match from English to Spanish, all failing in at least one case. Predicting which techniques will align which frames crosslinguistically is not possible from any known characteristic of the alignment technique or the frames. Although distributional techniques are a rich source of semantic information for many tasks, at present curated, knowledge-based semantics remains the only technique that can consistently align frames across languages.",https://aclanthology.org/2022.distcurate-1.4,Association for Computational Linguistics,2022,July,Proceedings of the Workshop on Dimensions of Meaning: Distributional and Curated Semantics (DistCurate 2022),"Baker, Collin F.  and
Ellsworth, Michael  and
Petruck, Miriam R. L.  and
Lorenzi, Arthur",Comparing Distributional and Curated Approaches for Cross-lingual Frame Alignment,10.18653/v1/2022.distcurate-1.4,distcurate,390
2020.dt4tp-1.1,"['Parsing', 'Dialogue Systems']","['Discourse Parsing', 'Chatbots']",,"In many task-oriented chatbot domains, an objective is to fully inform a user about a particular important piece of information. It is also crucial to make user believe this piece of information, relying on explanation and argumentation in as much degree as possible. In some cases, it is important to make a user believe in a particular short text. This should be done by thoroughly navigating a user through possible disagreements and misunderstanding, to make sure the user is being explained and communicated an issue exhaustively. Rather than throwing the whole paragraph of text at a user, we split it into logical parts and feed the user text fragment by fragment, following her interests and intents. To systematically implement this navigation, we follow a discourse-level structure for how the author of this text organized his thoughts. This can be done by navigating a discourse tree DT of this text. DT is a tree that is a labeled tree in which the leaves of the tree correspond to contiguous units for clauses elementary discourse units, EDUs. Adjacent EDUs, as well as higher-level larger discourse units, are organized in a hierarchy by rhetorical relation e.g., Reason, Temporal sequence provided by Rhetorical structure theory RST, Mann and Thompson, 1988. An anti-symmetric relation involves a pair of EDUs: nuclei, which are core parts of the relation, and satellites, which are the supportive parts of the rhetorical relation. A satellite can be delivered by the chatbot to a user as an utterance only if its nucleus has already been received and acknowledged in one way or another. We outline the chatbot algorithm of the DT traversal, covering a multitude of user intents at each iteration: 1. If a text is given, navigating a discourse tree of this text T is one of the most efficient ways to communicate it. The chatbot starts with making an introduction and then making the main statement M T . Then the user would ask for more details E T , disagree E T or ask a question on a topic outside of the scope of this text O T . 2. If the user asks for more details I T , the EDU connected with Elaboration with M T is provided as a reply. We denote this EDU as ElaborationI T . This is the easiest, most direct situation. 3. If the user disagrees, chatbot tries to find an EDU which is connected by Explanation or Cause with M T or I T. This EDU should be returned as a reply. 4. If the user asks a different question O T then it should be answered as a factoid question but nevertheless the chatbot needs to take the user back to T so the reply should end with ElaborationI T . If the user doubts about the validity of a claim in M T , the chatbot needs to deliver AttributionM T  as an answer. The procedure above should iterate till no more EDU in T is left or the user terminates the conversation. If the chatbot persistence is too high in trying to take the user back to T, this user would terminate the conversation too soon. Otherwise, if the chatbot persistence is too low, the user would deviate from T too far so will red less content of T EDUT. We want to optimize the chatbot to maintain the optimal persistence to maximize the number of delivered EDUT till the conversation is abandoned by the user. Let us take a text and show how a DT navigation leads a dialogue wrapped around this text. According to BBC, China has rejected calls for an independent international investigation into the origin of the coronavirus. A top diplomat in the UK, Chen Wen explained the BBC the demands were politically motivated and would divert China attention from fighting the pandemic. However, EU believes that information about how it initially spread could help countries tackle the disease. The virus is thought to have been caused by a poor hygiene emerged at a wildlife market in the city of Wuhan.",https://aclanthology.org/2020.dt4tp-1.1,Association for Computational Linguistics,2020,December,Proceedings of the Workshop on Discourse Theories for Text Planning,"Galitsky, Boris  and
Ilvovsky, Dmitry",Automatic planning of the dialogue between human and machine using discourse trees,,dt4tp,1128
S16-2017,"['Machine Translation (MT)', 'Low-resource Languages']",,,"We describe the implementation of a Word Sense Disambiguation WSD tool in a Dutch Text-to-Pictograph translation system, which converts textual messages into sequences of pictographic images. The system is used in an online platform for Augmentative and Alternative Communication AAC. In the original translation process, the appropriate sense of a word was not disambiguated before converting it into a pictograph. This often resulted in incorrect translations. The implementation of a WSD tool provides a better semantic understanding of the input messages.",https://aclanthology.org/S16-2017,Association for Computational Linguistics,2016,August,Proceedings of the Fifth Joint Conference on Lexical and Computational Semantics,"Sevens, Leen  and
Jacobs, Gilles  and
Vandeghinste, Vincent  and
Schuurman, Ineke  and
Van Eynde, Frank",Improving Text-to-Pictograph Translation Through Word Sense Disambiguation,10.18653/v1/S16-2017,S16,895
2020.fever-1.5,['Classification Applications'],['Misinformation Detection'],,"Recent work has suggested that language models LMs store both common-sense and factual knowledge learned from pre-training data. In this paper, we leverage this implicit knowledge to create an effective end-to-end fact checker using a solely a language model, without any external knowledge or explicit retrieval components. While previous work on extracting knowledge from LMs have focused on the task of open-domain question answering, to the best of our knowledge, this is the first work to examine the use of language models as fact checkers. In a closed-book setting, we show that our zero-shot LM approach outperforms a random baseline on the standard FEVER task, and that our finetuned LM compares favorably with standard baselines. Though we do not ultimately outperform methods which use explicit knowledge bases, we believe our exploration shows that this method is viable and has much room for exploration.",https://aclanthology.org/2020.fever-1.5,Association for Computational Linguistics,2020,July,Proceedings of the Third Workshop on Fact Extraction and VERification (FEVER),"Lee, Nayeon  and
Li, Belinda Z.  and
Wang, Sinong  and
Yih, Wen-tau  and
Ma, Hao  and
Khabsa, Madian",Language Models as Fact Checkers?,10.18653/v1/2020.fever-1.5,fever,566
C18-1293,"['Low-resource Languages', 'Classification Applications']",['Multilabel Text Classification'],,"In this paper, we describe experiments designed to explore and evaluate the impact of punctuation marks on the task of native language identification. Punctuation is specific to each language, and is part of the indicators that overtly represent the manner in which each language organizes and conveys information. Our experiments are organized in various set-ups: the usual multi-class classification for individual languages, also considering classification by language groups, across different proficiency levels, topics and even cross-corpus. The results support our hypothesis that punctuation marks are persistent and robust indicators of the native language of the author, which do not diminish in influence even when a high proficiency level in a non-native language is achieved.",https://aclanthology.org/C18-1293,Association for Computational Linguistics,2018,August,Proceedings of the 27th International Conference on Computational Linguistics,"Markov, Ilia  and
Nastase, Vivi  and
Strapparava, Carlo",Punctuation as Native Language Interference,,C18,808
2020.alw-1.11,"['Domain-specific NLP', 'Model Architectures', 'Classification Applications']","['NLP for News and Media', 'Hate and Offensive Speech Detection']",['NLP for Social Media'],"Cyberbullying is a prevalent social problem that inflicts detrimental consequences to the health and safety of victims such as psychological distress, anti-social behaviour, and suicide. The automation of cyberbullying detection is a recent but widely researched problem, with current research having a strong focus on a binary classification of bullying versus non-bullying. This paper proposes a novel approach to enhancing cyberbullying detection through role modeling. We utilise a dataset from ASKfm to perform multi-class classification to detect participant roles e.g. victim, harasser. Our preliminary results demonstrate promising performance including 0.83 and 0.76 of F1-score for cyberbullying and role classification respectively, outperforming baselines.",https://aclanthology.org/2020.alw-1.11,Association for Computational Linguistics,2020,November,Proceedings of the Fourth Workshop on Online Abuse and Harms,"Rathnayake, Gathika  and
Atapattu, Thushari  and
Herath, Mahen  and
Zhang, Georgia  and
Falkner, Katrina",Enhancing the Identification of Cyberbullying through Participant Roles,10.18653/v1/2020.alw-1.11,alw,1065
2020.nlp4call-1.4,['Low-resource Languages'],,,"This paper presents the NiinMikOli?! reading assistant for Finnish. The focus is upon the simplified presentation and visualisation of a wide range of word-level linguistic phenomena of the Finnish language in a unified form so as to benefit language learners. The system is available as a browser extension, intended to be used in-context, with authentic texts, in order to encourage free reading in language learners.",https://aclanthology.org/2020.nlp4call-1.4,LiU Electronic Press,2020,November,Proceedings of the 9th Workshop on NLP for Computer Assisted Language Learning,"Robertson, Frankie","Show, Don't Tell: Visualising Finnish Word Formation in a Browser-Based Reading Assistant",10.3384/ecp2017537,nlp4call,1060
2021.bsnlp-1.15,"['Data Management and Generation', 'Multilingual NLP', 'Information Extraction', 'Classification Applications', 'Learning Paradigms', 'Low-resource Languages', 'Cross-lingual Application']","['Transfer Learning', 'Data Preparation', 'Named Entity Recognition (NER)']",,"This paper describes Slav-NER: the 3 rd Multilingual Named Entity Challenge in Slavic languages. The tasks involve recognizing mentions of named entities in Web documents, normalization of the names, and crosslingual linking. The Challenge covers six languages and five entity types, and is organized as part of the 8 th Balto-Slavic Natural Language Processing Workshop, co-located with the EACL 2021 Conference. Ten teams participated in the competition. Performance for the named entity recognition task reached 90% Fmeasure, much higher than reported in the first edition of the Challenge. Seven teams covered all six languages. Detailed evaluation information is available on the shared task web page.",https://aclanthology.org/2021.bsnlp-1.15,Association for Computational Linguistics,2021,April,Proceedings of the 8th Workshop on Balto-Slavic Natural Language Processing,"Piskorski, Jakub  and
Babych, Bogdan  and
Kancheva, Zara  and
Kanishcheva, Olga  and
Lebedeva, Maria  and
Marci{\'n}czuk, Micha{\l}  and
Nakov, Preslav  and
Osenova, Petya  and
Pivovarova, Lidia  and
Pollak, Senja  and
P{\v{r}}ib{\'a}{\v{n}}, Pavel  and
Radev, Ivaylo  and
Robnik-Sikonja, Marko  and
Starko, Vasyl  and
Steinberger, Josef  and
Yangarber, Roman","Slav-NER: the 3rd Cross-lingual Challenge on Recognition, Normalization, Classification, and Linking of Named Entities across Slavic Languages",,bsnlp,1115
2021.nlp4if-1.14,"['Domain-specific NLP', 'Classification Applications']","['Misinformation Detection', 'NLP for News and Media']",['NLP for Social Media'],"The spread of COVID-19 has been accompanied with widespread misinformation on social media. In particular, Twitterverse has seen a huge increase in dissemination of distorted facts and figures. The present work aims at identifying tweets regarding COVID-19 which contains harmful and false information. We have experimented with a number of Deep Learning based models, including different word embeddings, such as Glove, ELMo, among others. BERTweet model achieved the best overall F1-score of 0.881 and secured the third rank on the above task.",https://aclanthology.org/2021.nlp4if-1.14,Association for Computational Linguistics,2021,June,"Proceedings of the Fourth Workshop on NLP for Internet Freedom: Censorship, Disinformation, and Propaganda","Kumar, Ankit  and
Jhunjhunwala, Naman  and
Agarwal, Raksha  and
Chatterjee, Niladri",NARNIA at NLP4IF-2021: Identification of Misinformation in COVID-19 Tweets Using BERTweet,10.18653/v1/2021.nlp4if-1.14,nlp4if,887
2021.alvr-1.7,"['Dialogue Systems', 'Data Management and Generation', 'Information Extraction', 'Image and Video Processing']","['Data Preparation', 'Coreference Resolution']",['Annotation Processes'],"In recent years, a large number of corpora have been developed for vision and language tasks. We argue that there is still significant room for corpora that increase the complexity of both visual and linguistic domains and which capture different varieties of perceptual and conversational contexts. Working with two corpora approaching this goal, we present a linguistic perspective on some of the challenges in creating and extending resources combining language and vision while preserving continuity with the existing best practices in the area of coreference annotation.",https://aclanthology.org/2021.alvr-1.7,Association for Computational Linguistics,2021,June,Proceedings of the Second Workshop on Advances in Language and Vision Research,"Lo{\'a}iciga, Sharid  and
Dobnik, Simon  and
Schlangen, David",Reference and coreference in situated dialogue,10.18653/v1/2021.alvr-1.7,alvr,199
2020.wildre-1.4,"['Data Management and Generation', 'Information Extraction', 'Low-resource Languages', 'Domain-specific NLP']","['Anaphora Resolution', 'Data Preparation', 'Coreference Resolution', 'NLP for News and Media']",,"Natural language understanding by automatic tools is the vital requirement for document processing tools. To achieve it, automatic system has to understand the coherence in the text. Co-reference chains bring coherence to the text. The commonly occurring reference markers which bring cohesiveness are Pronominal, Reflexives, Reciprocals, Distributives, One-anaphors, Noun-noun reference. Here in this paper, we deal with noun-noun reference in Tamil. We present the methodology to resolve these noun-noun anaphors and also present the challenges in handling the noun-noun anaphoric relations in Tamil.",https://aclanthology.org/2020.wildre-1.4,European Language Resources Association (ELRA),2020,May,Proceedings of the WILDRE5{--} 5th Workshop on Indian Language Data: Resources and Evaluation,"Sundar Ram, Vijay  and
Lalitha Devi, Sobha",Handling Noun-Noun Coreference in Tamil,,wildre,158
2020.parlaclarin-1.1,"['Domain-specific NLP', 'Data Management and Generation', 'Low-resource Languages']",['Data Preparation'],,"This short paper presents the current as of February 2020 state of preparation of the Polish Parliamentary Corpus PPC -an extensive collection of transcripts of Polish parliamentary proceedings dating from 1919 to present. The most evident developments as compared to the 2018 version is harmonization of metadata, standardization of document identifiers, uploading contents of all documents and metadata to the database to enable easier modification, maintenance and future development of the corpus, linking utterances to the political ontology, linking corpus texts to source data and processing historical documents.",https://aclanthology.org/2020.parlaclarin-1.1,European Language Resources Association,2020,May,Proceedings of the Second ParlaCLARIN Workshop,"Ogrodniczuk, Maciej  and
Nito{\'n}, Bart{\l}omiej",New Developments in the Polish Parliamentary Corpus,,parlaclarin,1036
2020.acl-main.63,"['Dialogue Systems', 'Model Architectures', 'Learning Paradigms']","['Unsupervised Learning', 'Supervised Learning', 'Response Generation', 'Reinforcement Learning']",,"In modular dialogue systems, natural language understanding NLU and natural language generation NLG are two critical components, where NLU extracts the semantics from the given texts and NLG is to construct corresponding natural language sentences based on the input semantic representations. However, the dual property between understanding and generation has been rarely explored. The prior work Su et al., 2019 is the first attempt that utilized the duality between NLU and NLG to improve the performance via a dual supervised learning framework. However, the prior work still learned both components in a supervised manner; instead, this paper introduces a general learning framework to effectively exploit such duality, providing flexibility of incorporating both supervised and unsupervised learning algorithms to train language understanding and generation models in a joint fashion. The benchmark experiments demonstrate that the proposed approach is capable of boosting the performance of both NLU and NLG. 1",https://aclanthology.org/2020.acl-main.63,Association for Computational Linguistics,2020,July,Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,"Su, Shang-Yu  and
Huang, Chao-Wei  and
Chen, Yun-Nung",Towards Unsupervised Language Understanding and Generation by Joint Dual Learning,10.18653/v1/2020.acl-main.63,acl,903
D19-5702,"['Embeddings', 'Domain-specific NLP', 'Data Management and Generation', 'Information Extraction', 'Low-resource Languages', 'Model Architectures']","['Data Preparation', 'Named Entity Recognition (NER)', 'Recurrent Neural Networks (RNNs)', 'Medical and Clinical NLP']","['Biomedical NLP', 'Long Short-Term Memory (LSTM) Models']","The recognition of pharmacological substances, compounds and proteins is an essential preliminary work for the recognition of relations between chemicals and other biomedically relevant units. In this paper, we describe an approach to Task 1 of the PharmaCoNER Challenge, which involves the recognition of mentions of chemicals and drugs in Spanish medical texts. We train a state-of-the-art BiLSTM-CRF sequence tagger with stacked Pooled Contextualized Embeddings, word and sub-word embeddings using the open-source framework FLAIR. We present a new corpus composed of articles and papers from Spanish health science journals, termed the Spanish Health Corpus, and use it to train domainspecific embeddings which we incorporate in our model training. We achieve a result of 89.76% F1-score using pre-trained embeddings and are able to improve these results to 90.52% F1-score using specialized embeddings.",https://aclanthology.org/D19-5702,Association for Computational Linguistics,2019,November,Proceedings of The 5th Workshop on BioNLP Open Shared Tasks,"Stoeckel, Manuel  and
Hemati, Wahed  and
Mehler, Alexander",When Specialization Helps: Using Pooled Contextualized Embeddings to Detect Chemical and Biomedical Entities in Spanish,10.18653/v1/D19-5702,D19,163
2020.sigtyp-1.6,['Model Architectures'],['Transformer Models'],,"The paper describes the Multitasking Selfattention based approach to constrained subtask within Sigtyp 2020 Shared task. Our model is simple neural network based architecture inspired by Transformers Vaswani et al.,  2017  model. The model uses Multitasking to compute values of all WALS features for a given input language simultaneously. Results show that our approach performs at par with the baseline approaches, even though our proposed approach requires only phylogenetic and geographical attributes namely Longitude, Latitude, Genus-index, Family-index and Country-index and do not use any of the known WALS features of the respective input language, to compute its missing WALS features.",https://aclanthology.org/2020.sigtyp-1.6,Association for Computational Linguistics,2020,November,Proceedings of the Second Workshop on Computational Research in Linguistic Typology,"Choudhary, Chinmay",NUIG: Multitasking Self-attention based approach to SigTyp 2020 Shared Task,10.18653/v1/2020.sigtyp-1.6,sigtyp,653
2022.lchange-1.21,"['Embeddings', 'Low-resource Languages', 'Language Change Analysis']",['Semantic Change Analysis'],,"This paper describes the methods used for lexical semantic change discovery in Spanish. We tried the method based on BERT embeddings with clustering, the method based on grammatical profiles and the grammatical profiles method enhanced with permutation tests. BERT embeddings with clustering turned out to show the best results for both graded and binary semantic change detection outperforming the baseline. Our best submission for graded discovery was the 3 rd best result, while for binary detection it was the 2 nd place precision and the 7 th place both F1-score and recall. Our highest precision for binary detection was 0.75 and it was achieved due to improving grammatical profiling with permutation tests.",https://aclanthology.org/2022.lchange-1.21,Association for Computational Linguistics,2022,May,Proceedings of the 3rd Workshop on Computational Approaches to Historical Language Change,"Kashleva, Kseniia  and
Shein, Alexander  and
Tukhtina, Elizaveta  and
Vydrina, Svetlana",HSE at LSCDiscovery in Spanish: Clustering and Profiling for Lexical Semantic Change Discovery,10.18653/v1/2022.lchange-1.21,lchange,1368
2022.naacl-srw.20,"['Data Management and Generation', 'Discourse Analysis', 'Dialogue Systems', 'Image and Video Processing']","['Open Domain Dialogue Systems', 'Image Captioning', 'Data Preparation', 'Data Analysis']",['Annotation Processes'],"This paper explores how humans conduct conversations with images by investigating an open-domain image conversation dataset, Im-ageChat. We examined the conversations with images from the perspectives of image relevancy and image information.We found that utterances/conversations are not always related to the given image, and conversation topics diverge within three turns about half of the time. Besides image objects, more comprehensive non-object image information is also indispensable. After inspecting the causes, we suggested that understanding the overall scenario of image and connecting objects based on their high-level attributes might be very helpful to generate more engaging open-domain conversations when an image is presented. We proposed enriching the image information with image caption and object tags based on our analysis. With our proposed image + features, we improved automatic metrics including BLEU and Bert Score, and increased the diversity and image-relevancy of generated responses to the strong SOTA baseline. The result verifies that our analysis provides valuable insights and could facilitate future research on open-domain conversations with images.",https://aclanthology.org/2022.naacl-srw.20,Association for Computational Linguistics,2022,July,Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Student Research Workshop,"Chen, Yi-Pei  and
Shimizu, Nobuyuki  and
Miyazaki, Takashi  and
Nakayama, Hideki",How do people talk about images? A study on open-domain conversations with images.,10.18653/v1/2022.naacl-srw.20,naacl,914
S19-2102,"['Domain-specific NLP', 'Classification Applications', 'Model Architectures']","['Hate and Offensive Speech Detection', 'NLP for News and Media']",['NLP for Social Media'],"This paper presents the application of two strong baseline systems for toxicity detection and evaluates their performance in identifying and categorizing offensive language in social media. Perspective is an API, that serves multiple machine learning models for the improvement of conversations online, as well as a toxicity detection system, trained on a wide variety of comments from platforms across the Internet. BERT is a recently popular language representation model, fine tuned per task and achieving state of the art performance in multiple NLP tasks. Perspective performed better than BERT in detecting toxicity, but BERT was much better in categorizing the offensive type. Both baselines were ranked surprisingly high in the SEMEVAL-2019 OFFENSE-VAL competition, Perspective in detecting an offensive post 12th and BERT in categorizing it 11th. The main contribution of this paper is the assessment of two strong baselines for the identification Perspective and the categorization BERT of offensive language with little or no additional training data.",https://aclanthology.org/S19-2102,Association for Computational Linguistics,2019,June,Proceedings of the 13th International Workshop on Semantic Evaluation,"Pavlopoulos, John  and
Thain, Nithum  and
Dixon, Lucas  and
Androutsopoulos, Ion",ConvAI at SemEval-2019 Task 6: Offensive Language Identification and Categorization with Perspective and BERT,10.18653/v1/S19-2102,S19,886
2021.fever-1.2,"['Information Retrieval', 'Information Extraction', 'Classification Applications']",['Claim Verification'],,"In Automated Claim Verification, we retrieve evidence from a knowledge base to determine the veracity of a claim. Intuitively, the retrieval of the correct evidence plays a crucial role in this process. Often, evidence selection is tackled as a pairwise sentence classification task, i.e., we train a model to predict for each sentence individually whether it is evidence for a claim. In this work, we fine-tune document level transformers to extract all evidence from a Wikipedia document at once. We show that this approach performs better than a comparable model classifying sentences individually on all relevant evidence selection metrics in FEVER. Our complete pipeline building on this evidence selection procedure produces a new state-of-the-art result on FEVER, a popular claim verification benchmark.",https://aclanthology.org/2021.fever-1.2,Association for Computational Linguistics,2021,November,Proceedings of the Fourth Workshop on Fact Extraction and VERification (FEVER),"Stammbach, Dominik",Evidence Selection as a Token-Level Prediction Task,10.18653/v1/2021.fever-1.2,fever,966
2021.cmcl-1.15,"['Embeddings', 'Model Architectures']",['Transformer Models'],,"This paper describes the submission of the team KonTra to the CMCL 2021 Shared Task on eye-tracking prediction. Our system combines the embeddings extracted from a finetuned BERT model with surface, linguistic and behavioral features, resulting in an average mean absolute error of 4.22 across all 5 eyetracking measures. We show that word length and features representing the expectedness of a word are consistently the strongest predictors across all 5 eye-tracking measures.",https://aclanthology.org/2021.cmcl-1.15,Association for Computational Linguistics,2021,June,Proceedings of the Workshop on Cognitive Modeling and Computational Linguistics,"Yu, Qi  and
Kalouli, Aikaterini-Lida  and
Frassinelli, Diego","KonTra at CMCL 2021 Shared Task: Predicting Eye Movements by Combining BERT with Surface, Linguistic and Behavioral Information",10.18653/v1/2021.cmcl-1.15,cmcl,1164
2020.sltu-1.3,"['Audio Generation and Processing', 'Data Management and Generation', 'Low-resource Languages']",['Data Preparation'],,"This paper introduces new open speech datasets for three of the languages of Spain: Basque, Catalan and Galician. Catalan is furthermore the official language of the Principality of Andorra. The datasets consist of high-quality multi-speaker recordings of the three languages along with the associated transcriptions. The resulting corpora include over 33 hours of crowd-sourced recordings of 132 male and female native speakers. The recording scripts also include material for elicitation of global and local place names, personal and business names. The datasets are released under a permissive license and are available for free download for commercial, academic and personal use. The high-quality annotated speech datasets described in this paper can be used to, among other things, build text-to-speech systems, serve as adaptation data in automatic speech recognition and provide useful phonetic and phonological insights in corpus linguistics.",https://aclanthology.org/2020.sltu-1.3,European Language Resources association,2020,May,Proceedings of the 1st Joint Workshop on Spoken Language Technologies for Under-resourced languages (SLTU) and Collaboration and Computing for Under-Resourced Languages (CCURL),"Kjartansson, Oddur  and
Gutkin, Alexander  and
Butryna, Alena  and
Demirsahin, Isin  and
Rivera, Clara","Open-Source High Quality Speech Datasets for Basque, Catalan and Galician",,sltu,530
2020.lt4hala-1.10,"['Data Management and Generation', 'Low-resource Languages', 'Knowledge Representation and Reasoning']",['Data Preparation'],,"We build a thesaurus for Biblical Hebrew, with connections between roots based on phonetic, semantic, and distributional similarity. To this end, we apply established algorithms to find connections between headwords based on existing lexicons and other digital resources. For semantic similarity, we utilize the cosine-similarity of tf-idf vectors of English gloss text of Hebrew headwords from Ernest Klein's A Comprehensive Etymological Dictionary of the Hebrew Language for Readers of English as well as from Brown-Driver-Brigg's Hebrew Lexicon. For phonetic similarity, we digitize part of Matityahu Clark's Etymological Dictionary of Biblical Hebrew, grouping Hebrew roots into phonemic classes, and establish phonetic relationships between headwords in Klein's Dictionary. For distributional similarity, we consider the cosine similarity of PPMI vectors of Hebrew roots and also, in a somewhat novel approach, apply Word2Vec to a Biblical corpus reduced to its lexemes. The resulting resource is helpful to those trying to understand Biblical Hebrew, and also stands as a good basis for programs trying to process the Biblical text.",https://aclanthology.org/2020.lt4hala-1.10,European Language Resources Association (ELRA),2020,May,Proceedings of LT4HALA 2020 - 1st Workshop on Language Technologies for Historical and Ancient Languages,"Azar, Miriam  and
Pahmer, Aliza  and
Waxman, Joshua",A Thesaurus for Biblical Hebrew,,lt4hala,1253
2021.maiworkshop-1.2,"['Text Generation', 'Embeddings', 'Machine Translation (MT)', 'Image and Video Processing', 'Model Architectures']","['Image Captioning', 'Word Embeddings', 'Neural MT (NMT)']",,"In natural language generation tasks, a neural language model is used for generating a sequence of words forming a sentence. The topmost weight matrix of the language model, known as the classification layer, can be viewed as a set of vectors, each representing a target word from the target dictionary. The target word vectors, along with the rest of the model parameters, are learned and updated during training. In this paper, we analyze the properties encoded in the target vectors and question the necessity of learning these vectors. We suggest to randomly draw the target vectors and set them as fixed so that no weights updates are being made during training. We show that by excluding the vectors from the optimization, the number of parameters drastically decreases with a marginal effect on the performance. We demonstrate the effectiveness of our method in image-captioning and machine-translation.",https://aclanthology.org/2021.maiworkshop-1.2,Association for Computational Linguistics,2021,June,Proceedings of the Third Workshop on Multimodal Artificial Intelligence,"Shalev, Gal-Lev  and
Shalev, Gabi  and
Keshet, Joseph",On Randomized Classification Layers and Their Implications in Natural Language Generation,10.18653/v1/2021.maiworkshop-1.2,maiworkshop,1052
2020.finnlp-1.11,"['Text Preprocessing', 'Domain-specific NLP', 'Low-resource Languages', 'Image and Video Processing']","['NLP for Finance', 'Text Segmentation', 'Document Layout Analysis (DLA)']",['Sentence Segmentation'],"In this paper, we present the method we have designed and implemented for identifying lists and sentences in PDF documents while participating to FinSBD-2 Financial Document Analysis Shared Task. We propose a model-driven approach for the French and English datasets. It relies on a top-down process from the PDF itself in order to keep control of the workflow. Our objective is to use PDF structure extraction to improve text segment boundaries detection in an end-to-end fashion.",https://aclanthology.org/2020.finnlp-1.11,-,2020,05-Jan,Proceedings of the Second Workshop on Financial Technology and Natural Language Processing,"Giguet, Emmanuel  and
Lejeune, Ga{\""e}l","Daniel at the FinSBD-2 Task: Extracting List and Sentence Boundaries from PDF Documents, a model-driven approach to PDF document analysis",,finnlp,434
N18-1161,"['Evaluation Techniques', 'Low-resource Languages', 'Automatic Text Summarization']","['Extractive Text Summarization', 'Document Summarization']",,"The task of automatic text summarization is to generate a short text that summarizes the most important information in a given set of documents. Sentence regression is an emerging branch in automatic text summarizations. Its key idea is to estimate the importance of information via learned utility scores for individual sentences. These scores are then used for selecting sentences from the source documents, typically according to a greedy selection strategy. Recently proposed state-ofthe-art models learn to predict ROUGE recall scores of individual sentences, which seems reasonable since the final summaries are evaluated according to ROUGE recall. In this paper, we show in extensive experiments that following this intuition leads to suboptimal results and that learning to predict ROUGE precision scores leads to better results. The crucial difference is to aim not at covering as much information as possible but at wasting as little space as possible in every greedy step.",https://aclanthology.org/N18-1161,Association for Computational Linguistics,2018,June,"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)","Zopf, Markus  and
Loza Menc{\'\i}a, Eneldo  and
F{\""u}rnkranz, Johannes",Which Scores to Predict in Sentence Regression for Text Summarization?,10.18653/v1/N18-1161,N18,176
2022.starsem-1.26,"['Figurative Language', 'Prompt Engineering', 'Discourse Analysis']",['Metaphors'],,"Given a specific discourse, which discourse properties trigger the use of metaphorical language, rather than using literal alternatives? For example, what drives people to say grasp the meaning rather than understand the meaning within a specific context? Many NLP approaches to metaphorical language rely on cognitive and psycho-linguistic insights and have successfully defined models of discourse coherence, abstractness and affect. In this work, we build five simple models relying on established cognitive and linguistic properties -frequency, abstractness, affect, discourse coherence and contextualized word representations -to predict the use of a metaphorical vs. synonymous literal expression in context. By comparing the models' outputs to human judgments, our study indicates that our selected properties are not sufficient to systematically explain metaphorical vs. literal language choices.",https://aclanthology.org/2022.starsem-1.26,Association for Computational Linguistics,2022,July,Proceedings of the 11th Joint Conference on Lexical and Computational Semantics,"Piccirilli, Prisca  and
Schulte Im Walde, Sabine","What Drives the Use of Metaphorical Language? Negative Insights from Abstractness, Affect, Discourse Coherence and Contextualized Word Representations",10.18653/v1/2022.starsem-1.26,starsem,593
2021.deelio-1.11,"['Domain-specific NLP', 'Classification Applications', 'Model Architectures']","['NLP for News and Media', 'Sentiment Analysis (SA)', 'Recurrent Neural Networks (RNNs)', 'Medical and Clinical NLP']",['NLP for Social Media'],"This paper presents a way to inject and leverage existing knowledge from external sources in a Deep Learning environment, extending the recently proposed Recurrent Independent Mechnisms RIMs architecture, which comprises a set of interacting yet independent modules. We show that this extension of the RIMs architecture is an effective framework with lower parameter implications compared to purely fine-tuned systems.",https://aclanthology.org/2021.deelio-1.11,Association for Computational Linguistics,2021,June,Proceedings of Deep Learning Inside Out (DeeLIO): The 2nd Workshop on Knowledge Extraction and Integration for Deep Learning Architectures,"Bagherzadeh, Parsa  and
Bergler, Sabine",Multi-input Recurrent Independent Mechanisms for leveraging knowledge sources: Case studies on sentiment analysis and health text mining,10.18653/v1/2021.deelio-1.11,deelio,996
2020.smm4h-1.31,"['Domain-specific NLP', 'Classification Applications', 'Model Architectures']","['Medical and Clinical NLP', 'NLP for News and Media']",['NLP for Social Media'],"This paper details the system description and approach used by our team for the SMM4H 2020 competition, Task 1. Task 1 targets the automatic classification of tweets that mention medication. We adapted the standard BERT pretrain-then-fine-tune approach to include an intermediate training stage with a biLSTM architecture neural network acting as a further fine-tuning stage. We were inspired by the effectiveness of within-task further pre-training and sentence encoders. We show that this approach works well for a highly imbalanced dataset. In this case, the positive class is only 0.2% of the entire dataset. Our model performed better in both F1 and precision scores compared to the mean score for all participants in the competition and had a competitive recall score.",https://aclanthology.org/2020.smm4h-1.31,Association for Computational Linguistics,2020,December,Proceedings of the Fifth Social Media Mining for Health Applications Workshop {\&} Shared Task,"Aduragba, Olanrewaju Tahir  and
Yu, Jialin  and
Senthilnathan, Gautham  and
Crsitea, Alexandra",Sentence Contextual Encoder with BERT and BiLSTM for Automatic Classification with Imbalanced Medication Tweets,,smm4h,1111
2022.gebnlp-1.24,"['Biases in NLP', 'Domain-specific NLP', 'Data Management and Generation', 'Classification Applications', 'Model Architectures']","['Transformer Models', 'Sentiment Analysis (SA)', 'Data Analysis']",,"Films are a rich source of data for natural language processing. OpenSubtitles Lison and Tiedemann, 2016  is a popular movie script dataset, used for training models for tasks such as machine translation and dialogue generation. However, movies often contain biases that reflect society at the time, and these biases may be introduced during pre-training and influence downstream models. We perform sentiment analysis on template infilling Kurita et al., 2019 and the Sentence Embedding Association Test May et al., 2019 to measure how BERT-based language models change after continued pre-training on OpenSubtitles. We consider gender bias as a primary motivating case for this analysis, while also measuring other social biases such as disability. We show that sentiment analysis on template infilling is not an effective measure of bias due to the rarity of disability and gender identifying tokens in the movie dialogue. We extend our analysis to a longitudinal study of bias in film dialogue over the last 110 years and find that continued pretraining on OpenSubtitles encodes additional bias into BERT. We show that BERT learns associations that reflect the biases and representation of each film era, suggesting that additional care must be taken when using historical data.",https://aclanthology.org/2022.gebnlp-1.24,Association for Computational Linguistics,2022,July,Proceedings of the 4th Workshop on Gender Bias in Natural Language Processing (GeBNLP),"Bertsch, Amanda  and
Oh, Ashley  and
Natu, Sanika  and
Gangu, Swetha  and
Black, Alan W.  and
Strubell, Emma",Evaluating Gender Bias Transfer from Film Data,10.18653/v1/2022.gebnlp-1.24,gebnlp,380
2021.emnlp-main.754,"['Question Answering (QA)', 'Learning Paradigms', 'Classification Applications', 'Model Architectures']","['Transformer Models', 'Transfer Learning', 'Open-Domain QA']",,"Question generation has recently shown impressive results in customizing question answering QA systems to new domains. These approaches circumvent the need for manually annotated training data from the new domain and, instead, generate synthetic questionanswer pairs that are used for training. However, existing methods for question generation rely on large amounts of synthetically generated datasets and costly computational resources, which render these techniques widely inaccessible when the text corpora is of limited size. This is problematic as many niche domains rely on small text corpora, which naturally restricts the amount of synthetic data that can be generated. In this paper, we propose a novel framework for domain adaptation called contrastive domain adaptation for QA CAQA. Specifically, CAQA combines techniques from question generation and domaininvariant learning to answer out-of-domain questions in settings with limited text corpora. Here, we train a QA system on both source data and generated data from the target domain with a contrastive adaptation loss that is incorporated in the training objective. By combining techniques from question generation and domain-invariant learning, our model achieved considerable improvements compared to stateof-the-art baselines.",https://aclanthology.org/2021.emnlp-main.754,Association for Computational Linguistics,2021,November,Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,"Yue, Zhenrui  and
Kratzwald, Bernhard  and
Feuerriegel, Stefan",Contrastive Domain Adaptation for Question Answering using Limited Text Corpora,10.18653/v1/2021.emnlp-main.754,emnlp,1282
2020.acl-main.486,"['Biases in NLP', 'Data Management and Generation', 'Classification Applications', 'Text Generation']",['Data Preparation'],['Annotation Processes'],"Warning: this paper contains content that may be offensive or upsetting. Language has the power to reinforce stereotypes and project social biases onto others. At the core of the challenge is that it is rarely what is stated explicitly, but rather the implied meanings, that frame people's judgments about others. For example, given a statement that ""we shouldn't lower our standards to hire more women,"" most listeners will infer the implicature intended by the speaker -that ""women candidates are less qualified."" Most semantic formalisms, to date, do not capture such pragmatic implications in which people express social biases and power differentials in language. We introduce SOCIAL BIAS FRAMES, a new conceptual formalism that aims to model the pragmatic frames in which people project social biases and stereotypes onto others. In addition, we introduce the Social Bias Inference Corpus to support large-scale modelling and evaluation with 150k structured annotations of social media posts, covering over 34k implications about a thousand demographic groups. We then establish baseline approaches that learn to recover SOCIAL BIAS FRAMES from unstructured text. We find that while stateof-the-art neural models are effective at highlevel categorization of whether a given statement projects unwanted social bias 80% F 1 , they are not effective at spelling out more detailed explanations in terms of SOCIAL BIAS FRAMES. Our study motivates future work that combines structured pragmatic inference with commonsense reasoning on social implications.",https://aclanthology.org/2020.acl-main.486,Association for Computational Linguistics,2020,July,Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,"Sap, Maarten  and
Gabriel, Saadia  and
Qin, Lianhui  and
Jurafsky, Dan  and
Smith, Noah A.  and
Choi, Yejin",Social Bias Frames: Reasoning about Social and Power Implications of Language,10.18653/v1/2020.acl-main.486,acl,241
C18-2035,"['Parsing', 'Multilingual NLP', 'Model Architectures']",['Semantic Parsing'],['Semantic Role Labeling'],"This paper presents DAMESRL 1 , a flexible and open source framework for deep semantic role labeling SRL. DAMESRL aims to facilitate easy exploration of model structures for multiple languages with different characteristics. It provides flexibility in its model construction in terms of word representation, sequence representation, output modeling, and inference styles and comes with clear output visualization. Additionally, it handles various input and output formats and comes with clear output visualization. The framework is available under the Apache 2.0 license. Task Definition Formally, the goal of end-to-end SRL is to predict a sequence l 1 , l 2 , . . . , l n  of semantic labels given a sentence w 1 , w 2 , . . . , w n , and its predicate w p as input. Each l i , which belongs to a discrete set of PropBank BIO tags, is the semantic tag corresponding to the word w i in the semantic frame evoked 1 The source code can be found at: https://liir.cs.kuleuven.be/software_pages/damesrl.php.",https://aclanthology.org/C18-2035,Association for Computational Linguistics,2018,August,Proceedings of the 27th International Conference on Computational Linguistics: System Demonstrations,"Do, Quynh Ngoc Thi  and
Leeuwenberg, Artuur  and
Heyman, Geert  and
Moens, Marie-Francine",A Flexible and Easy-to-use Semantic Role Labeling Framework for Different Languages,,C18,626
2021.gem-1.5,"['Evaluation Techniques', 'Embeddings', 'Dialogue Systems', 'Model Architectures']","['Large Language Models (LLMs)', 'Recurrent Neural Networks (RNNs)', 'Response Generation', 'Transformer Models']",['Long Short-Term Memory (LSTM) Models'],"Personalized response generation is essential for more human-like conversations. However, how to model user personalization information with no explicit user persona descriptions or demographics still remains underinvestigated. To tackle the data sparsity problem and the huge number of users, we utilize tensor factorization to model users' personalization information with their posting histories. Specifically, we introduce the personalized response embedding for all questionuser pairs and form them into a three-mode tensor, decomposed by Tucker decomposition. The personalized response embedding is fed to either the decoder of an LSTM-based Seq2Seq model or a transformer language model to help generate more personalized responses. To evaluate how personalized the generated responses are, we further propose a novel ranking-based metric called Per-Hits@k which measures how likely are the generated responses come from the corresponding users. Results on a large-scale English conversation dataset show that our proposed tensor factorization based models generate more personalized and higher quality responses compared to baselines. We have publicly released our code at https://github.com/GT-SALT/ personalized_response_generation.",https://aclanthology.org/2021.gem-1.5,Association for Computational Linguistics,2021,August,"Proceedings of the 1st Workshop on Natural Language Generation, Evaluation, and Metrics (GEM 2021)","Wang, Zhenghui  and
Luo, Lingxiao  and
Yang, Diyi",Personalized Response Generation with Tensor Factorization,10.18653/v1/2021.gem-1.5,gem,147
2020.alta-1.2,"['Domain-specific NLP', 'Model Architectures', 'Classification Applications']","['NLP for News and Media', 'Hate and Offensive Speech Detection']",['NLP for Social Media'],"Cyberbullying is a prevalent and growing social problem due to the surge of social media technology usage. Minorities, women, and adolescents are among the common victims of cyberbullying. Despite the advancement of NLP technologies, the automated cyberbullying detection remains challenging. This paper focuses on advancing the technology using state-of-the-art NLP techniques. We use a Twitter dataset from SemEval 2019 -Task 5 HatEval on hate speech against women and immigrants. Our best performing ensemble model based on DistilBERT has achieved 0.73 and 0.74 of F1 score in the task of classifying hate speech Task A and aggressiveness and target Task B respectively. We adapt the ensemble model developed for Task A to classify offensive language in external datasets and achieved 0.7 of F1 score using three benchmark datasets, enabling promising results for cross-domain adaptability. We conduct a qualitative analysis of misclassified tweets to provide insightful recommendations for future cyberbullying research.",https://aclanthology.org/2020.alta-1.2,Australasian Language Technology Association,2020,December,Proceedings of the The 18th Annual Workshop of the Australasian Language Technology Association,"Atapattu, Thushari  and
Herath, Mahen  and
Zhang, Georgia  and
Falkner, Katrina",Automated Detection of Cyberbullying Against Women and Immigrants and Cross-domain Adaptability,10.48550/arxiv.2012.02565,alta,551
2020.pam-1.9,['Discourse Analysis'],,,"Judgements about communicative agents evolve over the course of interactions both in how individuals are judged for testimonial reliability and for ideological trustworthiness. This paper combines a theory of social meaning and persona with a theory of reliability within a game-theoretic view of communication, giving a formal model involving interactional histories, repeated game models and ways of evaluating social meaning and trustworthiness.",https://aclanthology.org/2020.pam-1.9,Association for Computational Linguistics,2020,June,Proceedings of the Probability and Meaning Conference (PaM 2020),"McCready, Elin  and
Henderson, Robert",Social Meaning in Repeated Interactions,,pam,1226
Y18-1007,"['Domain-specific NLP', 'Data Management and Generation', 'Multilingual NLP', 'Low-resource Languages', 'Classification Applications', 'Model Architectures']","['Data Preparation', 'Recurrent Neural Networks (RNNs)']",['Long Short-Term Memory (LSTM) Models'],"Movies are one of the most prominent means of entertainment. The widespread use of the Internet in recent times has led to large volumes of data related to movies being generated and shared online. People often prefer to express their views online in English as compared to other local languages. This leaves us with a very little amount of data in languages apart from English to work on. To overcome this, we created the Multi-Language Movie Review Dataset MLMRD. The dataset consists of genre, rating, and synopsis of a movie across multiple languages, namely Hindi, Telugu, Tamil, Malayalam, Korean, French, and Japanese. The genre of a movie can be identified by its synopsis. Though the rating of a movie may depend on multiple factors like the performance of actors, screenplay, direction etc but in most of the cases, synopsis plays a crucial role in the movie rating. In this work, we provide various model architectures that can be used to predict the genre and the rating of a movie across various languages present in our dataset based on the synopsis.",https://aclanthology.org/Y18-1007,Association for Computational Linguistics,2018,1{--}3 December,"Proceedings of the 32nd Pacific Asia Conference on Language, Information and Computation","Battu, Varshit  and
Batchu, Vishal  and
Gangula, Rama Rohit Reddy  and
Dakannagari, Mohana Murali Krishna Reddy  and
Mamidi, Radhika",Predicting the Genre and Rating of a Movie Based on its Synopsis,,Y18,632
2020.emnlp-main.549,"['Model Architectures', 'Knowledge Representation and Reasoning']",['Graph Neural Networks (GNNs)'],,"Numerical reasoning over texts, such as addition, subtraction, sorting and counting, is a challenging machine reading comprehension task, since it requires both natural language understanding and arithmetic computation. To address this challenge, we propose a heterogeneous graph representation for the context of the passage and question needed for such reasoning, and design a question directed graph attention network to drive multi-step numerical reasoning over this context graph. Our model, which combines deep learning and graph reasoning, achieves remarkable results in benchmark datasets such as DROP 1 . * Corresponding author 1 https://leaderboard.allenai.org/drop/submissions/public. As of September 08, 2020, our models are ranked first in the case of fair comparison using the identical pre-training model.",https://aclanthology.org/2020.emnlp-main.549,Association for Computational Linguistics,2020,November,Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),"Chen, Kunlong  and
Xu, Weidi  and
Cheng, Xingyi  and
Xiaochuan, Zou  and
Zhang, Yuyu  and
Song, Le  and
Wang, Taifeng  and
Qi, Yuan  and
Chu, Wei",Question Directed Graph Attention Network for Numerical Reasoning over Text,10.18653/v1/2020.emnlp-main.549,emnlp,1421
2021.ranlp-srw.30,"['Domain-specific NLP', 'Data Management and Generation', 'Information Extraction', 'Low-resource Languages', 'Classification Applications', 'Model Architectures']","['Data Preparation', 'Medical and Clinical NLP']",,"Vast amounts of data in healthcare are available in unstructured text format, usually in the local language of the countries. These documents contain valuable information. Secondary use of clinical narratives and information extraction of key facts and relations from them about the patient disease history can foster preventive medicine and improve healthcare. In this paper, we propose a hybrid method for the automatic transformation of clinical text into a structured format. The documents are automatically sectioned into the following parts: diagnosis, patient history, patient status, lab results. For the ""Diagnosis"" section a deep learning text-based encoding into ICD-10 codes is applied using MBG-ClinicalBERT -a fine-tuned ClinicalBERT model for Bulgarian medical text. From the ""Patient History"" section, we identify patient symptoms using a rule-based approach enhanced with similarity search based on MBG-ClinicalBERT word embeddings. We also identify symptom relations like negation. For the ""Patient Status"" description, binary classification is used to determine the status of each anatomic organ. In this paper, we demonstrate different methods for adapting NLP tools for English and other languages to a low resource language like Bulgarian.",https://aclanthology.org/2021.ranlp-srw.30,INCOMA Ltd.,2021,September,Proceedings of the Student Research Workshop Associated with RANLP 2021,"Vassileva, Sylvia  and
Todorova, Gergana  and
Ivanova, Kristina  and
Velichkov, Boris  and
Koychev, Ivan  and
Angelova, Galia  and
Boytcheva, Svetla",Automatic Transformation of Clinical Narratives into Structured Format,10.26615/issn.2603-2821.2021_030,ranlp,392
2021.starsem-1.5,"['Evaluation Techniques', 'Model Architectures']",,,"Writers often repurpose material from existing texts when composing new documents. Because most documents have more than one source, we cannot trace these connections using only models of document-level similarity. Instead, this paper considers methods for local text reuse detection LTRD, detecting localized regions of lexically or semantically similar text embedded in otherwise unrelated material. In extensive experiments, we study the relative performance of four classes of neural and bag-of-words models on three LTRD tasks -detecting plagiarism, modeling journalists' use of press releases, and identifying scientists' citation of earlier papers. We conduct evaluations on three existing datasets and a new, publicly-available citation localization dataset. Our findings shed light on a number of previously-unexplored questions in the study of LTRD, including the importance of incorporating document-level context for predictions, the applicability of of-the-shelf neural models pretrained on ""general"" semantic textual similarity tasks such as paraphrase detection, and the trade-offs between more efficient bag-of-words and feature-based neural models and slower pairwise neural models.",https://aclanthology.org/2021.starsem-1.5,Association for Computational Linguistics,2021,August,Proceedings of *SEM 2021: The Tenth Joint Conference on Lexical and Computational Semantics,"MacLaughlin, Ansel  and
Xu, Shaobin  and
Smith, David A.",Recovering Lexically and Semantically Reused Texts,10.18653/v1/2021.starsem-1.5,starsem,458
2020.inlg-1.8,"['Model Architectures', 'Discourse Analysis', 'Text Generation']",['Large Language Models (LLMs)'],,"Recent advances in NLP have been attributed to the emergence of large-scale pre-trained language models. GPT-2 Radford et al., 2019, in particular, is suited for generation tasks given its left-to-right language modeling objective, yet the linguistic quality of its generated text has largely remain unexplored. Our work takes a step in understanding GPT-2's outputs in terms of discourse coherence. We perform a comprehensive study on the validity of explicit discourse relations in GPT-2's outputs under both organic generation and fine-tuned scenarios. Results show GPT-2 does not always generate text containing valid discourse relations; nevertheless, its text is more aligned with human expectation in the fine-tuned scenario. We propose a decoupled strategy to mitigate these problems and highlight the importance of explicitly modeling discourse information.",https://aclanthology.org/2020.inlg-1.8,Association for Computational Linguistics,2020,December,Proceedings of the 13th International Conference on Natural Language Generation,"Ko, Wei-Jen  and
Li, Junyi Jessy",Assessing Discourse Relations in Language Generation from GPT-2,10.18653/v1/2020.inlg-1.8,inlg,236
2020.framenet-1.8,"['Data Management and Generation', 'Knowledge Representation and Reasoning', 'Low-resource Languages']",['Data Analysis'],,"This paper presents the first investigation on using semantic frames to assess text difficulty. Based on Mandarin VerbNet, a verbal semantic database that adopts a frame-based approach, we examine usage patterns of ten verbs in a corpus of graded Chinese texts. We identify a number of characteristics in texts at advanced grades: more frequent use of non-core frame elements; more frequent omission of some core frame elements; increased preference for noun phrases rather than clauses as verb arguments; and more frequent metaphoric usage. These characteristics can potentially be useful for automatic prediction of text readability.",https://aclanthology.org/2020.framenet-1.8,European Language Resources Association,2020,May,"Proceedings of the International FrameNet Workshop 2020: Towards a Global, Multilingual FrameNet","Lee, John  and
Liu, Meichun  and
Cai, Tianyuan",Using Verb Frames for Text Difficulty Assessment,,framenet,197
2020.ecomnlp-1.9,"['Model Architectures', 'Text Generation']",['Transformer Models'],,"E-commerce sites include advertising slogans along with information regarding items. Slogans can attract viewers' attention to increase sales or visits by emphasizing advantages of items. The aim of this study is to generate a slogan from a description of an item. To generate a slogan, we apply an encoder-decoder model which has shown effectiveness in many kinds of natural language generation tasks, such as abstractive summarization. However, slogan generation task has three characteristics that distinguish it from other natural language generation tasks: distinctiveness, topic emphasis, and style difference. To handle these three characteristics, we propose a compressed representation-based reconstruction model with refer-attention and conversion layers. The results of experiments with automatic and human evaluations indicate that our method achieves higher performance than conventional methods.",https://aclanthology.org/2020.ecomnlp-1.9,Association for Computational Linguistics,2020,December,Proceedings of Workshop on Natural Language Processing in E-Commerce,"Misawa, Shotaro  and
Miura, Yasuhide  and
Taniguchi, Tomoki  and
Ohkuma, Tomoko",Distinctive Slogan Generation with Reconstruction,,ecomnlp,531
2020.nlp4convai-1.14,"['Model Architectures', 'Dialogue Systems']",['Chatbots'],,"Human-like chit-chat conversation requires agents to generate responses that are fluent, engaging and consistent. We propose Sketch-Fill-A-R, a framework that uses a personamemory to generate chit-chat responses in three phases. First, it generates dynamic sketch responses with open slots. Second, it generates candidate responses by filling slots with parts of its stored persona traits. Lastly, it ranks and selects the final response via a language model score. Sketch-Fill-A-R outperforms a state-of-the-art baseline both quantitatively 10-point lower perplexity and qualitatively preferred by 55% in head-to-head single-turn studies and 20% higher in consistency in multi-turn user studies on the Persona-Chat dataset. Finally, we extensively analyze Sketch-Fill-A-R's responses and human feedback, and show it is more consistent and engaging by using more relevant responses and questions. * Work done as an intern at Salesforce Research.",https://aclanthology.org/2020.nlp4convai-1.14,Association for Computational Linguistics,2020,July,Proceedings of the 2nd Workshop on Natural Language Processing for Conversational AI,"Shum, Michael  and
Zheng, Stephan  and
Kryscinski, Wojciech  and
Xiong, Caiming  and
Socher, Richard",Sketch-Fill-A-R: A Persona-Grounded Chit-Chat Generation Framework,10.18653/v1/2020.nlp4convai-1.14,nlp4convai,584
P16-2022,"['Classification Applications', 'Model Architectures']",,,"In this paper, we propose the TBCNNpair model to recognize entailment and contradiction between two sentences. In our model, a tree-based convolutional neural network TBCNN captures sentencelevel semantics; then heuristic matching layers like concatenation, element-wise product/difference combine the information in individual sentences. Experimental results show that our model outperforms existing sentence encoding-based approaches by a large margin.",https://aclanthology.org/P16-2022,Association for Computational Linguistics,2016,August,Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),"Mou, Lili  and
Men, Rui  and
Li, Ge  and
Xu, Yan  and
Zhang, Lu  and
Yan, Rui  and
Jin, Zhi",Natural Language Inference by Tree-Based Convolution and Heuristic Matching,10.18653/v1/P16-2022,P16,1003
P16-2052,"['Domain-specific NLP', 'Classification Applications', 'Data Management and Generation']","['Data Preparation', 'Personality Trait Prediction', 'NLP for News and Media']","['Annotation Processes', 'NLP for Social Media']","Optimism is linked to various personality factors as well as both psychological and physical health, but how does it relate to the way a person tweets? We analyze the online activity of a set of Twitter users in order to determine how well machine learning algorithms can detect a person's outlook on life by reading their tweets. A sample of tweets from each user is manually annotated in order to establish ground truth labels, and classifiers are trained to distinguish between optimistic and pessimistic users. Our results suggest that the words in people's tweets provide ample evidence to identify them as optimists, pessimists, or somewhere in between. Additionally, several applications of these trained models are explored.",https://aclanthology.org/P16-2052,Association for Computational Linguistics,2016,August,Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),"Ruan, Xianzhi  and
Wilson, Steven  and
Mihalcea, Rada",Finding Optimists and Pessimists on Twitter,10.18653/v1/P16-2052,P16,1054
2020.parlaclarin-1.11,"['Domain-specific NLP', 'Data Management and Generation', 'Low-resource Languages', 'Language Change Analysis']",['Data Analysis'],,"Most diachronic studies on either lexico-semantic change or political language usage are based on individual or structurally similar corpora. In this paper, we explore ways of studying the stability and changeability of lexical usage in political discourse across two corpora which are substantially different in structure and size. We present a case study focusing on lexical items associated with political parties in two diachronic corpora of Austrian German, namely a diachronic media corpus AMC and a corpus of parliamentary records ParlAT, and measure the cross-temporal stability of lexical usage over a period of 20 years. We conduct three sets of comparative analyses investigating a the stability of sets of lexical items associated with the three major political parties over time, b lexical similarity between parties, and c the similarity between the lexical choices in parliamentary speeches by members of the parties vis--vis the media's reporting on the parties. We employ time series modeling using generalized additive models GAMs to compare the lexical similarities and differences between parties within and across corpora. The results show that changes observed in these measures can be meaningfully related to political events during that time.",https://aclanthology.org/2020.parlaclarin-1.11,European Language Resources Association,2020,May,Proceedings of the Second ParlaCLARIN Workshop,"Hofmann, Klaus  and
Marakasova, Anna  and
Baumann, Andreas  and
Neidhardt, Julia  and
Wissik, Tanja",Comparing Lexical Usage in Political Discourse across Diachronic Corpora,,parlaclarin,776
2020.osact-1.18,"['Domain-specific NLP', 'Model Architectures', 'Classification Applications', 'Low-resource Languages']","['NLP for News and Media', 'Hate and Offensive Speech Detection']",['NLP for Social Media'],"As the use of social media platforms increases extensively to freely communicate and share opinions, hate speech becomes an outstanding problem that requires urgent attention. This paper focuses on the problem of detecting hate speech in Arabic tweets. To tackle the problem efficiently, we adopt a ""quick and simple"" approach by which we investigate the effectiveness of 15 classical e.g., SVM and neural e.g., CNN learning models, while exploring two different term representations. Our experiments on 8k labelled dataset show that the best neural learning models outperform the classical ones, while distributed term representation is more effective than statistical bag-of-words representation. Overall, our best classifier that combines both CNN and RNN in a joint architecture achieved 0.73 macro-F1 score on the dev set, which significantly outperforms the majority-class baseline that achieves 0.49, proving the effectiveness of our ""quick and simple"" approach.",https://aclanthology.org/2020.osact-1.18,European Language Resource Association,2020,May,"Proceedings of the 4th Workshop on Open-Source Arabic Corpora and Processing Tools, with a Shared Task on Offensive Language Detection","Abuzayed, Abeer  and
Elsayed, Tamer",Quick and Simple Approach for Detecting Hate Speech in Arabic Tweets,,osact,1467
2021.cmcl-1.10,['Model Architectures'],,,"A LightGBM model fed with target word lexical characteristics and features obtained from word frequency lists, psychometric data and bigram association measures has been optimized for the 2021 CMCL Shared Task on Eye-Tracking Data Prediction. It obtained the best performance of all teams on two of the five eye-tracking measures to predict, allowing it to rank first on the official challenge criterion and to outperform all deep-learning based systems participating in the challenge.",https://aclanthology.org/2021.cmcl-1.10,Association for Computational Linguistics,2021,June,Proceedings of the Workshop on Cognitive Modeling and Computational Linguistics,"Bestgen, Yves",LAST at CMCL 2021 Shared Task: Predicting Gaze Data During Reading with a Gradient Boosting Decision Tree Approach,10.18653/v1/2021.cmcl-1.10,cmcl,404
2016.eamt-2.18,"['Multilingual NLP', 'Low-resource Languages', 'Machine Translation (MT)']",,,"MTExpert is AMPLEXOR's proprietary machine translation MT system based on state-of-the-art statistical and linguistic algorithms, easily integrated with existing linguistic assets, delivering quality results tailored to different communication objectives.",https://aclanthology.org/2016.eamt-2.18,Baltic Journal of Modern Computing,2016,May 30{--}June 1,Proceedings of the 19th Annual Conference of the European Association for Machine Translation: Projects/Products,"Ceausu, Alexandru  and
Hunsicker, Sabine  and
Droumaguet, Tudy",Amplexor MTExpert -- machine translation adapted to the translation workflow,,eamt,1122
2016.amta-researchers.2,['Machine Translation (MT)'],,,"We assessed how different machine translation MT systems affect the post-editing PE process and product of professional English-Spanish translators. Our model found that for each 1-point increase in BLEU, there is a PE time decrease of 0.16 seconds per word, about 3-4%. The MT system with the lowest BLEU score produced the output that was post-edited to the lowest quality and with the highest PE effort, measured both in HTER and actual PE operations.",https://aclanthology.org/2016.amta-researchers.2,The Association for Machine Translation in the Americas,2016,October 28 - November 1,Conferences of the Association for Machine Translation in the Americas: MT Researchers' Track,"Sanchez-Torron, Marina  and
Koehn, Philipp",Machine Translation Quality and Post-Editor Productivity,,amta,1024
N18-4012,"['Domain-specific NLP', 'Model Architectures']",['NLP for News and Media'],,"Gated-Attention GA Reader has been effective for reading comprehension. GA Reader makes two assumptions: 1 a uni-directional attention that uses an input query to gate token encodings of a document; 2 encoding at the cloze position of an input query is considered for answer prediction. In this paper, we propose Collaborative Gating CG and Self-Belief Aggregation SBA to address the above assumptions respectively. In CG, we first use an input document to gate token encodings of an input query so that the influence of irrelevant query tokens may be reduced. Then the filtered query is used to gate token encodings of an document in a collaborative fashion. In SBA, we conjecture that query tokens other than the cloze token may be informative for answer prediction. We apply self-attention to link the cloze token with other tokens in a query so that the importance of query tokens with respect to the cloze position are weighted. Then their evidences are weighted, propagated and aggregated for better reading comprehension. Experiments show that our approaches advance the state-of-theart results in CNN, Daily Mail, and Who Did What public test sets.",https://aclanthology.org/N18-4012,Association for Computational Linguistics,2018,June,Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Student Research Workshop,"Deng, Haohui  and
Tam, Yik-Cheung",Read and Comprehend by Gated-Attention Reader with More Belief,10.18653/v1/N18-4012,N18,647
2020.computerm-1.11,"['Domain-specific NLP', 'Data Management and Generation', 'Information Extraction', 'Knowledge Representation and Reasoning', 'Low-resource Languages']",['Data Preparation'],['Annotation Processes'],"Our contribution is part of a wider research project on term variation in German and concentrates on the computational aspects of a frame-based model for term meaning representation in the technical field. We focus on the role of frames in the sense of Frame-Based Terminology as the semantic interface between concepts covered by a domain ontology and domain-specific terminology. In particular, we describe methods for performing frame-based corpus annotation and frame-based term extraction. The aim of the contribution is to discuss the capacity of the model to automatically acquire semantic knowledge suitable for terminographic information tools such as specialised dictionaries, and its applicability to further specialised languages.",https://aclanthology.org/2020.computerm-1.11,European Language Resources Association,2020,May,Proceedings of the 6th International Workshop on Computational Terminology,"Giacomini, Laura  and
Sch{\""a}fer, Johannes",Computational Aspects of Frame-based Meaning Representation in Terminology,,computerm,640
2022.fever-1.2,"['Classification Applications', 'Model Architectures']","['Transformer Models', 'Claim Verification', 'Graph Neural Networks (GNNs)']",,"Fact checking is a challenging task that requires corresponding evidences to verify the property of a claim based on reasoning. Previous studies generally i construct the graph by treating each evidence-claim pair as node which is a simple way that ignores to exploit their implicit interaction, or building a fully-connected graph among claim and evidences where the entailment relationship between claim and evidence would be considered equal to the semantic relationship among evidences; ii aggregate evidences equally without considering their different stances towards the verification of fact. Towards the above issues, we propose a novel heterogeneous-graph reasoning and finegrained aggregation model, with two following modules: 1 a heterogeneous graph attention network module to distinguish different types of relationships within the constructed graph; 2 fine-grained aggregation module which learns the implicit stance of evidences towards the prediction result in details. Extensive experiments on the benchmark dataset demonstrate that our proposed model achieves much better performance than state-of-the-art methods.",https://aclanthology.org/2022.fever-1.2,Association for Computational Linguistics,2022,May,Proceedings of the Fifth Fact Extraction and VERification Workshop (FEVER),"Lin, Hongbin  and
Fu, Xianghua",Heterogeneous-Graph Reasoning and Fine-Grained Aggregation for Fact Checking,10.18653/v1/2022.fever-1.2,fever,610
2021.alvr-1.1,"['Embeddings', 'Machine Translation (MT)', 'Image and Video Processing', 'Learning Paradigms', 'Low-resource Languages', 'Model Architectures']","['Multimodal Learning', 'Neural MT (NMT)']",,"Caption translation aims to translate image annotations captions for short. Recently, Multimodal Neural Machine Translation MNMT has been explored as the essential solution. Besides of linguistic features in captions, MNMT allows visual image featuresto be used. The integration of multimodal features reinforces the semantic representation and considerably improves translation performance. However, MNMT suffers from the incongruence between visual and linguistic features. To overcome the problem, we propose to extend MNMT architecture with a harmonization network, which harmonizes multimodal features linguistic and visual features by unidirectional modal space conversion. It enables multimodal translation to be carried out in a seemingly monomodal translation pipeline. We experiment on the golden Multi30k-16 and 17. Experimental results show that, compared to the baseline, the proposed method yields the improvements of 2.2% BLEU for the scenario of translating English captions into German EnDe at best, 7.6% for the case of Englishto-French translation EnFr and 1.5% for English-to-Czech EnCz. The utilization of harmonization network leads to the competitive performance to the-state-of-the-art.",https://aclanthology.org/2021.alvr-1.1,Association for Computational Linguistics,2021,June,Proceedings of the Second Workshop on Advances in Language and Vision Research,"Li, Zhifeng  and
Hong, Yu  and
Pan, Yuchen  and
Tang, Jian  and
Yao, Jianmin  and
Zhou, Guodong",Feature-level Incongruence Reduction for Multimodal Translation,10.18653/v1/2021.alvr-1.1,alvr,961
L16-1028,"['Data Management and Generation', 'Information Extraction', 'Domain-specific NLP']","['Data Preparation', 'Coreference Resolution']",['Annotation Processes'],"Characters form the focus of various studies of literary works, including social network analysis, archetype induction, and plot comparison. The recent rise in the computational modelling of literary works has produced a proportional rise in the demand for character-annotated literary corpora. However, automatically identifying characters is an open problem and there is low availability of literary texts with manually labelled characters. To address the latter problem, this work presents three contributions: 1 a comprehensive scheme for manually resolving mentions to characters in texts. 2 A novel collaborative annotation tool, CHARLES CHAracter Resolution Label-Entry System for character annotation and similiar cross-document tagging tasks. 3 The character annotations resulting from a pilot study on the novel Pride and Prejudice, demonstrating the scheme and tool facilitate the efficient production of high-quality annotations. We expect this work to motivate the further production of annotated literary corpora to help meet the demand of the community.",https://aclanthology.org/L16-1028,European Language Resources Association (ELRA),2016,May,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),"Vala, Hardik  and
Dimitrov, Stefan  and
Jurgens, David  and
Piper, Andrew  and
Ruths, Derek","Annotating Characters in Literary Corpora: A Scheme, the CHARLES Tool, and an Annotated Novel",,L16,336
2020.osact-1.2,"['Information Extraction', 'Classification Applications', 'Question Answering (QA)', 'Low-resource Languages', 'Model Architectures']","['Named Entity Recognition (NER)', 'Large Language Models (LLMs)', 'Transformer Models', 'Sentiment Analysis (SA)']",,"The Arabic language is a morphologically rich language with relatively few resources and a less explored syntax compared to English. Given these limitations, Arabic Natural Language Processing NLP tasks like Sentiment Analysis SA, Named Entity Recognition NER, and Question Answering QA, have proven to be very challenging to tackle. Recently, with the surge of transformers based models, language-specific BERT based models have proven to be very efficient at language understanding, provided they are pre-trained on a very large corpus. Such models were able to set new standards and achieve state-of-the-art results for most NLP tasks. In this paper, we pre-trained BERT specifically for the Arabic language in the pursuit of achieving the same success that BERT did for the English language. The performance of AraBERT is compared to multilingual BERT from Google and other state-of-the-art approaches. The results showed that the newly developed AraBERT achieved state-of-the-art performance on most tested Arabic NLP tasks. The pretrained araBERT models are publicly available on github.com/aub-mind/araBERT hoping to encourage research and applications for Arabic NLP.",https://aclanthology.org/2020.osact-1.2,European Language Resource Association,2020,May,"Proceedings of the 4th Workshop on Open-Source Arabic Corpora and Processing Tools, with a Shared Task on Offensive Language Detection","Antoun, Wissam  and
Baly, Fady  and
Hajj, Hazem",AraBERT: Transformer-based Model for Arabic Language Understanding,10.48550/arxiv.2003.00104,osact,271
2021.tlt-1.7,"['Data Management and Generation', 'Low-resource Languages', 'Classification Applications']",['Data Analysis'],,"This work provides the first in-depth analysis of genre in Universal Dependencies UD. In contrast to prior work on genre identification which uses small sets of well-defined labels in mono-/bilingual setups, UD contains 18 genres with varying degrees of specificity spread across 114 languages. As most treebanks are labeled with multiple genres while lacking annotations about which instances belong to which genre, we propose four methods for predicting instance-level genre using weak supervision from treebank metadata. The proposed methods recover instancelevel genre better than competitive baselines as measured on a subset of UD with labeled instances and adhere better to the global expected distribution. Our analysis sheds light on prior work using UD genre metadata for treebank selection, finding that metadata alone are a noisy signal and must be disentangled within treebanks before it can be universally applied.",https://aclanthology.org/2021.tlt-1.7,Association for Computational Linguistics,2021,December,"Proceedings of the 20th International Workshop on Treebanks and Linguistic Theories (TLT, SyntaxFest 2021)","M{\""u}ller-Eberstein, Max  and
van der Goot, Rob  and
Plank, Barbara",How Universal is Genre in Universal Dependencies?,10.48550/arxiv.2112.04971,tlt,1487
2020.coling-demos.14,"['Data Management and Generation', 'Dialogue Systems']","['Data Preparation', 'Chatbots']",['Annotation Processes'],"In this paper, we introduce Annobot: a platform for annotating and creating datasets through conversation with a chatbot. This natural form of interaction has allowed us to create a more accessible and flexible interface, especially for mobile devices. Our solution has a wide range of applications such as data labelling for binary, multi-class/label classification tasks, preparing data for regression problems, or creating sets for issues such as machine translation, question answering or text summarization. Additional features include pre-annotation, active sampling, online learning and real-time inter-annotator agreement. The system is integrated with the popular messaging platform: Facebook Messanger. Usability experiment showed the advantages of the proposed platform compared to other labelling tools. The source code of Annobot is available under the GNU LGPL license at https://github.com/rafalposwiata/annobot.",https://aclanthology.org/2020.coling-demos.14,International Committee on Computational Linguistics (ICCL),2020,December,Proceedings of the 28th International Conference on Computational Linguistics: System Demonstrations,"Po{\'s}wiata, Rafa{\l}  and
Pere{\l}kiewicz, Micha{\l}",Annobot: Platform for Annotating and Creating Datasets through Conversation with a Chatbot,10.18653/v1/2020.coling-demos.14,coling,257
2020.signlang-1.33,"['Image and Video Processing', 'Data Management and Generation', 'Low-resource Languages']","['Sign Language and Fingerspelling Recognition', 'Data Preparation']",['Annotation Processes'],"Representation of linguistic data is an issue of utmost importance when developing language resources, but the lack of a standard written form in sign languages presents a challenge. Different notation systems exist, but only SignWriting seems to have some use in the native signer community. It is, however, a difficult system to use computationally, not based on a linear sequence of characters. We present the project ""VisSE"", which aims to develop tools for the effective use of SignWriting in the computer. The first of these is an application which uses computer vision to interpret SignWriting, understanding the meaning of new or existing transcriptions, or even hand-written images. Two additional tools will be able to consume the result of this recognizer: first, a textual description of the features of the transcription will make it understandable for non-signers. Second, a three-dimensional avatar will be able to reproduce the configurations and movements contained within the transcription, making it understandable for signers even if not familiar with SignWriting. Additionally, the project will result in a corpus of annotated SignWriting data which will also be of use to the computational linguistics community.",https://aclanthology.org/2020.signlang-1.33,European Language Resources Association (ELRA),2020,May,"Proceedings of the LREC2020 9th Workshop on the Representation and Processing of Sign Languages: Sign Language Resources in the Service of the Language Community, Technological Challenges and Application Perspectives","Sevilla, Antonio F. G.  and
D{\'\i}az Esteban, Alberto  and
Lahoz-Bengoechea, Jos{\'e} Mar{\'\i}a",Tools for the Use of SignWriting as a Language Resource,,signlang,1002
2020.wnut-1.47,"['Domain-specific NLP', 'Classification Applications', 'Model Architectures']","['Transformer Models', 'Medical and Clinical NLP', 'Recurrent Neural Networks (RNNs)', 'NLP for News and Media']",['NLP for Social Media'],"This paper presents the approach that we employed to tackle the EMNLP WNUT-2020 Shared Task 2 : Identification of informative COVID-19 English Tweets. The task is to develop a system that automatically identifies whether an English Tweet related to the novel coronavirus COVID-19 is informative or not. We solve the task in three stages. The first stage involves pre-processing the dataset by filtering only relevant information. This is followed by experimenting with multiple deep learning models like CNNs, RNNs and Transformer based models. In the last stage, we propose an ensemble of the best model trained on different subsets of the provided dataset. Our final approach achieved an F1-score of 0.9037 and we were ranked sixth overall with F1-score as the evaluation criteria.",https://aclanthology.org/2020.wnut-1.47,Association for Computational Linguistics,2020,November,Proceedings of the Sixth Workshop on Noisy User-generated Text (W-NUT 2020),"Wadhawan, Anshul",Phonemer at WNUT-2020 Task 2: Sequence Classification Using COVID Twitter BERT and Bagging Ensemble Technique based on Plurality Voting,10.18653/v1/2020.wnut-1.47,wnut,928
2020.acl-main.216,"['Model Architectures', 'Audio Generation and Processing', 'Learning Paradigms']","['Transformer Models', 'Multimodal Learning', 'Automatic Speech Recognition (ASR)']",,"This paper presents an audio visual automatic speech recognition AV-ASR system using a Transformer-based architecture. We particularly focus on the scene context provided by the visual information, to ground the ASR. We extract representations for audio features in the encoder layers of the transformer and fuse video features using an additional crossmodal multihead attention layer. Additionally, we incorporate a multitask training criterion for multiresolution ASR, where we train the model to generate both character and subword level transcriptions. Experimental results on the How2 dataset, indicate that multiresolution training can speed up convergence by around 50% and relatively improves word error rate WER performance by upto 18% over subword prediction models. Further, incorporating visual information improves performance with relative gains upto 3.76% over audio only models. Our results are comparable to state-of-the-art Listen, Attend and Spell-based architectures.",https://aclanthology.org/2020.acl-main.216,Association for Computational Linguistics,2020,July,Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,"Paraskevopoulos, Georgios  and
Parthasarathy, Srinivas  and
Khare, Aparna  and
Sundaram, Shiva",Multimodal and Multiresolution Speech Recognition with Transformers,10.18653/v1/2020.acl-main.216,acl,1361
2020.aespen-1.7,"['Information Extraction', 'Domain-specific NLP', 'Data Management and Generation', 'Model Architectures']","['Data Preparation', 'Entity Linking', 'Event Extraction']",,"Previous efforts to automate the detection of social and political events in text have primarily focused on identifying events described within single sentences or documents. Within a corpus of documents, these automated systems are unable to link event referencesrecognize singular events across multiple sentences or documents. A separate literature in computational linguistics on event coreference resolution attempts to link known events to one another within and across documents. I provide a data set for evaluating methods to identify certain political events in text and to link related texts to one another based on shared events. The data set, Headlines of War, is built on the Militarized Interstate Disputes data set and offers headlines classified by dispute status and headline pairs labeled with coreference indicators. Additionally, I introduce a model capable of accomplishing both tasks. The multi-task convolutional neural network is shown to be capable of recognizing events and event coreferences given the headlines' texts and publication dates.",https://aclanthology.org/2020.aespen-1.7,European Language Resources Association (ELRA),2020,May,Proceedings of the Workshop on Automated Extraction of Socio-political Events from News 2020,"Radford, Benjamin",Seeing the Forest and the Trees: Detection and Cross-Document Coreference Resolution of Militarized Interstate Disputes,10.48550/arxiv.2005.02966,aespen,85
K16-2003,"['Information Extraction', 'Parsing', 'Low-resource Languages', 'Model Architectures']","['Discourse Parsing', 'Relation Extraction']",,"This paper describes our end-to-end discourse parser in the CoNLL-2016 Shared Task on Chinese Shallow Discourse Parsing. To adapt to the characteristics of Chinese, we implement a uniform framework for both explicit and non-explicit relation parsing. In this framework, we are the first to utilize a seed-expansion approach for the argument extraction subtask. In the official evaluation, our system achieves an F1 score of 26.90% in overall performance on the blind test set.",https://aclanthology.org/K16-2003,Association for Computational Linguistics,2016,August,Proceedings of the {C}o{NLL}-16 shared task,"Kang, Xiaomian  and
Li, Haoran  and
Zhou, Long  and
Zhang, Jiajun  and
Zong, Chengqing",An End-to-End Chinese Discourse Parser with Adaptation to Explicit and Non-explicit Relation Recognition,10.18653/v1/K16-2003,K16,968
2020.evalnlgeval-1.3,['Evaluation Techniques'],,,"NLG researchers often use uncontrolled corpora to train and evaluate their systems, using textual similarity metrics, such as BLEU. This position paper argues in favour of two alternative evaluation strategies, using grammars or rule-based systems. These strategies are particularly useful to identify the strengths and weaknesses of different systems. We contrast our proposals with the extended WebNLG dataset, which is revealed to have a skewed distribution of predicates. We predict that this distribution affects the quality of the predictions for systems trained on this data. However, this hypothesis can only be thoroughly tested without any confounds once we are able to systematically manipulate the skewness of the data, using a rule-based approach.",https://aclanthology.org/2020.evalnlgeval-1.3,Association for Computational Linguistics,2020,December,Proceedings of the 1st Workshop on Evaluating NLG Evaluation,"van Miltenburg, Emiel  and
van der Lee, Chris  and
Castro-Ferreira, Thiago  and
Krahmer, Emiel",Evaluation rules! On the use of grammars and rule-based systems for NLG evaluation,,evalnlgeval,375
2020.fnp-1.19,"['Domain-specific NLP', 'Automatic Text Summarization']","['NLP for Finance', 'Extractive Text Summarization']",,"This paper describes the systems proposed by HULAT research group from Universidad Carlos III de Madrid UC3M and MeaningCloud MC company to solve the FNS 2020 Shared Task on summarizing financial reports. We present a narrative extractive approach that implements a statistical model comprised of different features that measure the relevance of the sentences using a combination of statistical and machine learning methods. The key to the model's performance is its accurate representation of the text, since the word embeddings used by the model have been trained with the summaries of the training dataset and therefore capture the most salient information from the reports. The systems' code can be found at https://github.com/jaimebaldeon/FNS-2020.",https://aclanthology.org/2020.fnp-1.19,COLING,2020,December,Proceedings of the 1st Joint Workshop on Financial Narrative Processing and MultiLing Financial Summarisation,"Baldeon Suarez, Jaime  and
Mart{\'\i}nez, Paloma  and
Mart{\'\i}nez, Jose Luis",Combining financial word embeddings and knowledge-based features for financial text summarization UC3M-MC System at FNS-2020,,fnp,1243
2020.alw-1.9,"['Biases in NLP', 'Data Management and Generation', 'Classification Applications']","['Hate and Offensive Speech Detection', 'Data Analysis']",,"Abusive language detection is becoming increasingly important, but we still understand little about the biases in our datasets for abusive language detection, and how these biases affect the quality of abusive language detection. In the work reported here, we reproduce the investigation of Wiegand et al.  2019  to determine differences between different sampling strategies. They compared boosted random sampling, where abusive posts are upsampled, and biased topic sampling, which focuses on topics that are known to cause abusive language. Instead of comparing individual datasets created using these sampling strategies, we use the sampling strategies on a single, large dataset, thus eliminating the textual source of the dataset as a potential confounding factor. We show that differences in the textual source can have more effect than the chosen sampling strategy.",https://aclanthology.org/2020.alw-1.9,Association for Computational Linguistics,2020,November,Proceedings of the Fourth Workshop on Online Abuse and Harms,"Razo, Dante  and
K{\""u}bler, Sandra",Investigating Sampling Bias in Abusive Language Detection,10.18653/v1/2020.alw-1.9,alw,255
2022.naacl-demo.6,"['Classification Applications', 'Knowledge Representation and Reasoning']",,,"The recent growth of black-box machinelearning methods in data analysis has increased the demand for explanation methods and tools to understand their behaviour and assist human-ML model cooperation. In this paper, we demonstrate ContrXT , a novel approach that uses natural language explanations to help users to comprehend how a back-box model works. ContrXT provides time contrastive tcontrast explanations by computing the differences in the classification logic of two different trained models and then reasoning on their symbolic representations through Binary Decision Diagrams. ContrXT is publicly available at ContrXT.ai as a python pip package.",https://aclanthology.org/2022.naacl-demo.6,Association for Computational Linguistics,2022,July,Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: System Demonstrations,"Malandri, Lorenzo  and
Mercorio, Fabio  and
Mezzanzanica, Mario  and
Nobani, Navid  and
Seveso, Andrea",Contrastive Explanations of Text Classifiers as a Service,10.18653/v1/2022.naacl-demo.6,naacl,624
2020.ai4hi-1.1,"['Data Management and Generation', 'Image and Video Processing', 'Knowledge Representation and Reasoning']","['Image Captioning', 'Ontologies']",,"Cultural institutions such as galleries, libraries, archives and museums continue to make commitments to large scale digitization of collections. An ongoing challenge is how to increase discovery and access through structured data and the semantic web. In this paper we describe a method for using computer vision algorithms that automatically detect regions of ""stuff""-such as the sky, water, and roads-to produce rich and accurate structured data triples for describing the content of historic photography. We apply our method to a collection of 1610 documentary photographs produced in the 1930s and 1940 by the FSA-OWI division of the U.S. federal government. Manual verification of the extracted annotations yields an accuracy rate of 97.5%, compared to 70.7% for relations extracted from object detection and 31.5% for automatically generated captions. Our method also produces a rich set of features, providing more unique labels 1170 than either the captions 1040 or object detection 178 methods. We conclude by describing directions for a linguistically-focused ontology of region categories that can better enrich historical image data. Open source code and the extracted metadata from our corpus are made available as external resources.",https://aclanthology.org/2020.ai4hi-1.1,European Language Resources Association (ELRA),2020,May,Proceedings of the 1st International Workshop on Artificial Intelligence for Historical Image Enrichment and Access,"Arnold, Taylor  and
Tilton, Lauren",Enriching Historic Photography with Structured Data using Image Region Segmentation,,ai4hi,1138
2020.parlaclarin-1.10,"['Domain-specific NLP', 'Data Management and Generation', 'Classification Applications', 'Low-resource Languages']",['Data Analysis'],,"This paper addresses differences in the word use of two left-winged and two right-winged Danish parties, and how these differences, which reflect some of the basic stances of the parties, can be used to automatically identify the party of politicians from their speeches. In the first study, the most frequent and characteristic lemmas in the manifestos of the political parties as well as their language complexity are analysed. The analysis shows inter alia that the most frequently occurring lemmas in the manifestos reflect either the ideology or the position of the parties towards specific subjects, confirming for Danish preceding studies of English and German manifestos. Successively, we scaled our analysis applying NLP methods to the transcribed speeches by members of the same parties in the Parliament Hansards and trained machine learning algorithms in order to determine to what extent it is possible to predict the party of the politicians from the speeches. The speeches are a subset of the Danish Parliament corpus 2009-2017. The best results of the classification experiments gave a weighted F1-score of 0.57. These results are significantly better than the results obtained by the majority classifier weighted F1-score = 0.11 and by chance results. They show that the party of the politicians can be distinguished from their speeches in nearly 60% of the cases, even if they debate about the same subjects and thus often use the same terminology. In the future, we will include the subject of the speeches in the prediction experiments.",https://aclanthology.org/2020.parlaclarin-1.10,European Language Resources Association,2020,May,Proceedings of the Second ParlaCLARIN Workshop,"Navarretta, Costanza  and
Haltrup Hansen, Dorte",Identifying Parties in Manifestos and Parliament Speeches,,parlaclarin,859
2021.acl-long.141,"['Information Extraction', 'Model Architectures']","['Hypernymy Extraction', 'Transformer Models']",,"Recently, there is an effort to extend finegrained entity typing by using a richer and ultra-fine set of types, and labeling noun phrases including pronouns and nominal nouns instead of just named entity mentions. A key challenge for this ultra-fine entity typing task is that human annotated data are extremely scarce, and the annotation ability of existing distant or weak supervision approaches is very limited. To remedy this problem, in this paper, we propose to obtain training data for ultra-fine entity typing by using a BERT Masked Language Model MLM. Given a mention in a sentence, our approach constructs an input for the BERT MLM so that it predicts context dependent hypernyms of the mention, which can be used as type labels. Experimental results demonstrate that, with the help of these automatically generated labels, the performance of an ultra-fine entity typing model can be improved substantially. We also show that our approach can be applied to improve traditional fine-grained entity typing after performing simple type mapping.",https://aclanthology.org/2021.acl-long.141,Association for Computational Linguistics,2021,August,Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),"Dai, Hongliang  and
Song, Yangqiu  and
Wang, Haixun",Ultra-Fine Entity Typing with Weak Supervision from a Masked Language Model,10.18653/v1/2021.acl-long.141,acl,770
2021.acl-long.138,"['Parsing', 'Data Management and Generation', 'Learning Paradigms', 'Low-resource Languages', 'Model Architectures']","['Supervised Learning', 'Graph Neural Networks (GNNs)', 'Data Preparation', 'Discourse Parsing']",['Annotation Processes'],"In this paper, we present a neural model for joint dropped pronoun recovery DPR and conversational discourse parsing CDP in Chinese conversational speech. We show that DPR and CDP are closely related, and a joint model benefits both tasks. We refer to our model as DiscProReco, and it first encodes the tokens in each utterance in a conversation with a directed Graph Convolutional Network GCN. The token states for an utterance are then aggregated to produce a single state for each utterance. The utterance states are then fed into a biaffine classifier to construct a conversational discourse graph. A second multi-relational GCN is then applied to the utterance states to produce a discourse relation-augmented representation for the utterances, which are then fused together with token states in each utterance as input to a dropped pronoun recovery layer. The joint model is trained and evaluated on a new Structure Parsing-enhanced Dropped Pronoun Recovery SPDPR dataset that we annotated with both two types of information. Experimental results on the SPDPR dataset and other benchmarks show that DiscProReco significantly outperforms the state-of-the-art baselines of both tasks.",https://aclanthology.org/2021.acl-long.138,Association for Computational Linguistics,2021,August,Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),"Yang, Jingxuan  and
Xu, Kerui  and
Xu, Jun  and
Li, Si  and
Gao, Sheng  and
Guo, Jun  and
Xue, Nianwen  and
Wen, Ji-Rong",A Joint Model for Dropped Pronoun Recovery and Conversational Discourse Parsing in Chinese Conversational Speech,10.18653/v1/2021.acl-long.138,acl,957
2022.scil-1.6,['Model Architectures'],['Recurrent Neural Networks (RNNs)'],['Long Short-Term Memory (LSTM) Models'],"This paper revisits the question of what LSTMs know about the syntax of filler-gap dependencies in English. One contribution of this paper is to adjust the metrics used by Wilcox et al.  2018  and show that their language models LMs learn embedded wh-questions -a kind of filler-gap dependencies -better than they originally claimed. Another contribution of this paper is to examine four additional fillergap dependency constructions to see whether LMs perform equally on all types of filler-gap dependencies. We find that different constructions are learned to different extents, and there is a correlation between performance and frequency of constructions in the Penn Treebank Wall Street Journal corpus.",https://aclanthology.org/2022.scil-1.6,Association for Computational Linguistics,2022,February,Proceedings of the Society for Computation in Linguistics 2022,"Ozaki, Satoru  and
Yurovsky, Dan  and
Levin, Lori",How well do LSTM language models learn filler-gap dependencies?,,scil,218
O16-2004,"['Data Management and Generation', 'Information Extraction', 'Information Retrieval', 'Low-resource Languages', 'Model Architectures']","['Data Preparation', 'Search Engines']",['Annotation Processes'],"Conventional search engines usually consider a search query corresponding only to a simple task. Nevertheless, due to the explosive growth of web usage in recent years, more and more queries are driven by complex tasks. A complex task may consist of multiple sub-tasks. To accomplish a complex task, users may need to obtain information of various task-related entities corresponding to the sub-tasks. Users usually have to issue a series of queries for each entity during searching a complex search task. For example, the complex task ""travel to Beijing"" may involve several task-related entities, such as ""hotel room,"" ""flight tickets,"" and ""maps"". Understanding complex tasks with task-related entities can allow a search engine to suggest integrated search results for each sub-task simultaneously. To understand and improve user behavior when searching a complex task, we propose an entity-driven complex task model ECTM based on exploiting microblogs and query logs. Experimental results show that our ECTM is effective in identifying the comprehensive task-related entities for a complex task and generates good quality complex task names based on the identified task-related entities.",https://aclanthology.org/O16-2004,,2016,June,"International Journal of Computational Linguistics {\&} {C}hinese Language Processing, Volume 21, Number 1, June 2016","Wang, Ting-Xuan  and
Lu, Wen-Hsiang",Identifying the Names of Complex Search Tasks with Task-Related Entities,,O16,379
2021.hcinlp-1.7,"['Model Architectures', 'Learning Paradigms', 'Image and Video Processing']","['Multimodal Learning', 'Recurrent Neural Networks (RNNs)']",['Long Short-Term Memory (LSTM) Models'],"In this paper we argue that embodied multimodal agents, i.e., avatars, can play an important role in moving natural language processing toward ""deep understanding."" Fullyfeatured interactive agents, model encounters between two ""people,"" but a language-only agent has little environmental and situational awareness. Multimodal agents bring new opportunities for interpreting visuals, locational information, gestures, etc., which are more axes along which to communicate. We propose that multimodal agents, by facilitating an embodied form of human-computer interaction, provide additional structure that can be used to train models that move NLP systems closer to genuine ""understanding"" of grounded language, and we discuss ongoing studies using existing systems.",https://aclanthology.org/2021.hcinlp-1.7,Association for Computational Linguistics,2021,April,Proceedings of the First Workshop on Bridging Human{--}Computer Interaction and Natural Language Processing,"Krishnaswamy, Nikhil  and
Alalyani, Nada",Embodied Multimodal Agents to Bridge the Understanding Gap,,hcinlp,1452
2021.naacl-main.453,"['Learning Paradigms', 'Information Extraction', 'Knowledge Representation and Reasoning', 'Model Architectures']","['Knowledge Graphs', 'Supervised Learning', 'Relation Extraction']",,"Relational triple extraction is a crucial task for knowledge graph construction. Existing methods mainly focused on explicit relational triples that are directly expressed, but usually suffer from ignoring implicit triples that lack explicit expressions. This will lead to serious incompleteness of the constructed knowledge graphs. Fortunately, other triples in the sentence provide supplementary information for discovering entity pairs that may have implicit relations. Also, the relation types between the implicitly connected entity pairs can be identified with relational reasoning patterns in the real world. In this paper, we propose a unified framework to jointly extract explicit and implicit relational triples. To explore entity pairs that may be implicitly connected by relations, we propose a binary pointer network to extract overlapping relational triples relevant to each word sequentially and retain the information of previously extracted triples in an external memory. To infer the relation types of implicit relational triples, we propose to introduce real-world relational reasoning patterns in our model and capture these patterns with a relation network. We conduct experiments on several benchmark datasets, and the results prove the validity of our method.",https://aclanthology.org/2021.naacl-main.453,Association for Computational Linguistics,2021,June,Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,"Chen, Yubo  and
Zhang, Yunqi  and
Hu, Changran  and
Huang, Yongfeng",Jointly Extracting Explicit and Implicit Relational Triples with Reasoning Pattern Enhanced Binary Pointer Network,10.18653/v1/2021.naacl-main.453,naacl,1433
2020.repl4nlp-1.7,"['Bilingual Lexicon Induction (BLI)', 'Learning Paradigms', 'Low-resource Languages']",['Unsupervised Learning'],,"Work on projection-based induction of crosslingual word embedding spaces CLWEs predominantly focuses on the improvement of the projection i.e., mapping mechanisms. In this work, in contrast, we show that a simple method for post-processing monolingual embedding spaces facilitates learning of the crosslingual alignment and, in turn, substantially improves bilingual lexicon induction BLI. The post-processing method we examine is grounded in the generalisation of first-and second-order monolingual similarities to the n th -order similarity. By post-processing monolingual spaces before the cross-lingual alignment, the method can be coupled with any projection-based method for inducing CLWE spaces. We demonstrate the effectiveness of this simple monolingual post-processing across a set of 15 typologically diverse languages i.e., 1514 BLI setups, and in combination with two different projection methods.",https://aclanthology.org/2020.repl4nlp-1.7,Association for Computational Linguistics,2020,July,Proceedings of the 5th Workshop on Representation Learning for NLP,"Vuli{\'c}, Ivan  and
Korhonen, Anna  and
Glava{\v{s}}, Goran",Improving Bilingual Lexicon Induction with Unsupervised Post-Processing of Monolingual Word Vector Spaces,10.18653/v1/2020.repl4nlp-1.7,repl4nlp,754
Q17-1005,['Machine Translation (MT)'],['Statistical MT (SMT)'],,"Decoding of phrase-based translation models in the general case is known to be NPcomplete, by a reduction from the traveling salesman problem Knight, 1999 . In practice, phrase-based systems often impose a hard distortion limit that limits the movement of phrases during translation. However, the impact on complexity after imposing such a constraint is not well studied. In this paper, we describe a dynamic programming algorithm for phrase-based decoding with a fixed distortion limit. The runtime of the algorithm is Ond!lh d+1  where n is the sentence length, d is the distortion limit, l is a bound on the number of phrases starting at any position in the sentence, and h is related to the maximum number of target language translations for any source word. The algorithm makes use of a novel representation that gives a new perspective on decoding of phrase-based models.",https://aclanthology.org/Q17-1005,MIT Press,2017,,,"Chang, Yin-Wen  and
Collins, Michael",A Polynomial-Time Dynamic Programming Algorithm for Phrase-Based Decoding with a Fixed Distortion Limit,10.1162/tacl_a_00046,Q17,183
2022.csrr-1.1,"['Information Extraction', 'Knowledge Representation and Reasoning', 'Commonsense Reasoning']","['Knowledge Graphs', 'Entity Linking']",,"Knowledge graphs are often used to store common sense information that is useful for various tasks. However, the extraction of contextuallyrelevant knowledge is an unsolved problem, and current approaches are relatively simple. Here we introduce a triple selection method based on a ranking model and find that it improves question answering accuracy over existing methods. We additionally investigate methods to ensure that extracted triples form a connected graph. Graph connectivity is important for model interpretability, as paths are frequently used as explanations for the reasoning that connects question and answer.",https://aclanthology.org/2022.csrr-1.1,Association for Computational Linguistics,2022,May,Proceedings of the First Workshop on Commonsense Representation and Reasoning (CSRR 2022),"Aglionby, Guy  and
Teufel, Simone",Identifying relevant common sense information in knowledge graphs,10.18653/v1/2022.csrr-1.1,csrr,253
C16-1126,"['Text Preprocessing', 'Data Management and Generation', 'Low-resource Languages', 'Cross-lingual Application']","['Part-of-Speech (POS) Tagging', 'Data Analysis']",,"Several recent studies have shown that eye movements during reading provide information about grammatical and syntactic processing, which can assist the induction of NLP models. All these studies have been limited to English, however. This study shows that gaze and part of speech PoS correlations largely transfer across English and French. This means that we can replicate previous studies on gaze-based PoS tagging for French, but also that we can use English gaze data to assist the induction of French NLP models.",https://aclanthology.org/C16-1126,The COLING 2016 Organizing Committee,2016,December,"Proceedings of {COLING} 2016, the 26th International Conference on Computational Linguistics: Technical Papers","Barrett, Maria  and
Keller, Frank  and
S{\o}gaard, Anders",Cross-lingual Transfer of Correlations between Parts of Speech and Gaze Features,,C16,663
2021.latechclfl-1.8,"['Evaluation Techniques', 'Data Management and Generation', 'Low-resource Languages', 'Classification Applications', 'Model Architectures']","['Transformer Models', 'Data Preparation', 'Emotion Detection']",['Annotation Processes'],"We present results of a project on emotion classification on historical German plays of Enlightenment, Storm and Stress, and German Classicism. We have developed a hierarchical annotation scheme consisting of 13 subemotions like suffering, love and joy that sum up to 6 main and 2 polarity classes positive/negative. We have conducted textual annotations on 11 German plays and have acquired over 13,000 emotion annotations by two annotators per play. We have evaluated multiple traditional machine learning approaches as well as transformer-based models pretrained on historical and contemporary language for a single-label text sequence emotion classification for the different emotion categories. The evaluation is carried out on three different instances of the corpus: 1 taking all annotations, 2 filtering overlapping annotations by annotators, 3 applying a heuristic for speech-based analysis. Best results are achieved on the filtered corpus with the best models being large transformer-based models pretrained on contemporary German language. For the polarity classification accuracies of up to 90% are achieved. The accuracies become lower for settings with a higher number of classes, achieving 66% for 13 sub-emotions. Further pretraining of a historical model with a corpus of dramatic texts led to no improvements.",https://aclanthology.org/2021.latechclfl-1.8,Association for Computational Linguistics,2021,November,"Proceedings of the 5th Joint SIGHUM Workshop on Computational Linguistics for Cultural Heritage, Social Sciences, Humanities and Literature","Schmidt, Thomas  and
Dennerlein, Katrin  and
Wolff, Christian",Emotion Classification in German Plays with Transformer-based Language Models Pretrained on Historical and Contemporary Language,10.18653/v1/2021.latechclfl-1.8,latechclfl,618
2021.starsem-1.6,"['Learning Paradigms', 'Model Architectures', 'Commonsense Reasoning']","['Unsupervised Learning', 'Supervised Learning']",,"Abductive reasoning starts from some observations and aims at finding the most plausible explanation for these observations. To perform abduction, humans often make use of temporal and causal inferences, and knowledge about how some hypothetical situation can result in different outcomes. This work offers the first study of how such knowledge impacts the Abductive NLI task -which consists in choosing the more likely explanation for given observations. We train a specialized language model LM I that is tasked to generate what could happen next from a hypothetical scenario that evolves from a given event. We then propose a multi-task model MT L to solve the NLI task, which predicts a plausible explanation by a considering different possible events emerging from candidate hypothesesevents generated by LM I -and b selecting the one that is most similar to the observed outcome. We show that our MT L model improves over prior vanilla pre-trained LMs finetuned on NLI. Our manual evaluation and analysis suggest that learning about possible next events from different hypothetical scenarios supports abductive inference. O 1 : Priya decided to try a new restaurant. O 2 : Priya thought her food was delicious.",https://aclanthology.org/2021.starsem-1.6,Association for Computational Linguistics,2021,August,Proceedings of *SEM 2021: The Tenth Joint Conference on Lexical and Computational Semantics,"Paul, Debjit  and
Frank, Anette",Generating Hypothetical Events for Abductive Inference,10.18653/v1/2021.starsem-1.6,starsem,1335
2020.scil-1.40,['Knowledge Representation and Reasoning'],,,"We present a new logic-based inference engine for natural language inference NLI called MonaLog, which is based on natural logic and the monotonicity calculus. In contrast to existing logic-based approaches, our system is intentionally designed to be as lightweight as possible, and operates using a small set of well-known surface-level monotonicity facts about quantifiers, lexical items and tokenlevel polarity information. Despite its simplicity, we find our approach to be competitive with other logic-based NLI models on the SICK benchmark. We also use MonaLog in combination with the current state-of-the-art model BERT in a variety of settings, including for compositional data augmentation. We show that MonaLog is capable of generating large amounts of high-quality training data for BERT, improving its accuracy on SICK.",https://aclanthology.org/2020.scil-1.40,Association for Computational Linguistics,2020,January,Proceedings of the Society for Computation in Linguistics 2020,"Hu, Hai  and
Chen, Qi  and
Richardson, Kyle  and
Mukherjee, Atreyee  and
Moss, Lawrence S.  and
Kuebler, Sandra",MonaLog: a Lightweight System for Natural Language Inference Based on Monotonicity,10.48550/arxiv.1910.08772,scil,543
2021.cinlp-1.4,"['Domain-specific NLP', 'Classification Applications']",,,"Despite peer-reviewing being an essential component of academia since the 1600s, it has repeatedly received criticisms for lack of transparency and consistency. We posit that recent work in machine learning and explainable AI provide tools that enable insights into the decisions from a given peer-review process. We start by simulating the peer-review process using an ML classifier and extracting global explanations in the form of linguistic features that affect the acceptance of a scientific paper for publication on an open peerreview dataset. Second, since such global explanations do not justify causal interpretations, we propose a methodology for detecting confounding effects in natural language and generating explanations, disentangled from textual confounders, in the form of lexicons. Our proposed linguistic explanation methodology indicates the following on a case dataset of ICLR submissions: a the organising committee follows, for the most part, the recommendations of reviewers, and b the paper's main characteristics that led to reviewers recommending acceptance for publication are originality, clarity and substance.",https://aclanthology.org/2021.cinlp-1.4,Association for Computational Linguistics,2021,November,Proceedings of the First Workshop on Causal Inference and NLP,"Fytas, Panagiotis  and
Rizos, Georgios  and
Specia, Lucia",What Makes a Scientific Paper be Accepted for Publication?,10.18653/v1/2021.cinlp-1.4,cinlp,784
N16-1043,['Learning Paradigms'],['Multimodal Learning'],,"Children learn the meaning of words by being exposed to perceptually rich situations linguistic discourse, visual scenes, etc. Current computational learning models typically simulate these rich situations through impoverished symbolic approximations. In this work, we present a distributed word learning model that operates on child-directed speech paired with realistic visual scenes. The model integrates linguistic and extra-linguistic information visual and social cues, handles referential uncertainty, and correctly learns to associate words with objects, even in cases of limited linguistic exposure.",https://aclanthology.org/N16-1043,Association for Computational Linguistics,2016,June,Proceedings of the 2016 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies,"Lazaridou, Angeliki  and
Chrupa{\l}a, Grzegorz  and
Fern{\'a}ndez, Raquel  and
Baroni, Marco",Multimodal Semantic Learning from Child-Directed Input,10.18653/v1/N16-1043,N16,616
2019.lilt-18.6,"['Domain-specific NLP', 'Data Management and Generation']",,,"The principal barrier to the uptake of technologies in schools is not technological, but social and political. Teachers must be convinced of the pedagogical benefits of a particular curriculum before they will agree to learn the means to teach it. The teaching of formal grammar to first language students in schools is no exception to this rule. Over the last three decades, most schools in England have been legally required to teach grammatical subject knowledge, i.e. linguistic knowledge of grammar terms and structure, to children age five and upwards as part of the national curriculum in English. A mandatory set of curriculum specifications for England and Wales was published in 2014, and elsewhere similar requirements were imposed. However, few current English school teachers were taught grammar themselves, and the dominant view has long been in favour of 'real books' rather than the teaching of a formal grammar. English grammar teaching thus faces multiple challenges: to convince teachers of the value of grammar in their own teaching, to teach the teachers the knowledge they need, and to develop relevant resources to use in the classroom. Alongside subject knowledge, teachers need pedagogical knowledgehow to teach grammar effectively and how to integrate this teaching into other kinds of language learning. The paper introduces the Englicious 1 web platform for schools, and The Englicious project was funded by the UK research councils AHRC 1",https://aclanthology.org/2019.lilt-18.6,CSLI Publications,2019,July,"Linguistic Issues in Language Technology, Volume 18, 2019 - Exploiting Parsed Corpora: Applications in Research, Pedagogy, and Processing","Wallis, Sean  and
Cushing, Ian  and
Aarts, Bas",Exploiting parsed corpora in grammar teaching,10.33011/lilt.v18i.1437,lilt,1017
W19-3015,"['Audio Generation and Processing', 'Domain-specific NLP', 'Discourse Analysis', 'Data Management and Generation', 'Low-resource Languages']","['Data Preparation', 'Data Analysis', 'Medical and Clinical NLP']",['NLP for Mental Health'],"Incoherent discourse in schizophrenia has long been recognized as a dominant symptom of the mental disorder Bleuler,  1911Bleuler,   /1950. Recent studies have used modern sentence and word embeddings to compute coherence metrics for spontaneous speech in schizophrenia. While clinical ratings always have a subjective element, computational linguistic methodology allows quantification of speech abnormalities. Clinical and empirical knowledge from psychiatry provide the theoretical and conceptual basis for modelling. Our study is an interdisciplinary attempt at improving coherence models in schizophrenia. Speech samples were obtained from healthy controls and patients with a diagnosis of schizophrenia or schizoaffective disorder and different severity of positive formal thought disorder. Interviews were transcribed and coherence metrics derived from different embeddings. One model found higher coherence metrics for controls than patients. All other models remained non-significant. More detailed analysis of the data motivates different approaches to improving coherence models in schizophrenia, e.g. by assessing referential abnormalities.",https://aclanthology.org/W19-3015,Association for Computational Linguistics,2019,June,Proceedings of the Sixth Workshop on Computational Linguistics and Clinical Psychology,"Just, Sandra  and
Haegert, Erik  and
Ko{\v{r}}{\'a}nov{\'a}, Nora  and
Br{\""o}cker, Anna-Lena  and
Nenchev, Ivan  and
Funcke, Jakob  and
Montag, Christiane  and
Stede, Manfred",Coherence models in schizophrenia,10.18653/v1/W19-3015,W19,906
Q18-1042,"['Biases in NLP', 'Ethics', 'Data Management and Generation', 'Information Extraction']","['Gender Bias', 'Data Preparation', 'Coreference Resolution']",,"Coreference resolution is an important task for natural language understanding, and the resolution of ambiguous pronouns a longstanding challenge. Nonetheless, existing corpora do not capture ambiguous pronouns in sufficient volume or diversity to accurately indicate the practical utility of models. Furthermore, we find gender bias in existing corpora and systems favoring masculine entities. To address this, we present and release GAP, a genderbalanced labeled corpus of 8,908 ambiguous pronoun-name pairs sampled to provide diverse coverage of challenges posed by real-world text. We explore a range of baselines that demonstrate the complexity of the challenge, the best achieving just 66.9% F1. We show that syntactic structure and continuous neural models provide promising, complementary cues for approaching the challenge.",https://aclanthology.org/Q18-1042,MIT Press,2018,,,"Webster, Kellie  and
Recasens, Marta  and
Axelrod, Vera  and
Baldridge, Jason",Mind the GAP: A Balanced Corpus of Gendered Ambiguous Pronouns,10.1162/tacl_a_00240,Q18,134
U17-1010,['Information Extraction'],['Relation Extraction'],,"Extracting information from semistructured text has been studied only for limited domain sources due to its heterogeneous formats. This paper proposes a Ripple-Down Rules RDR based approach to extract relations from both semistructured and unstructured text in open domain Web pages. We find that RDR's 'case-by-case' incremental knowledge acquisition approach provides practical flexibility for 1 handling heterogeneous formats of semi-structured text; 2 conducting knowledge engineering on any Web pages with minimum start-up cost and  3  allowing open-ended settings on relation schema. The efficacy of the approach has been demonstrated by extracting contact information from randomly collected open domain Web pages. The rGALA system achieved 0.87 F1 score on a testing dataset of 100 Web pages, after only 7 hours of knowledge engineering on a training set of 100 Web pages.",https://aclanthology.org/U17-1010,,2017,December,Proceedings of the Australasian Language Technology Association Workshop 2017,"Kim, Maria Myung Hee",Incremental Knowledge Acquisition Approach for Information Extraction on both Semi-Structured and Unstructured Text from the Open Domain Web,,U17,1006
S16-1123,"['Knowledge Representation and Reasoning', 'Model Architectures']",,,"In our paper we present our rule-based system for semantic processing. In particular we show examples and solutions that may be challenge our approach. We then discuss problems and shortcomings of Task 2 -iSTS. We comment on the existence of a tension between the inherent need to on the one side, to make the task as much as possible ""semantically feasible"". Whereas the detailed presentation and some notes in the guidelines refer to inferential processes, paraphrases and the use of commonsense knowledge of the world for the interpretation to work. We then present results and some conclusions.",https://aclanthology.org/S16-1123,Association for Computational Linguistics,2016,June,Proceedings of the 10th International Workshop on Semantic Evaluation ({S}em{E}val-2016),"Delmonte, Rodolfo",VENSESEVAL at Semeval-2016 Task 2 iSTS - with a full-fledged rule-based approach,10.18653/v1/S16-1123,S16,1015
2020.peoples-1.8,"['Domain-specific NLP', 'Model Architectures', 'Classification Applications', 'Low-resource Languages']","['NLP for News and Media', 'Medical and Clinical NLP']","['NLP for Social Media', 'NLP for Mental Health']","The COVID-19 pandemic has caused international social tension and unrest. Besides the crisis itself, there are growing signs of rising conflict potential of societies around the world. Indicators of global mood changes are hard to detect and direct questionnaires suffer from social desirability biases. However, so-called implicit methods can reveal humans intrinsic desires from e.g. social media texts. We present psychologically validated social unrest predictors and replicate scalable and automated predictions, setting a new state of the art on a recent German shared task dataset. We employ this model to investigate a change of language towards social unrest during the COVID-19 pandemic by comparing established psychological predictors on samples of tweets from spring 2019 with spring 2020. The results show a significant increase of the conflict-indicating psychometrics. With this work, we demonstrate the applicability of automated NLP-based approaches to quantitative psychological research.",https://aclanthology.org/2020.peoples-1.8,Association for Computational Linguistics,2020,December,"Proceedings of the Third Workshop on Computational Modeling of People's Opinions, Personality, and Emotion's in Social Media","Johann{\ss}en, Dirk  and
Biemann, Chris",Social Media Unrest Prediction during the COVID-19 Pandemic: Neural Implicit Motive Pattern Recognition as Psychometric Signs of Severe Crises,,peoples,473
C16-2062,"['Text Clustering', 'Low-resource Languages', 'Knowledge Representation and Reasoning']",,,We aim at showing that lexical descriptions based on multifactorial and continuous models can be used by linguists and lexicographers and not only by machines so long as they are provided with a way to efficiently navigate data collections. We propose to demonstrate such a system.,https://aclanthology.org/C16-2062,The COLING 2016 Organizing Committee,2016,December,"Proceedings of {COLING} 2016, the 26th International Conference on Computational Linguistics: System Demonstrations","Marchal, Pierre  and
Poibeau, Thierry",Exploring a Continuous and Flexible Representation of the Lexicon,,C16,131
2021.nuse-1.3,"['Learning Paradigms', 'Data Management and Generation']","['Data Preparation', 'Supervised Learning']",,"While many different aspects of human experiences have been studied by the NLP community, none has captured its full richness. We propose a new task 1 to capture this richness based on an unlikely setting: movie characters. We sought to capture theme-level similarities between movie characters that were community-curated into 20,000 themes. By introducing a two-step approach that balances performance and efficiency, we managed to achieve 9-27% improvement over recent paragraph-embedding based methods. Finally, we demonstrate how the thematic information learnt from movie characters can potentially be used to understand themes in the experience of people, as indicated on Reddit posts.",https://aclanthology.org/2021.nuse-1.3,Association for Computational Linguistics,2021,June,Proceedings of the Third Workshop on Narrative Understanding,"Wang, Zhilin  and
Lin, Weizhe  and
Wu, Xiaodong",Learning Similarity between Movie Characters and Its Potential Implications on Understanding Human Experiences,10.18653/v1/2021.nuse-1.3,nuse,542
D19-1536,"['Parsing', 'Knowledge Representation and Reasoning', 'Domain-specific NLP']",['Semantic Parsing'],,"We propose Text2Math, a model for semantically parsing text into math expressions. The model can be used to solve different math related problems including arithmetic word problems Roy and Roth, 2017; Liang et al., 2018 and equation parsing problems Roy et al., 2016 . Unlike previous approaches, we tackle the problem from an end-to-end structured prediction perspective where our algorithm aims to predict the complete math expression at once as a tree structure, where minimal manual efforts are involved in the process. Empirical results on benchmark datasets demonstrate the efficacy of our approach.",https://aclanthology.org/D19-1536,Association for Computational Linguistics,2019,November,Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"Zou, Yanyan  and
Lu, Wei",Text2Math: End-to-end Parsing Text into Math Expressions,10.18653/v1/D19-1536,D19,1465
K19-1043,"['Biases in NLP', 'Embeddings', 'Low-resource Languages']","['Gender Bias', 'Word Embeddings']",,"Many natural languages assign grammatical gender also to inanimate nouns in the language. In such languages, words that relate to the gender-marked nouns are inflected to agree with the noun's gender. We show that this affects the word representations of inanimate nouns, resulting in nouns with the same gender being closer to each other than nouns with different gender. While ""embedding debiasing"" methods fail to remove the effect, we demonstrate that a careful application of methods that neutralize grammatical gender signals from the words' context when training word embeddings is effective in removing it. Fixing the grammatical gender bias yields a positive effect on the quality of the resulting word embeddings, both in monolingual and crosslingual settings. We note that successfully removing gender signals, while achievable, is not trivial to do and that a language-specific morphological analyzer, together with careful usage of it, are essential for achieving good results.",https://aclanthology.org/K19-1043,Association for Computational Linguistics,2019,November,Proceedings of the 23rd Conference on Computational Natural Language Learning (CoNLL),"Gonen, Hila  and
Kementchedjhieva, Yova  and
Goldberg, Yoav",How Does Grammatical Gender Affect Noun Representations in Gender-Marking Languages?,10.18653/v1/K19-1043,K19,279
2020.emnlp-main.218,"['Parsing', 'Data Management and Generation']","['Data Preparation', 'Syntactic Parsing']",['Annotation Processes'],A gapping construction consists of a coordinated structure where redundant elements are elided from all but one conjuncts. This paper proposes a method of parsing sentences with gapping to recover elided elements. The proposed method is based on constituent trees annotated with grammatical and semantic roles that are useful for identifying elided elements. Our method outperforms the previous method in terms of F-measure and recall.,https://aclanthology.org/2020.emnlp-main.218,Association for Computational Linguistics,2020,November,Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),"Kato, Yoshihide  and
Matsubara, Shigeki",Parsing Gapping Constructions Based on Grammatical and Semantic Roles,10.18653/v1/2020.emnlp-main.218,emnlp,741
J19-1001,"['Evaluation Techniques', 'Data Management and Generation', 'Multilingual NLP', 'Learning Paradigms', 'Low-resource Languages', 'Cross-lingual Application']","['Unsupervised Learning', 'Data Preparation']",,"Nominal compounds such as red wine and nut case display a continuum of compositionality, with varying contributions from the components of the compound to its semantics. This article proposes a framework for compound compositionality prediction using distributional semantic models, evaluating to what extent they capture idiomaticity compared to human judgments. For evaluation, we introduce data sets containing human judgments in three languages: English, French, and Portuguese. The results obtained reveal a high agreement between the models and human predictions, suggesting that they are able to incorporate information about idiomaticity. We also present an in-depth evaluation of various factors that can affect prediction, such as model and corpus parameters and compositionality operations. General crosslingual analyses reveal the impact of morphological variation and corpus size in the ability of the model to predict compositionality, and of a uniform combination of the components for best results.",https://aclanthology.org/J19-1001,MIT Press,2019,March,,"Cordeiro, Silvio  and
Villavicencio, Aline  and
Idiart, Marco  and
Ramisch, Carlos",Unsupervised Compositionality Prediction of Nominal Compounds,10.1162/coli_a_00341,J19,764
2020.coling-main.322,"['Learning Paradigms', 'Parsing', 'Model Architectures', 'Low-resource Languages']","['Unsupervised Learning', 'Syntactic Parsing']",['Constituency Parsing'],"Deep inside-outside recursive autoencoder DIORA is a neural-based model designed for unsupervised constituency parsing. During its forward computation, it provides phrase and contextual representations for all spans in the input sentence. By utilizing the contextual representation of each leaf-level span, the span of length 1, to reconstruct the word inside the span, the model is trained without labeled data. In this work, we extend the training objective of DIORA by making use of all spans instead of only leaf-level spans. We test our new training objective on datasets of two languages: English and Japanese, and empirically show that our method achieves improvement in parsing accuracy over the original DIORA.",https://aclanthology.org/2020.coling-main.322,International Committee on Computational Linguistics,2020,December,Proceedings of the 28th International Conference on Computational Linguistics,"Hong, Ruyue  and
Cai, Jiong  and
Tu, Kewei",Deep Inside-outside Recursive Autoencoder with All-span Objective,10.18653/v1/2020.coling-main.322,coling,376
2020.lt4hala-1.1,"['Data Management and Generation', 'Model Architectures', 'Low-resource Languages', 'Language Change Analysis']",['Data Analysis'],,"This paper introduces and evaluates a Bayesian mixture model that is designed for dating texts based on the distributions of linguistic features. The model is applied to the corpus of Vedic Sanskrit the historical structure of which is still unclear in many details. The evaluation concentrates on the interaction between time, genre and linguistic features, detecting those whose distributions are clearly coupled with the historical time. The evaluation also highlights the problems that arise when quantitative results need to be reconciled with philological insights.",https://aclanthology.org/2020.lt4hala-1.1,European Language Resources Association (ELRA),2020,May,Proceedings of LT4HALA 2020 - 1st Workshop on Language Technologies for Historical and Ancient Languages,"Hellwig, Oliver",Dating and Stratifying a Historical Corpus with a Bayesian Mixture Model,10.5167/uzh-192638,lt4hala,1190
2021.konvens-1.27,"['Embeddings', 'Domain-specific NLP', 'Model Architectures']",['Word Embeddings'],,"For domain-specific NLP tasks, applying word embeddings trained on general corpora is not optimal. Meanwhile, training domain-specific word representations poses challenges to dataset construction and embedding evaluation. In this paper, we present and compare ELMo and Word2Vec models trained/finetuned on philosophical data. For evaluation, a conceptual network was used. Results show that contextualized models provide better word embeddings than static models and that merging embeddings from different models boosts task performance.",https://aclanthology.org/2021.konvens-1.27,KONVENS 2021 Organizers,2021,6--9 September,Proceedings of the 17th Conference on Natural Language Processing (KONVENS 2021),"Zhou, Wei  and
Bloem, Jelke",Comparing Contextual and Static Word Embeddings with Small Data,,konvens,343
2022.slpat-1.3,['Domain-specific NLP'],['Medical and Clinical NLP'],,"Robitaille 2010 wrote 'if all technology companies have accessibility in their mind then people with disabilities won't be left behind.' Current technology has come a long way from where it stood decades ago; however, researchers and manufacturers often do not include people with disabilities in the design process and tend to accommodate them after the fact. In this paper we share feedback from four assistive technology users who rely on one or more assistive technology devices in their everyday lives. We believe end users should be part of the design process and that by bringing together experts and users, we can bridge the research/practice gap.",https://aclanthology.org/2022.slpat-1.3,Association for Computational Linguistics,2022,May,Ninth Workshop on Speech and Language Processing for Assistive Technologies (SLPAT-2022),"Vaidyanathan, Preethi  and
Wislon, Angela  and
Sawyer, Doug  and
Diego, Amy  and
Webster, Augustine  and
Fassov, Katerina  and
Brinton, James  and
Rubenstein, Jenn",A glimpse of assistive technology in daily life,10.18653/v1/2022.slpat-1.3,slpat,1081
2021.nlp4posimpact-1.1,"['Automatic Text Summarization', 'Text Generation', 'Domain-specific NLP', 'Dialogue Systems', 'Data Management and Generation', 'Low-resource Languages']","['Data Preparation', 'Chatbots', 'Medical and Clinical NLP']",['NLP for Mental Health'],"Amidst rising mental health needs in society, virtual agents are increasingly deployed in counselling. In order to give pertinent advice, counsellors must first gain an understanding of the issues at hand by eliciting sharing from the counsellee. It is thus important for the counsellor chatbot to encourage the user to open up and talk. One way to sustain the conversation flow is to acknowledge the counsellee's key points by restating them, or probing them further with questions. This paper applies models from two closely related NLP tasks -summarization and question generation -to restatement and question generation in the counselling context. We conducted experiments on a manually annotated dataset of Cantonese post-reply pairs on topics related to loneliness, academic anxiety and test anxiety. We obtained the best performance in both restatement and question generation by finetuning BertSum, a state-of-the-art summarization model, with the in-domain manual dataset augmented with a large-scale, automatically mined open-domain dataset.",https://aclanthology.org/2021.nlp4posimpact-1.1,Association for Computational Linguistics,2021,August,Proceedings of the 1st Workshop on NLP for Positive Impact,"Lee, John  and
Liang, Baikun  and
Fong, Haley",Restatement and Question Generation for Counsellor Chatbot,10.18653/v1/2021.nlp4posimpact-1.1,nlp4posimpact,1176
D18-1471,"['Data Management and Generation', 'Classification Applications', 'Domain-specific NLP']","['Data Preparation', 'Hate and Offensive Speech Detection', 'Data Analysis', 'NLP for News and Media']",['NLP for Social Media'],"Vulgar words are employed in language use for several different functions, ranging from expressing aggression to signaling group identity or the informality of the communication. This versatility of usage of a restricted set of words is challenging for downstream applications and has yet to be studied quantitatively or using natural language processing techniques. We introduce a novel data set of 7,800 tweets from users with known demographic traits where all instances of vulgar words are annotated with one of the six categories of vulgar word use. Using this data set, we present the first analysis of the pragmatic aspects of vulgarity and how they relate to social factors. We build a model able to predict the category of a vulgar word based on the immediate context it appears in with 67.4 macro F1 across six classes. Finally, we demonstrate the utility of modeling the type of vulgar word use in context by using this information to achieve state-of-the-art performance in hate speech detection on a benchmark data set.",https://aclanthology.org/D18-1471,Association for Computational Linguistics,2018,October-November,Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing,"Holgate, Eric  and
Cachola, Isabel  and
Preo{\c{t}}iuc-Pietro, Daniel  and
Li, Junyi Jessy",Why Swear? Analyzing and Inferring the Intentions of Vulgar Expressions,10.18653/v1/D18-1471,D18,252
2022.tacl-1.19,"['Data Management and Generation', 'Parsing', 'Knowledge Representation and Reasoning', 'Model Architectures']","['Data Preparation', 'Semantic Parsing']",['Annotation Processes'],"We introduce the Probabilistic Worldbuilding Model PWM, a new fully symbolic Bayesian model of semantic parsing and reasoning, as a first step in a research program toward more domain-and task-general NLU and AI. Humans create internal mental models of their observations that greatly aid in their ability to understand and reason about a large variety of problems. In PWM, the meanings of sentences, acquired facts about the world, and intermediate steps in reasoning are all expressed in a human-readable formal language, with the design goal of interpretability. PWM is Bayesian, designed specifically to be able to generalize to new domains and new tasks. We derive and implement an inference algorithm that reads sentences by parsing and abducing updates to its latent world model that capture the semantics of those sentences, and evaluate it on two out-of-domain question-answering datasets: 1 ProofWriter and 2 a new dataset we call FictionalGeoQA, designed to be more representative of real language but still simple enough to focus on evaluating reasoning ability, while being robust against heuristics. Our method outperforms baselines on both, thereby demonstrating its value as a proof-of-concept.",https://aclanthology.org/2022.tacl-1.19,MIT Press,2022,,,"Saparov, Abulhair  and
Mitchell, Tom M.",Towards General Natural Language Understanding with Probabilistic Worldbuilding,10.1162/tacl_a_00463,tacl,41
P19-1379,"['Embeddings', 'Language Change Analysis']","['Semantic Change Analysis', 'Word Embeddings']",,"Diachronic word embeddings have been widely used in detecting temporal changes. However, existing methods face the meaning conflation deficiency by representing a word as a single vector at each time period. To address this issue, this paper proposes a sense representation and tracking framework based on deep contextualized embeddings, aiming at answering not only what and when, but also how the word meaning changes. The experiments show that our framework is effective in representing fine-grained word senses, and it brings a significant improvement in word change detection task. Furthermore, we model the word change from an ecological viewpoint, and sketch two interesting sense behaviors in the process of language evolution, i.e. sense competition and sense cooperation.",https://aclanthology.org/P19-1379,Association for Computational Linguistics,2019,July,Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,"Hu, Renfen  and
Li, Shen  and
Liang, Shichen",Diachronic Sense Modeling with Deep Contextualized Word Embeddings: An Ecological View,10.18653/v1/P19-1379,P19,20
2020.aacl-main.31,"['Model Architectures', 'Classification Applications', 'Learning Paradigms']","['Humor Detection', 'Hate and Offensive Speech Detection', 'Sentiment Analysis (SA)', 'Sarcasm Detection', 'Multilabel Text Classification']",,"In this paper, we aim at learning the relationships and similarities of a variety of tasks, such as humour detection, sarcasm detection, offensive content detection, motivational content detection and sentiment analysis on a somewhat complicated form of information, i.e., memes. We propose a multi-task, multi-modal deep learning framework to solve multiple tasks simultaneously. For multi-tasking, we propose two attention-like mechanisms viz., Inter-task Relationship Module iTRM and Inter-class Relationship Module iCRM. The main motivation of iTRM is to learn the relationship between the tasks to realize how they help each other. In contrast, iCRM develops relations between the different classes of tasks. Finally, representations from both the attentions are concatenated and shared across the five tasks i.e., humour, sarcasm, offensive, motivational, and sentiment for multi-tasking. We use the recently released dataset in the Memotion Analysis task @ SemEval 2020, which consists of memes annotated for the classes as mentioned above. Empirical results on Memotion dataset show the efficacy of our proposed approach over the existing state-of-theart systems Baseline and SemEval 2020 winner. The evaluation also indicates that the proposed multi-task framework yields better performance over the single-task learning.",https://aclanthology.org/2020.aacl-main.31,Association for Computational Linguistics,2020,December,Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing,"Chauhan, Dushyant Singh  and
S R, Dhanush  and
Ekbal, Asif  and
Bhattacharyya, Pushpak","All-in-One: A Deep Attentive Multi-task Learning Framework for Humour, Sarcasm, Offensive, Motivation, and Sentiment on Memes",10.18653/v1/2020.aacl-main.31,aacl,1177
2020.ldl-1.1,"['Low-resource Languages', 'Knowledge Representation and Reasoning']","['Semantic Web', 'Ontologies']",['Ontology Construction'],"To empower end users in searching for historical linguistic content with a performance that far exceeds the research functions offered by websites of, e.g., historical dictionaries, is undoubtedly a major advantage of Linguistic Linked Open Data LLOD. An important aim of lexicography is to enable a language-independent, onomasiological approach, and the modelling of linguistic resources following the LOD paradigm facilitates the semantic mapping to ontologies making this approach possible. Hallig-Wartburg's Begriffssystem HW is a well-known extra-linguistic conceptual system used as an onomasiological framework by many historical lexicographical and lexicological works. Published in 1952, HW has meanwhile been digitised. With proprietary XML data as the starting point, our goal is the transformation of HW into Linked Open Data in order to facilitate its use by linguistic resources modelled as LOD. In this paper, we describe the particularities of the HW conceptual model and the method of converting HW: We discuss two approaches, i the representation of HW in RDF using SKOS, the SKOS thesaurus extension, and XKOS, and ii the creation of a lightweight ontology expressed in OWL, based on the RDF/SKOS model. The outcome is illustrated with use cases of medieval Gascon, and Italian.",https://aclanthology.org/2020.ldl-1.1,European Language Resources Association,2020,May,Proceedings of the 7th Workshop on Linked Data in Linguistics (LDL-2020),"Tittel, Sabine  and
Gillis-Webber, Frances  and
Nannini, Alessandro A.",Towards an Ontology Based on Hallig-Wartburg's Begriffssystem for Historical Linguistic Linked Data,,ldl,1466
2021.latechclfl-1.4,"['Evaluation Techniques', 'Data Management and Generation', 'Low-resource Languages', 'Domain-specific NLP']",['Data Preparation'],['Annotation Processes'],"We report on an inter-annotator agreement experiment involving instances of text reuse focusing on the well-known case of biblical intertextuality in medieval literature. We target the application use case of literary scholars whose aim is to document instances of biblical references in the 'apparatus fontium' of a prospective digital edition. We develop a Bayesian implementation of Cohen's  for multiple annotators that allows us to assess the influence of various contextual effects on the inter-annotator agreement, producing both more robust estimates of the agreement indices as well as insights into the annotation process that leads to the estimated indices. As a result, we are able to produce a novel and nuanced estimation of inter-annotator agreement in the context of intertextuality, exploring the challenges that arise from manually annotating a dataset of biblical references in the writings of Bernard of Clairvaux. Among others, our method is able to unveil the fact that the obtained agreement depends heavily on the biblical source book of the proposed reference, as well as the underlying algorithm used to retrieve the candidate match. Finally, a discussion of the hurdles encountered by annotators supplements the results of the statistical analysis, contributing a qualitative insight into the difficulties involved in the identification of literary text reuse.",https://aclanthology.org/2021.latechclfl-1.4,Association for Computational Linguistics,2021,November,"Proceedings of the 5th Joint SIGHUM Workshop on Computational Linguistics for Cultural Heritage, Social Sciences, Humanities and Literature","Manjavacas Arevalo, Enrique  and
Mellerin, Laurence  and
Kestemont, Mike",Quantifying Contextual Aspects of Inter-annotator Agreement in Intertextuality Research,10.18653/v1/2021.latechclfl-1.4,latechclfl,1493
2021.mrl-1.1,"['Cross-lingual Application', 'Multilingual NLP', 'Learning Paradigms', 'Low-resource Languages', 'Classification Applications', 'Model Architectures']","['Few-shot Learning', 'Transformer Models', 'Large Language Models (LLMs)']",,"General-purpose language models have demonstrated impressive capabilities, performing on par with state-of-the-art approaches on a range of downstream natural language processing NLP tasks and benchmarks when inferring instructions from very few examples. Here, we evaluate the multilingual skills of the GPT and T5 models in conducting multi-class classification on non-English languages without any parameter updates. We show that, given a few English examples as context, pre-trained language models can predict not only English test samples but also non-English ones. Finally, we find the in-context few-shot cross-lingual prediction results of language models are significantly better than random prediction, and they are competitive compared to the existing state-of-the-art cross-lingual models and translation models.",https://aclanthology.org/2021.mrl-1.1,Association for Computational Linguistics,2021,November,Proceedings of the 1st Workshop on Multilingual Representation Learning,"Winata, Genta Indra  and
Madotto, Andrea  and
Lin, Zhaojiang  and
Liu, Rosanne  and
Yosinski, Jason  and
Fung, Pascale",Language Models are Few-shot Multilingual Learners,10.18653/v1/2021.mrl-1.1,mrl,412
2022.bea-1.15,"['Domain-specific NLP', 'Low-resource Languages', 'Classification Applications', 'Data Management and Generation']",['Data Preparation'],['Annotation Processes'],"With the growth of online learning through MOOCs and other educational applications, it has become increasingly difficult for course providers to offer personalized feedback to students. Therefore asking students to provide feedback to each other has become one way to support learning. This peer-to-peer feedback has become increasingly important whether in MOOCs to provide feedback to thousands of students or in large-scale classes at universities. One of the challenges when allowing peer-to-peer feedback is that the feedback should be perceived as helpful, and an import factor determining helpfulness is how specific the feedback is. However, in classes including thousands of students, instructors do not have the resources to check the specificity of every piece of feedback between students. Therefore, we present an automatic classification model to measure sentence specificity in written feedback. The model was trained and tested on student feedback texts written in German where sentences have been labelled as general or specific. We find that we can automatically classify the sentences with an accuracy of 76.7% using a conventional feature-based approach, whereas transfer learning with BERT for German gives a classification accuracy of 81.1%. However, the feature-based approach comes with lower computational costs and preserves human interpretability of the coefficients. In addition we show that specificity of sentences in feedback texts has a weak positive correlation with perceptions of helpfulness. This indicates that specificity is one of the ingredients of good feedback, and invites further investigation.",https://aclanthology.org/2022.bea-1.15,Association for Computational Linguistics,2022,July,Proceedings of the 17th Workshop on Innovative Use of NLP for Building Educational Applications (BEA 2022),"Rietsche, Roman  and
Caines, Andrew  and
Schramm, Cornelius  and
Pf{\""u}tze, Dominik  and
Buttery, Paula",The Specificity and Helpfulness of Peer-to-Peer Feedback in Higher Education,10.18653/v1/2022.bea-1.15,bea,1350
2021.nllp-1.5,"['Domain-specific NLP', 'Data Management and Generation', 'Multilingual NLP', 'Knowledge Representation and Reasoning', 'Low-resource Languages', 'Classification Applications']","['Medical and Clinical NLP', 'Data Preparation', 'Multilabel Text Classification', 'NLP for the Legal Domain', 'Taxonomy Construction']",['Annotation Processes'],"The COVID-19 pandemic has witnessed the implementations of exceptional measures by governments across the world to counteract its impact. This work presents the initial results of an on-going project, EXCEPTIUS, aiming to automatically identify, classify and compare exceptional measures against COVID-19 across 32 countries in Europe. To this goal, we created a corpus of legal documents with sentence-level annotations of eight different classes of exceptional measures that are implemented across these countries. We evaluated multiple multi-label classifiers on a manually annotated corpus at sentence level. The XLM-RoBERTa model achieves highest performance on this multilingual multi-label classification task, with a macro-average F1 score of 59.8%.",https://aclanthology.org/2021.nllp-1.5,Association for Computational Linguistics,2021,November,Proceedings of the Natural Legal Language Processing Workshop 2021,"Tziafas, Georgios  and
de Saint-Phalle, Eugenie  and
de Vries, Wietse  and
Egger, Clara  and
Caselli, Tommaso",A Multilingual Approach to Identify and Classify Exceptional Measures against COVID-19,10.18653/v1/2021.nllp-1.5,nllp,874
O18-1017,"['Audio Generation and Processing', 'Data Management and Generation', 'Low-resource Languages']","['Data Preparation', 'Data Analysis']",,"This paper explores the relationship between children's vocalization and the linguistic input they received by using LENA Language Environment Analysis https://www.lena.org/, a computer system of automatic, objective and inexpensive device that collects and analyzes data for up to 16-hour-long recordings. It has been shown that the audio input children exposed to might largely affect their vocalizations 1, 2, 3, which in turn serve as an essential indicator of their later language development 4, 5.",https://aclanthology.org/O18-1017,The Association for Computational Linguistics and Chinese Language Processing (ACLCLP),2018,October,Proceedings of the 30th Conference on Computational Linguistics and Speech Processing ({ROCLING} 2018),"Chen, Li-Mei  and
Oller, D. Kimbrough  and
Lee, Chia-Cheng  and
Liu, Chin-Ting Jimbo",LENA computerized automatic analysis of speech development from birth to three,,O18,152
2021.smm4h-1.13,"['Information Extraction', 'Domain-specific NLP', 'Low-resource Languages', 'Classification Applications']","['NLP for News and Media', 'Medical and Clinical NLP']",['NLP for Social Media'],"This is the system description of the CA-SIA_Unisound team for Task 1, Task 7b, and Task 8 of the sixth Social Media Mining for Health Applications SMM4H shared task in 2021. To address two shared challenges among those tasks, the colloquial text and the imbalance annotation, we apply customized pre-trained language models and propose various training strategies. Experimental results show the effectiveness of our system. Moreover, we got an F1-score of 0.87 in task 8, which is the highest among all participates.",https://aclanthology.org/2021.smm4h-1.13,Association for Computational Linguistics,2021,June,Proceedings of the Sixth Social Media Mining for Health ({\#}SMM4H) Workshop and Shared Task,"Zhou, Tong  and
Li, Zhucong  and
Gan, Zhen  and
Zhang, Baoli  and
Chen, Yubo  and
Niu, Kun  and
Wan, Jing  and
Liu, Kang  and
Zhao, Jun  and
Shi, Yafei  and
Chong, Weifeng  and
Liu, Shengping","Classification, Extraction, and Normalization : CASIA\_Unisound Team at the Social Media Mining for Health 2021 Shared Tasks",10.18653/v1/2021.smm4h-1.13,smm4h,563
2020.computerm-1.1,"['Data Management and Generation', 'Domain-specific NLP', 'Information Extraction', 'Embeddings']","['NLP for News and Media', 'Data Preparation', 'Word Embeddings']",,"The first step of any terminological work is to setup a reliable, specialized corpus composed of documents written by specialists and then to apply automatic term extraction ATE methods to this corpus in order to retrieve a first list of potential terms. In this paper, the experiment we describe differs from this usual process. The corpus used for this study was built from newspaper articles retrieved from the Web using a short list of keywords. The general intuition on which this research is based is that ATE based corpus comparison techniques can be used to capture both similarities and dissimilarities between corpora. The former are exploited through a termhood measure and the latter through word embeddings. Our initial results were validated manually and show that combining a traditional ATE method that focuses on dissimilarities between corpora to newer methods that exploit similarities more specifically distributional features of candidates leads to promising results.",https://aclanthology.org/2020.computerm-1.1,European Language Resources Association,2020,May,Proceedings of the 6th International Workshop on Computational Terminology,"Drouin, Patrick  and
Morel, Jean-Beno{\^\i}t  and
L{'} Homme, Marie-Claude",Automatic Term Extraction from Newspaper Corpora: Making the Most of Specificity and Common Features,,computerm,489
2021.germeval-1.4,"['Domain-specific NLP', 'Data Management and Generation', 'Multilingual NLP', 'Low-resource Languages', 'Classification Applications', 'Model Architectures']","['Transformer Models', 'Data Augmentation', 'Hate and Offensive Speech Detection', 'NLP for News and Media']",['NLP for Social Media'],"We present our submission to the first subtask of GermEval 2021 classification of German Facebook comments as toxic or not. Binary sequence classification is a standard NLP task with known state-of-the-art methods. Therefore, we focus on data preparation by using two different techniques: taskspecific pre-training and data augmentation. First, we pre-train multilingual transformers XLM-RoBERTa and MT5 on 12 hatespeech detection datasets in nine different languages. In terms of F1, we notice an improvement of 10% on average, using task-specific pretraining. Second, we perform data augmentation by labelling unlabelled comments, taken from Facebook, to increase the size of the training dataset by 79%. Models trained on the augmented training dataset obtain on average +0.0282 +5% F1 score compared to models trained on the original training dataset. Finally, the combination of the two techniques allows us to obtain an F1 score of 0.6899 with XLM-RoBERTa and 0.6859 with MT5. The code of the project is available at: https://github.com/ airKlizz/germeval2021toxic.",https://aclanthology.org/2021.germeval-1.4,Association for Computational Linguistics,2021,September,"Proceedings of the GermEval 2021 Shared Task on the Identification of Toxic, Engaging, and Fact-Claiming Comments","Calizzano, Remi  and
Ostendorff, Malte  and
Rehm, Georg",DFKI SLT at GermEval 2021: Multilingual Pre-training and Data Augmentation for the Classification of Toxicity in Social Media Comments,,germeval,1099
2021.motra-1.7,"['Data Management and Generation', 'Low-resource Languages', 'Classification Applications']","['Data Analysis', 'Sentiment Analysis (SA)']",,"Translation can obscure the subjectivity of the sources and flatten down positive and negative aspects. Thus, we perform an explorative analysis of translation in terms of sentiment properties focusing on the differences between student and professional translations of various registers. However, we do not compare translations with their sources, but analyse polarity items in two translation variants from the same text sources. We propose a multi-step analysis to investigate the distribution of polarity items and report on small experiments on a corpus of English to German translations to identify the lack of experience in translation by students. Our results show that pragmatic differences expressed in the usage of polarity words is highly dependent on the register a text belongs to. Following this, we identify registers, such as popularscientific articles, where students translate sentiment using more and heavier polarity words.",https://aclanthology.org/2021.motra-1.7,Association for Computational Linguistics,2021,May,Proceedings for the First Workshop on Modelling Translation: Translatology in the Digital Age,"Lapshinova-Koltunski, Ekaterina  and
Kliche, Fritz  and
Moskvina, Anna  and
Sch{\""a}fer, Johannes",Polarity in Translation: Differences between Novice and Experts across Registers,,motra,1147
2021.ijclclp-1.4,"['Data Management and Generation', 'Domain-specific NLP']",['Data Preparation'],,"The study aims to investigate the use of conjunctive adverbials CA, hereafter performing various textual relations in the English writing by Chinese speakers across genres and over time. To begin with, a corpus of one million word was compiled and the corpus interface was constructed. Later, 45 pieces of writing by 5 college students during 4 semesters were selected for data annotation and analysis, with each student contributing 9 pieces for 9 text genres. The results show that there exists a distribution norm of CA-performed textual relations based on CA occurrence frequency and that the distribution is independent of genre and time effects. Compared with literature, the found distribution is also considered free from the first language influence. This suggests that the found distribution is a mental representation of mature human cognition, underlying English writing on global and coherent levels. Therefore, the found distribution is of great potential for developing automatic tools of discourse diagnosis.",https://aclanthology.org/2021.ijclclp-1.4,Association for Computational Linguistics and Chinese Language Processing,2021,June,"International Journal of Computational Linguistics {\&} {C}hinese Language Processing, Volume 26, Number 1, June 2021","Kao, Tung-Yu  and
Chen, Li-mei",Textual Relations with Conjunctive Adverbials in English Writing by Chinese Speakers: A corpus-based Approach,,ijclclp,23
U18-1007,"['Dialogue Systems', 'Classification Applications', 'Model Architectures']",['Multimodal Learning'],,"In spite of the recent success of Dialogue Act DA classification, the majority of prior works focus on text-based classification with oracle transcriptions, i.e. human transcriptions, instead of Automatic Speech Recognition ASR's transcriptions. Moreover, the performance of this classification task, because of speaker domain shift, may deteriorate. In this paper, we explore the effectiveness of using both acoustic and textual signals, either oracle or ASR transcriptions, and investigate speaker domain adaptation for DA classification. Our multimodal model proves to be superior to the unimodal models, particularly when the oracle transcriptions are not available. We also propose an effective method for speaker domain adaptation, which achieves competitive results.",https://aclanthology.org/U18-1007,,2018,December,Proceedings of the Australasian Language Technology Association Workshop 2018,"He, Xuanli  and
Tran, Quan  and
Havard, William  and
Besacier, Laurent  and
Zukerman, Ingrid  and
Haffari, Gholamreza",Exploring Textual and Speech information in Dialogue Act Classification with Speaker Domain Adaptation,10.48550/arxiv.1810.07455,U18,2
2021.nodalida-main.36,"['Audio Generation and Processing', 'Low-resource Languages']",['Automatic Speech Recognition (ASR)'],,"Forced alignment is an effective process to speed up linguistic research. However, most forced aligners are languagedependent, and under-resourced languages rarely have enough resources to train an acoustic model for an aligner. We present a new Finnish grapheme-based forced aligner and demonstrate its performance by aligning multiple Uralic languages and English as an unrelated language. We show that even a simple non-expert created grapheme-to-phoneme mapping can result in useful word alignments.",https://aclanthology.org/2021.nodalida-main.36,"Link{\""o}ping University Electronic Press, Sweden",2021,May 31--2 June,Proceedings of the 23rd Nordic Conference on Computational Linguistics (NoDaLiDa),"Leinonen, Juho  and
Virpioja, Sami  and
Kurimo, Mikko",Grapheme-Based Cross-Language Forced Alignment: Results with Uralic Languages,,nodalida,99
2020.vlsp-1.6,"['Learning Paradigms', 'Information Extraction', 'Model Architectures', 'Low-resource Languages']","['Relation Extraction', 'Supervised Learning', 'Transformer Models']",,"In recent years, BERT-based models have achieved the state-of-the-art performance over many Natural Language Language tasks. Because of that, BERT-based model becomes a trend and is widely used for so many NLP task. And in this paper, we present our approach on how we apply BERT-based model to Relation Extraction shared-task of VLSP 2020 campaign. In detail, we present: 1 our general idea to solve this task; 2 how we preprocess data to fit with the idea and to yield better result; 3 how we use BERT-based models for Relation Extraction task; and 4 our experiment and result on public development data and private test data of VLSP 2020.",https://aclanthology.org/2020.vlsp-1.6,Association for Computational Lingustics,2020,December,Proceedings of the 7th International Workshop on Vietnamese Language and Speech Processing,"Nguyn, Thut  and
Mn, Hiu",Vietnamese Relation Extraction with BERT-based Models at VLSP 2020,,vlsp,1126
2022.sigtyp-1.5,"['Information Extraction', 'Data Management and Generation', 'Multilingual NLP', 'Low-resource Languages']",['Data Preparation'],['Annotation Processes'],"We describe a methodology to extract with finer accuracy word order patterns from texts automatically annotated with Universal Dependency UD trained parsers. We use the methodology to quantify the word order entropy of determiners, quantifiers and numerals in ten Indo-European languages, using UDparsed texts from a parallel corpus of prosaic texts. Our results suggest that the combinations of different UD annotation layers, such as UD Relations, Universal Parts of Speech and lemma, and the introduction of languagespecific lists of closed-category lemmata has the two-fold effect of improving the quality of analysis and unveiling hidden areas of variability in word order patterns.",https://aclanthology.org/2022.sigtyp-1.5,Association for Computational Linguistics,2022,July,Proceedings of the 4th Workshop on Research in Computational Linguistic Typology and Multilingual NLP,"Talamo, Luigi","Tweaking UD Annotations to Investigate the Placement of Determiners, Quantifiers and Numerals in the Noun Phrase",10.18653/v1/2022.sigtyp-1.5,sigtyp,535
2020.smm4h-1.25,"['Domain-specific NLP', 'Information Extraction', 'Low-resource Languages', 'Classification Applications', 'Model Architectures']","['Medical and Clinical NLP', 'NLP for News and Media']",['NLP for Social Media'],"Identifying and extracting reports of medications, their abuse or adverse effects from social media is a challenging task. In social media, relevant reports are very infrequent, causes imbalanced class distribution for machine learning algorithms. Learning algorithms typically designed to optimize the overall accuracy without considering the relative distribution of each class. Thus, imbalanced class distribution is problematic as learning algorithms have low predictive accuracy for the infrequent class. Moreover, social media represents natural linguistic variation in creative language expressions. In this paper, we have used a combination of data balancing and neural language representation techniques to address the challenges. Specifically, we participated the shared tasks 1, 2 all languages, 4, and 3 only the span detection, no normalization was attempted in Social Media Mining for Health applications SMM4H 2020 Klein et al., 2020 . The results show that with the proposed methodology recall scores are better than the precision scores for the shared tasks. The recall score is also better compared to the mean score of the total submissions. However, the F1-score is worse than the mean score except for task 2 French.",https://aclanthology.org/2020.smm4h-1.25,Association for Computational Linguistics,2020,December,Proceedings of the Fifth Social Media Mining for Health Applications Workshop {\&} Shared Task,"Liza, Farhana Ferdousi",Sentence Classification with Imbalanced Data for Health Applications,,smm4h,1137
2021.maiworkshop-1.3,"['Learning Paradigms', 'Dialogue Systems', 'Classification Applications', 'Model Architectures']","['Multimodal Learning', 'Emotion Detection', 'Recurrent Neural Networks (RNNs)', 'Adversarial Learning']",,"Emotion recognition in conversation has received considerable attention recently because of its practical industrial applications. Existing methods tend to overlook the immediate mutual interaction between different speakers in the speaker-utterance level, or apply single speaker-agnostic RNN for utterances from different speakers. We propose COIN, a conversational interactive model to mitigate this problem by applying state mutual interaction within history contexts. In addition, we introduce a stacked global interaction module to capture the contextual and inter-dependency representation in a hierarchical manner. To improve the robustness and generalization during training, we generate adversarial examples by applying the minor perturbations on multimodal feature inputs, unveiling the benefits of adversarial examples for emotion detection. The proposed model empirically achieves the current state-of-the-art results on the IEMO-CAP benchmark dataset.",https://aclanthology.org/2021.maiworkshop-1.3,Association for Computational Linguistics,2021,June,Proceedings of the Third Workshop on Multimodal Artificial Intelligence,"Zhang, Haidong  and
Chai, Yekun",COIN: Conversational Interactive Networks for Emotion Recognition in Conversation,10.18653/v1/2021.maiworkshop-1.3,maiworkshop,894
2020.deelio-1.5,"['Commonsense Reasoning', 'Model Architectures', 'Knowledge Representation and Reasoning']","['Knowledge Graphs', 'Transformer Models']",,"Following the major success of neural language models LMs such as BERT or GPT-2 on a variety of language understanding tasks, recent work focused on injecting structured knowledge from external resources into these models. While on the one hand, joint pretraining i.e., training from scratch, adding objectives based on external knowledge to the primary LM objective may be prohibitively computationally expensive, post-hoc fine-tuning on external knowledge, on the other hand, may lead to the catastrophic forgetting of distributional knowledge. In this work, we investigate models for complementing the distributional knowledge of BERT with conceptual knowledge from ConceptNet and its corresponding Open Mind Common Sense OMCS corpus, respectively, using adapter training. While overall results on the GLUE benchmark paint an inconclusive picture, a deeper analysis reveals that our adapter-based models substantially outperform BERT up to 15-20 performance points on inference tasks that require the type of conceptual knowledge explicitly present in ConceptNet and OMCS. We also open source all our experiments and relevant code under: https://github.com/ wluper/retrograph.",https://aclanthology.org/2020.deelio-1.5,Association for Computational Linguistics,2020,November,Proceedings of Deep Learning Inside Out (DeeLIO): The First Workshop on Knowledge Extraction and Integration for Deep Learning Architectures,"Lauscher, Anne  and
Majewska, Olga  and
Ribeiro, Leonardo F. R.  and
Gurevych, Iryna  and
Rozanov, Nikolai  and
Glava{\v{s}}, Goran",Common Sense or World Knowledge? Investigating Adapter-Based Knowledge Injection into Pretrained Transformers,10.18653/v1/2020.deelio-1.5,deelio,841
2020.cllrd-1.3,"['Evaluation Techniques', 'Data Management and Generation']",['Data Preparation'],['Annotation Processes'],"Labelling, or annotation, is the process by which we assign labels to an item with regards to a task. In some Artificial Intelligence problems, such as Computer Vision tasks, the goal is to obtain objective labels. However, in problems such as text and sentiment analysis, subjective labelling is often required. More so when the sentiment analysis deals with actual emotions instead of polarity positive/negative . Scientists employ human experts to create these labels, but it is costly and time consuming. Crowdsourcing enables researchers to utilise non-expert knowledge for scientific tasks. From image analysis to semantic annotation, interested researchers can gather a large sample of answers via crowdsourcing platforms in a timely manner. However, non-expert contributions often need to be thoroughly assessed, particularly so when a task is subjective. Researchers have traditionally used 'Gold Standard', 'Thresholding' and 'Majority Voting' as methods to filter non-expert contributions. We argue that these methods are unsuitable for subjective tasks, such as lexicon acquisition and sentiment analysis. We discuss subjectivity in human centered tasks and present a filtering method that defines quality contributors, based on a set of objectively infused terms in a lexicon acquisition task. We evaluate our method against an established lexicon, the diversity of emotions -i.e. subjectivity-and the exclusion of contributions. Our proposed objective evaluation method can be used to assess contributors in subjective tasks that will provide domain agnostic, quality results, with at least 7% improvement over traditional methods.",https://aclanthology.org/2020.cllrd-1.3,European Language Resources Association,2020,May,Proceedings of the LREC 2020 Workshop on ``Citizen Linguistics in Language Resource Development'',"Haralabopoulos, Giannis  and
Tsikandilakis, Myron  and
Torres Torres, Mercedes  and
McAuley, Derek",Objective Assessment of Subjective Tasks in Crowdsourcing Applications,,cllrd,1288
2022.bea-1.10,"['Text Generation', 'Learning Paradigms', 'Domain-specific NLP']",['Unsupervised Learning'],,"In field of teaching, true/false questioning is an important educational method for assessing students' general understanding of learning materials. Manually creating such questions requires extensive human effort and expert knowledge. Question Generation QG technique offers the possibility to automatically generate a large number of questions. However, there is limited work on automatic true/false question generation due to the lack of training data and difficulty finding question-worthy content. In this paper, we propose an unsupervised True/False Question Generation approach TF-QG that automatically generates true/false questions from a given passage for reading comprehension test. TF-QG consists of a template-based framework that aims to test the specific knowledge in the passage by leveraging various NLP techniques, and a generative framework to generate more flexible and complicated questions by using a novel masking-and-infilling strategy. Human evaluation shows that our approach can generate high-quality and valuable true/false questions. In addition, simulated testing on the generated questions challenges the state-of-the-art inference models from NLI, QA, and fact verification tasks.",https://aclanthology.org/2022.bea-1.10,Association for Computational Linguistics,2022,July,Proceedings of the 17th Workshop on Innovative Use of NLP for Building Educational Applications (BEA 2022),"Zou, Bowei  and
Li, Pengfei  and
Pan, Liangming  and
Aw, Ai Ti",Automatic True/False Question Generation for Educational Purpose,10.18653/v1/2022.bea-1.10,bea,73
N16-1169,"['Text Generation', 'Data Management and Generation']","['Data Preparation', 'Data Analysis', 'Paraphrase and Rephrase Generation']",['Annotation Processes'],"This paper presents a methodology to extract positive interpretations from negated statements. First, we automatically generate plausible interpretations using well-known grammar rules and manipulating semantic roles. Second, we score plausible alternatives according to their likelihood. Manual annotations show that the positive interpretations are intuitive to humans, and experimental results show that the scoring task can be automated.",https://aclanthology.org/N16-1169,Association for Computational Linguistics,2016,June,Proceedings of the 2016 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies,"Blanco, Eduardo  and
Sarabi, Zahra",Automatic Generation and Scoring of Positive Interpretations from Negated Statements,10.18653/v1/N16-1169,N16,11
2020.cmlc-1.8,"['Data Management and Generation', 'Low-resource Languages']","['Data Preparation', 'Data Analysis']",,"The paper overviews the state of implementation of the Czech National Corpus CNC in all the main areas of its operation: corpus compilation, annotation, application development and user services. As the focus is on the recent development, some of the areas are described in more detail than the others. Close attention is paid to the data collection and, in particular, to the description of web application development. This is not only because CNC has recently seen a significant progress in this area, but also because we believe that end-user web applications shape the way linguists and other scholars think about the language data and about the range of possibilities they offer. This consideration is even more important given the variability of the CNC corpora.",https://aclanthology.org/2020.cmlc-1.8,European Language Ressources Association,2020,May,Proceedings of the 8th Workshop on Challenges in the Management of Large Corpora,"Kren, Michal",Czech National Corpus in 2020: Recent Developments and Future Outlook,,cmlc,1220
2020.emnlp-main.239,"['Model Architectures', 'Learning Paradigms']",['Adversarial Learning'],,"One approach to matching texts from asymmetrical domains is projecting the input sequences into a common semantic space as feature vectors upon which the matching function can be readily defined and learned. In realworld matching practices, it is often observed that with the training goes on, the feature vectors projected from different domains tend to be indistinguishable. The phenomenon, however, is often overlooked in existing matching models. As a result, the feature vectors are constructed without any regularization, which inevitably increases the difficulty of learning the downstream matching functions. In this paper, we propose a novel match method tailored for text matching in asymmetrical domains, called WD-Match. In WD-Match, a Wasserstein distance-based regularizer is defined to regularize the features vectors projected from different domains. As a result, the method enforces the feature projection function to generate vectors such that those correspond to different domains cannot be easily discriminated. The training process of WD-Match amounts to a game that minimizes the matching loss regularized by the Wasserstein distance. WD-Match can be used to improve different text matching methods, by using the method as its underlying matching model. Four popular text matching methods have been exploited in the paper. Experimental results based on four publicly available benchmarks showed that WD-Match consistently outperformed the underlying methods and the baselines.",https://aclanthology.org/2020.emnlp-main.239,Association for Computational Linguistics,2020,November,Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),"Yu, Weijie  and
Xu, Chen  and
Xu, Jun  and
Pang, Liang  and
Gao, Xiaopeng  and
Wang, Xiaozhao  and
Wen, Ji-Rong",Wasserstein Distance Regularized Sequence Representation for Text Matching in Asymmetrical Domains,10.18653/v1/2020.emnlp-main.239,emnlp,827
2021.triton-1.19,['Machine Translation (MT)'],,,"Despite the increasingly good quality of Machine Translation MT systems, MT outputs require corrections. Automatic Post-Editing APE models have been introduced to perform these corrections without human intervention. However, no system has been able to fully automate the Post-Editing PE process. Moreover, while numerous translation tools, such as Translation Memories TMs, largely benefit from translators' input, Human-Computer Interaction HCI remains limited when it comes to PE. This research-in-progress paper discusses APE models and suggests that they could be improved in more interactive scenarios, as previously done in MT with the creation of Interactive MT IMT systems. Based on the hypothesis that PE would benefit from HCI, two methodologies are proposed. Both suggest that traditional batch learning settings are not optimal for PE. Instead, online techniques are recommended to train and update PE models on the fly, via either real or simulated interactions with the translator.",https://aclanthology.org/2021.triton-1.19,INCOMA Ltd.,2021,July,Proceedings of the Translation and Interpreting Technology Online Conference,"Escribe, Marie  and
Mitkov, Ruslan",Interactive Models for Post-Editing,10.26615/978-954-452-071-7_019,triton,930
2020.clinicalnlp-1.19,"['Domain-specific NLP', 'Classification Applications', 'Model Architectures']",['Medical and Clinical NLP'],['NLP for Mental Health'],"While dementia with Lewy bodies DLB is the second most common type of neurodegenerative dementia following Alzheimer's disease AD, it is difficult to distinguish from AD. We propose a method for DLB detection by using mental health record MHR documents from a 3-month period before a patient has been diagnosed with DLB or AD. Our objective is to develop a model that could be clinically useful to differentiate between DLB and AD across various datasets from different healthcare institutions. We cast this as a classification task using convolutional neural network CNN, an efficient neural model for text classification. We experiment with different representation models, and explore the features that contribute to model performances. In addition, we apply temperature scaling, a simple but efficient model calibration method, to produce more reliable predictions. We believe the proposed method has important potential for clinical applications using routine healthcare records, and for generalising to other relevant clinical record datasets. To the best of our knowledge, this is the first attempt to distinguish DLB from AD using mental health records, and to improve the reliability of DLB predictions.",https://aclanthology.org/2020.clinicalnlp-1.19,Association for Computational Linguistics,2020,November,Proceedings of the 3rd Clinical Natural Language Processing Workshop,"Wang, Zixu  and
Ive, Julia  and
Moylett, Sinead  and
Mueller, Christoph  and
Cardinal, Rudolf  and
Velupillai, Sumithra  and
O{'}Brien, John  and
Stewart, Robert",Distinguishing between Dementia with Lewy bodies DLB and Alzheimer's Disease AD using Mental Health Records: a Classification Approach,10.18653/v1/2020.clinicalnlp-1.19,clinicalnlp,361
P17-1167,"['Question Answering (QA)', 'Parsing', 'Data Management and Generation']","['Data Preparation', 'Semantic Parsing']",,"Recent work in semantic parsing for question answering has focused on long and complicated questions, many of which would seem unnatural if asked in a normal conversation between two humans. In an effort to explore a conversational QA setting, we present a more realistic task: answering sequences of simple but inter-related questions. We collect a dataset of 6,066 question sequences that inquire about semistructured tables from Wikipedia, with 17,553 question-answer pairs in total. To solve this sequential question answering task, we propose a novel dynamic neural semantic parsing framework trained using a weakly supervised reward-guided search. Our model effectively leverages the sequential context to outperform state-of-the-art QA systems that are designed to answer highly complex questions.",https://aclanthology.org/P17-1167,Association for Computational Linguistics,2017,July,Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),"Iyyer, Mohit  and
Yih, Wen-tau  and
Chang, Ming-Wei",Search-based Neural Structured Learning for Sequential Question Answering,10.18653/v1/P17-1167,P17,547
2020.blackboxnlp-1.14,['Model Architectures'],,,"There is a recent surge of interest in using attention as explanation of model predictions, with mixed evidence on whether attention can be used as such. While attention conveniently gives us one weight per input token and is easily extracted, it is often unclear toward what goal it is used as explanation. We find that often that goal, whether explicitly stated or not, is to find out what input tokens are the most relevant to a prediction, and that the implied user for the explanation is a model developer. For this goal and user, we argue that input saliency methods are better suited, and that there are no compelling reasons to use attention, despite the coincidence that it provides a weight for each input. With this position paper, we hope to shift some of the recent focus on attention to saliency methods, and for authors to clearly state the goal and user for their explanations.",https://aclanthology.org/2020.blackboxnlp-1.14,Association for Computational Linguistics,2020,November,Proceedings of the Third BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP,"Bastings, Jasmijn  and
Filippova, Katja",The elephant in the interpretability room: Why use attention as explanation when we have saliency methods?,10.18653/v1/2020.blackboxnlp-1.14,blackboxnlp,1438
2020.coling-main.525,"['Learning Paradigms', 'Image and Video Processing', 'Model Architectures', 'Low-resource Languages']","['Transfer Learning', 'Transformer Models']",,"Sign Language Translation SLT first uses a Sign Language Recognition SLR system to extract sign language glosses from videos. Then, a translation system generates spoken language translations from the sign language glosses. This paper focuses on the translation system and introduces the STMC-Transformer which improves on the current state-of-the-art by over 5 and 7 BLEU respectively on gloss-to-text and video-to-text translation of the PHOENIX-Weather 2014T dataset. On the ASLG-PC12 corpus, we report an increase of over 16 BLEU. We also demonstrate the problem in current methods that rely on gloss supervision. The videoto-text translation of our STMC-Transformer outperforms translation of GT glosses. This contradicts previous claims that GT gloss translation acts as an upper bound for SLT performance and reveals that glosses are an inefficient representation of sign language. For future SLT research, we therefore suggest an end-to-end training of the recognition and translation models, or using a different sign language annotation scheme. *",https://aclanthology.org/2020.coling-main.525,International Committee on Computational Linguistics,2020,December,Proceedings of the 28th International Conference on Computational Linguistics,"Yin, Kayo  and
Read, Jesse",Better Sign Language Translation with STMC-Transformer,10.18653/v1/2020.coling-main.525,coling,1483
2020.framenet-1.5,"['Parsing', 'Data Management and Generation', 'Cross-lingual Application', 'Knowledge Representation and Reasoning']","['Data Preparation', 'Semantic Parsing']",['Annotation Processes'],"We introduce an annotation tool whose purpose is to gain insights into variation of framing by combining FrameNet annotation with referential annotation. English FrameNet enables researchers to study variation in framing at the conceptual level as well through its packaging in language. We enrich FrameNet annotations in two ways. First, we introduce the referential aspect. Secondly, we annotate on complete texts to encode connections between mentions. As a result, we can analyze the variation of framing for one particular event across multiple mentions and cross-lingual documents. We can examine how an event is framed over time and how core frame elements are expressed throughout a complete text. The data model starts with a representation of an event type. Each event type has many incidents linked to it, and each incident has several reference texts describing it as well as structured data about the incident. The user can apply two types of annotations: 1 mappings from expressions to frames and frame elements, 2 reference relations from mentions to events and participants of the structured data.",https://aclanthology.org/2020.framenet-1.5,European Language Resources Association,2020,May,"Proceedings of the International FrameNet Workshop 2020: Towards a Global, Multilingual FrameNet","Postma, Marten  and
Remijnse, Levi  and
Ilievski, Filip  and
Fokkens, Antske  and
Titarsolej, Sam  and
Vossen, Piek",Combining Conceptual and Referential Annotation to Study Variation in Framing,,framenet,929
2022.fl4nlp-1.2,"['Learning Paradigms', 'Model Architectures']","['Transformer Models', 'Large Language Models (LLMs)']",,"Most studies in cross-device federated learning focus on small models, due to the serverclient communication and on-device computation bottlenecks. In this work, we leverage various techniques for mitigating these bottlenecks to train larger language models in cross-device federated learning. With systematic applications of partial model training, quantization, efficient transfer learning, and communication-efficient optimizers, we are able to train a 21M parameter Transformer that achieves the same perplexity as that of a similarly sized LSTM with  10 smaller client-to-server communication cost and 11% lower perplexity than smaller LSTMs commonly studied in literature.",https://aclanthology.org/2022.fl4nlp-1.2,Association for Computational Linguistics,2022,May,Proceedings of the First Workshop on Federated Learning for Natural Language Processing (FL4NLP 2022),"Ro, Jae  and
Breiner, Theresa  and
McConnaughey, Lara  and
Chen, Mingqing  and
Suresh, Ananda  and
Kumar, Shankar  and
Mathews, Rajiv",Scaling Language Model Size in Cross-Device Federated Learning,10.18653/v1/2022.fl4nlp-1.2,fl4nlp,1133
2020.cl-1.5,"['Data Management and Generation', 'Low-resource Languages']","['Data Preparation', 'Data Analysis']",['Annotation Processes'],"Negation is a universal linguistic phenomenon with a great qualitative impact on natural language processing applications. The availability of corpora annotated with negation is essential to training negation processing systems. Currently, most corpora have been annotated for English, but the presence of languages other than English on the Internet, such as Chinese or Spanish, is greater every day. In this study, we present a review of the corpora annotated with negation information in several languages with the goal of evaluating what aspects of negation have been annotated and how compatible the corpora are. We conclude that it is very difficult to merge the existing corpora because we found differences in the annotation schemes used, and most importantly, in the annotation guidelines: the way in which each corpus was tokenized and the negation elements that have been annotated. Differently than for other well established tasks like semantic role labeling or parsing, for negation there is no standard annotation scheme nor guidelines, which hampers progress in its treatment.",https://aclanthology.org/2020.cl-1.5,MIT Press,2020,,,"Jim{\'e}nez-Zafra, Salud Mar{\'\i}a  and
Morante, Roser  and
Mart{\'\i}n-Valdivia, Mar{\'\i}a Teresa  and
Ure{\~n}a-L{\'o}pez, L. Alfonso",Corpora Annotated with Negation: An Overview,10.1162/coli_a_00371,cl,1427
S17-1017,"['Evaluation Techniques', 'Embeddings']",['Word Embeddings'],,"This paper explores the possibilities of analogical reasoning with vector space models. Given two pairs of words with the same relation e.g. man:woman :: king:queen, it was proposed that the offset between one pair of the corresponding word vectors can be used to identify the unknown member of the other pair  We argue against such ""linguistic regularities"" as a model for linguistic relations in vector space models and as a benchmark, and we show that the vector offset as well as two other, better-performing methods suffers from dependence on vector similarity.",https://aclanthology.org/S17-1017,Association for Computational Linguistics,2017,August,Proceedings of the 6th Joint Conference on Lexical and Computational Semantics (*{SEM} 2017),"Rogers, Anna  and
Drozd, Aleksandr  and
Li, Bofang",The too Many Problems of Analogical Reasoning with Word Vectors,10.18653/v1/S17-1017,S17,917
2021.mrl-1.6,"['Information Extraction', 'Multilingual NLP', 'Learning Paradigms', 'Low-resource Languages', 'Cross-lingual Application', 'Model Architectures']","['Transformer Models', 'Transfer Learning', 'Coreference Resolution', 'Adversarial Learning']",,"We study a new problem of cross-lingual transfer learning for event coreference resolution ECR where models trained on data from a source language are adapted for evaluations in different target languages. We introduce the first baseline model for this task based on XLM-RoBERTa, a state-of-the-art multilingual pre-trained language model. We also explore language adversarial neural networks LANN that present language discriminators to distinguish texts from the source and target languages to improve the language generalization for ECR. In addition, we introduce two novel mechanisms to further enhance the general representation learning of LANN, featuring: i multi-view alignment to penalize cross coreference-label alignment of examples in the source and target languages, and ii optimal transport to select close examples in the source and target languages to provide better training signals for the language discriminators. Finally, we perform extensive experiments for cross-lingual ECR from English to Spanish and Chinese to demonstrate the effectiveness of the proposed methods.",https://aclanthology.org/2021.mrl-1.6,Association for Computational Linguistics,2021,November,Proceedings of the 1st Workshop on Multilingual Representation Learning,"Phung, Duy  and
Minh Tran, Hieu  and
Nguyen, Minh Van  and
Nguyen, Thien Huu",Learning Cross-lingual Representations for Event Coreference Resolution with Multi-view Alignment and Optimal Transport,10.18653/v1/2021.mrl-1.6,mrl,717
2020.lrec-1.426,"['Audio Generation and Processing', 'Low-resource Languages', 'Learning Paradigms', 'Multilingual NLP', 'Model Architectures']",['Automatic Speech Recognition (ASR)'],,"This paper reports on the semi-supervised development of acoustic and language models for under-resourced, code-switched speech in five South African languages. Two approaches are considered. The first constructs four separate bilingual automatic speech recognisers ASRs corresponding to four different language pairs between which speakers switch frequently. The second uses a single, unified, five-lingual ASR system that represents all the languages English, isiZulu, isiXhosa, Setswana and Sesotho. We evaluate the effectiveness of these two approaches when used to add additional data to our extremely sparse training sets. Results indicate that batch-wise semi-supervised training yields better results than a non-batch-wise approach. Furthermore, while the separate bilingual systems achieved better recognition performance than the unified system, they benefited more from pseudo-labels generated by the five-lingual system than from those generated by the bilingual systems.",https://aclanthology.org/2020.lrec-1.426,European Language Resources Association,2020,May,Proceedings of the 12th Language Resources and Evaluation Conference,"Biswas, Astik  and
Yilmaz, Emre  and
De Wet, Febe  and
Van der westhuizen, Ewald  and
Niesler, Thomas",Semi-supervised Development of ASR Systems for Multilingual Code-switched Speech in Under-resourced Languages,10.48550/arxiv.2003.03135,lrec,27
2020.amta-research.4,"['Low-resource Languages', 'Model Architectures', 'Machine Translation (MT)']","['Neural MT (NMT)', 'Transformer Models']",,"Constrained decoding forces a certain set words or phrases to appear in the translation results and is very useful when adapting MT to a certain domain. In recent years, the Transformer model has outperformed other neural machine translation models to become the state-of-theart paradigm. However, constrained decoding for domain adaptation remains an open problem under the Transformer model. In this paper, we first investigate how a constrained decoding method -Grid Beam Search GBS -performs in the Transformer model, and then propose a source-informed heuristic method that can fully take advantage of the alignment information from the multi-head attention mechanism in Transformer to speed up the decoding in the GBS method and guide the placement of constraints during the expansion of hypotheses in GBS. Experiments on English-Chinese and English-German translation domain adaptation tasks show that the proposed method significantly outperforms the basic Transformer model in terms of BLEU and METEOR score, and prunes up to 30% hypotheses to save up to 20% decoding time compared to the GBS model while maintaining comparable translation performance.",https://aclanthology.org/2020.amta-research.4,Association for Machine Translation in the Americas,2020,October,Proceedings of the 14th Conference of the Association for Machine Translation in the Americas (Volume 1: Research Track),"Xie, Guodong  and
Way, Andy",Constraining the Transformer NMT Model with Heuristic Grid Beam Search,,amta,913
2020.iwclul-1.4,"['Multilingual NLP', 'Knowledge Representation and Reasoning', 'Low-resource Languages']",,,"We present an open online infrastructure for editing and visualization of dictionaries of different Uralic languages e.g. Erzya, Moksha, Skolt Sami and Komi-Zyrian. Our infrastructure integrates fully into the existing Giellatekno one in terms of XML dictionaries and FST morphology. Our code is open source, and the system is being actively used in editing a Skolt Sami dictionary set to be published in 2020.",https://aclanthology.org/2020.iwclul-1.4,Association for Computational Linguistics,2020,January,Proceedings of the Sixth International Workshop on Computational Linguistics of Uralic Languages,"Alnajjar, Khalid  and
H{\""a}m{\""a}l{\""a}inen, Mika  and
Rueter, Jack",On Editing Dictionaries for Uralic Languages in an Online Environment,10.18653/v1/2020.iwclul-1.4,iwclul,1260
2020.intellang-1.7,"['Text Generation', 'Domain-specific NLP', 'Data Management and Generation']","['Data Analysis', 'Text Simplification', 'Medical and Clinical NLP']",,"Can stress affect not only your life but also how you read and interpret a text? Healthcare has shown evidence of such dynamics and in this short paper we discuss customising texts based on user stress level, as it could represent a critical factor when it comes to user engagement and behavioural change. We first show a real-world example in which user behaviour is influenced by stress, then, after discussing which tools can be employed to assess and measure it, we propose an initial method for tailoring the document by exploiting complexity reduction and affect enforcement. The result is a short and encouraging text which requires less commitment to be read and understood. We believe this work in progress can raise some interesting questions on a topic that is often overlooked in NLG.",https://aclanthology.org/2020.intellang-1.7,Association for Computational Lingustics,2020,September,Proceedings of the Workshop on Intelligent Information Processing and Natural Language Generation,"Balloccu, Simone  and
Reiter, Ehud  and
Johnstone, Alexandra  and
Fyfe, Claire",How are you? Introducing stress-based text tailoring,10.48550/arxiv.2007.09970,intellang,1218
2020.clinicalnlp-1.20,"['Domain-specific NLP', 'Data Management and Generation', 'Information Extraction', 'Classification Applications', 'Model Architectures']","['Data Preparation', 'Medical and Clinical NLP']",,"Automated Medication Regimen MR extraction from medical conversations can not only improve recall and help patients follow through with their care plan, but also reduce the documentation burden for doctors. In this paper, we focus on extracting spans for frequency, route and change, corresponding to medications discussed in the conversation. We first describe a unique dataset of annotated doctor-patient conversations and then present a weakly supervised model architecture that can perform span extraction using noisy classification data. The model utilizes an attention bottleneck inside a classification model to perform the extraction. We experiment with several variants of attention scoring and projection functions and propose a novel transformer-based attention scoring function TAScore. The proposed combination of TAScore and Fusedmax projection achieves a 10 point increase in Longest Common Substring F1 compared to the baseline of additive scoring plus softmax projection.",https://aclanthology.org/2020.clinicalnlp-1.20,Association for Computational Linguistics,2020,November,Proceedings of the 3rd Clinical Natural Language Processing Workshop,"Patel, Dhruvesh  and
Konam, Sandeep  and
Prabhakar, Sai",Weakly Supervised Medication Regimen Extraction from Medical Conversations,10.18653/v1/2020.clinicalnlp-1.20,clinicalnlp,1074
2020.splu-1.7,"['Image and Video Processing', 'Data Management and Generation']",['Data Preparation'],,"The Touchdown dataset Chen et al., 2019 provides instructions by human annotators for navigation through New York City streets and for resolving spatial descriptions at a given location. To enable the wider research community to work effectively with the Touchdown tasks, we are publicly releasing the 29k raw Street View panoramas needed for Touchdown. We follow the process used for the StreetLearn data release Mirowski et al., 2019  to check panoramas for personally identifiable information and blur them as necessary. These have been added to the StreetLearn dataset and can be obtained via the same process as used previously for StreetLearn. We also provide a reference implementation for both Touchdown tasks: vision and language navigation VLN and spatial description resolution SDR. We compare our model results to those given in Chen et al.  2019  and show that the panoramas we have added to StreetLearn support both Touchdown tasks and can be used effectively for further research and comparison.",https://aclanthology.org/2020.splu-1.7,Association for Computational Linguistics,2020,November,Proceedings of the Third International Workshop on Spatial Language Understanding,"Mehta, Harsh  and
Artzi, Yoav  and
Baldridge, Jason  and
Ie, Eugene  and
Mirowski, Piotr",Retouchdown: Releasing Touchdown on StreetLearn as a Public Resource for Language Grounding Tasks in Street View,10.18653/v1/2020.splu-1.7,splu,445
C16-1193,"['Dialogue Systems', 'Classification Applications', 'Model Architectures']","['Recurrent Neural Networks (RNNs)', 'Multilabel Text Classification']",['Long Short-Term Memory (LSTM) Models'],"In many applications such as personal digital assistants, there is a constant need for new domains to increase the system's coverage of user queries. A conventional approach is to learn a separate model every time a new domain is introduced. This approach is slow, inefficient, and a bottleneck for scaling to a large number of domains. In this paper, we introduce a framework that allows us to have a single model that can handle all domains: including unknown domains that may be created in the future as long as they are covered in the master schema. The key idea is to remove the need for distinguishing domains by explicitly predicting the schema of queries. Given permitted schema of a query, we perform constrained decoding on a lattice of slot sequences allowed under the schema. The proposed model achieves competitive and often superior performance over the conventional model trained separately per domain.",https://aclanthology.org/C16-1193,The COLING 2016 Organizing Committee,2016,December,"Proceedings of {COLING} 2016, the 26th International Conference on Computational Linguistics: Technical Papers","Kim, Young-Bum  and
Stratos, Karl  and
Sarikaya, Ruhi",Domainless Adaptation by Constrained Decoding on a Schema Lattice,,C16,1097
2020.udw-1.5,"['Data Management and Generation', 'Low-resource Languages']",['Data Preparation'],['Annotation Processes'],"This article considers the annotation of subjects in UD treebanks. The identification of the subject poses a particular problem in Wolof, due to pronominal indices whose status as a pronoun or a pronominal affix is uncertain. In the UD treebank available for Wolof Dione, 2019, these have been annotated depending on the construction either as true subjects, or as morphosyntactic features agreeing with the verb. The study of this corpus of 40 000 words allows us to show that the problem is indeed difficult to solve, especially since Wolof has a rich system of auxiliaries and several basic constructions with different properties. Before addressing the case of Wolof, we will present the simpler, but partly comparable, case of French, where subject clitics also tend to behave like affixes, and subjecthood can move from the preverbal to the detached position. We will also make a several annotation recommendations that would avoid overwriting information regarding subjecthood.",https://aclanthology.org/2020.udw-1.5,Association for Computational Linguistics,2020,December,Proceedings of the Fourth Workshop on Universal Dependencies (UDW 2020),"Bond{\'e}elle, Olivier  and
Kahane, Sylvain",Subjecthood and annotation: The cases of French and Wolof,,udw,598
2021.argmining-1.20,"['Argument Mining', 'Classification Applications', 'Automatic Text Summarization']",,,"Key point analysis KPA has been proposed as a way to summarize arguments with shortsized pieces of text termed key points. This work aims at describing a solution for the Track 1 of the KPA 2021 shared task, analyzing different methodologies for the specific problem of key point matching, which consists in finding a reasonable mapping from arguments to key points. The analysis will focus on transformer based architectures, experimentally investigating the effectiveness of variants specifically tailored to the task.",https://aclanthology.org/2021.argmining-1.20,Association for Computational Linguistics,2021,November,Proceedings of the 8th Workshop on Argument Mining,"Cosenza, Emanuele",Key Point Matching with Transformers,10.18653/v1/2021.argmining-1.20,argmining,96
D19-1673,"['Data Management and Generation', 'Low-resource Languages']",['Data Preparation'],['Annotation Processes'],"Humor plays important role in human communication, which makes it important problem for natural language processing. Prior work on the analysis of humor focuses on whether text is humorous or not, or the degree of funniness, but this is insufficient to explain why it is funny. We therefore create a dataset on humor with 9,123 manually annotated jokes in Chinese. We propose a novel annotation scheme to give scenarios of how humor arises in text. Specifically, our annotations of linguistic humor not only contain the degree of funniness, like previous work, but they also contain key words that trigger humor as well as character relationship, scene, and humor categories. We report reasonable agreement between annotators. We also conduct an analysis and exploration of the dataset. To the best of our knowledge, we are the first to approach humor annotation for exploring the underlying mechanism of the use of humor, which may contribute to a significantly deeper analysis of humor. We also contribute with a scarce and valuable dataset, which we will release publicly.",https://aclanthology.org/D19-1673,Association for Computational Linguistics,2019,November,Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"Zhang, Dongyu  and
Zhang, Heting  and
Liu, Xikai  and
Lin, Hongfei  and
Xia, Feng",Telling the Whole Story: A Manually Annotated Chinese Dataset for the Analysis of Humor in Jokes,10.18653/v1/D19-1673,D19,294
Q17-1036,"['Learning Paradigms', 'Classification Applications', 'Model Architectures', 'Domain-specific NLP']","['Medical and Clinical NLP', 'Adversarial Learning', 'Unsupervised Learning', 'Sentiment Analysis (SA)', 'Supervised Learning', 'NLP for News and Media', 'Transfer Learning']",['NLP for Social Media'],"We introduce a neural method for transfer learning between two source and target classification tasks or aspects over the same domain. Rather than training on target labels, we use a few keywords pertaining to source and target aspects indicating sentence relevance instead of document class labels. Documents are encoded by learning to embed and softly select relevant sentences in an aspect-dependent manner. A shared classifier is trained on the source encoded documents and labels, and applied to target encoded documents. We ensure transfer through aspect-adversarial training so that encoded documents are, as sets, aspect-invariant. Experimental results demonstrate that our approach outperforms different baselines and model variants on two datasets, yielding an improvement of 27% on a pathology dataset and 5% on a review dataset. 1",https://aclanthology.org/Q17-1036,MIT Press,2017,,,"Zhang, Yuan  and
Barzilay, Regina  and
Jaakkola, Tommi",Aspect-augmented Adversarial Networks for Domain Adaptation,10.1162/tacl_a_00077,Q17,509
C16-1033,"['Parsing', 'Low-resource Languages']",['Morphological Parsing'],,"Parsing texts into universal dependencies UD in realistic scenarios requires infrastructure for morphological analysis and disambiguation MA&D of typologically different languages as a first tier. MA&D is particularly challenging in morphologically rich languages MRLs, where the ambiguous space-delimited tokens ought to be disambiguated with respect to their constituent morphemes. Here we present a novel, language-agnostic, framework for MA&D, based on a transition system with two variants, word-based and morpheme-based, and a dedicated transition to mitigate the biases of variable-length morpheme sequences. Our experiments on a Modern Hebrew case study outperform the state of the art, and we show that the morpheme-based MD consistently outperforms our word-based variant. We further illustrate the utility and multilingual coverage of our framework by morphologically analyzing and disambiguating the large set of languages in the UD treebanks.",https://aclanthology.org/C16-1033,The COLING 2016 Organizing Committee,2016,December,"Proceedings of {COLING} 2016, the 26th International Conference on Computational Linguistics: Technical Papers","More, Amir  and
Tsarfaty, Reut",Data-Driven Morphological Analysis and Disambiguation for Morphologically Rich Languages and Universal Dependencies,,C16,915
2020.readi-1.1,"['Data Management and Generation', 'Domain-specific NLP', 'Error Detection and Correction', 'Low-resource Languages']","['Data Preparation', 'Medical and Clinical NLP']",,"Spell checkers and other proofreading software are crucial tools for people with dyslexia and other reading disabilities. Most spell checkers automatically detect spelling mistakes by looking up individual words and seeing if they exist in the vocabulary. However, one of the biggest challenges of automatic spelling correction is how to deal with real-word errors, i.e. spelling mistakes which lead to a real but unintended word, such as when then is written in place of than. These errors account for 20% of all spelling mistakes made by people with dyslexia. As both words exist in the vocabulary, a simple dictionary lookup will not detect the mistake. The only way to disambiguate which word was actually intended is to look at the context in which the word appears. This problem is particularly apparent in languages with rich morphology where there is often minimal orthographic difference between grammatical items. In this paper, we present our novel confusion set corpus for Icelandic and discuss how it could be used for context-sensitive spelling correction. We have collected word pairs from seven different categories, chosen for their homophonous properties, along with sentence examples and frequency information from said pairs. We present a small-scale machine learning experiment using a decision tree binary classification which results range from 73% to 86% average accuracy with 10-fold cross validation. While not intended as a finalized result, the method shows potential and will be improved in future research.",https://aclanthology.org/2020.readi-1.1,European Language Resources Association,2020,May,Proceedings of the 1st Workshop on Tools and Resources to Empower People with REAding DIfficulties (READI),"Fri{\dh}riksd{\'o}ttir, Steinunn Rut  and
Ingason, Anton Karl",Disambiguating Confusion Sets as an Aid for Dyslexic Spelling,,readi,946
2020.finnlp-1.3,"['Evaluation Techniques', 'Domain-specific NLP', 'Data Management and Generation', 'Classification Applications']",['Data Preparation'],,"Regulators require most companies to publish yearly reports, describing their activities, results, future plans, and risk factors. Sometimes a risk factor can be omitted in a document, possiblyvoluntarily or not-misleading the readers. In this paper, we introduce a task for detecting omitted risk factors in Annual Reports. This new task requires to catch the risks mentions in multiple sentences, and to identify the ones that are specific to a sector or a period. To address it, we use a neural architecture to extract risk sentences from documents and cluster the risk factors from these sentences. Finally, we generate synthetic risk factor omissions and propose a metric to evaluate the omission detection method.",https://aclanthology.org/2020.finnlp-1.3,-,2020,05-Jan,Proceedings of the Second Workshop on Financial Technology and Natural Language Processing,"Masson, Corentin  and
Montariol, Syrielle",Detecting Omissions of Risk Factors in Company Annual Reports,,finnlp,524
W17-0407,"['Low-resource Languages', 'Finite State Machines']",,,"The problem of semi-automatic treebank conversion arises when converting between different schemas, such as from a language specific schema to Universal Dependencies, or when converting from one Universal Dependencies version to the next. We propose a formalism based on top-down tree transducers to convert dependency trees. Building on a well-defined mechanism yields a robust transformation system with clear semantics for rules and which guarantees that every transformation step results in a well formed tree, in contrast to previously proposed solutions. The rules only depend on the local context of the node to convert and rely on the dependency labels as well as the PoS tags. To exemplify the efficiency of our approach, we created a rule set based on only 45 manually transformed sentences from the Hamburg Dependency Treebank. These rules can already transform annotations with both coverage and precision of more than 90%.",https://aclanthology.org/W17-0407,Association for Computational Linguistics,2017,May,Proceedings of the {N}o{D}a{L}i{D}a 2017 Workshop on Universal Dependencies ({UDW} 2017),"Hennig, Felix  and
K{\""o}hn, Arne",Dependency Tree Transformation with Tree Transducers,,W17,1305
2020.nuse-1.2,"['Domain-specific NLP', 'Classification Applications']",['NLP for News and Media'],,"Deciding which scripts to turn into movies is a costly and time-consuming process for filmmakers. Thus, building a tool to aid script selection, an initial phase in movie production, can be very beneficial. Toward that goal, in this work, we present a method to evaluate the quality of a screenplay based on linguistic cues. We address this in a two-fold approach: 1 we define the task as predicting nominations of scripts at major film awards with the hypothesis that the peer-recognized scripts should have a greater chance to succeed. 2 based on industry opinions and narratology, we extract and integrate domain-specific features into common classification techniques. We face two challenges 1 scripts are much longer than other document datasets 2 nominated scripts are limited and thus difficult to collect. However, with narratology-inspired modeling and domain features, our approach offers clear improvements over strong baselines. Our work provides a new approach for future work in screenplay analysis.",https://aclanthology.org/2020.nuse-1.2,Association for Computational Linguistics,2020,July,"Proceedings of the First Joint Workshop on Narrative Understanding, Storylines, and Events","Chiu, Ming-Chang  and
Feng, Tiantian  and
Ren, Xiang  and
Narayanan, Shrikanth",Screenplay Quality Assessment: Can We Predict Who Gets Nominated?,10.18653/v1/2020.nuse-1.2,nuse,1314
2020.aacl-main.45,"['Learning Paradigms', 'Model Architectures', 'Machine Translation (MT)']",['Neural MT (NMT)'],,"Pairwise data automatically constructed from weakly supervised signals has been widely used for training deep learning models. Pairwise datasets such as parallel texts can have uneven quality levels overall, but usually contain data subsets that are more useful as learning examples. We present two methods to refine data that are aimed at obtaining that kind of subsets in a self-supervised way. Our methods are based on iteratively training dualencoder models to compute similarity scores. We evaluate our methods on de-noising parallel texts and training neural machine translation models. We find that: i The self-supervised refinement achieves most machine translation gains in the first iteration, but following iterations further improve its intrinsic evaluation. ii Machine translations can improve the de-noising performance when combined with selection steps. iii Our methods are able to reach the performance of a supervised method. Being entirely self-supervised, our methods are well-suited to handle pairwise data without the need of prior knowledge or human annotations.",https://aclanthology.org/2020.aacl-main.45,Association for Computational Linguistics,2020,December,Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing,"Hernandez Abrego, Gustavo  and
Liang, Bowen  and
Wang, Wei  and
Parekh, Zarana  and
Yang, Yinfei  and
Sung, Yunhsuan",Self-Supervised Learning for Pairwise Data Refinement,10.18653/v1/2020.aacl-main.45,aacl,617
2022.trustnlp-1.3,"['Ethics', 'Classification Applications', 'Model Architectures']",,,"In an effort to guarantee that machine learning model outputs conform with human moral values, recent work has begun exploring the possibility of explicitly training models to learn the difference between right and wrong. This is typically done in a bottom-up fashion, by exposing the model to different scenarios, annotated with human moral judgements. One question, however, is whether the trained models actually learn any consistent, higher-level ethical principles from these datasets -and if so, what? Here, we probe the Allen AI Delphi model with a set of standardized morality questionnaires, and find that, despite some inconsistencies, Delphi tends to mirror the moral principles associated with the demographic groups involved in the annotation process. We question whether this is desirable and discuss how we might move forward with this knowledge.",https://aclanthology.org/2022.trustnlp-1.3,Association for Computational Linguistics,2022,July,Proceedings of the 2nd Workshop on Trustworthy Natural Language Processing (TrustNLP 2022),"Fraser, Kathleen C.  and
Kiritchenko, Svetlana  and
Balkir, Esma",Does Moral Code have a Moral Code? Probing Delphi's Moral Philosophy,10.18653/v1/2022.trustnlp-1.3,trustnlp,865
2020.nlpcss-1.16,"['Ethics', 'Biases in NLP', 'Domain-specific NLP', 'Data Management and Generation', 'Learning Paradigms', 'Classification Applications']","['Bias Detection', 'Data Preparation', 'Multilabel Text Classification', 'Supervised Learning', 'NLP for News and Media']",,"Media organizations bear great reponsibility because of their considerable influence on shaping beliefs and positions of our society. Any form of media can contain overly biased content, e.g., by reporting on political events in a selective or incomplete manner. A relevant question hence is whether and how such form of imbalanced news coverage can be exposed. The research presented in this paper addresses not only the automatic detection of bias but goes one step further in that it explores how political bias and unfairness are manifested linguistically. In this regard we utilize a new corpus of 6964 news articles with labels derived from adfontesmedia.com and develop a neural model for bias assessment. By analyzing this model on article excerpts, we find insightful bias patterns at different levels of text granularity, from single words to the whole article discourse.",https://aclanthology.org/2020.nlpcss-1.16,Association for Computational Linguistics,2020,November,Proceedings of the Fourth Workshop on Natural Language Processing and Computational Social Science,"Chen, Wei-Fan  and
Al Khatib, Khalid  and
Wachsmuth, Henning  and
Stein, Benno",Analyzing Political Bias and Unfairness in News Articles at Different Levels of Granularity,10.18653/v1/2020.nlpcss-1.16,nlpcss,1114
W19-4828,"['Data Management and Generation', 'Model Architectures']","['Transformer Models', 'Data Analysis']",,"Large pre-trained neural networks such as BERT have had great recent success in NLP, motivating a growing body of research investigating what aspects of language they are able to learn from unlabeled data. Most recent analysis has focused on model outputs e.g., language model surprisal or internal vector representations e.g., probing classifiers. Complementary to these works, we propose methods for analyzing the attention mechanisms of pre-trained models and apply them to BERT. BERT's attention heads exhibit patterns such as attending to delimiter tokens, specific positional offsets, or broadly attending over the whole sentence, with heads in the same layer often exhibiting similar behaviors. We further show that certain attention heads correspond well to linguistic notions of syntax and coreference. For example, we find heads that attend to the direct objects of verbs, determiners of nouns, objects of prepositions, and coreferent mentions with remarkably high accuracy. Lastly, we propose an attention-based probing classifier and use it to further demonstrate that substantial syntactic information is captured in BERT's attention.",https://aclanthology.org/W19-4828,Association for Computational Linguistics,2019,August,Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP,"Clark, Kevin  and
Khandelwal, Urvashi  and
Levy, Omer  and
Manning, Christopher D.",What Does BERT Look at? An Analysis of BERT's Attention,10.18653/v1/W19-4828,W19,606
2020.signlang-1.12,"['Data Management and Generation', 'Image and Video Processing', 'Low-resource Languages']",['Sign Language and Fingerspelling Recognition'],,"In 2018 the DGS-Korpus project published the first full release of the Public DGS Corpus. This event marked a change of focus for the project. While before most attention had been on increasing the size of the corpus, now an increase in its depth became the priority. New data formats were added, corpus annotation conventions were released and OpenPose pose information was published for all transcripts. The community and research portal websites of the corpus also received upgrades, including persistent identifiers, archival copies of previous releases and improvements to their usability on mobile devices. The research portal was enhanced even further, improving its transcript web viewer, adding a KWIC concordance view, introducing cross-references to other linguistic resources of DGS and making its entire interface available in German in addition to English. This article provides an overview of these changes, chronicling the evolution of the Public DGS Corpus from its first release in 2018, through its second release in 2019 until its third release in 2020.",https://aclanthology.org/2020.signlang-1.12,European Language Resources Association (ELRA),2020,May,"Proceedings of the LREC2020 9th Workshop on the Representation and Processing of Sign Languages: Sign Language Resources in the Service of the Language Community, Technological Challenges and Application Perspectives","Hanke, Thomas  and
Schulder, Marc  and
Konrad, Reiner  and
Jahn, Elena",Extending the Public DGS Corpus in Size and Depth,,signlang,1488
2021.hcinlp-1.16,['Data Management and Generation'],['Data Analysis'],,"This paper presents a framework of opportunities and barriers/risks between the two research fields Natural Language Processing NLP and Human-Computer Interaction HCI. The framework is constructed by following an interdisciplinary research-model IDR, combining field-specific knowledge with existing work in the two fields. The resulting framework is intended as a departure point for discussion and inspiration for research collaborations.",https://aclanthology.org/2021.hcinlp-1.16,Association for Computational Linguistics,2021,April,Proceedings of the First Workshop on Bridging Human{--}Computer Interaction and Natural Language Processing,"Inie, Nanna  and
Derczynski, Leon",An IDR Framework of Opportunities and Barriers between HCI and NLP,,hcinlp,1494
2022.trustnlp-1.4,"['Ethics', 'Domain-specific NLP']",,,"Artificial Intelligence AI and Machine Learning ML are rapidly becoming musthave capabilities. According to a 2019 Forbes Insights Report, ""seventy-nine percent of executives agree that AI is already having a transformational impact on workflows and tools for knowledge workers, but only 5% of executives consider their companies to be industryleading in terms of taking advantage of AIpowered processes."" Forbes 2019 A major reason for this may be a shortage of on-staff expertise in AI/ML. This paper explores the intertwined issues of trust, adoption, training, and ethics of outsourcing AI development to a third party. We describe our experiences as a provider of outsourced natural language processing NLP. We discuss how trust and accountability co-evolve as solutions mature from proof-of-concept to production-ready.",https://aclanthology.org/2022.trustnlp-1.4,Association for Computational Linguistics,2022,July,Proceedings of the 2nd Workshop on Trustworthy Natural Language Processing (TrustNLP 2022),"Castelli, Maximilian  and
Moreau, Ph.D., Linda C.",The Cycle of Trust and Responsibility in Outsourced AI,10.18653/v1/2022.trustnlp-1.4,trustnlp,1444
W16-3410,"['Machine Translation (MT)', 'Evaluation Techniques', 'Low-resource Languages']","['Statistical MT (SMT)', 'Rule-based MT (RBMT)']",,"This work investigates the potential use of post-edited machine translation MT outputs as reference translations for automatic machine translation evaluation, focusing mainly on the following important question: Is it necessary to take into account the machine translation system and the source language from which the given post-edits are generated? In order to explore this, we investigated the use of post-edits originating from different machine translation systems two statistical systems and two rule-based systems, as well as the use of post-edits originating from two different source languages English and German. The obtained results shown that for comparison of different systems using automatic evaluation metrics, a good option is to use a post-edit originating from a high-quality possibly distinct system. A better option is to use it together with other references and post-edits, however post-edits originating from poor translation systems should be avoided. For tuning or development of a particular system, post-edited output of this same system seems to be the best reference translation.",https://aclanthology.org/W16-3410,,2016,,Proceedings of the 19th Annual Conference of the {E}uropean Association for Machine Translation,"Popovic, Maja  and
Ar{\v{c}}an, Mihael  and
Lommel, Arle",Potential and Limits of Using Post-edits as Reference Translations for MT Evaluation,,W16,118
L16-1487,"['Text Preprocessing', 'Data Management and Generation', 'Low-resource Languages', 'Domain-specific NLP']","['Text Segmentation', 'Data Preparation']",,"This paper describes the evaluation methodology followed to measure the impact of using a machine learning algorithm to automatically segment intralingual subtitles. The segmentation quality, productivity and self-reported post-editing effort achieved with such approach are shown to improve those obtained by the technique based in counting characters, mainly employed for automatic subtitle segmentation currently. The corpus used to train and test the proposed automated segmentation method is also described and shared with the community, in order to foster further research in this area.",https://aclanthology.org/L16-1487,European Language Resources Association (ELRA),2016,May,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),"{\'A}lvarez, Aitor  and
Balenciaga, Marina  and
del Pozo, Arantza  and
Arzelus, Haritz  and
Matamala, Anna  and
Mart{\'\i}nez-Hinarejos, Carlos-D.","Impact of Automatic Segmentation on the Quality, Productivity and Self-reported Post-editing Effort of Intralingual Subtitles",,L16,910
2022.starsem-1.12,"['Embeddings', 'Classification Applications']",['Sentence Embeddings'],,"There have been many successful applications of sentence embedding methods. However, it has not been well understood what properties are captured in the resulting sentence embeddings depending on the supervision signals. In this paper, we focus on two types of sentence embedding methods with similar architectures and tasks: one fine-tunes pre-trained language models on the natural language inference task, and the other fine-tunes pre-trained language models on word prediction task from its definition sentence, and investigate their properties. Specifically, we compare their performances on semantic textual similarity STS tasks using STS datasets partitioned from two perspectives: 1 sentence source and 2 superficial similarity of the sentence pairs, and compare their performances on the downstream and probing tasks. Furthermore, we attempt to combine the two methods and demonstrate that combining the two methods yields substantially better performance than the respective methods on unsupervised STS tasks and downstream tasks.",https://aclanthology.org/2022.starsem-1.12,Association for Computational Linguistics,2022,July,Proceedings of the 11th Joint Conference on Lexical and Computational Semantics,"Tsukagoshi, Hayato  and
Sasano, Ryohei  and
Takeda, Koichi",Comparison and Combination of Sentence Embeddings Derived from Different Supervision Signals,10.18653/v1/2022.starsem-1.12,starsem,1322
2020.alw-1.1,"['Ethics', 'Data Management and Generation']",['Data Analysis'],,"In 2020 The Workshop on Online Abuse and Harms WOAH held a satellite panel at RightsCons 2020, an international human rights conference. Our aim was to bridge the gap between human rights scholarship and Natural Language Processing NLP research communities in tackling online abuse. We report on the discussions that took place, and present an analysis of four key issues which emerged: Problems in tackling online abuse, Solutions, Meta concerns and the Ecosystem of content moderation and research. We argue there is a pressing need for NLP research communities to engage with human rights perspectives, and identify four key ways in which NLP research into online abuse could immediately be enhanced to create better and more ethical solutions.",https://aclanthology.org/2020.alw-1.1,Association for Computational Linguistics,2020,November,Proceedings of the Fourth Workshop on Online Abuse and Harms,"Prabhakaran, Vinodkumar  and
Waseem, Zeerak  and
Akiwowo, Seyi  and
Vidgen, Bertie",Online Abuse and Human Rights: WOAH Satellite Session at RightsCon 2020,10.18653/v1/2020.alw-1.1,alw,180
2022.ecnlp-1.26,"['Information Retrieval', 'Domain-specific NLP']",,,"Recently, semantic search has been successfully applied to e-commerce product search and the learned semantic spaces for query and product encoding are expected to generalize to unseen queries or products. Yet, whether generalization can conveniently emerge has not been thoroughly studied in the domain thus far. In this paper, we examine several general-domain and domain-specific pre-trained Roberta variants and discover that general-domain finetuning does not help generalization, which aligns with the discovery of prior art. Proper domain-specific fine-tuning with clickstream data can lead to better model generalization, based on a bucketed analysis of a publicly available manual annotated query-product pair data.",https://aclanthology.org/2022.ecnlp-1.26,Association for Computational Linguistics,2022,May,Proceedings of The Fifth Workshop on e-Commerce and NLP (ECNLP 5),"Liu, Zheng  and
Zhang, Wei  and
Chen, Yan  and
Sun, Weiyi  and
Du, Tianchuan  and
Schroeder, Benjamin",Towards Generalizeable Semantic Product Search by Text Similarity Pre-training on Search Click Logs,10.18653/v1/2022.ecnlp-1.26,ecnlp,128
2021.deelio-1.12,"['Biases in NLP', 'Domain-specific NLP', 'Data Management and Generation']",['Data Analysis'],,"Investigating brand perception is fundamental to marketing strategies. In this regard, brand image, defined by a set of attributes Aaker, 1997 , is recognized as a key element in indicating how a brand is perceived by various stakeholders such as consumers and competitors. Traditional approaches e.g., surveys to monitor brand perceptions are time-consuming and inefficient. In the era of digital marketing, both brand managers and consumers engage with a vast amount of digital marketing content. The exponential growth of digital content has propelled the emergence of pre-trained language models such as BERT and GPT as essential tools in solving myriads of challenges with textual data. This paper seeks to investigate the extent of brand perceptions i.e., brand and image attribute associations these language models encode. We believe that any kind of bias for a brand and attribute pair may influence customer-centric downstream tasks such as recommender systems, sentiment analysis, and question-answering, e.g., suggesting a specific brand consistently when queried for 'innovative' products. We use synthetic data and real-life data and report comparison results for five contextual LMs, viz. BERT, RoBERTa, DistilBERT, ALBERT and BART.",https://aclanthology.org/2021.deelio-1.12,Association for Computational Linguistics,2021,June,Proceedings of Deep Learning Inside Out (DeeLIO): The 2nd Workshop on Knowledge Extraction and Integration for Deep Learning Architectures,"Srivastava, Vivek  and
Pilli, Stephen  and
Bhat, Savita  and
Pedanekar, Niranjan  and
Karande, Shirish",What BERTs and GPTs know about your brand? Probing contextual language models for affect associations,10.18653/v1/2021.deelio-1.12,deelio,386
2020.nlpcss-1.19,"['Domain-specific NLP', 'Information Extraction', 'Embeddings', 'Language Change Analysis']","['Word Embeddings', 'NLP for News and Media', 'Named Entity Recognition (NER)']",,"Previous English-language diachronic change models based on word embeddings have typically used single tokens to represent entities, including names of people. This leads to issues with both ambiguity resulting in one embedding representing several distinct and unrelated people and unlinked references leading to several distinct embeddings which represent the same person. In this paper, we show that using named entity recognition and heuristic name linking steps before training a diachronic embedding model leads to more accurate representations of references to people, as compared to the token-only baseline. In large news corpus of articles from The Guardian, we provide examples of several types of analysis that can be performed using these new embeddings. Further, we show that real world events and context changes can be detected using our proposed model, with a focus on the examples of UK prime ministers and role changes in the football domain.",https://aclanthology.org/2020.nlpcss-1.19,Association for Computational Linguistics,2020,November,Proceedings of the Fourth Workshop on Natural Language Processing and Computational Social Science,"Hennig, Felix  and
Wilson, Steven",Diachronic Embeddings for People in the News,10.18653/v1/2020.nlpcss-1.19,nlpcss,574
2022.in2writing-1.7,"['Text Generation', 'Learning Paradigms', 'Model Architectures']","['Transformer Models', 'Transfer Learning', 'Text Simplification']",,"Text revision refers to a family of natural language generation tasks, where the source and target sequences share moderate resemblance in surface form but differentiate in attributes, such as text style transfer ",https://aclanthology.org/2022.in2writing-1.7,Association for Computational Linguistics,2022,May,Proceedings of the First Workshop on Intelligent and Interactive Writing Assistants (In2Writing 2022),"Li, Jingjing  and
Li, Zichao  and
Ge, Tao  and
King, Irwin  and
Lyu, Michael",Text Revision by On-the-Fly Representation Optimization,10.18653/v1/2022.in2writing-1.7,in2writing,1445
2020.splu-1.4,"['Question Answering (QA)', 'Learning Paradigms', 'Classification Applications', 'Model Architectures']","['Transformer Models', 'Multimodal Learning', 'Visual QA (VQA)', 'Multilabel Text Classification']",,"In this paper, we study the grounding skills required to answer spatial questions asked by humans while playing the GuessWhat?! game. We propose a classification for spatial questions dividing them into absolute, relational, and group questions. We build a new answerer model based on the LXMERT multimodal transformer and we compare a baseline with and without visual features of the scene. We are interested in studying how the attention mechanisms of LXMERT are used to answer spatial questions since they require putting attention on more than one region simultaneously and spotting the relation holding among them. We show that our proposed model outperforms the baseline by a large extent 9.70% on spatial questions and 6.27% overall. By analyzing LXMERT errors and its attention mechanisms, we find that our classification helps to gain a better understanding of the skills required to answer different spatial questions.",https://aclanthology.org/2020.splu-1.4,Association for Computational Linguistics,2020,November,Proceedings of the Third International Workshop on Spatial Language Understanding,"Testoni, Alberto  and
Greco, Claudio  and
Bianchi, Tobias  and
Mazuecos, Mauricio  and
Marcante, Agata  and
Benotti, Luciana  and
Bernardi, Raffaella",They Are Not All Alike: Answering Different Spatial Questions Requires Different Grounding Strategies,10.18653/v1/2020.splu-1.4,splu,1475
L16-1272,"['Domain-specific NLP', 'Data Management and Generation']","['Data Preparation', 'Medical and Clinical NLP']",['Annotation Processes'],"In this paper we present the Corpus of REcommendation STrength CREST, a collection of HTML-formatted clinical guidelines annotated with the location of recommendations. Recommendations are labelled with an author-provided indicator of their strength of importance. As data was drawn from many disparate authors, we define a unified scheme of importance labels, and provide a mapping for each guideline. We demonstrate the utility of the corpus and its annotations in some initial measurements investigating the type of language constructions associated with strong and weak recommendations, and experiments into promising features for recommendation classification, both with respect to strong and weak labels, and to all labels of the unified scheme. An error analysis indicates that, while there is a strong relationship between lexical choices and strength labels, there can be substantial variance in the choices made by different authors.",https://aclanthology.org/L16-1272,European Language Resources Association (ELRA),2016,May,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),"Read, Jonathon  and
Velldal, Erik  and
Cavazza, Marc  and
Georg, Gersende",A Corpus of Clinical Practice Guidelines Annotated with the Importance of Recommendations,,L16,867
2021.americasnlp-1.3,['Low-resource Languages'],,,"We describe experiments with character-based language modeling for written variants of Nahuatl. Using a standard LSTM model and publicly available Bible translations, we explore how character language models can be applied to the tasks of estimating mutual intelligibility, identifying genetic similarity, and distinguishing written variants. We demonstrate that these simple language models are able to capture similarities and differences that have been described in the linguistic literature. 1",https://aclanthology.org/2021.americasnlp-1.3,Association for Computational Linguistics,2021,June,Proceedings of the First Workshop on Natural Language Processing for Indigenous Languages of the Americas,"Pugh, Robert  and
Tyers, Francis",Investigating variation in written forms of Nahuatl using character-based language models,10.18653/v1/2021.americasnlp-1.3,americasnlp,807
N18-3004,"['Embeddings', 'Dialogue Systems', 'Model Architectures']","['Chatbots', 'Response Generation']",,"In task-oriented dialog, agents need to generate both fluent natural language responses and correct external actions like database queries and updates. We show that methods that achieve state of the art performance on synthetic datasets, perform poorly in real world dialog tasks. We propose a hybrid model, where nearest neighbor is used to generate fluent responses and Sequence-to-Sequence Seq2Seq type models ensure dialogue coherency and generate accurate external actions. The hybrid model on an internal customer support dataset achieves a 78% relative improvement in fluency, and a 200% improvement in external call accuracy.",https://aclanthology.org/N18-3004,Association for Computational Linguistics,2018,June,"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 3 (Industry Papers)","Gangadharaiah, Rashmi  and
Narayanaswamy, Balakrishnan  and
Elkan, Charles",What we need to learn if we want to do and not just talk,10.18653/v1/N18-3004,N18,136
2020.ijclclp-1.1,"['Data Management and Generation', 'Error Detection and Correction', 'Low-resource Languages', 'Learning Paradigms', 'Model Architectures', 'Machine Translation (MT)']","['Recurrent Neural Networks (RNNs)', 'Supervised Learning', 'Data Augmentation', 'Neural MT (NMT)', 'Data Preparation']",,"We present a method for Chinese spelling check that automatically learns to correct a sentence with potential spelling errors. In our approach, a character-based neural machine translation NMT model is trained to translate the potentially misspelled sentence into correct one, using right-and-wrong sentence pairs from newspaper edit logs and artificially generated data. The method involves extracting sentences contain edit of spelling correction from edit logs, using commonly confused right-and-wrong word pairs to generate artificial right-and-wrong sentence pairs in order to expand our training data , and training the NMT model. The evaluation on the United Daily News UDN Edit Logs and SIGHAN-7 Shared Task shows that adding artificial error data can significantly improve the performance of Chinese spelling check system.",https://aclanthology.org/2020.ijclclp-1.1,Association for Computational Linguistics and Chinese Language Processing,2020,June,"International Journal of Computational Linguistics {\&} {C}hinese Language Processing, Volume 25, Number 1, June 2020","Chen, Jhih-Jie  and
Tu, Hai-Lun  and
Yang, Ching-Yu  and
Li, Chiao-Wen  and
Chang, Jason S.",Chinese Spelling Check based on Neural Machine Translation,,ijclclp,105
C16-1216,"['Embeddings', 'Dialogue Systems', 'Data Management and Generation', 'Question Answering (QA)', 'Model Architectures']","['Data Preparation', 'Recurrent Neural Networks (RNNs)']",,"Recently, end-to-end memory networks have shown promising results on Question Answering task, which encode the past facts into an explicit memory and perform reasoning ability by making multiple computational steps on the memory. However, memory networks conduct the reasoning on sentence-level memory to output coarse semantic vectors and do not further take any attention mechanism to focus on words, which may lead to the model lose some detail information, especially when the answers are rare or unknown words. In this paper, we propose a novel Hierarchical Memory Networks, dubbed HMN. First, we encode the past facts into sentencelevel memory and word-level memory respectively. Then, k-max pooling is exploited following reasoning module on the sentence-level memory to sample the k most relevant sentences to a question and feed these sentences into attention mechanism on the word-level memory to focus the words in the selected sentences. Finally, the prediction is jointly learned over the outputs of the sentence-level reasoning module and the word-level attention mechanism. The experimental results demonstrate that our approach successfully conducts answer selection on unknown words and achieves a better performance than memory networks.",https://aclanthology.org/C16-1216,The COLING 2016 Organizing Committee,2016,December,"Proceedings of {COLING} 2016, the 26th International Conference on Computational Linguistics: Technical Papers","Xu, Jiaming  and
Shi, Jing  and
Yao, Yiqun  and
Zheng, Suncong  and
Xu, Bo  and
Xu, Bo",Hierarchical Memory Networks for Answer Selection on Unknown Words,10.48550/arxiv.1609.08843,C16,1208
D19-1119,"['Data Management and Generation', 'Error Detection and Correction']","['Data Augmentation', 'Grammatical Error Correction (GEC)']",,"The incorporation of pseudo data in the training of grammatical error correction models has been one of the main factors in improving the performance of such models. However, consensus is lacking on experimental configurations, namely, choosing how the pseudo data should be generated or used. In this study, these choices are investigated through extensive experiments, and state-of-the-art performance is achieved on the CoNLL-2014 test set F 0.5 = 65.0 and the official test set of the BEA-2019 shared task F 0.5 = 70.2 without making any modifications to the model architecture.",https://aclanthology.org/D19-1119,Association for Computational Linguistics,2019,November,Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"Kiyono, Shun  and
Suzuki, Jun  and
Mita, Masato  and
Mizumoto, Tomoya  and
Inui, Kentaro",An Empirical Study of Incorporating Pseudo Data into Grammatical Error Correction,10.18653/v1/D19-1119,D19,1296
W17-4713,"['Machine Translation (MT)', 'Learning Paradigms', 'Low-resource Languages']","['Unsupervised Learning', 'Neural MT (NMT)']",,"We investigate the application of Neural Machine Translation NMT under the following three conditions posed by realworld application scenarios. First, we operate with an input stream of sentences coming from many different domains and with no predefined order. Second, the sentences are presented without domain information. Third, the input stream should be processed by a single generic NMT model. To tackle the weaknesses of current NMT technology in this unsupervised multi-domain setting, we explore an efficient instance-based adaptation method that, by exploiting the similarity between the training instances and each test sentence, dynamically sets the hyperparameters of the learning algorithm and updates the generic model on-the-fly. The results of our experiments with multi-domain data show that local adaptation outperforms not only the original generic NMT system, but also a strong phrase-based system and even single-domain NMT models specifically optimized on each domain and applicable only by violating two of our aforementioned assumptions.",https://aclanthology.org/W17-4713,Association for Computational Linguistics,2017,September,Proceedings of the Second Conference on Machine Translation,"Farajian, M. Amin  and
Turchi, Marco  and
Negri, Matteo  and
Federico, Marcello",Multi-Domain Neural Machine Translation through Unsupervised Adaptation,10.18653/v1/W17-4713,W17,1228
2020.paclic-1.37,"['Data Management and Generation', 'Low-resource Languages', 'Domain-specific NLP']",['Data Preparation'],,"This paper describes the construction of an English-Chinese Parallel Corpus of wine reviews and elaborates on one of its applications -i.e. an E-C bilingual oenology term bank of wine tasting terms. The corpus is sourced from Decanter China, containing 1211 aligned wine reviews in both English and Chinese with 149,463 Chinese characters and 66,909 English words. It serves as a dataset for investigating crosslingual and cross-cultural differences in describing the sensory properties of wines. Our log-likelihood tests revealed good candidates for the Chinese translations of the English words in wine reviews. One of the most challenging features of this domain-specific bilingual term bank is the dominant many-to-many nature of term mapping. We focused on the one-to-many English-Chinese mapping relations of two major types: a the words without a single precise translation e.g. ""palate"" and b the words that are underspecified and involve 'place-holder' translation e.g. ""aroma"". Our study differs from previous bilingual CompuTerm studies by focusing on an area where cultural and sensory experiences favour many-to-many mappings instead of the default one-to-one mapping preferred in scientific and jurisprudential areas. This necessity for many-to-many mappings in turn challenges the basic design feature of many state-of-the-art automatic bilingual term-extraction approaches.",https://aclanthology.org/2020.paclic-1.37,Association for Computational Linguistics,2020,October,"Proceedings of the 34th Pacific Asia Conference on Language, Information and Computation","Wang, Vincent Xian  and
Chen, Xi  and
Quan, Songnan  and
Huang, Chu-Ren",A Parallel Corpus-driven Approach to Bilingual Oenology Term Banks: How Culture Differences Influence Wine Tasting Terms,,paclic,1151
2021.acl-long.426,"['Learning Paradigms', 'Adversarial Attacks and Robustness']",['Adversarial Learning'],,"Although deep neural networks have achieved prominent performance on many NLP tasks, they are vulnerable to adversarial examples. We propose Dirichlet Neighborhood Ensemble DNE, a randomized method for training a robust model to defense synonym substitutionbased attacks. During training, DNE forms virtual sentences by sampling embedding vectors for each word in an input sentence from a convex hull spanned by the word and its synonyms, and it augments them with the training data. In such a way, the model is robust to adversarial attacks while maintaining the performance on the original clean data. DNE is agnostic to the network architectures and scales to large models e.g., BERT for NLP applications. Through extensive experimentation, we demonstrate that our method consistently outperforms recently proposed defense methods by a significant margin across different network architectures and multiple data sets.",https://aclanthology.org/2021.acl-long.426,Association for Computational Linguistics,2021,August,Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),"Zhou, Yi  and
Zheng, Xiaoqing  and
Hsieh, Cho-Jui  and
Chang, Kai-Wei  and
Huang, Xuanjing",Defense against Synonym Substitution-based Adversarial Attacks via Dirichlet Neighborhood Ensemble,10.18653/v1/2021.acl-long.426,acl,74
2021.bsnlp-1.1,"['Text Preprocessing', 'Question Answering (QA)', 'Classification Applications', 'Learning Paradigms', 'Low-resource Languages', 'Cross-lingual Application', 'Model Architectures']","['Transformer Models', 'Transfer Learning', 'Part-of-Speech (POS) Tagging', 'Sentiment Analysis (SA)']",,"BERT-based models are currently used for solving nearly all Natural Language Processing NLP tasks and most often achieve stateof-the-art results. Therefore, the NLP community conducts extensive research on understanding these models, but above all on designing effective and efficient training procedures. Several ablation studies investigating how to train BERT-like models have been carried out, but the vast majority of them concerned only the English language. A training procedure designed for English does not have to be universal and applicable to other especially typologically different languages. Therefore, this paper presents the first ablation study focused on Polish, which, unlike the isolating English language, is a fusional language. We design and thoroughly evaluate a pretraining procedure of transferring knowledge from multilingual to monolingual BERT-based models. In addition to multilingual model initialization, other factors that possibly influence pretraining are also explored, i.e. training objective, corpus size, BPE-Dropout, and pretraining length. Based on the proposed procedure, a Polish BERTbased language model -HerBERT -is trained. This model achieves state-of-the-art results on multiple downstream tasks.",https://aclanthology.org/2021.bsnlp-1.1,Association for Computational Linguistics,2021,April,Proceedings of the 8th Workshop on Balto-Slavic Natural Language Processing,"Mroczkowski, Robert  and
Rybak, Piotr  and
Wr{\'o}blewska, Alina  and
Gawlik, Ireneusz",HerBERT: Efficiently Pretrained Transformer-based Language Model for Polish,10.48550/arxiv.2105.01735,bsnlp,795
2020.computerm-1.14,"['Information Extraction', 'Model Architectures']",,,"This paper describes RACAI's automatic term extraction system, which participated in the TermEval 2020 shared task on English monolingual term extraction. We discuss the system architecture, some of the challenges that we faced as well as present our results in the English competition.",https://aclanthology.org/2020.computerm-1.14,European Language Resources Association,2020,May,Proceedings of the 6th International Workshop on Computational Terminology,"Pais, Vasile  and
Ion, Radu",TermEval 2020: RACAI's automatic term extraction system,,computerm,484
N19-1106,['Classification Applications'],['Multilabel Text Classification'],,"Extreme classification is a classification task on an extremely large number of labels tags. User generated labels for any type of online data can be sparing per individual user but intractably large among all users. It would be useful to automatically select a smaller, standard set of labels to represent the whole label set. We can then solve efficiently the problem of multi-label learning with an intractably large number of interdependent labels, such as automatic tagging of Wikipedia pages. We propose a submodular maximization framework with linear cost to find informative labels which are most relevant to other labels yet least redundant with each other. A simple prediction model can then be trained on this label subset. Our framework includes both labellabel and label-feature dependencies, which aims to find the labels with the most representation and prediction ability. In addition, to avoid information loss, we extract and predict outlier labels with weak dependency on other labels. We apply our model to four standard natural language data sets including Bibsonomy entries with users assigned tags, web pages with user assigned tags, legal texts with EUROVOC descriptorsA topic hierarchy with almost 4000 categories regarding different aspects of European law and Wikipedia pages with tags from social bookmarking as well as news videos for automated label detection from a lexicon of semantic concepts. Experimental results show that our proposed approach improves label prediction quality, in terms of precision and nDCG, by 3% to 5% in three of the 5 tasks and is competitive in the others, even with a simple linear prediction model. An ablation study shows how different data sets benefit from different aspects of our model, with all aspects contributing substantially to at least one data set.",https://aclanthology.org/N19-1106,Association for Computational Linguistics,2019,June,"Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)","Barezi, Elham J.  and
Wood, Ian D.  and
Fung, Pascale  and
Rabiee, Hamid R.",A Submodular Feature-Aware Framework for Label Subset Selection in Extreme Classification Problems,10.18653/v1/N19-1106,N19,115
2020.lincr-1.2,"['Domain-specific NLP', 'Low-resource Languages']",['Medical and Clinical NLP'],,"Texts comprise a large part of visual information that we process every day, so one of the tasks of language science is to make them more accessible. However, often the text design process is focused on the font size, but not on its type; which might be crucial especially for the people with reading disabilities. The current paper represents a study on text accessibility and the first attempt to create a researchbased accessible font for Cyrillic letters. This resulted in the dyslexic-specific font, LexiaD. Its design rests on the reduction of interletter similarity of the Russian alphabet. In evaluation stage, dyslexic and non-dyslexic children were asked to read sentences from the Children version of the Russian Sentence Corpus. We tested the readability of LexiaD compared to PT Sans and PT Serif fonts. The results showed that all children had some advantage in letter feature extraction and information integration while reading in LexiaD, but lexical access was improved when sentences were rendered in PT Sans or PT Serif. Therefore, in several aspects, LexiaD proved to be faster to read and could be recommended to use by dyslexics who have visual deficiency or those who struggle with text understanding resulting in re-reading.",https://aclanthology.org/2020.lincr-1.2,European Language Resources Association,2020,May,Proceedings of the Second Workshop on Linguistic and Neurocognitive Resources,"Alexeeva, Svetlana  and
Dobrego, Aleksandra  and
Zubov, Vladislav",Towards the First Dyslexic Font in Russian,,lincr,1407
2021.acl-long.478,"['Question Answering (QA)', 'Learning Paradigms', 'Text Generation']","['Supervised Learning', 'Paraphrase and Rephrase Generation']",,"One of the main challenges in conversational question answering CQA is to resolve the conversational dependency, such as anaphora and ellipsis. However, existing approaches do not explicitly train QA models on how to resolve the dependency, and thus these models are limited in understanding human dialogues. In this paper, we propose a novel framework, EXCORD Explicit guidance on how to resolve Conversational Dependency to enhance the abilities of QA models in comprehending conversational context. EXCORD first generates self-contained questions that can be understood without the conversation history, then trains a QA model with the pairs of original and self-contained questions using a consistency-based regularizer. In our experiments, we demonstrate that EXCORD significantly improves the QA models' performance by up to 1.2 F1 on QuAC Choi et al., 2018,  and 5.2 F1 on CANARD Elgohary et al.,  2019, while addressing the limitations of the existing approaches. 1",https://aclanthology.org/2021.acl-long.478,Association for Computational Linguistics,2021,August,Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),"Kim, Gangwoo  and
Kim, Hyunjae  and
Park, Jungsoo  and
Kang, Jaewoo",Learn to Resolve Conversational Dependency: A Consistency Training Framework for Conversational Question Answering,10.18653/v1/2021.acl-long.478,acl,369
2020.nli-1.5,"['Text Preprocessing', 'Data Management and Generation', 'Model Architectures', 'Text Generation']","['Data Preparation', 'Transformer Models']",,"Text normalization and sanitization are intrinsic components of Natural Language Inferences. In Information Retrieval or Dialogue Generation, normalization of user queries or utterances enhances linguistic understanding by translating non-canonical text to its canonical form, on which many state-of-the-art language models are trained. On the other hand, text sanitization removes sensitive information to guarantee user privacy and anonymity. Existing approaches to normalization and sanitization mainly rely on hand-crafted heuristics and syntactic features of individual tokens while disregarding the linguistic context. Moreover, such context-unaware solutions cannot dynamically determine whether out-of-vocab tokens are misspelt or are entity names. In this work, we formulate text normalization and sanitization as a multi-task text generation approach and propose a neural pointer-generator network based on multihead attention. Its generator effectively captures linguistic context during normalization and sanitization while its pointer dynamically preserves the entities that are generally missing in the vocabulary. Experiments show that our generation approach outperforms both token-based text normalization and sanitization, while the pointer-generator improves the generator-only baseline in terms of BLEU4 score, and classical attentional pointer networks in terms of pointing accuracy.",https://aclanthology.org/2020.nli-1.5,Association for Computational Linguistics,2020,July,Proceedings of the First Workshop on Natural Language Interfaces,"Nguyen, Hoang  and
Cavallari, Sandro",Neural Multi-task Text Normalization and Sanitization with Pointer-Generator,10.18653/v1/2020.nli-1.5,nli,958
2022.woah-1.22,"['Ethics', 'Domain-specific NLP', 'Classification Applications', 'Model Architectures']","['Transformer Models', 'Multilabel Text Classification', 'NLP for News and Media']",['NLP for Social Media'],"The past decade has seen an abundance of work seeking to detect, characterize, and measure online hate speech. A related, but less studied problem, is the specification of identity groups targeted by that hate speech. Predictive accuracy on this task can supplement additional analyses beyond hate speech detection, motivating its study. Using the Measuring Hate Speech corpus, which provided annotations for targeted identity groups on roughly 50,000 social media comments, we create neural network models to perform multi-label binary prediction of identity groups targeted by a social media comment. Specifically, we study 8 broad identity groups and 12 identity sub-groups within race and gender identity. We find that these networks exhibited good predictive performance, achieving ROC AUCs of greater than 0.9 and PR AUCs of greater than 0.7 on several identity groups. At the same time, we find performance suffered on identity groups less represented in the dataset. We validate model performance on the HateCheck and Gab Hate Corpora, finding that predictive performance generalizes in most settings. We additionally examine the performance of the model on comments targeting multiple identity groups. Lastly, we discuss issues with a standardized conceptualization of a ""target"" in hate speech corpora, and its relation to intersectionality. Our results demonstrate the feasibility of simultaneously detecting a broad range of targeted groups in social media comments, and offer suggestions for future work on modeling and dataset annotation for this task.",https://aclanthology.org/2022.woah-1.22,Association for Computational Linguistics,2022,July,Proceedings of the Sixth Workshop on Online Abuse and Harms (WOAH),"Sachdeva, Pratik  and
Barreto, Renata  and
Von Vacano, Claudia  and
Kennedy, Chris",Targeted Identity Group Prediction in Hate Speech Corpora,10.18653/v1/2022.woah-1.22,woah,578
D18-1102,['Model Architectures'],,,Decipherment of homophonic substitution ciphers using language models LMs is a wellstudied task in NLP. Previous work in this topic scores short local spans of possible plaintext decipherments using n-gram LMs. The most widely used technique is the use of beam search with n-gram LMs proposed by Nuhn et al. 2013 . We propose a beam search algorithm that scores the entire candidate plaintext at each step of the decipherment using a neural LM. We augment beam search with a novel rest cost estimation that exploits the prediction power of a neural LM. We compare against the state of the art n-gram based methods on many different decipherment tasks. On challenging ciphers such as the Beale cipher we provide significantly better error rates with much smaller beam sizes.,https://aclanthology.org/D18-1102,Association for Computational Linguistics,2018,October-November,Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing,"Kambhatla, Nishant  and
Mansouri Bigvand, Anahita  and
Sarkar, Anoop",Decipherment of Substitution Ciphers with Neural Language Models,10.18653/v1/D18-1102,D18,878
2021.bionlp-1.12,"['Domain-specific NLP', 'Error Detection and Correction', 'Automatic Text Summarization', 'Knowledge Representation and Reasoning']","['Abstractive Text Summarization', 'Medical and Clinical NLP']",,"Medical question summarization is an important but difficult task, where the input is often complex and erroneous while annotated data is expensive to acquire. We report our participation in the MEDIQA 2021 question summarization task in which we are required to address these challenges. We start from pre-trained conditional generative language models, use knowledge bases to help correct input errors, and rerank single system outputs to boost coverage. Experimental results show significant improvement in stringbased metrics.",https://aclanthology.org/2021.bionlp-1.12,Association for Computational Linguistics,2021,June,Proceedings of the 20th Workshop on Biomedical Language Processing,"He, Yifan  and
Chen, Mosha  and
Huang, Songfang",damo\_nlp at MEDIQA 2021: Knowledge-based Preprocessing and Coverage-oriented Reranking for Medical Question Summarization,10.18653/v1/2021.bionlp-1.12,bionlp,682
2020.louhi-1.3,"['Embeddings', 'Domain-specific NLP', 'Classification Applications']","['Medical and Clinical NLP', 'Word Embeddings', 'NLP for News and Media']",['NLP for Social Media'],"Medical concept normalization helps in discovering standard concepts in free-form text i.e., maps health-related mentions to standard concepts in a clinical knowledge base. It is much beyond simple string matching and requires a deep semantic understanding of concept mentions. Recent research approach concept normalization as either text classification or text similarity. The main drawback in existing a text classification approach is ignoring valuable target concepts information in learning input concept mention representation b text similarity approach is the need to separately generate target concept embeddings which is time and resource consuming. Our proposed model overcomes these drawbacks by jointly learning the representations of input concept mention and target concepts. First, we learn input concept mention representation using RoBERTa. Second, we find cosine similarity between embeddings of input concept mention and all the target concepts. Here, embeddings of target concepts are randomly initialized and then updated during training. Finally, the target concept with maximum cosine similarity is assigned to the input concept mention. Our model surpasses all the existing methods across three standard datasets by improving accuracy up to 2.31%.",https://aclanthology.org/2020.louhi-1.3,Association for Computational Linguistics,2020,November,Proceedings of the 11th International Workshop on Health Text Mining and Information Analysis,"Kalyan, Katikapalli Subramanyam  and
Sangeetha, Sivanesan",Medical Concept Normalization in User-Generated Texts by Learning Target Concept Embeddings,10.18653/v1/2020.louhi-1.3,louhi,63
2020.finnlp-1.7,"['Domain-specific NLP', 'Data Management and Generation', 'Classification Applications']","['NLP for Finance', 'Data Analysis', 'Sentiment Analysis (SA)']",,"For some time, there has been significant disagreement as to whether financial measures that do not conform to the Generally Accepted Accounting Principles GAAP should be used in communication to stakeholders, as research points to these measures being used both altruistically and opportunistically. In this paper, we present a novel approach of using Sentiment Analysis to measure the impact that non-GAAP measures have on financial communication. We use an extractive approach in conjunction with the sentiment of four well known and robustly established dictionaries: General Inquirer, QDAP, Henry and Loughran-McDonald. We find that the sentiment declines once the non-GAAP measures are extracted with a statistical significance at the p = 0.01 level. We believe that this enhances NLP-based investment management and also has important implications for Know Your Customer KYC and text-based market provisioning.",https://aclanthology.org/2020.finnlp-1.7,-,2020,05-Jan,Proceedings of the Second Workshop on Financial Technology and Natural Language Processing,"Taylor, Stacey  and
Keselj, Vlado",Using Extractive Lexicon-based Sentiment Analysis to Enhance Understanding ofthe Impact of Non-GAAP Measures in Financial Reporting,,finnlp,800
L18-1494,"['Audio Generation and Processing', 'Model Architectures']",['Recurrent Neural Networks (RNNs)'],['Long Short-Term Memory (LSTM) Models'],"Long Short-Term Memory LSTM and its variants have been the standard solution to sequential data processing tasks because of their ability to preserve previous information weighted on distance. This feature provides the LSTM family with additional information in predictions, compared to regular Recurrent Neural Networks RNNs and Bag-of-Words BOW models. In other words, LSTM networks assume the data to be chain-structured. The longer the distance between two data points, the less related the data points are. However, this is usually not the case for real multimedia signals including text, sound and music. In real data, this chain-structured dependency exists only across meaningful groups of data units but not over single units directly. For example, in a prediction task over sound signals, a meaningful word could give a strong hint to its following word as a whole but not the first phoneme of that word. This undermines the ability of LSTM networks in modeling multimedia data, which is pattern-rich. In this paper we take advantage of Seq2Tree network, a dynamically extensible tree-structured neural network architecture which helps solve the problem LSTM networks face in sound signal processing tasks-the unbalanced connections among data units inside and outside semantic groups. Experiments show that Seq2Tree network outperforms the state-of-the-art Bidirectional LSTM BLSTM model on a signal and noise separation task CHiME Speech Separation and Recognition Challenge.",https://aclanthology.org/L18-1494,European Language Resources Association (ELRA),2018,May,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),"Ma, Weicheng  and
Cao, Kai  and
Ni, Zhaoheng  and
Chin, Peter  and
Li, Xiang",Sound Signal Processing with Seq2Tree Network,,L18,407
2018.ijclclp-2.4,"['Evaluation Techniques', 'Data Management and Generation', 'Question Answering (QA)', 'Low-resource Languages']",['Data Preparation'],,"This paper proposes a new n-gram matching approach for retrieving the supporting evidence, which is a question related text passage in the given document, for answering Yes/No questions. It locates the desired passage according to the question text with an efficient and simple n-gram matching algorithm. In comparison with those previous approaches, this model is more efficient and easy to implement. The proposed approach was tested on a task of answering Yes/No questions of Taiwan elementary school Social Studies lessons. Experimental results showed that the performance of our proposed approach is 5% higher than the well-known Apache Lucene search engine.",https://aclanthology.org/2018.ijclclp-2.4,Association for Computational Linguistics and Chinese Language Processing,2018,December,"International Journal of Computational Linguistics {\&} {C}hinese Language Processing, Volume 23, Number 2, December 2018","Wu, Meng-Tse  and
Lin, Yi-Chung  and
Su, Keh-Yih",Supporting Evidence Retrieval for Answering Yes/No Questions,,ijclclp,1021
2022.starsem-1.1,"['Prompt Engineering', 'Text Generation', 'Evaluation Techniques', 'Learning Paradigms', 'Model Architectures']",['Large Language Models (LLMs)'],,"Script Knowledge Schank and Abelson, 1975 has long been recognized as crucial for language understanding as it can help in filling in unstated information in a narrative. However, such knowledge is expensive to produce manually and difficult to induce from text due to reporting bias Gordon and Van Durme, 2013 . In this work, we are interested in the scientific question of whether explicit script knowledge is present and accessible through pre-trained generative language models LMs. To this end, we introduce the task of generating full event sequence descriptions ESDs given a scenario as a natural language prompt. Through zero-shot probing, we find that generative LMs produce poor ESDs with mostly omitted, irrelevant, repeated or misordered events. To address this, we propose a pipeline-based script induction framework SIF which can generate good quality ESDs for unseen scenarios e.g., bake a cake. SIF is a two-staged framework that fine-tunes LM on a small set of ESD examples in the first stage. In the second stage, ESD generated for an unseen scenario is post-processed using RoBERTa-based models to filter irrelevant events, remove repetitions, and reorder the temporally misordered events. Through automatic and manual evaluations, we demonstrate that SIF yields substantial improvements 1-3 BLEU points over a fine-tuned LM. However, manual analysis shows that there is great room for improvement, offering a new research direction for inducing script knowledge 1 .",https://aclanthology.org/2022.starsem-1.1,Association for Computational Linguistics,2022,July,Proceedings of the 11th Joint Conference on Lexical and Computational Semantics,"Sancheti, Abhilasha  and
Rudinger, Rachel",What do Large Language Models Learn about Scripts?,10.18653/v1/2022.starsem-1.1,starsem,528
N19-1264,"['Question Answering (QA)', 'Learning Paradigms', 'Automatic Text Summarization', 'Model Architectures']","['Extractive Text Summarization', 'Reinforcement Learning', 'Recurrent Neural Networks (RNNs)']",['Long Short-Term Memory (LSTM) Models'],"Highlighting while reading is a natural behavior for people to track salient content of a document. It would be desirable to teach an extractive summarizer to do the same. However, a major obstacle to the development of a supervised summarizer is the lack of ground-truth. Manual annotation of extraction units is costprohibitive, whereas acquiring labels by automatically aligning human abstracts and source documents can yield inferior results. In this paper we describe a novel framework to guide a supervised, extractive summarization system with question-answering rewards. We argue that quality summaries should serve as a document surrogate to answer important questions, and such question-answer pairs can be conveniently obtained from human abstracts. The system learns to promote summaries that are informative, fluent, and perform competitively on question-answering. Our results compare favorably with those reported by strong summarization baselines as evaluated by automatic metrics and human assessors.",https://aclanthology.org/N19-1264,Association for Computational Linguistics,2019,June,"Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)","Arumae, Kristjan  and
Liu, Fei",Guiding Extractive Summarization with Question-Answering Rewards,10.18653/v1/N19-1264,N19,1233
2020.mwe-1.10,"['Data Management and Generation', 'Classification Applications', 'Low-resource Languages']",['Hate and Offensive Speech Detection'],,"This paper presents our work on the refinement and improvement of the Serbian language part of Hurtlex, a multilingual lexicon of words to hurt. We pay special attention to adding Multi-word expressions that can be seen as abusive, as such lexical entries are very important in obtaining good results in a plethora of abusive language detection tasks. We use Serbian morphological dictionaries as a basis for data cleaning and MWE dictionary creation. A connection to other lexical and semantic resources in Serbian is outlined and building of abusive language detection systems based on that connection is foreseen.",https://aclanthology.org/2020.mwe-1.10,Association for Computational Linguistics,2020,December,Proceedings of the Joint Workshop on Multiword Expressions and Electronic Lexicons,"Stankovi{\'c}, Ranka  and
Mitrovi{\'c}, Jelena  and
Joki{\'c}, Danka  and
Krstev, Cvetana",Multi-word Expressions for Abusive Speech Detection in Serbian,,mwe,973
2020.udw-1.4,"['Data Management and Generation', 'Low-resource Languages']","['Data Preparation', 'Data Analysis']",,"We use Universal Dependencies treebanks to test whether a well-known typological trade-off between word order freedom and richness of morphological marking of core arguments holds within individual languages. Using Russian and German treebank data, we show that the following phenomenon sometimes dubbed word order freezing does occur: those sentences where core arguments cannot be distinguished by morphological means due to case syncretism or other kinds of ambiguity have more rigid order of subject, verb and object than those where unambiguous morphological marking is present. In ambiguous clauses, word order is more often equal to the one which is default or dominant most frequent in the language. While Russian and German differ with respect to how exactly they mark core arguments, the effect of morphological ambiguity is significant in both languages. It is, however, small, suggesting that languages do adapt to the evolutionary pressure on communicative efficiency and avoidance of redundancy, but that the pressure is weak in this particular respect.",https://aclanthology.org/2020.udw-1.4,Association for Computational Linguistics,2020,December,Proceedings of the Fourth Workshop on Universal Dependencies (UDW 2020),"Berdicevskis, Aleksandrs  and
Piperski, Alexander",Corpus evidence for word order freezing in Russian and German,,udw,650
2021.sdp-1.3,"['Domain-specific NLP', 'Data Management and Generation']","['NLP for Bibliometrics and Scientometrics', 'Data Preparation']",['Citation Analysis'],"With the increase in the number of published academic papers, growing expectations have been placed on research related to supporting the writing process of scientific papers. Recently, research has been conducted on various tasks such as citation worthiness judging whether a sentence requires citation, citation recommendation, and citation-text generation. However, since each task has been studied and evaluated using data that has been independently developed, it is currently impossible to verify whether such tasks can be successfully pipelined to effective use in scientificdocument writing. In this paper, we first define a series of tasks related to scientific-document writing that can be pipelined. Then, we create a dataset of academic papers that can be used for the evaluation of each task as well as a series of these tasks. Finally, using the dataset, we evaluate the tasks of citation worthiness and citation recommendation as well as both of these tasks integrated. The results of our evaluations show that the proposed approach is promising.",https://aclanthology.org/2021.sdp-1.3,Association for Computational Linguistics,2021,June,Proceedings of the Second Workshop on Scholarly Document Processing,"Narimatsu, Hiromi  and
Koyama, Kohei  and
Dohsaka, Kohji  and
Higashinaka, Ryuichiro  and
Minami, Yasuhiro  and
Taira, Hirotoshi",Task Definition and Integration For Scientific-Document Writing Support,10.18653/v1/2021.sdp-1.3,sdp,532
W16-0707,"['Data Management and Generation', 'Multilingual NLP', 'Information Extraction', 'Low-resource Languages', 'Cross-lingual Application']","['Data Analysis', 'Coreference Resolution']",,"This paper aims at a cross-lingual analysis of coreference to abstract entities in Czech and German, two languages that are typologically not very close, since they belong to two different language groups -Slavic and Germanic. We will specifically focus on coreference chains to abstract entities, i.e. verbal phrases, clauses, sentences or even longer text passages. To our knowledge, this type of relation is underinvestigated in the current stateof-the-art literature.",https://aclanthology.org/W16-0707,Association for Computational Linguistics,2016,June,Proceedings of the Workshop on Coreference Resolution Beyond {O}nto{N}otes ({CORBON} 2016),"Nedoluzhko, Anna  and
Lapshinova-Koltunski, Ekaterina",Abstract Coreference in a Multilingual Perspective: a View on Czech and German,10.18653/v1/W16-0707,W16,1497
2021.lantern-1.5,"['Learning Paradigms', 'Information Extraction']",['Multimodal Learning'],,"Multi-modal texts are abundant and diverse in structure, yet Vision & Language research of these naturally occurring texts has mostly focused on genres that are comparatively light on text, like tweets. In this paper, we discuss the challenges and potential benefits of a V&L framework that explicitly models referential relations, taking Wikipedia articles about buildings as an example. We briefly survey existing related tasks in V&L and propose multi-modal information extraction as a general direction for future research.",https://aclanthology.org/2021.lantern-1.5,Association for Computational Linguistics,2021,April,Proceedings of the Third Workshop on Beyond Vision and LANguage: inTEgrating Real-world kNowledge (LANTERN),"Utescher, Ronja  and
Zarrie{\ss}, Sina",What Did This Castle Look like before? Exploring Referential Relations in Naturally Occurring Multimodal Texts,,lantern,627
2021.finnlp-1.5,"['Domain-specific NLP', 'Data Management and Generation', 'Information Extraction', 'Classification Applications', 'Model Architectures']","['Hypernymy Extraction', 'NLP for Finance', 'Data Preparation']",['Annotation Processes'],"The FinSim-3 is the third edition of FinSim shared task on Learning Semantic Similarities for the Financial Domain, held in conjunction with IJCAI 2021 @Online as part of the FinNLP-2021 workshop. FinSim Shared Task proposes the challenge to automatically learn effective and precise semantic models for the financial domain. The third edition of the FinSim offered an extended dataset with more diversified financial concepts in order to increase its coverage in the semantic models, which were limited to those of instruments and market indices in the previous editions, and interested in systems which make creative use of relevant resources such as ontologies and lexica, as well as systems which make use of contextual word embeddings such as BERT Devlin et al., 2019. This year, 11 system runs were submitted by 5 teams and systems were ranked according to two metrics, Accuracy and Mean rank. All the systems largely beat our baselines 1 and 2, and the best system of this edition beats the performance of the previous editions Mansar et al., 2021 in both of the metrics.",https://aclanthology.org/2021.finnlp-1.5,-,2021,19-Aug,Proceedings of the Third Workshop on Financial Technology and Natural Language Processing,"Kang, Juyeon  and
Maarouf, Ismail El  and
Bellato, Sandra  and
Gan, Mei",FinSim-3: The 3rd Shared Task on Learning Semantic Similarities for the Financial Domain,,finnlp,209
2020.blackboxnlp-1.24,"['Model Architectures', 'Classification Applications']","['Transformer Models', 'Sentiment Analysis (SA)']",,"Neural attention, especially the self-attention made popular by the Transformer, has become the workhorse of state-of-the-art natural language processing NLP models. Very recent work suggests that the self-attention in the Transformer encodes syntactic information; Here, we show that self-attention scores encode semantics by considering sentiment analysis tasks. In contrast to gradient-based feature attribution methods, we propose a simple and effective Layer-wise Attention Tracing LAT method to analyze structured attention weights. We apply our method to Transformer models trained on two tasks that have surface dissimilarities, but share common semanticssentiment analysis of movie reviews and timeseries valence prediction in life story narratives. Across both tasks, words with high aggregated attention weights were rich in emotional semantics, as quantitatively validated by an emotion lexicon labeled by human annotators. Our results show that structured attention weights encode rich semantics in sentiment analysis, and match human interpretations of semantics.",https://aclanthology.org/2020.blackboxnlp-1.24,Association for Computational Linguistics,2020,November,Proceedings of the Third BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP,"Wu, Zhengxuan  and
Nguyen, Thanh-Son  and
Ong, Desmond",Structured Self-AttentionWeights Encode Semantics in Sentiment Analysis,10.18653/v1/2020.blackboxnlp-1.24,blackboxnlp,1437
2021.naloma-1.6,"['Classification Applications', 'Knowledge Representation and Reasoning', 'Model Architectures']",['Transformer Models'],,"Natural language contexts display logical regularities with respect to substitutions of related concepts: these are captured in a functional order-theoretic property called monotonicity. For a certain class of NLI problems where the resulting entailment label depends only on the context monotonicity and the relation between the substituted concepts, we build on previous techniques that aim to improve the performance of NLI models for these problems, as consistent performance across both upward and downward monotone contexts still seems difficult to attain even for state of the art models. To this end, we reframe the problem of context monotonicity classification to make it compatible with transformer-based pre-trained NLI models and add this task to the training pipeline. Furthermore, we introduce a sound and complete simplified monotonicity logic formalism which describes our treatment of contexts as abstract units. Using the notions in our formalism, we adapt targeted challenge sets to investigate whether an intermediate context monotonicity classification task can aid NLI models' performance on examples exhibiting monotonicity reasoning.",https://aclanthology.org/2021.naloma-1.6,Association for Computational Linguistics,2021,June,Proceedings of the 1st and 2nd Workshops on Natural Logic Meets Machine Learning (NALOMA),"Rozanova, Julia  and
Ferreira, Deborah  and
Thayaparan, Mokanarangan  and
Valentino, Marco  and
Freitas, Andr{\'e}",Supporting Context Monotonicity Abstractions in Neural NLI Models,10.48550/arxiv.2105.08008,naloma,936
2020.nlptea-1.14,"['Error Detection and Correction', 'Model Architectures', 'Low-resource Languages']",['Grammatical Error Correction (GEC)'],,"In the process of learning Chinese, second language learners may have various grammatical errors due to the negative transfer of native language. This paper describes our submission to the NLPTEA 2020 shared task on CGED. We present a hybrid system that utilizes both detection and correction stages. The detection stage is a sequential labelling model based on BiLSTM-CRF and BERT contextual word representation. The correction stage is a hybrid model based on the n-gram and Seq2Seq. Without adding additional features and external data, the BERT contextual word representation can effectively improve the performance metrics of Chinese grammatical error detection and correction.",https://aclanthology.org/2020.nlptea-1.14,Association for Computational Linguistics,2020,December,Proceedings of the 6th Workshop on Natural Language Processing Techniques for Educational Applications,"Zan, Hongying  and
Han, Yangchao  and
Huang, Haotian  and
Yan, Yingjie  and
Wang, Yuke  and
Han, Yingjie",Chinese Grammatical Errors Diagnosis System Based on BERT at NLPTEA-2020 CGED Shared Task,10.18653/v1/2020.nlptea-1.14,nlptea,1195
Y16-3015,"['Data Management and Generation', 'Low-resource Languages', 'Domain-specific NLP']","['Data Preparation', 'NLP for News and Media']",['NLP for Social Media'],"In this paper, we present our collective effort to gather, annotate, and model various language resources for use in different research projects. This includes those that are available online such as tweets, Wikipedia articles, game chat, online radio, and religious text. The different applications, issues and directions are also discussed in the paper. Future works include developing a language web service. A subset of the resources will be made temporarily available online at: http://bit.ly/1MpcFoT.",https://aclanthology.org/Y16-3015,,2016,October,"Proceedings of the 30th Pacific Asia Conference on Language, Information and Computation: Posters","Oco, Nathaniel  and
Syliongka, Leif Romeritch  and
Allman, Tod  and
Roxas, Rachel Edita","Philippine Language Resources: Applications, Issues, and Directions",,Y16,1217
D16-1224,"['Text Generation', 'Knowledge Representation and Reasoning']","['Data-to-Text Generation', 'Abstract Meaning Representation (AMR)']",,"The task of AMR-to-text generation is to generate grammatical text that sustains the semantic meaning for a given AMR graph. We attack the task by first partitioning the AMR graph into smaller fragments, and then generating the translation for each fragment, before finally deciding the order by solving an asymmetric generalized traveling salesman problem AGTSP. A Maximum Entropy classifier is trained to estimate the traveling costs, and a TSP solver is used to find the optimized solution. The final model reports a BLEU score of 22.44 on the SemEval-2016 Task8 dataset.",https://aclanthology.org/D16-1224,Association for Computational Linguistics,2016,November,Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing,"Song, Linfeng  and
Zhang, Yue  and
Peng, Xiaochang  and
Wang, Zhiguo  and
Gildea, Daniel",AMR-to-text generation as a Traveling Salesman Problem,10.18653/v1/D16-1224,D16,1210
D18-2025,"['Dialogue Systems', 'Knowledge Representation and Reasoning']",,,"We present LIA, an intelligent personal assistant that can be programmed using natural language. Our system demonstrates multiple competencies towards learning from humanlike interactions. These include i the ability to be taught reusable conditional procedures, ii ability to be taught new knowledge about the world concepts in an ontology and iii the ability to be taught how to ground that knowledge in a set of sensors and effectors. Building such a system highlights design questions regarding the overall architecture that such an agent should have, as well as questions about parsing and grounding language in situational contexts. We outline key properties of this architecture, and demonstrate a prototype that embodies them in the form of a personal assistant on an Android device.",https://aclanthology.org/D18-2025,Association for Computational Linguistics,2018,November,Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations,"Labutov, Igor  and
Srivastava, Shashank  and
Mitchell, Tom",LIA: A Natural Language Programmable Personal Assistant,10.18653/v1/D18-2025,D18,1146
2021.nlpmc-1.6,"['Domain-specific NLP', 'Data Management and Generation', 'Information Extraction', 'Classification Applications', 'Model Architectures']","['Data Preparation', 'Recurrent Neural Networks (RNNs)', 'Medical and Clinical NLP']","['Annotation Processes', 'Long Short-Term Memory (LSTM) Models']","Extracting structured information from medical conversations can reduce the documentation burden for doctors and help patients follow through with their care plan. In this paper, we introduce a novel task of extracting appointment spans from medical conversations. We frame this task as a sequence tagging problem and focus on extracting spans for appointment reason and time. However, annotating medical conversations is expensive, time-consuming, and requires considerable domain expertise. Hence, we propose to leverage weak supervision approaches, namely incomplete supervision, inaccurate supervision, and a hybrid supervision approach and evaluate both generic and domain-specific, ELMo, and BERT embeddings using sequence tagging models. The best performing model is the domain-specific BERT variant using weak hybrid supervision and obtains an F1 score of 79.32.",https://aclanthology.org/2021.nlpmc-1.6,Association for Computational Linguistics,2021,June,Proceedings of the Second Workshop on Natural Language Processing for Medical Conversations,"Meripo, Nimshi Venkat  and
Konam, Sandeep",Extracting Appointment Spans from Medical Conversations,10.18653/v1/2021.nlpmc-1.6,nlpmc,70
2022.lnls-1.4,"['Evaluation Techniques', 'Data Management and Generation']",['Data Preparation'],,"Many methods now exist for conditioning models on task instructions and user-provided explanations for individual data points. These methods show great promise for improving task performance of language models beyond what can be achieved by learning from individual x, y pairs. In this paper, we 1 provide a formal framework for characterizing approaches to learning from explanation data, and 2 we propose a synthetic task for studying how models learn from explanation data. In the first direction, we give graphical models for the available modeling approaches, in which explanation data can be used as model inputs, as targets, or as a prior. In the second direction, we introduce a carefully designed synthetic task with several properties making it useful for studying a model's ability to learn from explanation data. Each data point in this binary classification task is accompanied by a string that is essentially an answer to the why question: ""why does data point x have label y?"" Miller, 2019 . We aim to encourage research into this area by identifying key considerations for the modeling problem and providing an empirical test bed for theories of how models can best learn from explanation data. 1",https://aclanthology.org/2022.lnls-1.4,Association for Computational Linguistics,2022,May,Proceedings of the First Workshop on Learning with Natural Language Supervision,"Hase, Peter  and
Bansal, Mohit",When Can Models Learn From Explanations? A Formal Framework for Understanding the Roles of Explanation Data,10.18653/v1/2022.lnls-1.4,lnls,449
2020.acl-main.316,['Model Architectures'],,,"Variational autoencoders VAEs combine latent variables with amortized variational inference, whose optimization usually converges into a trivial local optimum termed posterior collapse, especially in text modeling. By tracking the optimization dynamics, we observe the encoder-decoder incompatibility that leads to poor parameterizations of the data manifold. We argue that the trivial local optimum may be avoided by improving the encoder and decoder parameterizations since the posterior network is part of a transition map between them. To this end, we propose Coupled-VAE, which couples a VAE model with a deterministic autoencoder with the same structure and improves the encoder and decoder parameterizations via encoder weight sharing and decoder signal matching. We apply the proposed Coupled-VAE approach to various VAE models with different regularization, posterior family, decoder structure, and optimization strategy. Experiments on benchmark datasets i.e., PTB, Yelp, and Yahoo show consistently improved results in terms of probability estimation and richness of the latent space. We also generalize our method to conditional language modeling and propose Coupled-CVAE, which largely improves the diversity of dialogue generation on the Switchboard dataset. 1",https://aclanthology.org/2020.acl-main.316,Association for Computational Linguistics,2020,July,Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,"Wu, Chen  and
Wang, Prince Zizhuang  and
Wang, William Yang",On the Encoder-Decoder Incompatibility in Variational Text Modeling and Beyond,10.18653/v1/2020.acl-main.316,acl,1473
2021.nodalida-main.12,"['Evaluation Techniques', 'Data Management and Generation', 'Low-resource Languages', 'Automatic Text Summarization']",['Data Preparation'],,"This paper explores how to automatically measure the quality of human-generated summaries, based on a Norwegian corpus of real estate condition reports and their corresponding summaries. The proposed approach proceeds in two steps. First, the real estate reports and their associated summaries are automatically labelled using a set of heuristic rules gathered from human experts and aggregated using weak supervision. The aggregated labels are then employed to learn a neural model that takes a document and its summary as inputs and outputs a score reflecting the predicted quality of the summary. The neural model maps the document and its summary to a shared ""summary content space"" and computes the cosine similarity between the two document embeddings to predict the final summary quality score. The best performance is achieved by a CNN-based model with an accuracy measured against the aggregated labels obtained via weak supervision of 89.5%, compared to 72.6% for the best unsupervised model. Manual inspection of examples indicate that the weak supervision labels do capture important indicators of summary quality, but the correlation of those labels with human judgements remains to be validated. Our models of summary quality predict that approximately 30% of the real estate reports in the corpus have a summary of poor quality.",https://aclanthology.org/2021.nodalida-main.12,"Link{\""o}ping University Electronic Press, Sweden",2021,May 31--2 June,Proceedings of the 23rd Nordic Conference on Computational Linguistics (NoDaLiDa),"Olsen, Joakim  and
N{\ae}ss, Arild Brandrud  and
Lison, Pierre",Assessing the Quality of Human-Generated Summaries with Weakly Supervised Learning,,nodalida,882
2020.wnut-1.68,"['Learning Paradigms', 'Domain-specific NLP', 'Classification Applications', 'Model Architectures']","['Transformer Models', 'Medical and Clinical NLP', 'Supervised Learning', 'NLP for News and Media']",['NLP for Social Media'],"In this system paper, we present a transformerbased approach to the detection of informativeness in English tweets on the topic of the current COVID-19 pandemic. Our models distinguish informative tweets, i.e. tweets containing statistics on recovery, suspected and confirmed cases and COVID-19 related deaths, from uninformative tweets. We present two transformer-based approaches as well as a Naive Bayes classifier and a support vector machine as baseline systems. The transformer models outperform the baselines by more than 0.1 in F1-score, with F1-scores of 0.9091 and 0.9036. Our models were submitted to the shared task Identification of informative COVID-19 English tweets WNUT-2020 Task 2.",https://aclanthology.org/2020.wnut-1.68,Association for Computational Linguistics,2020,November,Proceedings of the Sixth Workshop on Noisy User-generated Text (W-NUT 2020),"Varachkina, Hanna  and
Ziehe, Stefan  and
D{\""o}nicke, Tillmann  and
Pannach, Franziska",\#GCDH at WNUT-2020 Task 2: BERT-Based Models for the Detection of Informativeness in English COVID-19 Related Tweets,10.18653/v1/2020.wnut-1.68,wnut,1307
2022.nlppower-1.5,"['Classification Applications', 'Domain-specific NLP', 'Automatic Text Summarization', 'Data Management and Generation']","['Abstractive Text Summarization', 'Data Preparation', 'NLP for News and Media']",['Annotation Processes'],"Recent improvements in automatic news summarization fundamentally rely on large corpora of news articles and their summaries. These corpora are often constructed by scraping news websites, which results in including not only summaries but also other kinds of texts. Apart from more generic noise, we identify straplines as a form of text scraped from news websites that commonly turn out not to be summaries. The presence of these nonsummaries threatens the validity of scraped corpora as benchmarks for news summarization. We have annotated extracts from two news sources that form part of the Newsroom corpus Grusky et al., 2018 , labeling those which were straplines, those which were summaries, and those which were both. We present a rule-based strapline detection method that achieves good performance on a manually annotated test set 1 . Automatic evaluation indicates that removing straplines and noise from the training data of a news summarizer results in higher quality summaries, with improvements as high as 7 points ROUGE score.",https://aclanthology.org/2022.nlppower-1.5,Association for Computational Linguistics,2022,May,Proceedings of NLP Power! The First Workshop on Efficient Benchmarking in NLP,"Keleg, Amr  and
Lindemann, Matthias  and
Liu, Danyang  and
Long, Wanqiu  and
Webber, Bonnie L.",Automatically Discarding Straplines to Improve Data Quality for Abstractive News Summarization,10.18653/v1/2022.nlppower-1.5,nlppower,1373
2020.lt4hala-1.8,"['Text Preprocessing', 'Learning Paradigms', 'Low-resource Languages', 'Data Management and Generation']","['Text Segmentation', 'Data Preparation', 'Part-of-Speech (POS) Tagging', 'Supervised Learning']","['Word Segmentation', 'Sentence Segmentation']","The basic tasks of ancient Chinese information processing include automatic sentence segmentation, word segmentation, part-of-speech tagging and named entity recognition. Tasks such as lexical analysis need to be based on sentence segmentation because of the reason that a plenty of ancient books are not punctuated. However, step-by-step processing is prone to cause multi-level diffusion of errors. This paper designs and implements an integrated annotation system of sentence segmentation and lexical analysis. The BiLSTM-CRF neural network model is used to verify the generalization ability and the effect of sentence segmentation and lexical analysis on different label levels on four cross-age test sets. Research shows that the integration method adopted in ancient Chinese improves the F1-score of sentence segmentation, word segmentation and part of speech tagging. Based on the experimental results of each test set, the F1-score of sentence segmentation reached 78.95, with an average increase of 3.5%; the F1-score of word segmentation reached 85.73%, with an average increase of 0.18%; and the F1-score of part-of-speech tagging reached 72.65, with an average increase of 0.35%.",https://aclanthology.org/2020.lt4hala-1.8,European Language Resources Association (ELRA),2020,May,Proceedings of LT4HALA 2020 - 1st Workshop on Language Technologies for Historical and Ancient Languages,"Cheng, Ning  and
Li, Bin  and
Xiao, Liming  and
Xu, Changwei  and
Ge, Sijia  and
Hao, Xingyue  and
Feng, Minxuan",Integration of Automatic Sentence Segmentation and Lexical Analysis of Ancient Chinese based on BiLSTM-CRF Model,,lt4hala,453
2021.mmsr-1.10,"['Image and Video Processing', 'Learning Paradigms', 'Low-resource Languages', 'Data Management and Generation']","['Multimodal Learning', 'Data Preparation', 'Video Captioning']",['Annotation Processes'],"This paper introduces a new video-andlanguage dataset with human actions for multimodal logical inference, which focuses on intentional and aspectual expressions that describe dynamic human actions. The dataset consists of 200 videos, 5,554 action labels, and 1,942 action triplets of the form subject, predicate, object that can be translated into logical semantic representations. The dataset is expected to be useful for evaluating multimodal inference systems between videos and semantically complicated sentences including negation and quantification.",https://aclanthology.org/2021.mmsr-1.10,Association for Computational Linguistics,2021,June,Proceedings of the 1st Workshop on Multimodal Semantic Representations (MMSR),"Suzuki, Riko  and
Yanaka, Hitomi  and
Mineshima, Koji  and
Bekki, Daisuke",Building a Video-and-Language Dataset with Human Actions for Multimodal Logical Inference,10.48550/arxiv.2106.14137,mmsr,568
S18-2022,"['Information Extraction', 'Classification Applications', 'Model Architectures']",['Named Entity Recognition (NER)'],,"Fine-grained entity typing is the task of assigning fine-grained semantic types to entity mentions. We propose a neural architecture which learns a distributional semantic representation that leverages a greater amount of semantic context -both document and sentence level information -than prior work. We find that additional context improves performance, with further improvements gained by utilizing adaptive classification thresholds. Experiments show that our approach without reliance on hand-crafted features achieves the state-ofthe-art results on three benchmark datasets.",https://aclanthology.org/S18-2022,Association for Computational Linguistics,2018,June,Proceedings of the Seventh Joint Conference on Lexical and Computational Semantics,"Zhang, Sheng  and
Duh, Kevin  and
Van Durme, Benjamin",Fine-grained Entity Typing through Increased Discourse Context and Adaptive Classification Thresholds,10.18653/v1/S18-2022,S18,931
R19-1044,"['Question Answering (QA)', 'Discourse Analysis', 'Knowledge Representation and Reasoning']",,,"We introduce a concept of a virtual discourse tree to improve question answering Q/A recall for complex, multisentence questions. Augmenting the discourse tree of an answer with tree fragments obtained from text corpora playing the role of ontology, we obtain on the fly a canonical discourse representation of this answer that is independent of the thought structure of a given author. This mechanism is critical for finding an answer that is not only relevant in terms of questions entities but also in terms of interrelations between these entities in an answer and its style. We evaluate the Q/A system enabled with virtual discourse trees and observe a substantial increase of performance answering complex questions such as Yahoo! Answers and www.2carpros.com.",https://aclanthology.org/R19-1044,INCOMA Ltd.,2019,September,Proceedings of the International Conference on Recent Advances in Natural Language Processing (RANLP 2019),"Galitsky, Boris  and
Ilvovsky, Dmitry",Discourse-Based Approach to Involvement of Background Knowledge for Question Answering,10.26615/978-954-452-056-4_044,R19,492
2022.sigmorphon-1.2,"['Text Preprocessing', 'Learning Paradigms', 'Low-resource Languages']","['Unsupervised Learning', 'Text Segmentation']",['Word Segmentation'],"We present an extension of the Morfessor Baseline model of unsupervised morphological segmentation Creutz and Lagus, 2007 that incorporates abstract templates for reduplication, a typologically common but computationally underaddressed process. Through a detailed investigation that applies the model to Mori, the Indigenous language of Aotearoa New Zealand, we show that incorporating templates improves Morfessor's ability to identify instances of reduplication, and does so most when there are multiple minimally-overlapping templates. We present an error analysis that reveals important factors to consider when applying the extended model and suggests useful future directions.",https://aclanthology.org/2022.sigmorphon-1.2,Association for Computational Linguistics,2022,July,"Proceedings of the 19th SIGMORPHON Workshop on Computational Research in Phonetics, Phonology, and Morphology","Todd, Simon  and
Huang, Annie  and
Needle, Jeremy  and
Hay, Jennifer  and
King, Jeanette",Unsupervised morphological segmentation in a language with reduplication,10.18653/v1/2022.sigmorphon-1.2,sigmorphon,985
2022.ltedi-1.52,"['Audio Generation and Processing', 'Data Management and Generation', 'Low-resource Languages', 'Model Architectures']","['Transformer Models', 'Data Preparation', 'Automatic Speech Recognition (ASR)']",,"This paper illustrates the overview of the shared task on automatic speech recognition in the Tamil language. In the shared task, spontaneous Tamil speech data gathered from elderly and transgender people was given for recognition and evaluation. These utterances were collected from people when they communicated in the public locations such as hospitals, markets, vegetable shop, etc. The speech corpus includes utterances of male, female, and transgender and was split into training and testing data. The given task was evaluated using WER Word Error Rate. The participants used the transformer-based model for automatic speech recognition. Different results using different pre-trained transformer models are discussed in this overview paper.",https://aclanthology.org/2022.ltedi-1.52,Association for Computational Linguistics,2022,May,"Proceedings of the Second Workshop on Language Technology for Equality, Diversity and Inclusion","B, Bharathi  and
Chakravarthi, Bharathi Raja  and
Cn, Subalalitha  and
N, Sripriya  and
Pandian, Arunaggiri  and
Valli, Swetha",Findings of the Shared Task on Speech Recognition for Vulnerable Individuals in Tamil,10.18653/v1/2022.ltedi-1.52,ltedi,1312
2022.jeptalnrecital-taln.47,"['Evaluation Techniques', 'Text Generation', 'Data Management and Generation']","['Data Analysis', 'Text Simplification']",,"The purpose of automatic text simplification is to adapt the content of documents in order to make them easier to understand by a given population or to improve the performance of NLP tasks such as summarization or information extraction. The main steps for the automatic text simplification systems are quite well defined and researched in existing work but the evaluation of the simplification output remains understudied. Indeed, contrary to other NLP tasks, like information retrieval and extraction, terminology structuring, or question-answering, which expect factual and consensual outputs of the systems, it is difficult to define a standard output of simplification. There is considerable subjectivity in the simplification process, and it is not consensual because it is heavily based on own knowledge of people. Hence, several factors are involved in the simplification process and its assessment. In this paper, we present and discuss some of these factors : the role of end users, the reference data, the domain of source documents, and the evaluation measures.",https://aclanthology.org/2022.jeptalnrecital-taln.47,ATALA,2022,6,Actes de la 29e Conf{\'e}rence sur le Traitement Automatique des Langues Naturelles. Volume 1 : conf{\'e}rence principale,"Grabar, Natalia  and
Saggion, Horacio","Evaluation of Automatic Text Simplification: Where are we now, where should we go from here",,jeptalnrecital,969
2021.nlp4if-1.2,"['Figurative Language', 'Domain-specific NLP', 'Data Management and Generation', 'Low-resource Languages', 'Classification Applications']","['Data Preparation', 'Hate and Offensive Speech Detection', 'Metaphors', 'NLP for News and Media']","['Annotation Processes', 'NLP for Social Media']","We study the usefulness of hateful metaphors as features for the identification of the type and target of hate speech in Dutch Facebook comments. For this purpose, all hateful metaphors in the Dutch LiLaH corpus were annotated and interpreted in line with Conceptual Metaphor Theory and Critical Metaphor Analysis. We provide SVM and BERT/RoBERTa results, and investigate the effect of different metaphor information encoding methods on hate speech type and target detection accuracy. The results of the conducted experiments show that hateful metaphor features improve model performance for the both tasks. To our knowledge, it is the first time that the effectiveness of hateful metaphors as an information source for hate speech classification is investigated.",https://aclanthology.org/2021.nlp4if-1.2,Association for Computational Linguistics,2021,June,"Proceedings of the Fourth Workshop on NLP for Internet Freedom: Censorship, Disinformation, and Propaganda","Lemmens, Jens  and
Markov, Ilia  and
Daelemans, Walter",Improving Hate Speech Type and Target Detection with Hateful Metaphor Features,10.18653/v1/2021.nlp4if-1.2,nlp4if,1172
P17-2058,['Learning Paradigms'],,,"We demonstrate that a continuous relaxation of the argmax operation can be used to create a differentiable approximation to greedy decoding for sequence-tosequence seq2seq models. By incorporating this approximation into the scheduled sampling training procedure Bengio  et al., 2015-a well-known technique for correcting exposure bias-we introduce a new training objective that is continuous and differentiable everywhere and that can provide informative gradients near points where previous decoding decisions change their value. In addition, by using a related approximation, we demonstrate a similar approach to sampled-based training. Finally, we show that our approach outperforms cross-entropy training and scheduled sampling procedures in two sequence prediction tasks: named entity recognition and machine translation.",https://aclanthology.org/P17-2058,Association for Computational Linguistics,2017,July,Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),"Goyal, Kartik  and
Dyer, Chris  and
Berg-Kirkpatrick, Taylor",Differentiable Scheduled Sampling for Credit Assignment,10.18653/v1/P17-2058,P17,654
2021.bionlp-1.2,"['Domain-specific NLP', 'Information Extraction']","['Medical and Clinical NLP', 'Entity Linking']",['Biomedical NLP'],"Concept normalization, the task of linking textual mentions of concepts to concepts in an ontology, is critical for mining and analyzing biomedical texts. We propose a vector-space model for concept normalization, where mentions and concepts are encoded via transformer networks that are trained via a triplet objective with online hard triplet mining. The transformer networks refine existing pre-trained models, and the online triplet mining makes training efficient even with hundreds of thousands of concepts by sampling training triples within each mini-batch. We introduce a variety of strategies for searching with the trained vector-space model, including approaches that incorporate domain-specific synonyms at search time with no model retraining. Across five datasets, our models that are trained only once on their corresponding ontologies are within 3 points of state-of-the-art models that are retrained for each new domain. Our models can also be trained for each domain, achieving new state-of-the-art on multiple datasets.",https://aclanthology.org/2021.bionlp-1.2,Association for Computational Linguistics,2021,June,Proceedings of the 20th Workshop on Biomedical Language Processing,"Xu, Dongfang  and
Bethard, Steven",Triplet-Trained Vector Space and Sieve-Based Search Improve Biomedical Concept Normalization,10.18653/v1/2021.bionlp-1.2,bionlp,1127
2020.conll-1.25,['Embeddings'],['Word Embeddings'],,"This article establishes that, unlike the legacy tf*idf representation, recent natural language representations word embedding vectors tend to exhibit a so-called concentration of measure phenomenon, in the sense that, as the representation size p and database size n are both large, their behavior is similar to that of large dimensional Gaussian random vectors. This phenomenon may have important consequences as machine learning algorithms for natural language data could be amenable to improvement, thereby providing new theoretical insights into the field of natural language processing.",https://aclanthology.org/2020.conll-1.25,Association for Computational Linguistics,2020,November,Proceedings of the 24th Conference on Computational Natural Language Learning,"Couillet, Romain  and
Cinar, Yagmur Gizem  and
Gaussier, Eric  and
Imran, Muhammad",Word Representations Concentrate and This is Good News!,10.18653/v1/2020.conll-1.25,conll,307
2020.figlang-1.9,"['Domain-specific NLP', 'Model Architectures', 'Classification Applications']","['Sarcasm Detection', 'NLP for News and Media', 'Transformer Models', 'Sentiment Analysis (SA)']","['NLP for Social Media', 'Aspect-Based SA (ABSA)']","Sarcasm is a type of figurative language broadly adopted in social media and daily conversations. The sarcasm can ultimately alter the meaning of the sentence, which makes the opinion analysis process error-prone. In this paper, we propose to employ bidirectional encoder representations transformers BERT, and aspect-based sentiment analysis approaches in order to extract the relation between context dialogue sequence and response and determine whether or not the response is sarcastic. The best performing method of ours obtains an F1 score of 0.73 on the Twitter dataset and 0.734 over the Reddit dataset at the second workshop on figurative language processing Shared Task 2020.",https://aclanthology.org/2020.figlang-1.9,Association for Computational Linguistics,2020,July,Proceedings of the Second Workshop on Figurative Language Processing,"Shangipour ataei, Taha  and
Javdan, Soroush  and
Minaei-Bidgoli, Behrouz",Applying Transformers and Aspect-based Sentiment Analysis approaches on Sarcasm Detection,10.18653/v1/2020.figlang-1.9,figlang,851
2020.ecomnlp-1.5,"['Dialogue Systems', 'Domain-specific NLP', 'Low-resource Languages', 'Data Management and Generation']","['Data Preparation', 'Response Generation']",,"Online customer reviews are of growing importance for many businesses in the hospitality industry, particularly restaurants and hotels. Managerial responses to such reviews provide businesses with the opportunity to influence the public discourse and to attain improved ratings over time. However, responding to each and every review is a time-consuming endeavour. Therefore, we investigate automatic generation of review responses in the hospitality domain for two languages, English and German. We apply an existing system, originally proposed for review response generation for smartphone apps. This approach employs an extended neural network sequence-to-sequence architecture and performs well in the original domain. However, as shown through our experiments, when applied to a new domain, such as hospitality, performance drops considerably. Therefore, we analyse potential causes for the differences in performance and provide evidence to suggest that review response generation in the hospitality domain is a more challenging task and thus requires further study and additional domain adaptation techniques.",https://aclanthology.org/2020.ecomnlp-1.5,Association for Computational Linguistics,2020,December,Proceedings of Workshop on Natural Language Processing in E-Commerce,"Kew, Tannon  and
Amsler, Michael  and
Ebling, Sarah",Benchmarking Automated Review Response Generation for the Hospitality Domain,,ecomnlp,1069
2021.icnlsp-1.15,"['Domain-specific NLP', 'Data Management and Generation', 'Information Extraction', 'Low-resource Languages', 'Classification Applications', 'Model Architectures']",['Data Preparation'],,"While the development of online recruitment platforms has allowed companies to post their job offers and for job seekers to submit their resumes simply and efficiently, the recruitment process remains time and resources consuming. Accordingly, models are needed to support the automatic matching between candidate resumes and job offers, which is called person-job fit issue. Recent works have focused on modeling the matching between resumes and job requirements through deep learning techniques. However, due to the complex internal transformations that will subject to the input, these models suffer from the interpretability problem. Yet, in real deployment, it is necessary to explain why a candidate is accepted or rejected for a given job offer. To this end, we propose a hybrid approach that takes benefit from deep learning techniques while making the matching results human-readable. This was achieved by extracting several features from the resume and job description and use them to perform classification and ranking. The obtained results on French resumes dataset show an accuracy of 93.7% in the case of classification and 70% of accepted resumes were ranked on the top 5 candidates, and this in the case where the problem is processed as a ranking issue.",https://aclanthology.org/2021.icnlsp-1.15,Association for Computational Linguistics,2021,12--13 November,Proceedings of The Fourth International Conference on Natural Language and Speech Processing (ICNLSP 2021),"Menacer, Mohamed Amine  and
Hamda, Fatma Ben  and
Mighri, Ghada  and
Hamidene, Sabeur Ben  and
Cariou, Maxime",An interpretable person-job fitting approach based on classification and ranking,,icnlsp,213
2020.gebnlp-1.2,"['Biases in NLP', 'Embeddings']","['Word Embeddings', 'Gender Bias']",,"Recent years have seen a surge in research on the biases in word embeddings with respect to gender and, to a lesser extent, race. Few of these studies, however, have given attention to the critical intersection of race and gender. In this case study, we analyze the dimensions of gender and race in contextualized word embeddings of given names, taken from BERT, and investigate the nature and nuance of their interaction. We find that these demographic axes, though typically treated as physically and conceptually separate, are in fact interdependent and thus inadvisable to consider in isolation. Further, we show that demographic dimensions predicated on default settings in language, such as in pronouns, may risk rendering groups with multiple marginalized identities invisible. We conclude by discussing the importance and implications of intersectionality for future studies on bias and debiasing in NLP.",https://aclanthology.org/2020.gebnlp-1.2,Association for Computational Linguistics,2020,December,Proceedings of the Second Workshop on Gender Bias in Natural Language Processing,"Jiang, May  and
Fellbaum, Christiane",Interdependencies of Gender and Race in Contextualized Word Embeddings,,gebnlp,1426
P16-3014,"['Parsing', 'Knowledge Representation and Reasoning']",,,"The computing cost of many NLP tasks increases faster than linearly with the length of the representation of a sentence. For parsing the representation is tokens, while for operations on syntax and semantics it will be more complex. In this paper we propose a new task of sentence chunking: splitting sentence representations into coherent substructures. Its aim is to make further processing of long sentences more tractable. We investigate this idea experimentally using the Dependency Minimal Recursion Semantics DMRS representation.",https://aclanthology.org/P16-3014,Association for Computational Linguistics,2016,August,Proceedings of the {ACL} 2016 Student Research Workshop,"Muszy{\'n}ska, Ewa",Graph- and surface-level sentence chunking,10.18653/v1/P16-3014,P16,19
2021.acl-demo.35,"['Domain-specific NLP', 'Data Management and Generation', 'Embeddings', 'Language Change Analysis']","['NLP for News and Media', 'Semantic Change Analysis', 'Word Embeddings', 'Data Preparation', 'Data Analysis']",,"The usage of individual words can change over time, for example, when words experience a semantic shift. As text datasets generally comprise documents that were collected over a longer period of time, examining word usage changes in a corpus can often reveal interesting patterns. In this paper, we introduce a simple and intuitive way to track word usage changes via continuously evolving embeddings, computed as a weighted running average of transformer-based contextualized embeddings. We demonstrate our approach on a corpus of recent New York Times article snippets and provide code for an easy to use web app to conveniently explore semantic shifts with interactive plots.",https://aclanthology.org/2021.acl-demo.35,Association for Computational Linguistics,2021,August,Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: System Demonstrations,"Horn, Franziska",Exploring Word Usage Change with Continuously Evolving Embeddings,10.18653/v1/2021.acl-demo.35,acl,330
2021.privatenlp-1.3,"['Ethics', 'Domain-specific NLP', 'Information Extraction', 'Model Architectures']","['Transformer Models', 'Medical and Clinical NLP']",,"Contextual language models have led to significantly better results on a plethora of language understanding tasks, especially when pre-trained on the same data as the downstream task. While this additional pre-training usually improves performance, it can lead to information leakage and therefore risks the privacy of individuals mentioned in the training data. One method to guarantee the privacy of such individuals is to train a differentiallyprivate model, but this usually comes at the expense of model performance. Moreover, it is hard to tell given a privacy parameter what was the effect on the trained representation. In this work we aim to guide future practitioners and researchers on how to improve privacy while maintaining good model performance. We demonstrate how to train a differentially-private pre-trained language model i.e., BERT with a privacy guarantee of = 1 and with only a small degradation in performance. We experiment on a dataset of clinical notes with a model trained on a target entity extraction task, and compare it to a similar model trained without differential privacy. Finally, we present experiments showing how to interpret the differentially-private representation and understand the information lost and maintained in this process.",https://aclanthology.org/2021.privatenlp-1.3,Association for Computational Linguistics,2021,June,Proceedings of the Third Workshop on Privacy in Natural Language Processing,"Hoory, Shlomo  and
Feder, Amir  and
Tendler, Avichai  and
Cohen, Alon  and
Erell, Sofia  and
Laish, Itay  and
Nakhost, Hootan  and
Stemmer, Uri  and
Benjamini, Ayelet  and
Hassidim, Avinatan  and
Matias, Yossi",Learning and Evaluating a Differentially Private Pre-trained Language Model,10.18653/v1/2021.privatenlp-1.3,privatenlp,1449
2021.conll-1.18,"['Evaluation Techniques', 'Data Management and Generation', 'Low-resource Languages', 'Machine Translation (MT)']","['Data Preparation', 'Data Analysis']",['Annotation Processes'],"This work describes an analysis of interannotator disagreements in human evaluation of machine translation output. The errors in the analysed texts were marked by multiple annotators under guidance of different quality criteria: adequacy, comprehension, and an unspecified generic mixture of adequacy and fluency. Our results show that different criteria result in different disagreements, and indicate that a clear definition of quality criterion can improve the inter-annotator agreement. Furthermore, our results show that for certain linguistic phenomena which are not limited to one or two words such as word ambiguity or gender but span over several words or even entire phrases such as negation or relative clause, disagreements do not necessarily represent ""errors"" or ""noise"" but are rather inherent to the evaluation process. On the other hand, for some other phenomena such as omission or verb forms agreement can be easily improved by providing more precise and detailed instructions to the evaluators.",https://aclanthology.org/2021.conll-1.18,Association for Computational Linguistics,2021,November,Proceedings of the 25th Conference on Computational Natural Language Learning,"Popovi{\'c}, Maja",Agree to Disagree: Analysis of Inter-Annotator Disagreements in Human Evaluation of Machine Translation Output,10.18653/v1/2021.conll-1.18,conll,587
N19-2007,"['Dialogue Systems', 'Model Architectures']","['Transformer Models', 'Response Generation']",,"End-to-end neural models for goal-oriented conversational systems have become an increasingly active area of research, though results in real-world settings are few. We present real-world results for two issue types in the customer service domain. We train models on historical chat transcripts and test on live contacts using a human-in-the-loop research platform. Additionally, we incorporate customer profile features to assess their impact on model performance. We experiment with two approaches for response generation: 1 sequence-to-sequence generation and 2 template ranking. To test our models, a customer service agent handles live contacts and at each turn we present the top four model responses and allow the agent to select and optionally edit one of the suggestions or to type their own. We present results for turn acceptance rate, response coverage, and edit rate based on approximately 600 contacts, as well as qualitative analysis on patterns of turn rejection and edit behavior. Top-4 turn acceptance rate across all models ranges from 63%-80%. Our results suggest that these models are promising for an agent-support application.",https://aclanthology.org/N19-2007,Association for Computational Linguistics,2019,June,"Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Industry Papers)","Lu, Yichao  and
Srivastava, Manisha  and
Kramer, Jared  and
Elfardy, Heba  and
Kahn, Andrea  and
Wang, Song  and
Bhardwaj, Vikas",Goal-Oriented End-to-End Conversational Models with Profile Features in a Real-World Setting,10.18653/v1/N19-2007,N19,1211
U17-1002,"['Question Answering (QA)', 'Knowledge Representation and Reasoning', 'Model Architectures']",,,"Answering questions while reasoning over multiple supporting facts has long been a goal of artificial intelligence. Recently, remarkable advances have been made, focusing on reasoning over natural language-based stories. In particular, end-to-end memory networks N2N, have achieved state-of-the-art results over such tasks. However, N2Ns are limited by the necessity to choose between two weight tying schemes, neither of which performs consistently well over all tasks. We propose a unified model generalising weight tying and in doing so, make the model more expressive. The proposed model achieves uniformly high performance, improving on the best results for memory network-based models on the bAbI dataset, and competitive results on Dialog bAbI.",https://aclanthology.org/U17-1002,,2017,December,Proceedings of the Australasian Language Technology Association Workshop 2017,"Liu, Fei  and
Cohn, Trevor  and
Baldwin, Timothy",Improving End-to-End Memory Networks with Unified Weight Tying,,U17,570
2021.sigtyp-1.8,"['Information Extraction', 'Data Management and Generation', 'Low-resource Languages']",['Data Analysis'],,"It has long been recognized that suffixing is more common than prefixing in the languages of the world. More detailed statistics on this tendency are needed to sharpen proposed explanations for this tendency. The classic approach to gathering data on the prefix/suffix preference is for a human to read grammatical descriptions 948 languages, which is timeconsuming and involves discretization judgments. In this paper we explore two machinedriven approaches for prefix and suffix statistics which are crude approximations, but have advantages in terms of time and replicability. The first simply searches a large collection of grammatical descriptions for occurrences of the terms 'prefix' and 'suffix' 4 287 languages. The second counts substrings from raw text data in a way indirectly reflecting prefixation and suffixation 1 030 languages, using New Testament translations. The three approaches largely agree in their measurements but there are important theoretical and practical differences. In all measurements, there is an overall preference for suffixation, albeit only slightly, at ratios ranging between 0.51 and 0.68.",https://aclanthology.org/2021.sigtyp-1.8,Association for Computational Linguistics,2021,June,Proceedings of the Third Workshop on Computational Typology and Multilingual NLP,"Hammarstr{\""o}m, Harald",Measuring Prefixation and Suffixation in the Languages of the World,10.18653/v1/2021.sigtyp-1.8,sigtyp,1423
2021.wat-1.28,"['Machine Translation (MT)', 'Model Architectures', 'Multilingual NLP', 'Low-resource Languages']","['Transformer Models', 'Neural MT (NMT)']",,"In this paper, we present the details of the systems that we have submitted for the WAT 2021 MultiIndicMT: An Indic Language Multilingual Task. We have submitted two separate multilingual NMT models: one for English to 10 Indic languages and another for 10 Indic languages to English. We discuss the implementation details of two separate multilingual NMT approaches, namely one-to-many and many-to-one, that makes use of a shared decoder and a shared encoder, respectively. From our experiments, we observe that the multilingual NMT systems outperforms the bilingual baseline MT systems for each of the language pairs under consideration.",https://aclanthology.org/2021.wat-1.28,Association for Computational Linguistics,2021,August,Proceedings of the 8th Workshop on Asian Translation (WAT2021),"Mhaskar, Shivam  and
Jain, Aditya  and
Banerjee, Aakash  and
Bhattacharyya, Pushpak",Multilingual Machine Translation Systems at WAT 2021: One-to-Many and Many-to-One Transformer based NMT,10.18653/v1/2021.wat-1.28,wat,1462
L18-1278,"['Domain-specific NLP', 'Multilingual NLP', 'Information Extraction', 'Low-resource Languages', 'Model Architectures']","['Named Entity Recognition (NER)', 'NLP for News and Media']",['NLP for Social Media'],"The rise in accessibility of web to the mass has led to a spurt in the use of social media making it convenient and powerful way to express and exchange information in their own languages. India, being enormously diversified country have more than 168 millions users on social media. This diversity is also reflected in their scripts where a majority of users often switch between their native languages to be more expressive. These linguistic variations make automatic entity extraction both a necessary and a challenging problem. In this paper, we report our work for entity extraction in a code-mixed environment. Our proposed approach is based on the popular deep neural network based Gated Recurrent Unit GRU archirecture that automatically discovers the higher level features from the text. We do not make use of any handcrafted features or rules, and therefore our proposed model is quite generic in nature. Our experiments on two benchmark datasets of English-Hindi and English-Tamil language pairs show the F-scores of 66.04% and 53.85%, respectively.",https://aclanthology.org/L18-1278,European Language Resources Association (ELRA),2018,May,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),"Gupta, Deepak  and
Ekbal, Asif  and
Bhattacharyya, Pushpak",A Deep Neural Network based Approach for Entity Extraction in Code-Mixed Indian Social Media Text,,L18,793
2021.semeval-1.74,['Model Architectures'],,,"This paper describes our submission to the SemEval-2021 shared task on Lexical Complexity Prediction. We approached it as a regression problem and present an ensemble combining four systems, one feature-based and three neural with fine-tuning, frequency pre-training and multi-task learning, achieving Pearson scores of 0.8264 and 0.7556 on the trial and test sets respectively sub-task 1. We further present our analysis of the results and discuss our findings.",https://aclanthology.org/2021.semeval-1.74,Association for Computational Linguistics,2021,August,Proceedings of the 15th International Workshop on Semantic Evaluation (SemEval-2021),"Yuan, Zheng  and
Tyen, Gladys  and
Strohmaier, David",Cambridge at SemEval-2021 Task 1: An Ensemble of Feature-Based and Neural Models for Lexical Complexity Prediction,10.18653/v1/2021.semeval-1.74,semeval,290
2021.dravidianlangtech-1.29,"['Domain-specific NLP', 'Multilingual NLP', 'Low-resource Languages', 'Classification Applications', 'Model Architectures']","['Hate and Offensive Speech Detection', 'NLP for News and Media']",['NLP for Social Media'],"The development of online media platforms has given users more opportunities to post and comment freely, but the negative impact of offensive language has become increasingly apparent. It is very necessary for the automatic identification system of offensive language. This paper describes our work on the task of Offensive Language Identification in Dravidian language-EACL 2021. To complete this task, we propose a system based on the multilingual model XLM-Roberta and DPCNN. The test results on the official test data set confirm the effectiveness of our system. The weighted average F1-score of Kannada, Malayalam, and Tamil language are 0.69, 0.92, and 0.76 respectively, ranked 6th, 6th, and 3rd.",https://aclanthology.org/2021.dravidianlangtech-1.29,Association for Computational Linguistics,2021,April,Proceedings of the First Workshop on Speech and Language Technologies for Dravidian Languages,"Zhao, Yingjia  and
Tao, Xin",ZYJ123@DravidianLangTech-EACL2021: Offensive Language Identification based on XLM-RoBERTa with DPCNN,,dravidianlangtech,662
2020.starsem-1.18,"['Commonsense Reasoning', 'Question Answering (QA)', 'Data Management and Generation']","['Multiple Choice QA (MCQA)', 'Data Preparation']",['Annotation Processes'],"We examine a new commonsense reasoning task: given a narrative describing a social interaction that centers on two protagonists, systems make inferences about the underlying relationship trajectory. Specifically, we propose two evaluation tasks: Relationship Outlook Prediction MCQ and Resolution Prediction MCQ. In Relationship Outlook Prediction, a system maps an interaction to a relationship outlook that captures how the interaction is expected to change the relationship. In Resolution Prediction, a system attributes a given relationship outlook to a particular resolution that explains the outcome. These two tasks parallel two real-life questions that people frequently ponder upon as they navigate different social situations: ""where is this relationship going?"" and ""how did we end up here?"". To facilitate the investigation of human social relationships through these two tasks, we construct a new dataset, Social Narrative Tree, which consists of 1250 stories documenting a variety of daily social interactions. The narratives encode a multitude of social elements that interweave to give rise to rich commonsense knowledge of how relationships evolve with respect to social interactions. We establish baseline performances using language models and the accuracies are significantly lower than human performance. The results demonstrate that models need to look beyond syntactic and semantic signals to comprehend complex human relationships.",https://aclanthology.org/2020.starsem-1.18,Association for Computational Linguistics,2020,December,Proceedings of the Ninth Joint Conference on Lexical and Computational Semantics,"You, Keen  and
Goldwasser, Dan",``where is this relationship going?'': Understanding Relationship Trajectories in Narrative Text,,starsem,1215
N18-2029,"['Multilingual NLP', 'Learning Paradigms', 'Low-resource Languages', 'Classification Applications', 'Model Architectures']",['Transfer Learning'],,"We present a simple and effective feedforward neural architecture for discriminating between lexico-semantic relations synonymy, antonymy, hypernymy, and meronymy. Our Specialization Tensor Model STM simultaneously produces multiple different specializations of input distributional word vectors, tailored for predicting lexico-semantic relations for word pairs. STM outperforms more complex state-of-the-art architectures on two benchmark datasets and exhibits stable performance across languages. We also show that, if coupled with a lingual distributional space, the proposed model can transfer the prediction of lexico-semantic relations to a resource-lean target language without any training data.",https://aclanthology.org/N18-2029,Association for Computational Linguistics,2018,June,"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers)","Glava{\v{s}}, Goran  and
Vuli{\'c}, Ivan",Discriminating between Lexico-Semantic Relations with the Specialization Tensor Model,10.18653/v1/N18-2029,N18,1425
D19-5548,"['Biases in NLP', 'Domain-specific NLP', 'Data Management and Generation', 'Learning Paradigms', 'Low-resource Languages', 'Classification Applications', 'Model Architectures']","['Bias Detection', 'Data Preparation', 'Transformer Models', 'NLP for News and Media']",,"This study analyzes the political slants of user comments on Korean partisan media. We built a BERT-based classifier to detect political leaning of short comments via the use of semiunsupervised deep learning methods that produced an F1 score of 0.83. As a result of classifying 21.6K comments, we found the high presence of conservative bias on both conservative and liberal news outlets. Moreover, this study discloses an asymmetry across the partisan spectrum in that more liberals 48.0% than conservatives 23.6% comment not only on news stories resonating with their political perspectives but also on those challenging their viewpoints. These findings advance the current understanding of online echo chambers.",https://aclanthology.org/D19-5548,Association for Computational Linguistics,2019,November,Proceedings of the 5th Workshop on Noisy User-generated Text (W-NUT 2019),"Han, Jiyoung  and
Lee, Youngin  and
Lee, Junbum  and
Cha, Meeyoung",The Fallacy of Echo Chambers: Analyzing the Political Slants of User-Generated News Comments in Korean Media,10.18653/v1/D19-5548,D19,267
2022.dravidianlangtech-1.2,"['Data Management and Generation', 'Low-resource Languages', 'Classification Applications', 'Domain-specific NLP']","['Humor Detection', 'Data Preparation', 'NLP for News and Media']",['NLP for Social Media'],"Increased use of online social media sites has given rise to tremendous amounts of user generated data. Social media sites have become a platform where users express and voice their opinions in a real-time environment. Social media sites such as Twitter limit the number of characters used to express a thought in a tweet, leading to increased use of creative, humorous and confusing language in order to convey the message. Due to this, automatic humor detection has become a difficult task, especially for low-resource languages such as the Dravidian languages. Humor detection has been a well studied area for resource rich languages due to the availability of rich and accurate data. In this paper, we have attempted to solve this issue by working on low-resource languages, such as, Telugu, a Dravidian language, by collecting and annotating Telugu tweets and performing automatic humor detection on the collected data. We experimented on the corpus using various transformer models such as Multilingual BERT, Multilingual DistillBERT and XLM-RoBERTa to establish a baseline classification system. We concluded that XLM-RoBERTa was the best-performing model and it achieved an F1-score of 0.82 with 81.5% accuracy.",https://aclanthology.org/2022.dravidianlangtech-1.2,Association for Computational Linguistics,2022,May,Proceedings of the Second Workshop on Speech and Language Technologies for Dravidian Languages,"Bellamkonda, Sriphani  and
Lohakare, Maithili  and
Patel, Shaswat",A Dataset for Detecting Humor in Telugu Social Media Text,10.18653/v1/2022.dravidianlangtech-1.2,dravidianlangtech,950
2021.eacl-demos.15,"['Information Extraction', 'Knowledge Representation and Reasoning']","['Knowledge Graphs', 'Entity Linking']",,"In this paper we present COCO-EX, a tool for Extracting Concepts from texts and linking them to the ConceptNet knowledge graph. COCO-EX extracts meaningful concepts from natural language texts and maps them to conjunct concept nodes in ConceptNet, utilizing the maximum of relational information stored in the ConceptNet knowledge graph. COCO-EX takes into account the challenging characteristics of ConceptNet, namely that -unlike conventional knowledge graphs -nodes are represented as non-canonicalized, free-form text. This means that i concepts are not normalized; ii they often consist of several different, nested phrase types; and iii many of them are uninformative, over-specific, or misspelled. A commonly used shortcut to circumvent these problems is to apply string matching. We compare COCO-EX to this method and show that COCO-EX enables the extraction of meaningful, important rather than overspecific or uninformative concepts, and allows to assess more relational information stored in the knowledge graph. 1",https://aclanthology.org/2021.eacl-demos.15,Association for Computational Linguistics,2021,April,Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: System Demonstrations,"Becker, Maria  and
Korfhage, Katharina  and
Frank, Anette",COCO-EX: A Tool for Linking Concepts from Texts to ConceptNet,10.18653/v1/2021.eacl-demos.15,eacl,433
2020.lr4sshoc-1.9,"['Audio Generation and Processing', 'Domain-specific NLP']",['Automatic Speech Recognition (ASR)'],,"Spoken audio data, such as interview data, is a scientific instrument used by researchers in various disciplines crossing the boundaries of social sciences and humanities. In this paper, we will have a closer look at a portal designed to perform speech-to-text conversion on audio recordings through Automatic Speech Recognition ASR in the CLARIN infrastructure. Within the cluster cross-domain EU project SSHOC the potential value of such a linguistic tool kit for processing spoken language recording has found uptake in a webinar about the topic, and in a task addressing audio analysis of panel survey data. The objective of this contribution is to show that the processing of interviews as a research instrument has opened up a fascinating and fruitful area of collaboration between Social Sciences and Humanities SSH.",https://aclanthology.org/2020.lr4sshoc-1.9,European Language Resources Association,2020,May,Proceedings of the Workshop about Language Resources for the SSH Cloud,"van den Heuvel, Henk",Crossing the SSH Bridge with Interview Data,,lr4sshoc,1035
2021.nodalida-main.26,"['Text Generation', 'Data Management and Generation', 'Low-resource Languages']","['Data Analysis', 'Text Simplification']",,"In this article, we explore the use of basiclevel nouns in texts of different complexity, and hypothesise that hypernyms with characteristics of basic-level words could be useful for the task of lexical simplification. Basic-level terms have been described as the most important to human categorisation. They are the earliest emerging words in children's language acquisition, and seem to be more frequently occurring in language in general. We conducted two corpus studies using four different corpora, two corpora of standard Swedish and two corpora of simple Swedish, and explored whether corpora of simple texts contain a higher proportion of basic-level nouns than corpora of standard Swedish. Based on insights from the corpus studies, we developed a novel algorithm for choosing the best synonym by rewarding high relative frequencies and monolexemity, and restricting the climb in the word hierarchy not to suggest synonyms of a too high level of inclusiveness.",https://aclanthology.org/2021.nodalida-main.26,"Link{\""o}ping University Electronic Press, Sweden",2021,May 31--2 June,Proceedings of the 23rd Nordic Conference on Computational Linguistics (NoDaLiDa),"Rennes, Evelina  and
J{\""o}nsson, Arne",Synonym Replacement based on a Study of Basic-level Nouns in Swedish Texts of Different Complexity,,nodalida,719
2022.humeval-1.2,"['Evaluation Techniques', 'Data Management and Generation', 'Information Extraction', 'Low-resource Languages']","['Data Preparation', 'Data Analysis', 'Coreference Resolution']",['Annotation Processes'],"We propose a method for investigating the interpretability of metrics used for the coreference resolution task through comparisons with human judgments. We provide a corpus with annotations of different error types and human evaluations of their gravity. Our preliminary analysis shows that metrics considerably overlook several error types and overlook errors in general in comparison to humans. This study is conducted on French texts, but the methodology should be language-independent.",https://aclanthology.org/2022.humeval-1.2,Association for Computational Linguistics,2022,May,Proceedings of the 2nd Workshop on Human Evaluation of NLP Systems (HumEval),"Borovikova, Mariya  and
Grobol, Lo{\""\i}c  and
Halftermeyer, Ana{\""\i}s  and
Billot, Sylvie",A Methodology for the Comparison of Human Judgments With Metrics for Coreference Resolution,10.18653/v1/2022.humeval-1.2,humeval,67
2021.ranlp-srw.10,"['Domain-specific NLP', 'Low-resource Languages', 'Classification Applications', 'Data Management and Generation']","['Data Preparation', 'Sentiment Analysis (SA)', 'NLP for News and Media']","['Aspect-Based SA (ABSA)', 'NLP for Social Media']","In the pandemic period, the stay-at-home trend forced businesses to switch their activities to digital mode, for example, app-based payment methods, social distancing via social media platforms, and other digital means have become an integral part of our lives. Sentiment analysis of textual information in user comments is a topical task in emotion AI because user comments or reviews are not homogeneous, they contain sparse context behind, and are misleading both for human and computer. Barriers arise from the emotional language enriched with slang, peculiar spelling, transliteration, use of emoji and their symbolic counterparts, and codeswitching.",https://aclanthology.org/2021.ranlp-srw.10,INCOMA Ltd.,2021,September,Proceedings of the Student Research Workshop Associated with RANLP 2021,"Gimadi, Dinara",Web-sentiment analysis of public comments public reviews for languages with limited resources such as the Kazakh language,10.26615/issn.2603-2821.2021_010,ranlp,934
2020.nlp4if-1.1,"['Domain-specific NLP', 'Data Management and Generation', 'Model Architectures', 'Classification Applications']","['NLP for News and Media', 'Data Preparation', 'Misinformation Detection', 'Medical and Clinical NLP']","['NLP for Social Media', 'Fake News Detection']","The rapid advancement of technology in online communication via social media platforms has led to a prolific rise in the spread of misinformation and fake news. Fake news is especially rampant in the current COVID-19 pandemic, leading to people believing in false and potentially harmful claims and stories. Detecting fake news quickly can alleviate the spread of panic, chaos and potential health hazards. We developed a two stage automated pipeline for COVID-19 fake news detection using state of the art machine learning models for natural language processing. The first model leverages a novel fact checking algorithm that retrieves the most relevant facts concerning user claims about particular COVID-19 claims. The second model verifies the level of ""truth"" in the claim by computing the textual entailment between the claim and the true facts retrieved from a manually curated COVID-19 dataset. The dataset is based on a publicly available knowledge source consisting of more than 5000 COVID-19 false claims and verified explanations, a subset of which was internally annotated and cross-validated to train and evaluate our models. We evaluate a series of models based on classical text-based features to more contextual Transformer based models and observe that a model pipeline based on BERT and ALBERT for the two stages respectively yields the best results.",https://aclanthology.org/2020.nlp4if-1.1,International Committee on Computational Linguistics (ICCL),2020,December,"Proceedings of the 3rd NLP4IF Workshop on NLP for Internet Freedom: Censorship, Disinformation, and Propaganda","Vijjali, Rutvik  and
Potluri, Prathyush  and
Kumar, Siddharth  and
Teki, Sundeep",Two Stage Transformer Model for COVID-19 Fake News Detection and Fact Checking,10.48550/arxiv.2011.13253,nlp4if,383
W17-1713,['Low-resource Languages'],,,"We use word alignment variance as an indicator for the non-compositionality of German and English noun compounds. Our work-in-progress results are on their own not competitive with state-of-the art approaches, but they show that alignment variance is correlated with compositionality and thus worth a closer look in the future.",https://aclanthology.org/W17-1713,Association for Computational Linguistics,2017,April,Proceedings of the 13th Workshop on Multiword Expressions ({MWE} 2017),"Cap, Fabienne",Show Me Your Variance and I Tell You Who You Are - Deriving Compound Compositionality from Word Alignments,10.18653/v1/W17-1713,W17,1008
2020.calcs-1.7,"['Audio Generation and Processing', 'Learning Paradigms', 'Low-resource Languages', 'Multilingual NLP', 'Model Architectures']",['Automatic Speech Recognition (ASR)'],,"We present an analysis of semi-supervised acoustic and language model training for English-isiZulu code-switched ASR using soap opera speech. Approximately 11 hours of untranscribed multilingual speech was transcribed automatically using four bilingual code-switching transcription systems operating in English-isiZulu, English-isiXhosa, English-Setswana and English-Sesotho. These transcriptions were incorporated into the acoustic and language model training sets. Results showed that the TDNN-F acoustic models benefit from the additional semi-supervised data and that even better performance could be achieved by including additional CNN layers. Using these CNN-TDNN-F acoustic models, a first iteration of semi-supervised training achieved an absolute mixed-language WER reduction of 3.4%, and a further 2.2% after a second iteration. Although the languages in the untranscribed data were unknown, the best results were obtained when all automatically transcribed data was used for training and not just the utterances classified as English-isiZulu. Despite reducing perplexity, the semi-supervised language model was not able to improve the ASR performance.",https://aclanthology.org/2020.calcs-1.7,European Language Resources Association,2020,May,Proceedings of the The 4th Workshop on Computational Approaches to Code Switching,"Biswas, Astik  and
De Wet, Febe  and
Van der westhuizen, Ewald  and
Niesler, Thomas",Semi-supervised acoustic and language model training for English-isiZulu code-switched speech recognition,10.48550/arxiv.2004.04054,calcs,113
2020.computerm-1.3,"['Embeddings', 'Domain-specific NLP', 'Evaluation Techniques', 'Bilingual Lexicon Induction (BLI)', 'Knowledge Representation and Reasoning', 'Classification Applications', 'Learning Paradigms', 'Low-resource Languages', 'Cross-lingual Application']","['Unsupervised Learning', 'Word Embeddings', 'NLP for News and Media', 'Multilabel Text Classification']",,"A common method of structuring information extracted from textual data is using a knowledge model e.g. a thesaurus to organise the information semantically. Creating and managing a knowledge model is already a costly task in terms of human effort, not to mention making it multilingual. Multilingual knowledge modelling is a common problem for both transnational organisations and organisations providing text analytics that want to analyse information in more than one language. Many organisations tend to develop their language resources first in one language often English. When it comes to analysing data sources in other languages, either a lot of effort has to be invested in recreating the same knowledge base in a different language or the data itself has to be translated into the language of the knowledge model. In this paper, we propose an unsupervised method to automatically induce a given thesaurus into another language using only comparable monolingual corpora. The aim of this proposal is to employ cross-lingual word embeddings to map the set of topics in an already-existing English thesaurus into Spanish. With this in mind, we describe different approaches to generate the Spanish thesaurus terms and offer an extrinsic evaluation by using the obtained thesaurus, which covers non-financial topics in a multi-label document classification task, and we compare the results across these approaches.",https://aclanthology.org/2020.computerm-1.3,European Language Resources Association,2020,May,Proceedings of the 6th International Workshop on Computational Terminology,"Quesada Zaragoza, Mart{\'\i}n  and
Sep{\'u}lveda Torres, Lianet  and
Basdevant, J{\'e}r{\^o}me",Translating Knowledge Representations with Monolingual Word Embeddings: the Case of a Thesaurus on Corporate Non-Financial Reporting,,computerm,1216
W18-5903,"['Domain-specific NLP', 'Classification Applications', 'Data Management and Generation']","['Medical and Clinical NLP', 'Data Preparation', 'NLP for News and Media']","['NLP for Mental Health', 'NLP for Social Media']","This paper presents a set of classification experiments for identifying depression in posts gathered from social media platforms. In addition to the data gathered previously by other researchers, we collect additional data from the social media platform Reddit. Our experiments show promising results for identifying depression from social media texts. More importantly, however, we show that the choice of corpora is crucial in identifying depression and can lead to misleading conclusions in case of poor choice of data.",https://aclanthology.org/W18-5903,Association for Computational Linguistics,2018,October,Proceedings of the 2018 {EMNLP} Workshop {SMM}4{H}: The 3rd Social Media Mining for Health Applications Workshop {\&} Shared Task,"Pirina, Inna  and
{\c{C}}{\""o}ltekin, {\c{C}}a{\u{g}}r{\i}",Identifying Depression on Reddit: The Effect of Training Data,10.18653/v1/W18-5903,W18,57
2020.wosp-1.4,"['Learning Paradigms', 'Domain-specific NLP', 'Classification Applications', 'Data Management and Generation']","['NLP for Bibliometrics and Scientometrics', 'Data Augmentation', 'Data Preparation', 'Supervised Learning']",['Citation Analysis'],"Citation parsing, particularly with deep neural networks, suffers from a lack of training data as available datasets typically contain only a few thousand training instances. Manually labelling citation strings is very timeconsuming, hence, synthetically created training data could be a solution. However, as of now, it is unknown if synthetically created reference-strings are suitable to train machine learning algorithms for citation parsing. To find out, we train Grobid, which uses Conditional Random Fields, with a humanlabelled reference strings from 'real' bibliographies and b synthetically created reference strings from the GIANT dataset. We find 1 that both synthetic and organic reference strings are equally suited for training Grobid F1 = 0.74. We additionally find that retraining Grobid has a notable impact on its performance, for both synthetic and real data +30% in F1. Having as many types of labelled fields as possible during training also improves effectiveness, even if these fields are not available in the evaluation data +13.5% F1. We conclude that synthetic data is suitable for training deep citation parsing models. We further suggest that in future evaluations of reference parsing tools, both evaluation data being similar and data being dissimilar to the training data should be used to obtain more meaningful results.",https://aclanthology.org/2020.wosp-1.4,Association for Computational Linguistics,2020,05-Aug,Proceedings of the 8th International Workshop on Mining Scientific Publications,"Grennan, Mark  and
Beel, Joeran","Synthetic vs. Real Reference Strings for Citation Parsing, and the Importance of Re-training and Out-Of-Sample Data for Meaningful Evaluations: Experiments with GROBID, GIANT and CORA",,wosp,1167
2021.naacl-main.315,"['Question Answering (QA)', 'Learning Paradigms', 'Domain-specific NLP', 'Model Architectures']",['Transformer Models'],,"We propose a practical instant question answering QA system on product pages of ecommerce services, where for each user query, relevant community question answer CQA pairs are retrieved. User queries and CQA pairs differ significantly in language characteristics making relevance learning difficult. Our proposed transformer-based model learns a robust relevance function by jointly learning unified syntactic and semantic representations without the need for human labeled data. This is achieved by distantly supervising our model by distilling from predictions of a syntactic matching system on user queries and simultaneously training with CQA pairs. Training with CQA pairs helps our model learning semantic QA relevance and distant supervision enables learning of syntactic features as well as the nuances of user querying language. Additionally, our model encodes queries and candidate responses independently allowing offline candidate embedding generation thereby minimizing the need for real-time transformer model execution. Consequently, our framework is able to scale to large e-commerce QA traffic. Extensive evaluation on user queries shows that our framework significantly outperforms both syntactic and semantic baselines in offline as well as large scale online A/B setups of a popular e-commerce service.",https://aclanthology.org/2021.naacl-main.315,Association for Computational Linguistics,2021,June,Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,"Mittal, Happy  and
Chakrabarti, Aniket  and
Bayar, Belhassen  and
Sharma, Animesh Anant  and
Rasiwasia, Nikhil",Distantly Supervised Transformers For E-Commerce Product QA,10.18653/v1/2021.naacl-main.315,naacl,1499
Q18-1001,"['Learning Paradigms', 'Data Management and Generation', 'Domain-specific NLP']","['Multimodal Learning', 'Data Preparation']",['Annotation Processes'],"In this paper we argue that crime drama exemplified in television programs such as CSI: Crime Scene Investigation is an ideal testbed for approximating real-world natural language understanding and the complex inferences associated with it. We propose to treat crime drama as a new inference task, capitalizing on the fact that each episode poses the same basic question i.e., who committed the crime and naturally provides the answer when the perpetrator is revealed. We develop a new dataset 1 based on CSI episodes, formalize perpetrator identification as a sequence labeling problem, and develop an LSTM-based model which learns from multi-modal data. Experimental results show that an incremental inference strategy is key to making accurate guesses as well as learning from representations fusing textual, visual, and acoustic input.",https://aclanthology.org/Q18-1001,MIT Press,2018,,,"Frermann, Lea  and
Cohen, Shay B.  and
Lapata, Mirella",Whodunnit? Crime Drama as a Case for Natural Language Understanding,10.1162/tacl_a_00001,Q18,713
2020.sltu-1.5,"['Data Management and Generation', 'Low-resource Languages', 'Cross-lingual Application', 'Model Architectures', 'Finite State Machines']","['Data Augmentation', 'Recurrent Neural Networks (RNNs)']",,"We present a method for conducting morphological disambiguation for South Smi, which is an endangered language. Our method uses an FST-based morphological analyzer to produce an ambiguous set of morphological readings for each word in a sentence. These readings are disambiguated with a Bi-RNN model trained on the related North Smi UD Treebank and some synthetically generated South Smi data. The disambiguation is done on the level of morphological tags ignoring word forms and lemmas; this makes it possible to use North Smi training data for South Smi without the need for a bilingual dictionary or aligned word embeddings. Our approach requires only minimal resources for South Smi, which makes it usable and applicable in the contexts of any other endangered language as well.",https://aclanthology.org/2020.sltu-1.5,European Language Resources association,2020,May,Proceedings of the 1st Joint Workshop on Spoken Language Technologies for Under-resourced languages (SLTU) and Collaboration and Computing for Under-Resourced Languages (CCURL),"H{\""a}m{\""a}l{\""a}inen, Mika  and
Wiechetek, Linda",Morphological Disambiguation of South S\'ami with FSTs and Neural Networks,,sltu,537
Q19-1026,"['Evaluation Techniques', 'Data Management and Generation', 'Question Answering (QA)']","['Data Preparation', 'Long Form QA']",['Annotation Processes'],"We present the Natural Questions corpus, a question answering data set. Questions consist of real anonymized, aggregated queries issued to the Google search engine. An annotator is presented with a question along with a Wikipedia page from the top 5 search results, and annotates a long answer typically a paragraph and a short answer one or more entities if present on the page, or marks null if no long/short answer is present. The public release consists of 307,373 training examples with single annotations; 7,830 examples with 5-way annotations for development data; and a further 7,842 examples with 5-way annotated sequestered as test data. We present experiments validating quality of the data. We also describe analysis of 25-way annotations on 302 examples, giving insights into human variability on the annotation task. We introduce robust metrics for the purposes of evaluating question answering systems; demonstrate high human upper bounds on these metrics; and establish baseline results using competitive methods drawn from related literature. *  Project initiation;  Project design;  Data creation;  Model development;  Project support;  Also affiliated with Columbia University, work done at Google;  No longer at Google, work done at Google.",https://aclanthology.org/Q19-1026,MIT Press,2019,,,"Kwiatkowski, Tom  and
Palomaki, Jennimaria  and
Redfield, Olivia  and
Collins, Michael  and
Parikh, Ankur  and
Alberti, Chris  and
Epstein, Danielle  and
Polosukhin, Illia  and
Devlin, Jacob  and
Lee, Kenton  and
Toutanova, Kristina  and
Jones, Llion  and
Kelcey, Matthew  and
Chang, Ming-Wei  and
Dai, Andrew M.  and
Uszkoreit, Jakob  and
Le, Quoc  and
Petrov, Slav",Natural Questions: A Benchmark for Question Answering Research,10.1162/tacl_a_00276,Q19,1118
2022.semeval-1.123,"['Domain-specific NLP', 'Low-resource Languages', 'Classification Applications']","['Sarcasm Detection', 'NLP for News and Media']",['NLP for Social Media'],"This paper describes the systems submitted to iSarcasm shared task. The aim of iSarcasm is to identify the sarcastic contents in Arabic and English text. Our team participated in iSarcasm for the Arabic language. A multi-Layer machine learning based model has been submitted for Arabic sarcasm detection. In this model, a vector space TF-IDF has been used as for feature representation. The submitted system is simple and does not need any external resources. The test results show encouraging results.",https://aclanthology.org/2022.semeval-1.123,Association for Computational Linguistics,2022,July,Proceedings of the 16th International Workshop on Semantic Evaluation (SemEval-2022),"Ashraf, Nsrin  and
Elkazzaz, Fathy  and
Taha, Mohamed  and
Nayel, Hamada  and
Elshishtawy, Tarek",BFCAI at SemEval-2022 Task 6: Multi-Layer Perceptron for Sarcasm Detection in Arabic Texts,10.18653/v1/2022.semeval-1.123,semeval,1381
2021.dravidianlangtech-1.16,"['Data Management and Generation', 'Multilingual NLP', 'Learning Paradigms', 'Low-resource Languages', 'Classification Applications']","['Multimodal Learning', 'Data Preparation', 'Hate and Offensive Speech Detection']",,"The internet has facilitated its user-base with a platform to communicate and express their views without any censorship. On the other hand, this freedom of expression or free speech can be abused by its user or a troll to demean an individual or a group. Demeaning people based on their gender, sexual orientation, religious believes or any other characteristics -trolling-could cause significant distress in the online community. Hence, the content posted by a troll needs to be identified and dealt with before causing any more damage. Amongst all the forms of troll content, memes are most prevalent due to their popularity and ability to propagate across cultures. A troll uses a meme to demean, attack or offend its targetted audience. In this shared task, we provide a resource TamilMemes that could be used to train a system capable of identifying a troll meme in the Tamil language. In our TamilMemes dataset, each meme has been categorized into either a ""troll"" or a ""not troll"" class. Along with the meme images, we also provided the Latin transcripted text from memes. We received ten system submissions from the participants, which were evaluated using the weighted average F1-score. The system with the weighted average F1-score of 0.55 secured the first rank.",https://aclanthology.org/2021.dravidianlangtech-1.16,Association for Computational Linguistics,2021,April,Proceedings of the First Workshop on Speech and Language Technologies for Dravidian Languages,"Suryawanshi, Shardul  and
Chakravarthi, Bharathi Raja",Findings of the Shared Task on Troll Meme Classification in Tamil,,dravidianlangtech,133
2020.cogalex-1.14,"['Low-resource Languages', 'Machine Translation (MT)']",,,"Existing dictionaries may help collocation translation by suggesting associated words in the form of collocations, thesaurus, and example sentences. We propose to enhance them with taskdriven word associations, illustrating the need by a few scenarios and outlining a possible approach based on word embedding. An example is given, using pre-trained word embedding, while more extensive investigation with more refined methods and resources is underway.",https://aclanthology.org/2020.cogalex-1.14,Association for Computational Linguistics,2020,December,Proceedings of the Workshop on the Cognitive Aspects of the Lexicon,"Kwong, Oi Yee",Translating Collocations: The Need for Task-driven Word Associations,,cogalex,1287
2020.nlpcss-1.4,"['Learning Paradigms', 'Domain-specific NLP', 'Classification Applications', 'Data Management and Generation']","['Medical and Clinical NLP', 'Data Preparation', 'Supervised Learning', 'NLP for News and Media']","['Annotation Processes', 'NLP for Mental Health', 'NLP for Social Media']","Individuals recovering from substance use often seek social support emotional and informational on online recovery forums, where they can both write and comment on posts, expressing their struggles and successes. A common challenge in these forums is that certain posts some of which may be support seeking receive no comments. In this work, we use data from two Reddit substance recovery forums: /r/Leaves and /r/OpiatesRecovery, to determine the relationship between the social supports expressed in the titles of posts and the number of comments they receive. We show that the types of social support expressed in post titles that elicit comments vary from one substance use recovery forum to the other.",https://aclanthology.org/2020.nlpcss-1.4,Association for Computational Linguistics,2020,November,Proceedings of the Fourth Workshop on Natural Language Processing and Computational Social Science,"Andy, Anietie  and
Guntuku, Sharath Chandra",Does Social Support Expressed in Post Titles Elicit Comments in Online Substance Use Recovery Forums?,10.18653/v1/2020.nlpcss-1.4,nlpcss,823
2021.spnlp-1.3,['Parsing'],"['Syntactic Parsing', 'Semantic Parsing']",['Dependency Parsing'],"AM dependency parsing is a method for neural semantic graph parsing that exploits the principle of compositionality. While AM dependency parsers have been shown to be fast and accurate across several graphbanks, they require explicit annotations of the compositional tree structures for training. In the past, these were obtained using complex graphbankspecific heuristics written by experts. Here we show how they can instead be trained directly on the graphs with a neural latent-variable model, drastically reducing the amount and complexity of manual heuristics. We demonstrate that our model picks up on several linguistic phenomena on its own and achieves comparable accuracy to supervised training, greatly facilitating the use of AM dependency parsing for new sembanks.",https://aclanthology.org/2021.spnlp-1.3,Association for Computational Linguistics,2021,August,Proceedings of the 5th Workshop on Structured Prediction for NLP (SPNLP 2021),"Groschwitz, Jonas  and
Fowlie, Meaghan  and
Koller, Alexander",Learning compositional structures for semantic graph parsing,10.18653/v1/2021.spnlp-1.3,spnlp,214
N19-1044,"['Machine Translation (MT)', 'Data Management and Generation', 'Low-resource Languages']","['Data Augmentation', 'Neural MT (NMT)']",,"Leveraging user-provided translation to constrain NMT has practical significance. Existing methods can be classified into two main categories, namely the use of placeholder tags for lexicon words and the use of hard constraints during decoding. Both methods can hurt translation fidelity for various reasons. We investigate a data augmentation method, making code-switched training data by replacing source phrases with their target translations. Our method does not change the NMT model or decoding algorithm, allowing the model to learn lexicon translations by copying source-side target words. Extensive experiments show that our method achieves consistent improvements over existing approaches, improving translation of constrained words without hurting unconstrained words.",https://aclanthology.org/N19-1044,Association for Computational Linguistics,2019,June,"Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)","Song, Kai  and
Zhang, Yue  and
Yu, Heng  and
Luo, Weihua  and
Wang, Kun  and
Zhang, Min",Code-Switching for Enhancing NMT with Pre-Specified Translation,10.18653/v1/N19-1044,N19,1326
E17-2033,"['Audio Generation and Processing', 'Dialogue Systems', 'Model Architectures']","['Dialogue State Tracking (DST)', 'Automatic Speech Recognition (ASR)']",,"This paper presents a hybrid dialog state tracker enhanced by trainable Spoken Language Understanding SLU for slotfilling dialog systems. Our architecture is inspired by previously proposed neuralnetwork-based belief-tracking systems. In addition we extended some parts of our modular architecture with differentiable rules to allow end-to-end training. We hypothesize that these rules allow our tracker to generalize better than pure machinelearning based systems. For evaluation we used the Dialog State Tracking Challenge DSTC 2 dataset -a popular belief tracking testbed with dialogs from restaurant information system. To our knowledge, our hybrid tracker sets a new stateof-the-art result in three out of four categories within the DSTC2.",https://aclanthology.org/E17-2033,Association for Computational Linguistics,2017,April,"Proceedings of the 15th Conference of the {E}uropean Chapter of the Association for Computational Linguistics: Volume 2, Short Papers","Vodol{\'a}n, Miroslav  and
Kadlec, Rudolf  and
Kleindienst, Jan",Hybrid Dialog State Tracker with ASR Features,10.18653/v1/e17-2033,E17,589
2022.jeptalnrecital-taln.11,"['Domain-specific NLP', 'Low-resource Languages', 'Classification Applications', 'Data Management and Generation']","['Data Preparation', 'Medical and Clinical NLP']",['Biomedical NLP'],"The purpose of automatic text simplification is to provide a new version of documents that are easier to understand by a given population or easier to process by other NLP applications. However, it is important to know what should be simplified exactly within the documents before the simplification is done. Indeed, even in technical and specialized documents, it is unnecessary to simplify everything but just those segments that present understanding difficulty. Typically, the purpose of complex word identification is to diagnose the difficulty of a given document to detect complex words or passages within it. We propose to address the issue of identifying complex words and passages within biomedical documents in French.",https://aclanthology.org/2022.jeptalnrecital-taln.11,ATALA,2022,6,Actes de la 29e Conf{\'e}rence sur le Traitement Automatique des Langues Naturelles. Volume 1 : conf{\'e}rence principale,"Cheng Sheang, Kim  and
Koptient, Ana{\""\i}s  and
Grabar, Natalia  and
Saggion, Horacio",Identification of complex words and passages in medical documents in French,,jeptalnrecital,1428
2020.emnlp-main.400,"['Question Answering (QA)', 'Information Extraction', 'Model Architectures', 'Knowledge Representation and Reasoning']","['Named Entity Recognition (NER)', 'Open-Domain QA', 'Transformer Models']",,"We focus on the problem of capturing declarative knowledge about entities in the learned parameters of a language model. We introduce a new model-Entities as Experts EAEthat can access distinct memories of the entities mentioned in a piece of text. Unlike previous efforts to integrate entity knowledge into sequence models, EAE's entity representations are learned directly from text. We show that EAE's learned representations capture sufficient knowledge to answer TriviaQA questions such as ""Which Dr. Who villain has been played by Roger Delgado, Anthony Ainley, Eric Roberts?"", outperforming an encodergenerator Transformer model with 10 the parameters. According to the LAMA knowledge probes, EAE contains more factual knowledge than a similarly sized BERT, as well as previous approaches that integrate external sources of entity knowledge. Because EAE associates parameters with specific entities, it only needs to access a fraction of its parameters at inference time, and we show that the correct identification and representation of entities is essential to EAE's performance.",https://aclanthology.org/2020.emnlp-main.400,Association for Computational Linguistics,2020,November,Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),"F{\'e}vry, Thibault  and
Baldini Soares, Livio  and
FitzGerald, Nicholas  and
Choi, Eunsol  and
Kwiatkowski, Tom",Entities as Experts: Sparse Memory Access with Entity Supervision,10.18653/v1/2020.emnlp-main.400,emnlp,893
2021.ranlp-1.41,"['Domain-specific NLP', 'Classification Applications', 'Model Architectures']","['NLP for News and Media', 'Medical and Clinical NLP']","['NLP for Mental Health', 'NLP for Social Media']","Mental health is getting more and more attention recently, depression being a very common illness nowadays, but also other disorders like anxiety, obsessive-compulsive disorders, feeding disorders, autism, or attention-deficit/hyperactivity disorders. The huge amount of data from social media and the recent advances of deep learning models provide valuable means to automatically detecting mental disorders from plain text. In this article, we experiment with state-of-the-art methods on the SMHD mental health conditions dataset from Reddit Cohan et al., 2018 . Our contribution is threefold: using a dataset consisting of more illnesses than most studies, focusing on general text rather than mental health support groups and classification by posts rather than individuals or groups. For the automatic classification of the diseases, we employ three deep learning models: BERT, RoBERTa and XLNET. We double the baseline established by Cohan et al. 2018 , on just a sample of their dataset. We improve the results obtained by Jiang et al.  2020  on post-level classification. The accuracy obtained by the eating disorder classifier is the highest due to the pregnant presence of discussions related to calories, diets, recipes etc., whereas depression had the lowest F1 score, probably because depression is more difficult to identify in linguistic acts. UEFISCDI, project number 108, COTOHILI, within PNCDI III.",https://aclanthology.org/2021.ranlp-1.41,INCOMA Ltd.,2021,September,Proceedings of the International Conference on Recent Advances in Natural Language Processing (RANLP 2021),"Dinu, Anca  and
Moldovan, Andreea-Codrina",Automatic Detection and Classification of Mental Illnesses from General Social Media Texts,10.26615/978-954-452-072-4_041,ranlp,426
2022.acl-long.219,"['Question Answering (QA)', 'Data Management and Generation']",['Data Preparation'],,"We present the Berkeley Crossword Solver, a state-of-the-art approach for automatically solving crossword puzzles. Our system works by generating answer candidates for each crossword clue using neural question answering models and then combines loopy belief propagation with local search to find full puzzle solutions. Compared to existing approaches, our system improves exact puzzle accuracy from 57% to 82% on crosswords from The New York Times and obtains 99.9% letter accuracy on themeless puzzles. Our system also won first place at the top human crossword tournament, which marks the first time that a computer program has surpassed human performance at this event. To facilitate research on question answering and crossword solving, we analyze our system's remaining errors and release a dataset of over six million question-answer pairs.",https://aclanthology.org/2022.acl-long.219,Association for Computational Linguistics,2022,May,Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),"Wallace, Eric  and
Tomlin, Nicholas  and
Xu, Albert  and
Yang, Kevin  and
Pathak, Eshaan  and
Ginsberg, Matthew  and
Klein, Dan",Automated Crossword Solving,10.18653/v1/2022.acl-long.219,acl,385
2021.icnlsp-1.5,"['Information Extraction', 'Classification Applications', 'Model Architectures']","['Transformer Models', 'Sentiment Analysis (SA)']",['Aspect-Based SA (ABSA)'],"Aspect-Based Sentiment Analysis ABSA addresses the problem of extracting sentiments and their targets from opinionated data such as consumer product reviews. Analyzing the language used in a review is a difficult task that requires a deep understanding of the language. In recent years, deep language models, such as BERT, have shown great progress in this regard. In this work, we propose two simple modules called Parallel Aggregation and Hierarchical Aggregation to be utilized on top of BERT for two main ABSA tasks namely Aspect Extraction AE and Aspect Sentiment Classification ASC. With the proposed modules, we show that the intermediate layers of the BERT architecture can be utilized for the enhancement of the model performance 1 .",https://aclanthology.org/2021.icnlsp-1.5,Association for Computational Linguistics,2021,12--13 November,Proceedings of The Fourth International Conference on Natural Language and Speech Processing (ICNLSP 2021),"Karimi, Akbar  and
Rossi, Leonardo  and
Prati, Andrea",Improving BERT Performance for Aspect-Based Sentiment Analysis,10.48550/arxiv.2010.11731,icnlsp,697
2021.maiworkshop-1.10,"['Learning Paradigms', 'Model Architectures']","['Transformer Models', 'Multimodal Learning']",,"Recent progress in natural language processing has led to Transformer architectures becoming the predominant model used for natural language tasks. However, in many realworld datasets, additional modalities are included which the Transformer does not directly leverage. We present Multimodal-Toolkit, 1 an open-source Python package to incorporate text and tabular categorical and numerical data with Transformers for downstream applications. Our toolkit integrates well with Hugging Face's existing API such as tokenization and the model hub 2 which allows easy download of different pre-trained models.",https://aclanthology.org/2021.maiworkshop-1.10,Association for Computational Linguistics,2021,June,Proceedings of the Third Workshop on Multimodal Artificial Intelligence,"Gu, Ken  and
Budhkar, Akshay",A Package for Learning on Tabular and Text Data with Transformers,10.18653/v1/2021.maiworkshop-1.10,maiworkshop,83
2021.metanlp-1.1,['Learning Paradigms'],['Reinforcement Learning'],,"Text-based games can be used to develop taskoriented text agents for accomplishing tasks with high-level language instructions, which has potential applications in domains such as human-robot interaction. Given a text instruction, reinforcement learning is commonly used to train agents to complete the intended task owing to its convenience of learning policies automatically. However, because of the large space of combinatorial text actions, learning a policy network that generates an action word by word with reinforcement learning is challenging. Recent research works show that imitation learning provides an effective way of training a generation-based policy network. However, trained agents with imitation learning are hard to master a wide spectrum of task types or skills, and it is also difficult for them to generalize to new environments. In this paper, we propose a meta-reinforcement learning based method to train text agents through learning-to-explore. In particular, the text agent first explores the environment to gather task-specific information and then adapts the execution policy for solving the task with this information. On the publicly available testbed ALFWorld, we conducted a comparison study with imitation learning and show the superiority of our method.",https://aclanthology.org/2021.metanlp-1.1,Association for Computational Linguistics,2021,August,Proceedings of the 1st Workshop on Meta Learning and Its Applications to Natural Language Processing,"Zhao, Zhenjie  and
Sun, Mingfei  and
Ma, Xiaojuan",Meta-Reinforcement Learning for Mastering Multiple Skills and Generalizing across Environments in Text-based Games,10.18653/v1/2021.metanlp-1.1,metanlp,1339
2021.repl4nlp-1.11,"['Learning Paradigms', 'Classification Applications', 'Model Architectures']","['Transformer Models', 'Transfer Learning']",,"Pretrained language models have served as the backbone for many state-of-the-art NLP results. These models are large and expensive to train. Recent work suggests that continued pretraining on task-specific data is worth the effort as pretraining leads to improved performance on downstream tasks. We explore alternatives to full-scale task-specific pretraining of language models through the use of adapter modules, a parameter-efficient approach to transfer learning. We find that adapter-based pretraining is able to achieve comparable results to task-specific pretraining while using a fraction of the overall trainable parameters. We further explore direct use of adapters without pretraining and find that the direct finetuning performs mostly on par with pretrained adapter models, contradicting previously proposed benefits of continual pretraining in full pretraining fine-tuning strategies. Lastly, we perform an ablation study on task-adaptive pretraining to investigate how different hyperparameter settings can change the effectiveness of the pretraining.",https://aclanthology.org/2021.repl4nlp-1.11,Association for Computational Linguistics,2021,August,Proceedings of the 6th Workshop on Representation Learning for NLP (RepL4NLP-2021),"Kim, Seungwon  and
Shum, Alex  and
Susanj, Nathan  and
Hilgart, Jonathan",Revisiting Pretraining with Adapters,10.18653/v1/2021.repl4nlp-1.11,repl4nlp,725
W19-6626,"['Machine Translation (MT)', 'Domain-specific NLP', 'Low-resource Languages']","['NLP for Finance', 'Neural MT (NMT)']",,"Neural machine translation NMT has set new quality standards in automatic translation, yet its effect on post-editing productivity is still pending thorough investigation. We empirically test how the inclusion of NMT, in addition to domain-specific translation memories and termbases, impacts speed and quality in professional translation of financial texts. We find that even with language pairs that have received little attention in research settings and small amounts of in-domain data for system adaptation, NMT post-editing allows for substantial time savings and leads to equal or slightly better quality.",https://aclanthology.org/W19-6626,European Association for Machine Translation,2019,August,Proceedings of Machine Translation Summit XVII: Research Track,"L{\""a}ubli, Samuel  and
Amrhein, Chantal  and
D{\""u}ggelin, Patrick  and
Gonzalez, Beatriz  and
Zwahlen, Alena  and
Volk, Martin",Post-editing Productivity with Neural Machine Translation: An Empirical Assessment of Speed and Quality in the Banking and Finance Domain,10.48550/arxiv.1906.01685,W19,1424
2020.lr4sshoc-1.7,['Domain-specific NLP'],,,"This paper aims to give some insights on how the European Open Science Cloud EOSC will be able to influence the Social Sciences and Humanities SSH sector, thus paving the way towards innovation. Points of discussion on how the LRs and RIs community can contribute to the revolution in the practice of research areas are provided.",https://aclanthology.org/2020.lr4sshoc-1.7,European Language Resources Association,2020,May,Proceedings of the Workshop about Language Resources for the SSH Cloud,"Castelli, Donatella",EOSC as a game-changer in the Social Sciences and Humanities research activities,,lr4sshoc,1271
2020.findings-emnlp.326,"['Argument Mining', 'Parsing', 'Information Extraction', 'Model Architectures']","['Syntactic Parsing', 'Semantic Parsing', 'Event Extraction']",,"The goal of Event Argument Extraction EAE is to find the role of each entity mention for a given event trigger word. It has been shown in the previous works that the syntactic structures of the sentences are helpful for the deep learning models for EAE. However, a major problem in such prior works is that they fail to exploit the semantic structures of the sentences to induce effective representations for EAE. Consequently, in this work, we propose a novel model for EAE that exploits both syntactic and semantic structures of the sentences with the Graph Transformer Networks GTNs to learn more effective sentence structures for EAE. In addition, we introduce a novel inductive bias based on information bottleneck to improve generalization of the EAE models. Extensive experiments are performed to demonstrate the benefits of the proposed model, leading to state-of-the-art performance for EAE on standard datasets.",https://aclanthology.org/2020.findings-emnlp.326,Association for Computational Linguistics,2020,November,Findings of the Association for Computational Linguistics: EMNLP 2020,"Pouran Ben Veyseh, Amir  and
Nguyen, Tuan Ngo  and
Nguyen, Thien Huu",Graph Transformer Networks with Syntactic and Semantic Structures for Event Argument Extraction,10.18653/v1/2020.findings-emnlp.326,findings,773
2021.sigdial-1.41,"['Evaluation Techniques', 'Discourse Analysis', 'Dialogue Systems']","['Open Domain Dialogue Systems', 'Chatbots']",,"Open-domain chatbots are supposed to converse freely with humans without being restricted to a topic, task or domain. However, the boundaries and/or contents of opendomain conversations are not clear. To clarify the boundaries of ""openness"", we conduct two studies: First, we classify the types of ""speech events"" encountered in a chatbot evaluation data set i.e., Meena by Google and find that these conversations mainly cover the ""small talk"" category and exclude the other speech event categories encountered in real life human-human communication. Second, we conduct a small-scale pilot study to generate online conversations covering a wider range of speech event categories between two humans vs. a human and a state-of-the-art chatbot i.e., Blender by Facebook. A human evaluation of these generated conversations indicates a preference for human-human conversations, since the human-chatbot conversations lack coherence in most speech event categories. Based on these results, we suggest a using the term ""small talk"" instead of ""opendomain"" for the current chatbots which are not that ""open"" in terms of conversational abilities yet, and b revising the evaluation methods to test the chatbot conversations against other speech events.",https://aclanthology.org/2021.sigdial-1.41,Association for Computational Linguistics,2021,July,Proceedings of the 22nd Annual Meeting of the Special Interest Group on Discourse and Dialogue,"Do{\u{g}}ru{\""o}z, A. Seza  and
Skantze, Gabriel",How ``open'' are the conversations with open-domain chatbots? A proposal for Speech Event based evaluation,,sigdial,1292
2020.lt4gov-1.6,"['Embeddings', 'Domain-specific NLP', 'Topic Modeling', 'Data Management and Generation', 'Low-resource Languages']","['NLP for the Legal Domain', 'Data Preparation', 'Word Embeddings']",['Annotation Processes'],"This paper presents work on progress aiming at the development of Legal-ES. Legal-ES is a set of resources for Spanish legal text processing including a large scale corpus with calculated models for word embeddings and topics. The large scale Spanish legal corpus consists of over 2000 million words from open public legislative, jurisprudential and administrative texts representing a variety of sources from international, national and regional entities. The corpus is pre-processed and tokenized. A word embedding is calculated over raw text and over lemmatised texts in addition to some experiments with topic modelling on the legislative subset of the corpus representing the text from the Spanish Official Bulletin of State Boletin Oficial del Estado-BOE. Within the framework of the Workshop on Language Technologies for Government and Public Administration LT4Gov, the present paper showcases how Public Data is a valuable input for developing Language Resources. It fits within the second dimension of the workshop, i.e. PublicData4LRs. Legal-ES is the result of an initiative by the team of the Spanish Plan for the Advancement of Language Technologies Plan TL aiming at developing resources for the HLT community to promote intelligent solutions by industry and academia destined to Public Administration and the Legal Domain.",https://aclanthology.org/2020.lt4gov-1.6,European Language Resources Association,2020,May,Proceedings of the 1st Workshop on Language Technologies for Government and Public Administration (LT4Gov),"Samy, Doaa  and
Arenas-Garc{\'\i}a, Jer{\'o}nimo  and
P{\'e}rez-Fern{\'a}ndez, David",Legal-ES: A Set of Large Scale Resources for Spanish Legal Text Processing,,lt4gov,825
2020.tacl-1.9,"['Biases in NLP', 'Model Architectures']","['Bias Detection', 'Recurrent Neural Networks (RNNs)']",['Long Short-Term Memory (LSTM) Models'],"Learners that are exposed to the same training data might generalize differently due to differing inductive biases. In neural network models, inductive biases could in theory arise from any aspect of the model architecture. We investigate which architectural factors affect the generalization behavior of neural sequence-to-sequence models trained on two syntactic tasks, English question formation and English tense reinflection. For both tasks, the training set is consistent with a generalization based on hierarchical structure and a generalization based on linear order. All architectural factors that we investigated qualitatively affected how models generalized, including factors with no clear connection to hierarchical structure. For example, LSTMs and GRUs displayed qualitatively different inductive biases. However, the only factor that consistently contributed a hierarchical bias across tasks was the use of a treestructured model rather than a model with sequential recurrence, suggesting that humanlike syntactic generalization requires architectural syntactic structure.",https://aclanthology.org/2020.tacl-1.9,MIT Press,2020,,,"McCoy, R. Thomas  and
Frank, Robert  and
Linzen, Tal",Does Syntax Need to Grow on Trees? Sources of Hierarchical Inductive Bias in Sequence-to-Sequence Networks,10.1162/tacl_a_00304,tacl,828
2020.nlposs-1.18,"['Data Management and Generation', 'Learning Paradigms', 'Adversarial Attacks and Robustness']","['Adversarial Learning', 'Data Augmentation']",,"TextAttack is an open-source Python toolkit for adversarial attacks, adversarial training, and data augmentation in NLP. TextAttack unites 15+ papers from the NLP adversarial attack literature into a single framework, with many components reused across attacks. This framework allows both researchers and developers to test and study the weaknesses of their NLP models. To build such an open-source NLP toolkit requires solving some common problems: How do we enable users to supply models from different deep learning frameworks? How can we build tools to support as many different datasets as possible? We share our insights into developing a well-written, well-documented NLP Python framework in hope that they can aid future development of similar packages.",https://aclanthology.org/2020.nlposs-1.18,Association for Computational Linguistics,2020,November,Proceedings of Second Workshop for NLP Open Source Software (NLP-OSS),"Morris, John  and
Yoo, Jin Yong  and
Qi, Yanjun",TextAttack: Lessons learned in designing Python frameworks for NLP,10.18653/v1/2020.nlposs-1.18,nlposs,534
P18-1005,"['Machine Translation (MT)', 'Learning Paradigms', 'Low-resource Languages', 'Model Architectures']","['Unsupervised Learning', 'Neural MT (NMT)']",,"Unsupervised neural machine translation NMT is a recently proposed approach for machine translation which aims to train the model without using any labeled data. The models proposed for unsupervised NMT often use only one shared encoder to map the pairs of sentences from different languages to a shared-latent space, which is weak in keeping the unique and internal characteristics of each language, such as the style, terminology, and sentence structure. To address this issue, we introduce an extension by utilizing two independent encoders but sharing some partial weights which are responsible for extracting high-level representations of the input sentences. Besides, two different generative adversarial networks GANs, namely the local GAN and global GAN, are proposed to enhance the cross-language translation. With this new approach, we achieve significant improvements on English-German, English-French and Chinese-to-English translation tasks.",https://aclanthology.org/P18-1005,Association for Computational Linguistics,2018,July,Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),"Yang, Zhen  and
Chen, Wei  and
Wang, Feng  and
Xu, Bo",Unsupervised Neural Machine Translation with Weight Sharing,10.18653/v1/P18-1005,P18,1041
2022.naacl-main.139,"['Learning Paradigms', 'Low-resource Languages', 'Cross-lingual Application', 'Model Architectures']",,,"Large-scale cross-lingual pre-trained language models xPLMs have shown effectiveness in cross-lingual sequence labeling tasks xSL, such as cross-lingual machine reading comprehension xMRC by transferring knowledge from a high-resource language to low-resource languages. Despite the great success, we draw an empirical observation that there is a training objective gap between pre-training and finetuning stages: e.g., mask language modeling objective requires local understanding of the masked token and the span-extraction objective requires global understanding and reasoning of the input passage/paragraph and question, leading to the discrepancy between pretraining and xMRC. In this paper, we first design a pre-training task tailored for xSL named Cross-lingual Language Informative Span Masking CLISM to eliminate the objective gap in a self-supervised manner. Second, we present ContrAstive-Consistency Regularization CACR, which utilizes contrastive learning to encourage the consistency between representations of input parallel sequences via unsupervised cross-lingual instance-wise training signals during pre-training. By these means, our methods not only bridge the gap between pretrain-finetune, but also enhance PLMs to better capture the alignment between different languages. Extensive experiments prove that our method achieves clearly superior results on multiple xSL benchmarks with limited pretraining data. Our methods also surpass the previous state-of-the-art methods by a large margin in few-shot data settings, where only a few hundred training examples are available.",https://aclanthology.org/2022.naacl-main.139,Association for Computational Linguistics,2022,July,Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,"Chen, Nuo  and
Shou, Linjun  and
Gong, Ming  and
Pei, Jian  and
Jiang, Daxin",Bridging the Gap between Language Models and Cross-Lingual Sequence Labeling,10.18653/v1/2022.naacl-main.139,naacl,819
U17-1016,"['Model Architectures', 'Image and Video Processing']","['Optical Character Recognition (OCR)', 'Recurrent Neural Networks (RNNs)']",['Long Short-Term Memory (LSTM) Models'],"This paper describes the SuperOCR system submitted for the ALTA 2017 shared task, which aims at correcting noisy OCR output for the Trove database. We used heuristic rules and patterns in submitted system and we apply language model to further improve our system. Experiment shows that language model plays an vital role in performance. Surprisingly, a trigram language model outperforms LSTM language model in this task. OCR Post-Correction ALTA 2017 shared task 1 aims at Optical Character Recognition OCR post-correction for Trove database Holley, 2010 2 . OCR extracts text from image, allowing further language analysis. However, OCR is inherently error-prone, in particular for old scanned documents. High quality OCR analysis result benefits downstream NLP task, including named entity recognition NER Mac Kim and Cassidy, 2015 and information extraction Taghva et al., 2006 . In this shared task, given a set of OCR raw-correction pairs, participators are required to build a system to automatically and accurately correct the OCR-ed documents. Our submitted system achieved averaged F1 score 16.82%, which is 2 nd best system. Later, we further improved our submitted system to 20.72% by using context and weighted OCR error information. However, The wining system had achieved averaged F1 score 32.99%, indicating that our system still has large margin to be improved. Our submitted system mainly targeted in a correcting word-level errors, e.g. words with char-",https://aclanthology.org/U17-1016,,2017,December,Proceedings of the Australasian Language Technology Association Workshop 2017,"Wang, Yufei",SuperOCR for ALTA 2017 Shared Task,,U17,304
2020.socialnlp-1.1,"['Biases in NLP', 'Domain-specific NLP', 'Data Management and Generation', 'Classification Applications', 'Discourse Analysis']","['NLP for News and Media', 'Data Preparation', 'Bias Detection']",['Annotation Processes'],"Usage of presuppositions in social media and news discourse can be a powerful way to influence the readers as they usually tend to not examine the truth value of the hidden or indirectly expressed information. Fairclough and Wodak 1997 discuss presupposition at a discourse level where some implicit claims are taken for granted in the explicit meaning of a text or utterance. From the Gricean perspective, the presuppositions of a sentence determine the class of contexts in which the sentence could be felicitously uttered. This paper aims to correlate the type of knowledge presupposed in a news article to the bias present in it. We propose a set of guidelines to identify various kinds of presuppositions in news articles and present a dataset consisting of 1050 articles which are annotated for bias positive, negative or neutral and the magnitude of presupposition. We introduce a supervised classification approach for detecting bias in political news which significantly outperforms the existing systems.",https://aclanthology.org/2020.socialnlp-1.1,Association for Computational Linguistics,2020,July,Proceedings of the Eighth International Workshop on Natural Language Processing for Social Media,"Kameswari, Lalitha  and
Sravani, Dama  and
Mamidi, Radhika",Enhancing Bias Detection in Political News Using Pragmatic Presupposition,10.18653/v1/2020.socialnlp-1.1,socialnlp,900
2022.csrr-1.4,"['Question Answering (QA)', 'Learning Paradigms', 'Image and Video Processing', 'Commonsense Reasoning']","['Multimodal Learning', 'Image Captioning', 'Visual QA (VQA)']",,"Large-scale visual-linguistic pre-training aims to capture the generic representations from multimodal features, which are essential for downstream vision-language tasks. Existing methods mostly focus on learning the semantic connections between visual objects and linguistic content, which tend to be recognitionlevel information and may not be sufficient for commonsensical reasoning tasks like VCR. In this paper, we propose a novel commonsensical vision-language pre-training framework to bridge the gap. We first augment the conventional image-caption pre-training datasets with commonsense inferences from a visuallinguistic GPT-2. To pre-train models on image, caption and commonsense inferences together, we propose two new tasks: masked commonsense modeling MCM and commonsense type prediction CTP. To reduce the shortcut effect between captions and commonsense inferences, we further introduce the domain-wise adaptive masking that dynamically adjusts the masking ratio. Experimental results on downstream tasks, VCR and VQA, show the improvement of our pre-training strategy over previous methods. Human evaluation also validates the relevance, informativeness, and diversity of the generated commonsense inferences. Overall, we demonstrate the potential of incorporating commonsense knowledge into the conventional recognition-level visual-linguistic pre-training.",https://aclanthology.org/2022.csrr-1.4,Association for Computational Linguistics,2022,May,Proceedings of the First Workshop on Commonsense Representation and Reasoning (CSRR 2022),"Wan, Yue  and
Ma, Yueen  and
You, Haoxuan  and
Wang, Zhecan  and
Chang, Shih-Fu",Bridging the Gap between Recognition-level Pre-training and Commonsensical Vision-language Tasks,10.18653/v1/2022.csrr-1.4,csrr,1457
2020.semeval-1.252,"['Domain-specific NLP', 'Classification Applications', 'Low-resource Languages', 'Multilingual NLP', 'Model Architectures']","['NLP for News and Media', 'Hate and Offensive Speech Detection', 'Transformer Models']",['NLP for Social Media'],"With today's proliferation of maliciously intended communication across all social media platforms, finding ways of effectively combating these messages grows increasingly important. We present our submission and results for SemEval-2020 Task 12: Multilingual Offensive Language Identification in Social Media OffensEval 2020 where we participated in offensive tweet classification tasks in English, Arabic, Greek, Turkish and Danish. Our approach included classical machine learning architectures such as support vector machines and logistic regression combined in an ensemble with a multilingual transformer-based model XLM-R. The transformer model is trained on all languages combined in order to create a fully multilingual model which can leverage knowledge between languages. The machine learning model hyperparameters are fine-tuned and the statistically best performing ones included in the final ensemble. We further discuss the results of our model and see that our broad approach provides competitive but not task-winning performance. We also include an error analysis and potential improvements for future work.",https://aclanthology.org/2020.semeval-1.252,International Committee for Computational Linguistics,2020,December,Proceedings of the Fourteenth Workshop on Semantic Evaluation,"Chapman, Kathryn  and
Bernhard, Johannes  and
Klakow, Dietrich",CoLi at UdS at SemEval-2020 Task 12: Offensive Tweet Detection with Ensembling,10.18653/v1/2020.semeval-1.252,semeval,695
P16-1087,"['Information Extraction', 'Model Architectures']","['Named Entity Recognition (NER)', 'Recurrent Neural Networks (RNNs)', 'Relation Extraction']",['Long Short-Term Memory (LSTM) Models'],"We investigate the use of deep bidirectional LSTMs for joint extraction of opinion entities and the IS-FROM and IS-ABOUT relations that connect them -the first such attempt using a deep learning approach. Perhaps surprisingly, we find that standard LSTMs are not competitive with a state-of-the-art CRF+ILP joint inference approach Yang and Cardie, 2013 to opinion entities extraction, performing below even the standalone sequencetagging CRF. Incorporating sentence-level and a novel relation-level optimization, however, allows the LSTM to identify opinion relations and to perform within 1-3% of the state-of-the-art joint model for opinion entities and the IS-FROM relation; and to perform as well as the state-of-theart for the IS-ABOUT relation -all without access to opinion lexicons, parsers and other preprocessing components required for the feature-rich CRF+ILP approach.",https://aclanthology.org/P16-1087,Association for Computational Linguistics,2016,August,Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),"Katiyar, Arzoo  and
Cardie, Claire",Investigating LSTMs for Joint Extraction of Opinion Entities and Relations,10.18653/v1/P16-1087,P16,1196
2020.sltu-1.48,"['Audio Generation and Processing', 'Domain-specific NLP', 'Dialogue Systems', 'Data Management and Generation', 'Information Retrieval', 'Low-resource Languages']","['Search Engines', 'Chatbots', 'NLP for News and Media']",['NLP for Social Media'],"Despite recent advances in natural language processing and other language technology, the application of such technology to language documentation and conservation has been limited. In August 2019, a workshop was held at Carnegie Mellon University in Pittsburgh to attempt to bring together language community members, documentary linguists, and technologists to discuss how to bridge this gap and create prototypes of novel and practical language revitalization technologies. This paper reports the results of this workshop, including issues discussed, and various conceived and implemented technologies for nine languages: Arapaho,",https://aclanthology.org/2020.sltu-1.48,European Language Resources association,2020,May,Proceedings of the 1st Joint Workshop on Spoken Language Technologies for Under-resourced languages (SLTU) and Collaboration and Computing for Under-Resourced Languages (CCURL),"Neubig, Graham  and
Rijhwani, Shruti  and
Palmer, Alexis  and
MacKenzie, Jordan  and
Cruz, Hilaria  and
Li, Xinjian  and
Lee, Matthew  and
Chaudhary, Aditi  and
Gessler, Luke  and
Abney, Steven  and
Hayati, Shirley Anugrah  and
Anastasopoulos, Antonios  and
Zamaraeva, Olga  and
Prud{'}hommeaux, Emily  and
Child, Jennette  and
Child, Sara  and
Knowles, Rebecca  and
Moeller, Sarah  and
Micher, Jeffrey  and
Li, Yiyuan  and
Zink, Sydney  and
Xia, Mengzhou  and
Sharma, Roshan S  and
Littell, Patrick",A Summary of the First Workshop on Language Technology for Language Documentation and Revitalization,10.48550/arxiv.2004.13203,sltu,634
2020.coling-industry.6,"['Evaluation Techniques', 'Data Management and Generation', 'Model Architectures', 'Text Generation']","['Transformer Models', 'Data Preparation', 'Paraphrase and Rephrase Generation', 'Text Style Transfer']",,"The topic of this paper is neural multi-task training for text style transfer. We present an efficient method for neutral-to-style transformation using the transformer framework. We demonstrate how to prepare a robust model utilizing large paraphrases corpora together with a small parallel style transfer corpus. We study how much style transfer data is needed for a model on the example of two transformations: neutral-to-cute on internal corpus and modern-to-antique on publicly available Bible corpora. Additionally, we propose a synthetic measure for the automatic evaluation of style transfer models. We hope our research is a step towards replacing common but limited rule-based style transfer systems by more flexible machine learning models for both public and commercial usage.",https://aclanthology.org/2020.coling-industry.6,International Committee on Computational Linguistics,2020,December,Proceedings of the 28th International Conference on Computational Linguistics: Industry Track,"Bujnowski, Pawel  and
Ryzhova, Kseniia  and
Choi, Hyungtak  and
Witkowska, Katarzyna  and
Piersa, Jaroslaw  and
Krumholc, Tymoteusz  and
Beksa, Katarzyna",An Empirical Study on Multi-Task Learning for Text Style Transfer and Paraphrase Generation,10.18653/v1/2020.coling-industry.6,coling,1338
2022.deeplo-1.11,['Information Extraction'],['Event Extraction'],,"Supervised event extraction models require a substantial amount of training data to perform well. However, event annotation requires a lot of human effort and costs much time, which limits the application of existing supervised approaches to new event types. In order to reduce manual labor and shorten the time to build an event extraction system for an arbitrary event ontology, we present a new framework to train such systems much more efficiently without large annotations. Our event trigger labeling model uses a weak supervision approach, which only requires a set of keywords, a small number of examples and an unlabeled corpus, on which our approach automatically collects weakly supervised annotations. Our argument role labeling component performs zero-shot learning, which only requires the names of the argument roles of new event types. The source codes of our event trigger detection 1 and event argument extraction 2 models are publicly available for research purposes. We also release a dockerized system connecting the two models into an unified event extraction pipeline 3 .",https://aclanthology.org/2022.deeplo-1.11,Association for Computational Linguistics,2022,July,Proceedings of the Third Workshop on Deep Learning for Low-Resource Natural Language Processing,"Yu, Pengfei  and
Zhang, Zixuan  and
Voss, Clare  and
May, Jonathan  and
Ji, Heng",Building an Event Extractor with Only a Few Examples,10.18653/v1/2022.deeplo-1.11,deeplo,200
W18-5905,"['Domain-specific NLP', 'Data Management and Generation']","['Data Preparation', 'NLP for News and Media', 'Medical and Clinical NLP']",['NLP for Social Media'],"Previous research has linked psychological and social variables to physical health. At the same time, psychological and social variables have been successfully predicted from the language used by individuals in social media. In this paper, we conduct an initial exploratory study linking these two areas. Using the social media platform of Twitter, we identify users self-reporting symptoms that are descriptive of influenza-like illness ILI. We analyze the tweets of those users in the periods before, during, and after the reported symptoms, exploring emotional, cognitive, and structural components of language. We observe a post-ILI increase in social activity and cognitive processes, possibly supporting previous offline findings linking more active social activities and stronger cognitive coping skills to a better immune status.",https://aclanthology.org/W18-5905,Association for Computational Linguistics,2018,October,Proceedings of the 2018 {EMNLP} Workshop {SMM}4{H}: The 3rd Social Media Mining for Health Applications Workshop {\&} Shared Task,"Flekova, Lucie  and
Lampos, Vasileios  and
Cox, Ingemar","Changes in Psycholinguistic Attributes of Social Media Users Before, During, and After Self-Reported Influenza Symptoms",10.18653/v1/W18-5905,W18,575
2020.textgraphs-1.10,['Question Answering (QA)'],['Knowledge Base QA'],,"The 2020 Shared Task on Multi-Hop Inference for Explanation Regeneration tasks participants with regenerating large detailed multi-fact explanations for standardized science exam questions. Given a question, correct answer, and knowledge base, models must rank each fact in the knowledge base such that facts most likely to appear in the explanation are ranked highest. Explanations consist of an average of 6 and as many as 16 facts that span both core scientific knowledge and world knowledge, and form an explicit lexically-connected ""explanation graph"" describing how the facts interrelate. In this second iteration of the explanation regeneration shared task, participants are supplied with more than double the training and evaluation data of the first shared task, as well as a knowledge base nearly double in size, both of which expand into more challenging scientific topics that increase the difficulty of the task. In total 10 teams participated, and 5 teams submitted system description papers. The best-performing teams significantly increased state-of-the-art performance both in terms of ranking mean average precision and inference speed on this challenge task.",https://aclanthology.org/2020.textgraphs-1.10,Association for Computational Linguistics,2020,December,Proceedings of the Graph-based Methods for Natural Language Processing (TextGraphs),"Jansen, Peter  and
Ustalov, Dmitry",TextGraphs 2020 Shared Task on Multi-Hop Inference for Explanation Regeneration,10.18653/v1/2020.textgraphs-1.10,textgraphs,358
D17-1038,['Learning Paradigms'],['Transfer Learning'],,"Domain similarity measures can be used to gauge adaptability and select suitable data for transfer learning, but existing approaches define ad hoc measures that are deemed suitable for respective tasks. Inspired by work on curriculum learning, we propose to learn data selection measures using Bayesian Optimization and evaluate them across models, domains and tasks. Our learned measures outperform existing domain similarity measures significantly on three tasks: sentiment analysis, partof-speech tagging, and parsing. We show the importance of complementing similarity with diversity, and that learned measures are-to some degree-transferable across models, domains, and even tasks.",https://aclanthology.org/D17-1038,Association for Computational Linguistics,2017,September,Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing,"Ruder, Sebastian  and
Plank, Barbara",Learning to select data for transfer learning with Bayesian Optimization,10.18653/v1/D17-1038,D17,927
2021.teachingnlp-1.13,['Domain-specific NLP'],,,"In this article, we show and discuss our experience in applying the flipped classroom method for teaching Conditional Random Fields in a Natural Language Processing course. We present the activities that we developed together with their relationship to a cognitive complexity model Bloom's taxonomy. After this, we provide our own reflections and expectations of the model itself. Based on the evaluation got from students, it seems that students learn about the topic and also that the method is rewarding for some students. Additionally, we discuss some shortcomings and we propose possible solutions to them. We conclude the paper with some possible future work.",https://aclanthology.org/2021.teachingnlp-1.13,Association for Computational Linguistics,2021,June,Proceedings of the Fifth Workshop on Teaching NLP,"Agirrezabal, Manex",The Flipped Classroom model for teaching Conditional Random Fields in an NLP course,10.18653/v1/2021.teachingnlp-1.13,teachingnlp,939
K16-2021,"['Learning Paradigms', 'Parsing', 'Classification Applications']","['Discourse Parsing', 'Supervised Learning']",,"This paper describes our system for the CoNLL-2016 Shared Task on Shallow Discourse Parsing on English. We adopt a cascaded framework consisting of nine components, among which six are casted as sequence labeling tasks and the remaining three are treated as classification problems. All our sequence labeling and classification models are implemented based on linear models with averaged perceptron training. Our feature sets are mostly borrowed from previous works. The main focus of our effort is to recall cases when Arg1 locates at sentences far before the connective phrase, with some yet limited success.",https://aclanthology.org/K16-2021,Association for Computational Linguistics,2016,August,Proceedings of the {C}o{NLL}-16 shared task,"Fan, Ziwei  and
Li, Zhenghua  and
Zhang, Min",Finding Arguments as Sequence Labeling in Discourse Parsing,10.18653/v1/K16-2021,K16,1204
2021.naloma-1.3,"['Model Architectures', 'Commonsense Reasoning']",['Recurrent Neural Networks (RNNs)'],['Long Short-Term Memory (LSTM) Models'],"Many state-of-art neural models designed for monotonicity reasoning perform poorly on downward inference. To address this shortcoming, we developed an attentive treestructured neural network. It consists of a treebased long-short-term-memory network Tree-LSTM with soft attention. It is designed to model the syntactic parse tree information from the sentence pair of a reasoning task. A self-attentive aggregator is used for aligning the representations of the premise and the hypothesis. We present our model and evaluate it using the Monotonicity Entailment Dataset MED. We show and attempt to explain that our model outperforms existing models on MED.",https://aclanthology.org/2021.naloma-1.3,Association for Computational Linguistics,2021,June,Proceedings of the 1st and 2nd Workshops on Natural Logic Meets Machine Learning (NALOMA),"Chen, Zeming",Attentive Tree-structured Network for Monotonicity Reasoning,,naloma,1029
2022.nlp4convai-1.7,"['Question Answering (QA)', 'Data Management and Generation', 'Dialogue Systems']","['Data Preparation', 'Open-Domain QA']",,"In conversational QA, models have to leverage information in previous turns to answer upcoming questions. Current approaches, such as Question Rewriting, struggle to extract relevant information as the conversation unwinds. We introduce the Common Ground CG, an approach to accumulate conversational information as it emerges and select the relevant information at every turn. We show that CG offers a more efficient and human-like way to exploit conversational information compared to existing approaches, leading to improvements on Open Domain Conversational QA.",https://aclanthology.org/2022.nlp4convai-1.7,Association for Computational Linguistics,2022,May,Proceedings of the 4th Workshop on NLP for Conversational AI,"Del Tredici, Marco  and
Shen, Xiaoyu  and
Barlacchi, Gianni  and
Byrne, Bill  and
Gispert, Adri{\`a} de",From Rewriting to Remembering: Common Ground for Conversational QA Models,10.18653/v1/2022.nlp4convai-1.7,nlp4convai,427
2020.ngt-1.13,"['Machine Translation (MT)', 'Learning Paradigms', 'Data Management and Generation', 'Low-resource Languages']","['Data Augmentation', 'Neural MT (NMT)', 'Supervised Learning']",,"In this paper, we introduce a system built for the Duolingo Simultaneous Translation And Paraphrase for Language Education STA-PLE shared task at the 4th Workshop on Neural Generation and Translation WNGT 2020. We participated in the English-to-Japanese track with a Transformer model pretrained on the JParaCrawl corpus and finetuned in two steps on the JESC corpus and then the smaller Duolingo training corpus. First, during training, we find it is essential to deliberately expose the model to higher-quality translations more often during training for optimal translation performance. For inference, encouraging a small amount of diversity with Diverse Beam Search to improve translation coverage yielded marginal improvement over regular Beam Search. Finally, using an auxiliary filtering model to filter out unlikely candidates from Beam Search improves performance further. We achieve a weighted F1 score of 27.56% on our own test set, outperforming the STAPLE AWS translations baseline score of 4.31%.",https://aclanthology.org/2020.ngt-1.13,Association for Computational Linguistics,2020,July,Proceedings of the Fourth Workshop on Neural Generation and Translation,"Yang, Michael  and
Liu, Yixin  and
Mayuranath, Rahul",Training and Inference Methods for High-Coverage Neural Machine Translation,10.18653/v1/2020.ngt-1.13,ngt,1034
2021.naloma-1.2,"['Domain-specific NLP', 'Information Extraction']",['Medical and Clinical NLP'],,"Logical Observation Identifiers Names and Codes LOINC is a standard set of codes that enable clinicians to communicate about medical tests. Laboratories depend on LOINC to identify what tests a doctor orders for a patient. However, clinicians often use sitespecific, custom codes in their medical records systems that can include shorthand, spelling mistakes, and invented acronyms. Software solutions must map from these custom codes to the LOINC standard to support data interoperability. A key challenge is that LOINC is comprised of six elements. Mapping requires not only extracting those elements, but also combining them according to LOINC logic. We found that character-based deep learning excels at extracting LOINC elements while logicbased methods are more effective for combining those elements into complete LOINC values. In this paper, we present an ensemble of machine learning and logic that is currently used in several medical facilities to map from custom codes to standard LOINC values.",https://aclanthology.org/2021.naloma-1.2,Association for Computational Linguistics,2021,June,Proceedings of the 1st and 2nd Workshops on Natural Logic Meets Machine Learning (NALOMA),"Langton, John  and
Srihasam, Krishna",Applied Medical Code Mapping with Character-based Deep Learning Models and Word-based Logic,,naloma,1156
D19-5309,"['Learning Paradigms', 'Knowledge Representation and Reasoning', 'Model Architectures']","['Multihop Reasoning', 'Supervised Learning']",,"While automated question answering systems are increasingly able to retrieve answers to natural language questions, their ability to generate detailed human-readable explanations for their answers is still quite limited. The Shared Task on Multi-Hop Inference for Explanation Regeneration tasks participants with regenerating detailed gold explanations for standardized elementary science exam questions by selecting facts from a knowledge base of semistructured tables. Each explanation contains between 1 and 16 interconnected facts that form an ""explanation graph"" spanning core scientific knowledge and detailed world knowledge. It is expected that successfully combining these facts to generate detailed explanations will require advancing methods in multihop inference and information combination, and will make use of the supervised training data provided by the WorldTree explanation corpus. The top-performing system achieved a mean average precision MAP of 0.56, substantially advancing the state-of-the-art over a baseline information retrieval model. Detailed extended analyses of all submitted systems showed large relative improvements in accessing the most challenging multi-hop inference problems, while absolute performance remains low, highlighting the difficulty of generating detailed explanations through multihop reasoning.",https://aclanthology.org/D19-5309,Association for Computational Linguistics,2019,November,Proceedings of the Thirteenth Workshop on Graph-Based Methods for Natural Language Processing (TextGraphs-13),"Jansen, Peter  and
Ustalov, Dmitry",TextGraphs 2019 Shared Task on Multi-Hop Inference for Explanation Regeneration,10.18653/v1/D19-5309,D19,1496
R19-1001,"['Data Management and Generation', 'Information Extraction', 'Image and Video Processing', 'Classification Applications', 'Model Architectures']","['Data Preparation', 'Document Layout Analysis (DLA)']",,"In this paper, we present a relationship extraction based methodology for table structure recognition in PDF documents. The proposed deep learning-based method takes a bottom-up approach to table recognition in PDF documents. We outline the shortcomings of conventional approaches based on heuristics and machine learningbased top-down approaches. In this work, we explain how the task of table structure recognition can be modeled as a cell relationship extraction task and the importance of the bottom-up approach in recognizing the table cells. We use Multilayer Feedforward Neural Network for table structure recognition and compare the results of three feature sets. To gauge the performance of the proposed method, we prepared a training dataset using 250 tables in PDF documents, carefully selecting the table structures that are most commonly found in the documents. Our model achieves an overall accuracy of 97.95% and an F1-Score of 92.62% on the test dataset.",https://aclanthology.org/R19-1001,INCOMA Ltd.,2019,September,Proceedings of the International Conference on Recent Advances in Natural Language Processing (RANLP 2019),"Adiga, Darshan  and
Bhat, Shabir Ahmad  and
Shah, Muzaffar Bashir  and
Vyeth, Viveka","Table Structure Recognition Based on Cell Relationship, a Bottom-Up Approach",10.26615/978-954-452-056-4_001,R19,1485
2021.nuse-1.6,"['Automatic Text Summarization', 'Model Architectures']","['Transformer Models', 'Extractive Text Summarization']",,"Screenplay summarization is the task of extracting informative scenes from a screenplay. The screenplay contains turning point TP events that change the story direction and thus define the story structure decisively. Accordingly, this task can be defined as the TP identification task. We suggest using dialogue information, one attribute of screenplays, motivated by previous work that discovered that TPs have a relation with dialogues appearing in screenplays. To teach a model this characteristic, we add a dialogue feature to the input embedding. Moreover, in an attempt to improve the model architecture of previous studies, we replace LSTM with Transformer. We observed that the model can better identify TPs in a screenplay by using dialogue information and that a model adopting Transformer outperforms LSTM-based models.",https://aclanthology.org/2021.nuse-1.6,Association for Computational Linguistics,2021,June,Proceedings of the Third Workshop on Narrative Understanding,"Lee, Myungji  and
Kwon, Hongseok  and
Shin, Jaehun  and
Lee, WonKee  and
Jung, Baikjin  and
Lee, Jong-Hyeok",Transformer-based Screenplay Summarization Using Augmented Learning Representation with Dialogue Information,10.18653/v1/2021.nuse-1.6,nuse,869
2020.coling-main.477,"['Domain-specific NLP', 'Data Management and Generation', 'Model Architectures', 'Classification Applications']","['NLP for News and Media', 'Misinformation Detection', 'Data Analysis']",['Fake News Detection'],"Cognitive and social traits of individuals are reflected in language use. Moreover, individuals who are prone to spread fake news online often share common traits. Building on these ideas, we introduce a model that creates representations of individuals on social media based only on the language they produce, and use them to detect fake news. We show that language-based user representations are beneficial for this task. We also present an extended analysis of the language of fake news spreaders, showing that its main features are mostly domain independent and consistent across two English datasets. Finally, we exploit the relation between language use and connections in the social graph to assess the presence of the Echo Chamber effect in our data.",https://aclanthology.org/2020.coling-main.477,International Committee on Computational Linguistics,2020,December,Proceedings of the 28th International Conference on Computational Linguistics,"Del Tredici, Marco  and
Fern{\'a}ndez, Raquel",Words are the Window to the Soul: Language-based User Representations for Fake News Detection,10.18653/v1/2020.coling-main.477,coling,1372
R19-1156,"['Domain-specific NLP', 'Parsing', 'Low-resource Languages', 'Finite State Machines']","['Morphological Parsing', 'NLP for News and Media']",,"In this paper, we present a two-level morphological analyzer for Turkish which consists of five main components: finite state transducer, rule engine for suffixation, lexicon, trie data structure, and LRU cache. We use Java language to implement finite state machine logic and rule engine, Xml language to describe the finite state transducer rules of the Turkish language, which makes the morphological analyzer both easily extendible and easily applicable to other languages. Empowered with a comprehensive lexicon of 54,000 bare-forms including 19,000 proper nouns, our morphological analyzer is amongst the most reliable analyzers produced so far. The analyzer is compared with Turkish morphological analyzers in the literature. By using LRU cache and a trie data structure, the system can analyze 100,000 words per second, which enables users to analyze huge corpora in a few hours.",https://aclanthology.org/R19-1156,INCOMA Ltd.,2019,September,Proceedings of the International Conference on Recent Advances in Natural Language Processing (RANLP 2019),"Y{\i}ld{\i}z, Olcay Taner  and
Avar, Beg{\""u}m  and
Ercan, G{\""o}khan","An Open, Extendible, and Fast Turkish Morphological Analyzer",10.26615/978-954-452-056-4_156,R19,1319
Y17-1037,"['Low-resource Languages', 'Classification Applications']",['Plagiarism Detection'],,"Arabic plagiarism detection is a difficult task because of the great richness of Arabic language characteristics of which it is a productive, derivational and inflectional language, on the one hand, and a word can has more than one lexical category in different contexts allows us to have different meanings of the word what changes the meaning of the sentence, on the other hand. In this context, Arabic paraphrase identification allows quantifying how much a suspect Arabic text and source Arabic text are similar based on their contexts. In this paper, we proposed a semantic similarity approach for paraphrase identification in Arabic texts by combining different techniques of Natural Language Processing NLP, such as: Term Frequency-Inverse Document Frequency TF-IDF technique to improve the identification of words that are highly descriptive in each sentence; and distributed word vector representations using word2vec algorithm to reduce computational complexity and to optimize the probability of predicting words in the context given the current center word, which they would be subsequently used to generate a sentence vector representations and after applying a similarity measurement operation based on different metrics of comparison, such as: Cosine Similarity and Euclidean Distance. Finally, our proposed approach was evaluated on the Open Source Arabic Corpus OSAC and obtained a promising rate.",https://aclanthology.org/Y17-1037,The National University (Phillippines),2017,November,"Proceedings of the 31st Pacific Asia Conference on Language, Information and Computation","Mahmoud, Adnen  and
Zrigui, Mounir",Semantic Similarity Analysis for Paraphrase Identification in Arabic Texts,,Y17,1293
2020.vlsp-1.3,"['Information Extraction', 'Learning Paradigms', 'Low-resource Languages', 'Model Architectures']","['Transformer Models', 'Relation Extraction']",,"In this paper, we present an empirical study of using pre-trained BERT models for the relation extraction task at the VLSP 2020 Evaluation Campaign. We applied two state-of-theart BERT-based models: R-BERT and BERT model with entity starts. For each model, we compared two pre-trained BERT models: FPTAI/vibert and NlpHUST/vibert4news. We found that NlpHUST/vibert4news model significantly outperforms FPTAI/vibert for the Vietnamese relation extraction task. Finally, we proposed an ensemble model that combines R-BERT and BERT with entity starts. Our proposed ensemble model slightly improved against two single models on the development data and the test data provided by the task organizers.",https://aclanthology.org/2020.vlsp-1.3,Association for Computational Lingustics,2020,December,Proceedings of the 7th International Workshop on Vietnamese Language and Speech Processing,"Pham, Minh Quang Nhat",An Empirical Study of Using Pre-trained BERT Models for Vietnamese Relation Extraction Task at VLSP 2020,10.48550/arxiv.2012.10275,vlsp,333
2022.ltedi-1.57,"['Domain-specific NLP', 'Data Management and Generation', 'Multilingual NLP', 'Low-resource Languages', 'Classification Applications', 'Model Architectures']","['Data Preparation', 'Hate and Offensive Speech Detection', 'NLP for News and Media']",['NLP for Social Media'],"Homophobia and Transphobia Detection is the task of identifying homophobia, transphobia, and non-anti-LGBT+ content from the given corpus. Homophobia and transphobia are both toxic languages directed at LGBTQ+ individuals that are described as hate speech. This paper summarizes our findings on the ""Homophobia and Transphobia Detection in social media comments"" shared task held at LT-EDI 2022 -ACL 2022 1 . This shared task focused on three sub-tasks for Tamil, English, and Tamil-English code-mixed languages. It received 10 systems for Tamil, 13 systems for English, and 11 systems for Tamil-English. The best systems for Tamil, English, and Tamil-English scored 0.570, 0.870, and 0.610, respectively, on average macro F1-score.",https://aclanthology.org/2022.ltedi-1.57,Association for Computational Linguistics,2022,May,"Proceedings of the Second Workshop on Language Technology for Equality, Diversity and Inclusion","Chakravarthi, Bharathi Raja  and
Priyadharshini, Ruba  and
Durairaj, Thenmozhi  and
McCrae, John  and
Buitelaar, Paul  and
Kumaresan, Prasanna  and
Ponnusamy, Rahul",Overview of The Shared Task on Homophobia and Transphobia Detection in Social Media Comments,10.18653/v1/2022.ltedi-1.57,ltedi,75
2020.coling-main.422,"['Evaluation Techniques', 'Data Management and Generation']","['Data Preparation', 'Data Analysis']",['Annotation Processes'],"Semantic annotation tasks contain ambiguity and vagueness and require varying degrees of world knowledge. Disagreement is an important indication of these phenomena. Most traditional evaluation methods, however, critically hinge upon the notion of inter-annotator agreement. While alternative frameworks have been proposed, they do not move beyond agreement as the most important indicator of quality. Critically, evaluations usually do not distinguish between instances in which agreement is expected and instances in which disagreement is not only valid but desired because it captures the linguistic and cognitive phenomena in the data. We attempt to overcome these limitations using the example of a dataset that provides semantic representations for diagnostic experiments on language models. Ambiguity, vagueness, and difficulty are not only highly relevant for this use-case, but also play an important role in other types of semantic annotation tasks. We establish an additional, agreement-independent quality metric based on answer-coherence and evaluate it in comparison to existing metrics. We compare against a gold standard and evaluate on expected disagreement. Despite generally low agreement, annotations follow expected behavior and have high accuracy when selected based on coherence. We show that combining different quality metrics enables a more comprehensive evaluation than relying exclusively on agreement.",https://aclanthology.org/2020.coling-main.422,International Committee on Computational Linguistics,2020,December,Proceedings of the 28th International Conference on Computational Linguistics,"Sommerauer, Pia  and
Fokkens, Antske  and
Vossen, Piek",Would you describe a leopard as yellow? Evaluating crowd-annotations with justified and informative disagreement,10.18653/v1/2020.coling-main.422,coling,1327
2020.loresmt-1.7,"['Learning Paradigms', 'Multilingual NLP', 'Low-resource Languages', 'Machine Translation (MT)']",['Neural MT (NMT)'],,"Standard neural machine translation NMT allows a model to perform translation between a pair of languages. Multilingual neural machine translation NMT, on the other hand, allows a model to perform translation between several language pairs, even between language pairs for which no sentences pair has been seen during training zero-shot translation. This paper presents experiments with zero-shot translation on low resource Indian languages with a very small amount of data for each language pair. We first report results on balanced data over all considered language pairs. We then expand our experiments for additional three rounds by increasing the training data with 2,000 sentence pairs in each round for some of the language pairs. We obtain an increase in translation accuracy with its balanced data settings score multiplied by 7 for Manipuri to Hindi during Round-III of zeroshot translation.",https://aclanthology.org/2020.loresmt-1.7,Association for Computational Linguistics,2020,December,Proceedings of the 3rd Workshop on Technologies for MT of Low Resource Languages,"Huidrom, Rudali  and
Lepage, Yves",Zero-shot translation among Indian languages,10.18653/v1/2020.loresmt-1.7,loresmt,567
2021.scil-1.29,"['Multilingual NLP', 'Parsing', 'Low-resource Languages', 'Data Management and Generation']","['Syntactic Parsing', 'Data Analysis']",['Constituency Parsing'],"Constituency parsing is generally evaluated superficially, particularly in a multiple language setting, with only F-scores being reported. As new state-of-the-art chart-based parsers have resulted in a transition from traditional PCFG-based grammars to span-based approaches Stern et al., 2017; Gaddy et al., 2018 , we do not have a good understanding of how such fundamentally different approaches interact with various treebanks as results show improvements across treebanks Kitaev and Klein, 2018, but it is unclear what influence annotation schemes have on various treebank performance Kitaev et al., 2019 . In particular, a span-based parser's capability of creating novel rules is an unknown factor. We perform an analysis of how span-based parsing performs across 11 treebanks in order to examine the overall behavior of this parsing approach and the effect of the treebanks' specific annotations on results. We find that the parser tends to prefer flatter trees, but the approach works well because it is robust enough to adapt to differences in annotation schemes across treebanks and languages.",https://aclanthology.org/2021.scil-1.29,Association for Computational Linguistics,2021,February,Proceedings of the Society for Computation in Linguistics 2021,"Dakota, Daniel  and
K{\""u}bler, Sandra",What's in a Span? Evaluating the Creativity of a Span-Based Neural Constituency Parser,,scil,897
2021.case-1.9,"['Learning Paradigms', 'Information Extraction']","['Transfer Learning', 'Event Extraction']",,"Incidents in industries have huge social and political impact and minimizing the consequent damage has been a high priority. However, automated analysis of repositories of incident reports has remained a challenge. In this paper, we focus on automatically extracting events from incident reports. Due to absence of event annotated datasets for industrial incidents we employ a transfer learning based approach which is shown to outperform several baselines. We further provide detailed analysis regarding effect of increase in pre-training data and provide explainability of why pre-training improves the performance.",https://aclanthology.org/2021.case-1.9,Association for Computational Linguistics,2021,August,Proceedings of the 4th Workshop on Challenges and Applications of Automated Extraction of Socio-political Events from Text (CASE 2021),"Ramrakhiyani, Nitin  and
Hingmire, Swapnil  and
Patil, Sangameshwar  and
Kumar, Alok  and
Palshikar, Girish",Extracting Events from Industrial Incident Reports,10.18653/v1/2021.case-1.9,case,1442
N16-1121,"['Learning Paradigms', 'Parsing', 'Low-resource Languages', 'Cross-lingual Application']","['Transfer Learning', 'Syntactic Parsing']",['Dependency Parsing'],"In this paper, we present a straightforward strategy for transferring dependency parsers across languages. The proposed method learns a parser from partially annotated data obtained through the projection of annotations across unambiguous word alignments. It does not rely on any modeling of the reliability of dependency and/or alignment links and is therefore easy to implement and parameter free. Experiments on six languages show that our method is at par with recent algorithmically demanding methods, at a much cheaper computational cost. It can thus serve as a fair baseline for transferring dependencies across languages with the use of parallel corpora.",https://aclanthology.org/N16-1121,Association for Computational Linguistics,2016,June,Proceedings of the 2016 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies,"Lacroix, Oph{\'e}lie  and
Aufrant, Lauriane  and
Wisniewski, Guillaume  and
Yvon, Fran{\c{c}}ois",Frustratingly Easy Cross-Lingual Transfer for Transition-Based Dependency Parsing,10.18653/v1/N16-1121,N16,346
S16-1019,"['Learning Paradigms', 'Domain-specific NLP', 'Classification Applications', 'Model Architectures']","['Sentiment Analysis (SA)', 'Supervised Learning', 'NLP for News and Media']",['NLP for Social Media'],"This work presents our team solution for task 4a Message Polarity Classification at the Se-mEval 2016 challenge. Our experiments have been carried out over the Twitter dataset provided by the challenge. We follow a supervised approach, exploiting a SVM polynomial kernel classifier trained with the challenge data. The classifier takes as input advanced NLP features. This paper details the features and discusses the achieved results.",https://aclanthology.org/S16-1019,Association for Computational Linguistics,2016,June,Proceedings of the 10th International Workshop on Semantic Evaluation ({S}em{E}val-2016),"Cozza, Vittoria  and
Petrocchi, Marinella",mib at SemEval-2016 Task 4a: Exploiting lexicon based features for Sentiment Analysis in Twitter,10.18653/v1/S16-1019,S16,162
2020.lrec-1.853,"['Error Detection and Correction', 'Classification Applications', 'Low-resource Languages']",,,"In this work we address the processing of negation in Spanish. We first present a machine learning system that processes negation in Spanish. Specifically, we focus on two tasks: i negation cue detection and ii scope identification. The corpus used in the experimental framework is the SFU Review SP -NEG. The results for cue detection outperform state-of-the-art results, whereas for scope detection this is the first system that performs the task for Spanish. Moreover, we provide a qualitative error analysis aimed at understanding the limitations of the system and showing which negation cues and scopes are straightforward to predict automatically, and which ones are challenging.",https://aclanthology.org/2020.lrec-1.853,European Language Resources Association,2020,May,Proceedings of the 12th Language Resources and Evaluation Conference,"Jim{\'e}nez-Zafra, Salud Mar{\'\i}a  and
Morante, Roser  and
Blanco, Eduardo  and
Mart{\'\i}n Valdivia, Mar{\'\i}a Teresa  and
Ure{\~n}a L{\'o}pez, L. Alfonso",Detecting Negation Cues and Scopes in Spanish,,lrec,842
2020.challengehml-1.6,"['Domain-specific NLP', 'Classification Applications', 'Learning Paradigms']",['Multimodal Learning'],,"Behavioral cues play a significant part in human communication and cognitive perception. In most professional domains, employee recruitment policies are framed such that both professional skills and personality traits are adequately assessed. Hiring interviews are structured to evaluate expansively a potential employee's suitability for the position -their professional qualifications, interpersonal skills, ability to perform in critical and stressful situations, in the presence of time and resource constraints, etc. Therefore, candidates need to be aware of their positive and negative attributes and be mindful of behavioral cues that might have adverse effects on their success. We propose a multimodal analytical framework that analyzes the candidate in an interview scenario and provides feedback for predefined labels such as engagement, speaking rate, eye contact, etc. We perform a comprehensive analysis that includes the interviewee's facial expressions, speech, and prosodic information, using the video, audio, and text transcripts obtained from the recorded interview. We use these multimodal data sources to construct a composite representation, which is used for training machine learning classifiers to predict the class labels. Such analysis is then used to provide constructive feedback to the interviewee for their behavioral cues and body language. Experimental validation showed that the proposed methodology achieved promising results.",https://aclanthology.org/2020.challengehml-1.6,Association for Computational Linguistics,2020,July,Second Grand-Challenge and Workshop on Multimodal Language (Challenge-HML),"Agrawal, Anumeha  and
Anil George, Rosa  and
Ravi, Selvan Sunitha  and
Kamath S, Sowmya  and
Kumar, Anand",Leveraging Multimodal Behavioral Analytics for Automated Job Interview Performance Assessment and Feedback,10.18653/v1/2020.challengehml-1.6,challengehml,1181
J19-1005,"['Information Retrieval', 'Dialogue Systems', 'Model Architectures']","['Recurrent Neural Networks (RNNs)', 'Chatbots']",,"We study the problem of response selection for multi-turn conversation in retrieval-based chatbots. The task involves matching a response candidate with a conversation context, the challenges for which include how to recognize important parts of the context, and how to model the relationships among utterances in the context. Existing matching methods may lose important information in contexts as we can interpret them with a unified framework in which contexts are transformed to fixed-length vectors without any interaction with responses before matching. This motivates us to propose a new matching framework that can sufficiently carry important information in contexts to matching and model relationships among utterances at the same time. The new framework, which we call a sequential matching framework SMF, lets each utterance in a context interact with a response candidate at the first step and transforms the pair to a matching vector. The matching vectors are then accumulated following the order of the utterances in the context with a recurrent neural network RNN that models relationships among utterances. Context-response matching is then calculated with the hidden states of the RNN. Under SMF, we propose a sequential convolutional network and sequential attention network and conduct experiments on two public data sets to test their performance. Experiment results show that both models can significantly outperform state-of-the-art matching methods. We also show that the models are interpretable with visualizations that provide us insights on how they capture and leverage important information in contexts for matching.",https://aclanthology.org/J19-1005,MIT Press,2019,March,,"Wu, Yu  and
Wu, Wei  and
Xing, Chen  and
Xu, Can  and
Li, Zhoujun  and
Zhou, Ming",A Sequential Matching Framework for Multi-Turn Response Selection in Retrieval-Based Chatbots,10.1162/coli_a_00345,J19,767
2021.nlp4musa-1.8,"['Audio Generation and Processing', 'Text Generation', 'Data Management and Generation', 'Learning Paradigms', 'Model Architectures']","['Multimodal Learning', 'Lyrics Generation', 'Data Preparation']",,"This paper addresses the novel task of lyrics completion for creative support. Our proposed task aims to suggest words that are 1 atypical but 2 suitable for musical audio signals. Previous approaches focused on fully automatic lyrics generation tasks using language models that tend to generate frequent phrases, despite the importance of atypicality for creative support. In this study, we propose a novel vector space model and hypothesize that embedding multimodal aspects words, draft sentences, and music audio in a unified vector space contributes to capturing 1 the atypicality of words and 2 the relationships between words and the moods of music audio. To test our hypothesis, we used a large-scale dataset to investigate whether the proposed model suggests atypical words.",https://aclanthology.org/2021.nlp4musa-1.8,Association for Computational Linguistics,2021,November,Proceedings of the 2nd Workshop on NLP for Music and Spoken Audio (NLP4MusA),"Watanabe, Kento  and
Goto, Masataka",Atypical Lyrics Completion Considering Musical Audio Signals,10.1007/978-3-030-67832-6_15,nlp4musa,538
2022.wnu-1.6,"['Learning Paradigms', 'Information Extraction', 'Model Architectures']",['Large Language Models (LLMs)'],,"This paper shows how to use large-scale pretrained language models to extract character roles from narrative texts without domainspecific training data. Queried with a zero-shot question-answering prompt, GPT-3 can identify the hero, villain, and victim in diverse domains: newspaper articles, movie plot summaries, and political speeches.",https://aclanthology.org/2022.wnu-1.6,Association for Computational Linguistics,2022,July,Proceedings of the 4th Workshop of Narrative Understanding (WNU2022),"Stammbach, Dominik  and
Antoniak, Maria  and
Ash, Elliott","Heroes, Villains, and Victims, and GPT-3: Automated Extraction of Character Roles Without Training Data",10.18653/v1/2022.wnu-1.6,wnu,736
2020.spnlp-1.6,"['Text Generation', 'Domain-specific NLP', 'Information Extraction', 'Model Architectures']",['Recurrent Neural Networks (RNNs)'],['Long Short-Term Memory (LSTM) Models'],"The predominant approaches for extracting key information from documents resort to classifiers predicting the information type of each word. However, the word level ground truth used for learning is expensive to obtain since it is not naturally produced by the extraction task. In this paper, we discuss a new method for training extraction models directly from the textual value of information. The extracted information of a document is represented as a sequence of tokens in the XML language. We learn to output this representation with a pointer-generator network that alternately copies the document words carrying information and generates the XML tags delimiting the types of information. The ability of our end-to-end method to retrieve structured information is assessed on a large set of business documents. We show that it performs competitively with a standard word classifier without requiring costly word level supervision.",https://aclanthology.org/2020.spnlp-1.6,Association for Computational Linguistics,2020,November,Proceedings of the Fourth Workshop on Structured Prediction for NLP,"Sage, Cl{\'e}ment  and
Aussem, Alex  and
Eglin, V{\'e}ronique  and
Elghazel, Haytham  and
Espinas, J{\'e}r{\'e}my",End-to-End Extraction of Structured Information from Business Documents with Pointer-Generator Networks,10.18653/v1/2020.spnlp-1.6,spnlp,809
2022.nlppower-1.4,"['Evaluation Techniques', 'Learning Paradigms', 'Classification Applications']",['Supervised Learning'],,"Relation classification models are conventionally evaluated using only a single measure, e.g., micro-F 1 , macro-F 1 or AUC. In this work, we analyze weighting schemes, such as micro and macro, for imbalanced datasets. We introduce a framework for weighting schemes, where existing schemes are extremes, and two new intermediate schemes. We show that reporting results of different weighting schemes better highlights strengths and weaknesses of a model.",https://aclanthology.org/2022.nlppower-1.4,Association for Computational Linguistics,2022,May,Proceedings of NLP Power! The First Workshop on Efficient Benchmarking in NLP,"Harbecke, David  and
Chen, Yuxuan  and
Hennig, Leonhard  and
Alt, Christoph",Why only Micro-F1? Class Weighting of Measures for Relation Classification,10.18653/v1/2022.nlppower-1.4,nlppower,223
2020.nlpcss-1.15,"['Domain-specific NLP', 'Data Management and Generation', 'Classification Applications', 'Learning Paradigms']","['Supervised Learning', 'Data Preparation']",['Annotation Processes'],"In social care environments, the main goal of social workers is to foster independent living by their clients. An important task is thus to monitor progress towards reaching independence in different areas of their patients' life. To support this task, we present an approach that extracts indications of independence on different life aspects from the day-to-day documentation that social workers create. We describe the process of collecting and annotating a corresponding corpus created from data records of two social work institutions with a focus on disability care. We show that the agreement on the task of annotating the observations of social workers with respect to discrete independent levels yields a high agreement of .74 as measured by Fleiss' Kappa. We present a classification approach towards automatically classifying an observation into the discrete independence levels and present results for different types of classifiers. Against our original expectation, we show that we reach F-Measures macro of 95% averaged across topics, showing that this task can be automatically solved.",https://aclanthology.org/2020.nlpcss-1.15,Association for Computational Linguistics,2020,November,Proceedings of the Fourth Workshop on Natural Language Processing and Computational Social Science,"Maier, Angelika  and
Cimiano, Philipp",Predicting independent living outcomes from written reports of social workers,10.18653/v1/2020.nlpcss-1.15,nlpcss,728
2021.emnlp-main.155,"['Biases in NLP', 'Domain-specific NLP', 'Learning Paradigms', 'Classification Applications', 'Model Architectures']","['Adversarial Learning', 'Sentiment Analysis (SA)', 'NLP for News and Media']",['NLP for Social Media'],"Class imbalance is a common challenge in many NLP tasks, and has clear connections to bias, in that bias in training data often leads to higher accuracy for majority groups at the expense of minority groups. However there has traditionally been a disconnect between research on class-imbalanced learning and mitigating bias, and only recently have the two been looked at through a common lens. In this work we evaluate long-tail learning methods for tweet sentiment and occupation classification, and extend a margin-loss based approach with methods to enforce fairness. We empirically show through controlled experiments that the proposed approaches help mitigate both class imbalance and demographic biases. 1",https://aclanthology.org/2021.emnlp-main.155,Association for Computational Linguistics,2021,November,Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,"Subramanian, Shivashankar  and
Rahimi, Afshin  and
Baldwin, Timothy  and
Cohn, Trevor  and
Frermann, Lea",Fairness-aware Class Imbalanced Learning,10.18653/v1/2021.emnlp-main.155,emnlp,291
2022.bigscience-1.7,"['Domain-specific NLP', 'Information Extraction', 'Multilingual NLP', 'Learning Paradigms', 'Low-resource Languages']","['Few-shot Learning', 'Named Entity Recognition (NER)', 'NLP for News and Media']",,"In this work, we explore whether the recently demonstrated zero-shot abilities of the T0 model extend to Named Entity Recognition for out-of-distribution languages and time periods. Using a historical newspaper corpus in 3 languages as test-bed, we use prompts to extract possible named entities. Our results show that a naive approach for prompt-based zero-shot multilingual Named Entity Recognition is errorprone, but highlights the potential of such an approach for historical languages lacking labeled datasets. Moreover, we also find that T0-like models can be probed to predict the publication date and language of a document, which could be very relevant for the study of historical texts * .",https://aclanthology.org/2022.bigscience-1.7,Association for Computational Linguistics,2022,May,Proceedings of BigScience Episode {\#}5 -- Workshop on Challenges {\&} Perspectives in Creating Large Language Models,"De Toni, Francesco  and
Akiki, Christopher  and
De La Rosa, Javier  and
Fourrier, Cl{\'e}mentine  and
Manjavacas, Enrique  and
Schweter, Stefan  and
Van Strien, Daniel","Entities, Dates, and Languages: Zero-Shot on Historical Texts with T0",10.18653/v1/2022.bigscience-1.7,bigscience,555
2021.dash-1.12,"['Text Generation', 'Evaluation Techniques']",,,"Current methods for evaluation of natural language generation models focus on measuring text quality but fail to probe the model creativity, i.e., its ability to generate novel but coherent text sequences not seen in the training corpus. We present the GenX tool which is designed to enable interactive exploration and explanation of natural language generation outputs with a focus on the detection of memorization. We demonstrate the tool on two domainconditioned generation use cases -phishing emails and ACL abstracts.",https://aclanthology.org/2021.dash-1.12,Association for Computational Linguistics,2021,June,Proceedings of the Second Workshop on Data Science with Human in the Loop: Language Advances,"Duskin, Kayla  and
Sharma, Shivam  and
Yun, Ji Young  and
Saldanha, Emily  and
Arendt, Dustin",Evaluating and Explaining Natural Language Generation with GenX,10.18653/v1/2021.dash-1.12,dash,510
2020.alw-1.3,"['Domain-specific NLP', 'Model Architectures', 'Classification Applications', 'Learning Paradigms']","['NLP for News and Media', 'Supervised Learning', 'Transfer Learning', 'Hate and Offensive Speech Detection', 'Transformer Models']",['NLP for Social Media'],"Distinguishing hate speech from non-hate offensive language is challenging, as hate speech not always includes offensive slurs and offensive language not always express hate. Here, four deep learners based on the Bidirectional Encoder Representations from Transformers BERT, with either general or domain-specific language models, were tested against two datasets containing tweets labelled as either 'Hateful', 'Normal' or 'Offensive'. The results indicate that the attention-based models profoundly confuse hate speech with offensive and normal language. However, the pre-trained models outperform state-of-the-art results in terms of accurately predicting the hateful instances.",https://aclanthology.org/2020.alw-1.3,Association for Computational Linguistics,2020,November,Proceedings of the Fourth Workshop on Online Abuse and Harms,"Isaksen, Vebj{\o}rn  and
Gamb{\""a}ck, Bj{\""o}rn",Using Transfer-based Language Models to Detect Hateful and Offensive Language Online,10.18653/v1/2020.alw-1.3,alw,743
Q18-1003,"['Learning Paradigms', 'Parsing', 'Low-resource Languages']","['Morphological Parsing', 'Supervised Learning']",,"Much like sentences are composed of words, words themselves are composed of smaller units. For example, the English word questionably can be analyzed as question+able+ly. However, this structural decomposition of the word does not directly give us a semantic representation of the word's meaning. Since morphology obeys the principle of compositionality, the semantics of the word can be systematically derived from the meaning of its parts. In this work, we propose a novel probabilistic model of word formation that captures both the analysis of a word w into its constituent segments and the synthesis of the meaning of w from the meanings of those segments. Our model jointly learns to segment words into morphemes and compose distributional semantic vectors of those morphemes. We experiment with the model on English CELEX data and German DErivBase Zeller et al., 2013 data. We show that jointly modeling semantics increases both segmentation accuracy and morpheme F 1 by between 3% and 5%. Additionally, we investigate different models of vector composition, showing that recurrent neural networks yield an improvement over simple additive models. Finally, we study the degree to which the representations correspond to a linguist's notion of morphological productivity.",https://aclanthology.org/Q18-1003,MIT Press,2018,,,"Cotterell, Ryan  and
Sch{\""u}tze, Hinrich",Joint Semantic Synthesis and Morphological Analysis of the Derived Word,10.1162/tacl_a_00003,Q18,932
2022.sigmorphon-1.25,"['Low-resource Languages', 'Model Architectures']",,,"The paper describes the Flexica team's submission to the SIGMORPHON 2022 Shared Task 1 Part 1: Typologically Diverse Morphological Inflection. Our team submitted a nonneural system that extracted transformation patterns from alignments between a lemma and inflected forms. For each inflection category, we chose a pattern based on its abstractness score. The system outperformed the non-neural baseline, the extracted patterns covered a substantial part of possible inflections. However, we discovered that such score that does not account for all possible combinations of string segments as well as morphosyntactic features is not sufficient for a certain proportion of inflection cases.",https://aclanthology.org/2022.sigmorphon-1.25,Association for Computational Linguistics,2022,July,"Proceedings of the 19th SIGMORPHON Workshop on Computational Research in Phonetics, Phonology, and Morphology","Sherbakov, Andreas  and
Vylomova, Ekaterina",Morphology is not just a naive Bayes -- UniMelb Submission to SIGMORPHON 2022 ST on Morphological Inflection,10.18653/v1/2022.sigmorphon-1.25,sigmorphon,394
2021.hackashop-1.9,"['Text Generation', 'Domain-specific NLP', 'Learning Paradigms', 'Low-resource Languages', 'Cross-lingual Application']","['Transfer Learning', 'Paraphrase and Rephrase Generation', 'NLP for News and Media']",,"In this work, we describe our efforts in improving the variety of language generated from a rule-based NLG system for automated journalism. We present two approaches: one based on inserting completely new words into sentences generated from templates, and another based on replacing words with synonyms. Our initial results from a human evaluation conducted in English indicate that these approaches successfully improve the variety of the language without significantly modifying sentence meaning. We also present variations of the methods applicable to low-resource languages, simulated here using Finnish, where cross-lingual aligned embeddings are harnessed to make use of linguistic resources in a high-resource language. A human evaluation indicates that while proposed methods show potential in the low-resource case, additional work is needed to improve their performance.",https://aclanthology.org/2021.hackashop-1.9,Association for Computational Linguistics,2021,April,Proceedings of the EACL Hackashop on News Media Content Analysis and Automated Report Generation,"R{\""a}m{\""o}, Miia  and
Lepp{\""a}nen, Leo",Using contextual and cross-lingual word embeddings to improve variety in template-based NLG for automated journalism,,hackashop,689
2022.in2writing-1.3,"['Text Generation', 'Learning Paradigms', 'Data Management and Generation']","['Multimodal Learning', 'Data Analysis']",,"While developing a story, novices and published writers alike have had to look outside themselves for inspiration. Language models have recently been able to generate text fluently, producing new stochastic narratives upon request. However, effectively integrating such capabilities with human cognitive faculties and creative processes remains challenging. We propose to investigate this integration with a multimodal writing support interface that offers writing suggestions textually, visually, and aurally. We conduct an extensive study that combines elicitation of prior expectations before writing, observation and semi-structured interviews during writing, and outcome evaluations after writing. Our results illustrate individual and situational variation in machine-in-the-loop writing approaches, suggestion acceptance, and ways the system is helpful. Centrally, we report how participants perform integrative leaps, by which they do cognitive work to integrate suggestions of varying semantic relevance into their developing stories. We interpret these findings, offering modeling and design recommendations for future creative writing support technologies. 1",https://aclanthology.org/2022.in2writing-1.3,Association for Computational Linguistics,2022,May,Proceedings of the First Workshop on Intelligent and Interactive Writing Assistants (In2Writing 2022),"Singh, Nikhil  and
Bernal, Guillermo  and
Savchenko, Daria  and
Glassman, Elena",A Selective Summary of Where to Hide a Stolen Elephant: Leaps in Creative Writing with Multimodal Machine Intelligence,10.18653/v1/2022.in2writing-1.3,in2writing,941
2021.bppf-1.4,"['Audio Generation and Processing', 'Evaluation Techniques', 'Knowledge Representation and Reasoning']","['Taxonomy Construction', 'Automatic Speech Recognition (ASR)']",,"The applications of automatic speech recognition ASR systems are proliferating, in part due to recent significant quality improvements. However, as recent work indicates, even state-of-the-art speech recognition systems -some which deliver impressive benchmark results, struggle to generalize across use cases. We review relevant work, and, hoping to inform future benchmark development, outline a taxonomy of speech recognition use cases, proposed for the next generation of ASR benchmarks. We also survey work on metrics, in addition to the de facto standard Word Error Rate WER metric, and we introduce a versatile framework designed to describe interactions between linguistic variation and ASR performance metrics.",https://aclanthology.org/2021.bppf-1.4,Association for Computational Linguistics,2021,August,"Proceedings of the 1st Workshop on Benchmarking: Past, Present and Future","Aks{\""e}nova, Al{\""e}na  and
van Esch, Daan  and
Flynn, James  and
Golik, Pavel",How Might We Create Better Benchmarks for Speech Recognition?,10.18653/v1/2021.bppf-1.4,bppf,1033
2020.osact-1.4,"['Data Management and Generation', 'Low-resource Languages', 'Figurative Language']","['Idiomatic Expressions', 'Data Preparation']",['Annotation Processes'],"The current Arabic natural language processing resources are mainly build to address the Modern Standard Arabic MSA, while we witnessed some scattered efforts to build resources for various Arabic dialects such as the Levantine and the Egyptian dialects. We observed a lack of resources for Gulf Arabic and especially the Qatari variety. In this paper, we present the first Qatari idioms and expression corpus of 1000 entries. The corpus was created from on-line and printed sources in addition to transcribed recorded interviews. The corpus covers various Qatari traditional expressions and idioms. To this end, audio recordings were collected from interviews and an online survey questionnaire was conducted to validate our data. This corpus aims to help advance the dialectal Arabic Speech and Natural Language Processing tools and applications for the Qatari dialect.",https://aclanthology.org/2020.osact-1.4,European Language Resource Association,2020,May,"Proceedings of the 4th Workshop on Open-Source Arabic Corpora and Processing Tools, with a Shared Task on Offensive Language Detection","Al-Mulla, Sara  and
Zaghouani, Wajdi",Building a Corpus of Qatari Arabic Expressions,,osact,986
2021.wmt-1.87,"['Machine Translation (MT)', 'Domain-specific NLP', 'Low-resource Languages']","['Neural MT (NMT)', 'Medical and Clinical NLP']",['Biomedical NLP'],"This paper reports the optimization of using the out-of-domain data in the Biomedical translation task. We firstly optimized our parallel training dataset using the BabelNet in-domain terminology words. Afterward, to increase the training set, we studied the effects of the out-of-domain data on biomedical translation tasks, and we created a mixture of in-domain and out-of-domain training sets and added more in-domain data using forward translation in the English-Spanish task. Finally, with a simple bpe optimization method, we increased the number of in-domain subwords in our mixed training set and trained the Transformer model on the generated data. Results show improvements using our proposed method. 041 using Babelnet to include biomedical sentences 2 042 Implementing subwords bpe optimization on the 043 train set to study the adaptation of out-of-domain",https://aclanthology.org/2021.wmt-1.87,Association for Computational Linguistics,2021,November,Proceedings of the Sixth Conference on Machine Translation,"Rafieian, Bardia  and
Costa-jussa, Marta R.",High Frequent In-domain Words Segmentation and Forward Translation for the WMT21 Biomedical Task,,wmt,876
2021.finnlp-1.3,"['Embeddings', 'Domain-specific NLP', 'Topic Modeling', 'Low-resource Languages', 'Model Architectures']","['NLP for Finance', 'NLP for News and Media']",,"In this paper, we aim to predict stock price return rates by analyzing text data in financial news articles. A promising text analysis technique is word embedding that maps words into a lowdimensional continuous embedding space by exploiting the local word collocation patterns in a small context window. Another means of analyzing text is topic modeling that maps each document into a low-dimensional topic space. Recently developed topic embedding takes advantage of those two approaches by modeling latent topics of each document in a word embedding space. In this paper, by incorporating regression into the topic embedding model, we propose a topic embedding regression model called TopicVec-Reg to jointly model each document and a response variable associated with the document. Moreover, our method predicts the stock price return rate for unseen unlabeled financial articles. We evaluated the effectiveness of TopicVec-Reg through experiments in the task of stock return rate prediction using news articles provided by Thomson Reuters and stock prices by the Tokyo Stock Exchange. The result of closed test experiments showed that our method brought meaningful improvement on prediction performance in comparison to performing linear regression as post-processing of TopicVec. Through an open test, our method showed better prediction accuracy with a statistically significant difference.",https://aclanthology.org/2021.finnlp-1.3,-,2021,19-Aug,Proceedings of the Third Workshop on Financial Technology and Natural Language Processing,"Xu, Weiran  and
Eguchi, Koji",Topic Embedding Regression Model and its Application to Financial Texts,,finnlp,1393
R19-1116,['Model Architectures'],['Recurrent Neural Networks (RNNs)'],,"Calculating the Semantic Textual Similarity STS is an important research area in natural language processing which plays a significant role in many applications such as question answering, document summarisation, information retrieval and information extraction. This paper evaluates Siamese recurrent architectures, a special type of neural networks, which are used here to measure STS. Several variants of the architecture are compared with existing methods.",https://aclanthology.org/R19-1116,INCOMA Ltd.,2019,September,Proceedings of the International Conference on Recent Advances in Natural Language Processing (RANLP 2019),"Ranasinghe, Tharindu  and
Orasan, Constantin  and
Mitkov, Ruslan",Semantic Textual Similarity with Siamese Neural Networks,10.26615/978-954-452-056-4_116,R19,904
2019.icon-1.9,"['Data Management and Generation', 'Parsing', 'Cross-lingual Application', 'Low-resource Languages']","['Data Augmentation', 'Syntactic Parsing']",['Dependency Parsing'],"We present an approach for cross-lingual transfer of dependency parser so that the parser trained on a single source language can more effectively cater to diverse target languages. In this work, we show that the cross-lingual performance of the parsers can be enhanced by over-generating the source language treebank. For this, the source language treebank is augmented with its perturbed version in which controlled perturbation is introduced in the parse trees by stochastically reordering the positions of the dependents with respect to their heads while keeping the structure of the parse trees unchanged. This enables the parser to capture diverse syntactic patterns in addition to those that are found in the source language. The resulting parser is found to more effectively parse target languages with different syntactic structures. With English as the source language, our system shows an average improvement of 6.7% and 7.7% in terms of UAS and LAS over 29 target languages compared to the baseline single source parser trained using unperturbed source language treebank. This also results in significant improvement over the transfer parser proposed by Ahmad et al. 2019 that involves an ""orderfree"" parser algorithm.",https://aclanthology.org/2019.icon-1.9,NLP Association of India,2019,December,Proceedings of the 16th International Conference on Natural Language Processing,"Das, Ayan  and
Sarkar, Sudeshna",A little perturbation makes a difference: Treebank augmentation by perturbation improves transfer parsing,,icon,154
K19-1014,"['Data Management and Generation', 'Multilingual NLP', 'Knowledge Representation and Reasoning', 'Low-resource Languages']","['Taxonomy Construction', 'Data Preparation', 'Data Analysis']",['Annotation Processes'],"We conduct a manual error analysis of the CoNLL-SIGMORPHON 2017 Shared Task on Morphological Reinflection. In this task, systems are given a word in citation form e.g., hug and asked to produce the corresponding inflected form e.g., the simple past hugged. This design lets us analyze errors much like we might analyze children's production errors. We propose an error taxonomy and use it to annotate errors made by the top two systems across twelve languages. Many of the observed errors are related to inflectional patterns sensitive to inherent linguistic properties such as animacy or affect; many others are failures to predict truly unpredictable inflectional behaviors. We also find nearly one quarter of the residual ""errors"" reflect errors in the gold data.",https://aclanthology.org/K19-1014,Association for Computational Linguistics,2019,November,Proceedings of the 23rd Conference on Computational Natural Language Learning (CoNLL),"Gorman, Kyle  and
McCarthy, Arya D.  and
Cotterell, Ryan  and
Vylomova, Ekaterina  and
Silfverberg, Miikka  and
Markowska, Magdalena",Weird Inflects but OK: Making Sense of Morphological Generation Errors,10.18653/v1/K19-1014,K19,972
2020.sltu-1.7,"['Audio Generation and Processing', 'Data Management and Generation', 'Multilingual NLP', 'Low-resource Languages', 'Model Architectures']","['Data Augmentation', 'Automatic Speech Recognition (ASR)']",,"Towards developing high-performing ASR for low-resource languages, approaches to address the lack of resources are to make use of data from multiple languages, and to augment the training data by creating acoustic variations. In this work we present a single grapheme-based ASR model learned on 7 geographically proximal languages, using standard hybrid BLSTM-HMM acoustic models with lattice-free MMI objective. We build the single ASR grapheme set via taking the union over each language-specific grapheme set, and we find such multilingual graphemic hybrid ASR model can perform language-independent recognition on all 7 languages, and substantially outperform each monolingual ASR model. Secondly, we evaluate the efficacy of multiple data augmentation alternatives within language, as well as their complementarity with multilingual modeling. Overall, we show that the proposed multilingual graphemic hybrid ASR with various data augmentation can not only recognize any within training set languages, but also provide large ASR performance improvements.",https://aclanthology.org/2020.sltu-1.7,European Language Resources association,2020,May,Proceedings of the 1st Joint Workshop on Spoken Language Technologies for Under-resourced languages (SLTU) and Collaboration and Computing for Under-Resourced Languages (CCURL),"Liu, Chunxi  and
Zhang, Qiaochu  and
Zhang, Xiaohui  and
Singh, Kritika  and
Saraf, Yatharth  and
Zweig, Geoffrey",Multilingual Graphemic Hybrid ASR with Massive Data Augmentation,10.48550/arxiv.1909.06522,sltu,631
P16-1061,"['Embeddings', 'Information Extraction', 'Multilingual NLP', 'Learning Paradigms', 'Low-resource Languages', 'Model Architectures']",['Coreference Resolution'],,"A long-standing challenge in coreference resolution has been the incorporation of entity-level information -features defined over clusters of mentions instead of mention pairs. We present a neural network based coreference system that produces high-dimensional vector representations for pairs of coreference clusters. Using these representations, our system learns when combining clusters is desirable. We train the system with a learning-to-search algorithm that teaches it which local decisions cluster merges will lead to a high-scoring final coreference partition. The system substantially outperforms the current state-of-the-art on the English and Chinese portions of the CoNLL 2012 Shared Task dataset despite using few hand-engineered features.",https://aclanthology.org/P16-1061,Association for Computational Linguistics,2016,August,Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),"Clark, Kevin  and
Manning, Christopher D.",Improving Coreference Resolution by Learning Entity-Level Distributed Representations,10.18653/v1/P16-1061,P16,1061
2020.cmlc-1.3,"['Parsing', 'Information Extraction', 'Data Management and Generation', 'Embeddings', 'Low-resource Languages', 'Text Preprocessing']","['Part-of-Speech (POS) Tagging', 'Word Embeddings', 'Named Entity Recognition (NER)', 'Syntactic Parsing', 'Data Preparation']",['Dependency Parsing'],"This paper investigates the impact of different types and size of training corpora on language models. By asking the fundamental question of quality versus quantity, we compare four French corpora by pre-training four different ELMOs and evaluating them on dependency parsing, POS-tagging and Named Entities Recognition downstream tasks. We present and asses the relevance of a new balanced French corpus, CaBeRnet, that features a representative range of language usage, including a balanced variety of genres oral transcriptions, newspapers, popular magazines, technical reports, fiction, academic texts, in oral and written styles. We hypothesize that a linguistically representative corpus will allow the language models to be more efficient, and therefore yield better evaluation scores on different evaluation sets and tasks.",https://aclanthology.org/2020.cmlc-1.3,European Language Ressources Association,2020,May,Proceedings of the 8th Workshop on Challenges in the Management of Large Corpora,"Popa-Fabre, Murielle  and
Ortiz Su{\'a}rez, Pedro Javier  and
Sagot, Beno{\^\i}t  and
de la Clergerie, {\'E}ric",French Contextualized Word-Embeddings with a sip of CaBeRnet: a New French Balanced Reference Corpus,,cmlc,1066
P19-2012,"['Learning Paradigms', 'Parsing', 'Classification Applications']","['Unsupervised Learning', 'Syntactic Parsing']",['Dependency Parsing'],"Neural models at the sentence level often operate on the constituent words/tokens in a way that encodes the inductive bias of processing the input in a similar fashion to how humans do. However, there is no guarantee that the standard ordering of words is computationally efficient or optimal. To help mitigate this, we consider a dependency parse as a proxy for the inter-word dependencies in a sentence and simplify the sentence with respect to combinatorial objectives imposed on the sentenceparse pair. The associated optimization results in permuted sentences that are provably approximately optimal with respect to minimizing dependency parse lengths and that are demonstrably simpler. We evaluate our general-purpose permutations within a finetuning schema for the downstream task of subjectivity analysis. Our fine-tuned baselines reflect a new state of the art for the SUBJ dataset and the permutations we introduce lead to further improvements with a 2.0% increase in classification accuracy absolute and a 45% reduction in classification error relative over the previous state of the art.",https://aclanthology.org/P19-2012,Association for Computational Linguistics,2019,July,Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: Student Research Workshop,"Bommasani, Rishi",Long-Distance Dependencies Don't Have to Be Long: Simplifying through Provably Approximately Optimal Permutations,10.18653/v1/P19-2012,P19,347
Q19-1018,"['Parsing', 'Low-resource Languages']",['Syntactic Parsing'],['Dependency Parsing'],"We present a new cubic-time algorithm to calculate the optimal next step in shift-reduce dependency parsing, relative to ground truth, commonly referred to as dynamic oracle. Unlike existing algorithms, it is applicable if the training corpus contains non-projective structures. We then show that for a projective training corpus, the time complexity can be improved from cubic to linear.",https://aclanthology.org/Q19-1018,MIT Press,2019,,,"Nederhof, Mark-Jan",Calculating the Optimal Step in Shift-Reduce Dependency Parsing: From Cubic to Linear Time,10.1162/tacl_a_00268,Q19,1291
2020.autosimtrans-1.6,"['Data Management and Generation', 'Audio Generation and Processing', 'Learning Paradigms', 'Low-resource Languages', 'Model Architectures', 'Text Preprocessing', 'Machine Translation (MT)']","['Data Augmentation', 'Transfer Learning', 'Text Segmentation', 'Neural MT (NMT)', 'Automatic Speech Recognition (ASR)']",['Sentence Segmentation'],This paper describes our machine translation systems for the streaming Chinese-to-English translation task of AutoSimTrans 2020. We present a sentence length based method and a sentence boundary detection model based method for the streaming input segmentation. Experimental results of the transcription and the ASR output translation on the development data sets show that the translation system with the detection model based method outperforms the one with the length based method in BLEU score by 1.19 and 0.99 respectively under similar or better latency.,https://aclanthology.org/2020.autosimtrans-1.6,Association for Computational Linguistics,2020,July,Proceedings of the First Workshop on Automatic Simultaneous Translation,"Li, Minqin  and
Cheng, Haodong  and
Wang, Yuanjie  and
Zhang, Sijia  and
Wu, Liting  and
Guo, Yuhang",BIT's system for the AutoSimTrans 2020,10.18653/v1/2020.autosimtrans-1.6,autosimtrans,322
2021.bea-1.23,"['Domain-specific NLP', 'Classification Applications', 'Data Management and Generation']","['Data Preparation', 'Medical and Clinical NLP']",,"This study examines the relationship between the linguistic characteristics of a test item and the complexity of the response process required to answer it correctly. Using data from a large-scale medical licensing exam, clustering methods identified items that were similar with respect to their relative difficulty and relative response-time intensiveness to create low response process complexity and high response process complexity item classes. Interpretable models were used to investigate the linguistic features that best differentiated between these classes from a descriptive and predictive framework. Results suggest that nuanced features such as the number of ambiguous medical terms help explain response process complexity beyond superficial item characteristics such as word count. Yet, although linguistic features carry signal relevant to response process complexity, the classification of individual items remains challenging.",https://aclanthology.org/2021.bea-1.23,Association for Computational Linguistics,2021,April,Proceedings of the 16th Workshop on Innovative Use of NLP for Building Educational Applications,"Yaneva, Victoria  and
Jurich, Daniel  and
Ha, Le An  and
Baldwin, Peter",Using Linguistic Features to Predict the Response Process Complexity Associated with Answering Clinical MCQs,,bea,446
2020.socialnlp-1.5,"['Data Management and Generation', 'Classification Applications', 'Model Architectures']","['Data Preparation', 'Stance Detection', 'Recurrent Neural Networks (RNNs)']",,"We investigate whether pre-trained bidirectional transformers with sentiment and emotion information improve stance detection in long discussions of contemporary issues. As a part of this work, we create a novel stance detection dataset covering 419 different controversial issues and their related pros and cons collected by procon.org in nonpartisan format. Experimental results show that a shallow recurrent neural network with sentiment or emotion information can reach competitive results compared to fine-tuned BERT with 20 fewer parameters. We also use a simple approach that explains which input phrases contribute to stance detection.",https://aclanthology.org/2020.socialnlp-1.5,Association for Computational Linguistics,2020,July,Proceedings of the Eighth International Workshop on Natural Language Processing for Social Media,"Hosseinia, Marjan  and
Dragut, Eduard  and
Mukherjee, Arjun",Stance Prediction for Contemporary Issues: Data and Experiments,10.18653/v1/2020.socialnlp-1.5,socialnlp,676
P19-1613,"['Question Answering (QA)', 'Learning Paradigms', 'Knowledge Representation and Reasoning']","['Multihop Reasoning', 'Supervised Learning']",,"Multi-hop Reading Comprehension RC requires reasoning and aggregation across several paragraphs. We propose a system for multi-hop RC that decomposes a compositional question into simpler sub-questions that can be answered by off-the-shelf single-hop RC models. Since annotations for such decomposition are expensive, we recast subquestion generation as a span prediction problem and show that our method, trained using only 400 labeled examples, generates sub-questions that are as effective as humanauthored sub-questions. We also introduce a new global rescoring approach that considers each decomposition i.e. the sub-questions and their answers to select the best final answer, greatly improving overall performance. Our experiments on HOTPOTQA show that this approach achieves the state-of-the-art results, while providing explainable evidence for its decision making in the form of sub-questions.",https://aclanthology.org/P19-1613,Association for Computational Linguistics,2019,July,Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,"Min, Sewon  and
Zhong, Victor  and
Zettlemoyer, Luke  and
Hajishirzi, Hannaneh",Multi-hop Reading Comprehension through Question Decomposition and Rescoring,10.18653/v1/P19-1613,P19,444
2020.blackboxnlp-1.21,['Model Architectures'],,,"If the same neural network architecture is trained multiple times on the same dataset, will it make similar linguistic generalizations across runs? To study this question, we finetuned 100 instances of BERT on the Multigenre Natural Language Inference MNLI dataset and evaluated them on the HANS dataset, which evaluates syntactic generalization in natural language inference. On the MNLI development set, the behavior of all instances was remarkably consistent, with accuracy ranging between 83.6% and 84.8%. In stark contrast, the same models varied widely in their generalization performance. For example, on the simple case of subject-object swap e.g., determining that the doctor visited the lawyer does not entail the lawyer visited the doctor, accuracy ranged from 0.0% to 66.2%. Such variation is likely due to the presence of many local minima in the loss surface that are equally attractive to a low-bias learner such as a neural network; decreasing the variability may therefore require models with stronger inductive biases.",https://aclanthology.org/2020.blackboxnlp-1.21,Association for Computational Linguistics,2020,November,Proceedings of the Third BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP,"McCoy, R. Thomas  and
Min, Junghyun  and
Linzen, Tal",BERTs of a feather do not generalize together: Large variability in generalization across models with similar test set performance,10.18653/v1/2020.blackboxnlp-1.21,blackboxnlp,1086
2022.repl4nlp-1.25,"['Learning Paradigms', 'Low-resource Languages', 'Cross-lingual Application']",['Transfer Learning'],,"Pretrained multilingual encoders enable zeroshot cross-lingual transfer, but often produce unreliable models that exhibit high performance variance on the target language. We postulate that this high variance results from zero-shot cross-lingual transfer solving an under-specified optimization problem. We show that any linear-interpolated model between the source language monolingual model and source + target bilingual model has equally low source language generalization error, yet the target language generalization error reduces smoothly and linearly as we move from the monolingual to bilingual model, suggesting that the model struggles to identify good solutions for both source and target languages using the source language alone. Additionally, we show that zero-shot solution lies in non-flat region of target language error generalization surface, causing the high variance.",https://aclanthology.org/2022.repl4nlp-1.25,Association for Computational Linguistics,2022,May,Proceedings of the 7th Workshop on Representation Learning for NLP,"Wu, Shijie  and
Van Durme, Benjamin  and
Dredze, Mark",Zero-shot Cross-lingual Transfer is Under-specified Optimization,10.18653/v1/2022.repl4nlp-1.25,repl4nlp,1254
2021.conll-1.16,"['Evaluation Techniques', 'Data Management and Generation', 'Question Answering (QA)']",['Data Preparation'],,"The capabilities of today's natural language processing systems are typically evaluated using large datasets of curated questions and answers. While these are critical benchmarks of progress, they also suffer from weakness due to artificial distributions and incomplete knowledge. Artifacts arising from artificial distributions can overstate language model performance, while incomplete knowledge limits fine-grained analysis. In this work, we introduce a complementary benchmarking approach based on SimPlified Language Activity Traces SPLAT. SPLATs are corpora of language encodings of activity in some closed domain we study traces from chess and baseball games in this work. SPLAT datasets use naturally-arising distributions, allow the generation of question-answer pairs at scale, and afford complete knowledge in their closed domains. We show that language models of three different architectures can answer questions about world states using only verb-like encodings of activity. Our approach is extensible to new language models and additional question-answering tasks.",https://aclanthology.org/2021.conll-1.16,Association for Computational Linguistics,2021,November,Proceedings of the 25th Conference on Computational Natural Language Learning,"Demeter, David  and
Downey, Doug",Who's on First?: Probing the Learning and Representation Capabilities of Language Models on Deterministic Closed Domains,10.18653/v1/2021.conll-1.16,conll,1209
2021.vardial-1.13,"['Learning Paradigms', 'Low-resource Languages', 'Classification Applications', 'Model Architectures']","['Transformer Models', 'Adversarial Learning']",,"Dialect identification is a task with applicability in a vast array of domains, ranging from automatic speech recognition to opinion mining. This work presents our architectures used for the VarDial 2021 Romanian Dialect Identification subtask. We introduced a series of solutions based on Romanian or multilingual Transformers, as well as adversarial training techniques. At the same time, we experimented with a knowledge distillation tool in order to check whether a smaller model can maintain the performance of our best approach. Our best solution managed to obtain a weighted F1-score of 0.7324, allowing us to obtain the 2 nd place on the leaderboard.",https://aclanthology.org/2021.vardial-1.13,Association for Computational Linguistics,2021,April,"Proceedings of the Eighth Workshop on NLP for Similar Languages, Varieties and Dialects","Zaharia, George-Eduard  and
Avram, Andrei-Marius  and
Cercel, Dumitru-Clementin  and
Rebedea, Traian",Dialect Identification through Adversarial Learning and Knowledge Distillation on Romanian BERT,,vardial,965
2021.dravidianlangtech-1.19,"['Domain-specific NLP', 'Multilingual NLP', 'Low-resource Languages', 'Classification Applications', 'Model Architectures']","['Hate and Offensive Speech Detection', 'NLP for News and Media']",['NLP for Social Media'],"The intensity of online abuse has increased in recent years. Automated tools are being developed to prevent the use of hate speech and offensive content. Most of the technologies use natural language and machine learning tools to identify offensive text. In a multilingual society, where code-mixing is a norm, the hate content would be delivered in a code-mixed form in social media, which makes offensive content identification, further challenging. In this work, we participated in the EACL task to detect offensive content in the code-mixed social media scenario. The methodology uses a transformer model with transliteration and class balancing loss for offensive content identification. In this task, our model has been ranked 2 nd in Malayalam-English and 4 th in Tamil-English code-mixed languages.",https://aclanthology.org/2021.dravidianlangtech-1.19,Association for Computational Linguistics,2021,April,Proceedings of the First Workshop on Speech and Language Technologies for Dravidian Languages,"Dowlagar, Suman  and
Mamidi, Radhika",OFFLangOne@DravidianLangTech-EACL2021: Transformers with the Class Balanced Loss for Offensive Language Identification in Dravidian Code-Mixed text.,,dravidianlangtech,1321
2021.udw-1.13,"['Parsing', 'Data Management and Generation', 'Low-resource Languages']","['Data Preparation', 'Data Analysis', 'Syntactic Parsing']","['Dependency Parsing', 'Annotation Processes']","This study discusses the way different numerals and related expressions are currently annotated in the Universal Dependencies project, with a specific focus on the Uralic language family and only occasional references to the other language groups. We analyse different annotation conventions between individual treebanks, and aim to highlight some areas where further development work and systematization could prove beneficial. At the same time, the Universal Dependencies project already offers a wide range of conventions to mark nuanced variation in numerals and counting expressions, and the harmonization of conventions between different languages could be the next step to take. The discussion here makes specific reference to Universal Dependencies version 2.8, and some differences found may already have been harmonized in version 2.9. Regardless of whether this takes place or not, we believe that the study still forms an important documentation of this period in the project.",https://aclanthology.org/2021.udw-1.13,Association for Computational Linguistics,2021,December,"Proceedings of the Fifth Workshop on Universal Dependencies (UDW, SyntaxFest 2021)","Rueter, Jack  and
Partanen, Niko  and
Pirinen, Flammie A.",Numerals and what counts,10.1515/9783110220933.11,udw,962
2022.ecnlp-1.24,"['Biases in NLP', 'Question Answering (QA)', 'Domain-specific NLP', 'Model Architectures']","['Transformer Models', 'Recurrent Neural Networks (RNNs)']",['Long Short-Term Memory (LSTM) Models'],"Many e-commerce websites provide Productrelated Question Answering PQA platform where potential customers can ask questions related to a product, and other consumers can post an answer to that question based on their experience. Recently, there has been a growing interest in providing automated responses to product questions. In this paper, we investigate the suitability of the generative approach for PQA. We use state-of-the-art generative models proposed by Deng et al.  2020  and Lu et al.  2020  for this purpose. On closer examination, we find several drawbacks in this approach: 1 input reviews are not always utilized significantly for answer generation, 2 the performance of the models is abysmal while answering the numerical questions, 3 many of the generated answers contain phrases like ""I do not know"" which are taken from the reference answer in training data, and these answers do not convey any information to the customer. Although these approaches achieve a high ROUGE score, it does not reflect upon these shortcomings of the generated answers. We hope that our analysis will lead to more rigorous PQA approaches, and future research will focus on addressing these shortcomings in PQA.",https://aclanthology.org/2022.ecnlp-1.24,Association for Computational Linguistics,2022,May,Proceedings of The Fifth Workshop on e-Commerce and NLP (ECNLP 5),"Roy, Kalyani  and
Balapanuru, Vineeth  and
Nayak, Tapas  and
Goyal, Pawan",Investigating the Generative Approach for Question Answering in E-Commerce,10.18653/v1/2022.ecnlp-1.24,ecnlp,1351
2021.eval4nlp-1.21,"['Evaluation Techniques', 'Data Management and Generation', 'Domain-specific NLP']","['NLP for Bibliometrics and Scientometrics', 'Data Analysis']",,"SemEval is the primary venue in the NLP community for the proposal of new challenges and for the systematic empirical evaluation of NLP systems. This paper provides a systematic quantitative analysis of SemEval aiming to evidence the patterns of the contributions behind SemEval. By understanding the distribution of task types, metrics, architectures, participation and citations over time we aim to answer the question on what is being evaluated by Se-mEval.",https://aclanthology.org/2021.eval4nlp-1.21,Association for Computational Linguistics,2021,November,Proceedings of the 2nd Workshop on Evaluation and Comparison of NLP Systems,"Wysocki, Oskar  and
Florea, Malina  and
Landers, D{\'o}nal  and
Freitas, Andr{\'e}",What is SemEval evaluating? A Systematic Analysis of Evaluation Campaigns in NLP,10.18653/v1/2021.eval4nlp-1.21,eval4nlp,293
2021.bsnlp-1.2,"['Data Management and Generation', 'Low-resource Languages', 'Text Generation']","['Data Preparation', 'Data Augmentation', 'Paraphrase and Rephrase Generation']",,"This paper focuses on generation methods for paraphrasing in the Russian language. There are several transformer-based models Russian and multilingual trained on a collected corpus of paraphrases. We compare different models, contrast the quality of paraphrases using different ranking methods and apply paraphrasing methods in the context of augmentation procedure for different tasks. The contributions of the work are the combined paraphrasing dataset, fine-tuned generated models for Russian paraphrasing task and additionally the open source tool for simple usage of the paraphrasers.",https://aclanthology.org/2021.bsnlp-1.2,Association for Computational Linguistics,2021,April,Proceedings of the 8th Workshop on Balto-Slavic Natural Language Processing,"Fenogenova, Alena",Russian Paraphrasers: Paraphrase with Transformers,,bsnlp,81
2022.naacl-main.286,"['Model Architectures', 'Question Answering (QA)', 'Knowledge Representation and Reasoning', 'Image and Video Processing']","['Visual QA (VQA)', 'Graph Neural Networks (GNNs)']",,"Existing video question answering video QA models lack the capacity for deep video understanding and flexible multistep reasoning. We propose for video QA a novel model which performs dynamic multistep reasoning between questions and videos. It creates video semantic representation based on the video scene graph composed of semantic elements of the video and semantic relations among these elements. Then, it performs multistep reasoning for better answer decision between the representations of the question and the video, and dynamically integrate the reasoning results. Experiments show the significant advantage of the proposed model against previous methods in accuracy and interpretability. Against the existing stateof-the-art model, the proposed model dramatically improves more than 4%/3.1%/2% on the three widely used video QA datasets, MSRVTT-QA, MSRVTT multi-choice, and TGIF-QA, and displays better interpretability by backtracing along with the attention mechanisms to the video scene graphs.",https://aclanthology.org/2022.naacl-main.286,Association for Computational Linguistics,2022,July,Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,"Mao, Jianguo  and
Jiang, Wenbin  and
Wang, Xiangdong  and
Feng, Zhifan  and
Lyu, Yajuan  and
Liu, Hong  and
Zhu, Yong",Dynamic Multistep Reasoning based on Video Scene Graph for Video Question Answering,10.18653/v1/2022.naacl-main.286,naacl,1441
J17-4002,"['Learning Paradigms', 'Error Detection and Correction', 'Domain-specific NLP']","['Grammatical Error Correction (GEC)', 'Supervised Learning']",,"This article considers the problem of correcting errors made by English as a Second Language writers from a machine learning perspective, and addresses an important issue of developing an appropriate training paradigm for the task, one that accounts for error patterns of non-native writers using minimal supervision. Existing training approaches present a trade-off between large amounts of cheap data offered by the native-trained models and additional knowledge of learner error patterns provided by the more expensive method of training on annotated learner data. We propose a novel training approach that draws on the strengths offered by the two standard training paradigms-of training either on native or on annotated learner data-and that outperforms both of these standard methods. Using the key observation that parameters relating to error regularities exhibited by non-native writers are relatively simple, we develop models that can incorporate knowledge about error regularities based on a small annotated sample but that are otherwise trained on native English data. The key contribution of this article is the introduction and analysis of two methods for adapting the learned models to error patterns of non-native writers; one method that applies to generative classifiers and a second that applies to discriminative classifiers. Both methods demonstrated state-of-the-art performance in several text correction competitions. In particular,",https://aclanthology.org/J17-4002,MIT Press,2017,December,,"Rozovskaya, Alla  and
Roth, Dan  and
Sammons, Mark",Adapting to Learner Errors with Minimal Supervision,10.1162/COLI_a_00299,J17,129
W18-1113,['Classification Applications'],['Author Detection'],,"Written text transmits a good deal of nonverbal information related to the author's identity and social factors, such as age, gender and personality. However, it is less known to what extent behavioral biometric traces transmit such information. We use typist data to study the predictiveness of authorship, and present first experiments on predicting both age and gender from keystroke dynamics. Our results show that the model based on keystroke features leads to significantly higher accuracies for authorship than the text-based system, while being two orders of magnitude smaller. For user attribute prediction, the best approach is to combine the two, suggesting that extralinguistic factors are disclosed to a larger degree in written text, while author identity is better transmitted in typing behavior.",https://aclanthology.org/W18-1113,Association for Computational Linguistics,2018,June,"Proceedings of the Second Workshop on Computational Modeling of People{'}s Opinions, Personality, and Emotions in Social Media","Plank, Barbara",Predicting Authorship and Author Traits from Keystroke Dynamics,10.18653/v1/W18-1113,W18,1495
2016.tc-1.8,"['Low-resource Languages', 'Knowledge Representation and Reasoning']",['Semantic Web'],,"This work describes a teaching method to enrich and improve the translators' and interpreters' multilingual terminology to translate specialized texts from a single link. This method consists of using controlled vocabularies such as thesauri, classification schemes, subject heading systems and taxonomies that employ Linked Open Data LOD technology in the framework of Semantic Web.",https://aclanthology.org/2016.tc-1.8,AsLing,2016,November 17-18,Proceedings of Translating and the Computer 38,"Gomez-Camarero, Carmen  and
Palomares Perraut, Rocio",How translators can improve multilingual terminology in a link: teaching case study examples,,tc,1301
2020.alw-1.13,"['Domain-specific NLP', 'Data Management and Generation', 'Classification Applications', 'Low-resource Languages']","['NLP for News and Media', 'Hate and Offensive Speech Detection', 'Data Preparation']",['NLP for Social Media'],"Hateful rhetoric is plaguing online discourse, fostering extreme societal movements and possibly giving rise to real-world violence. A potential solution to this growing global problem is citizen-generated counter speech where citizens actively engage with hate speech to restore civil non-polarized discourse. However, its actual effectiveness in curbing the spread of hatred is unknown and hard to quantify. One major obstacle to researching this question is a lack of large labeled data sets for training automated classifiers to identify counter speech. Here we use a unique situation in Germany where self-labeling groups engaged in organized online hate and counter speech. We use an ensemble learning algorithm which pairs a variety of paragraph embeddings with regularized logistic regression functions to classify both hate and counter speech in a corpus of millions of relevant tweets from these two groups. Our pipeline achieves macro F1 scores on out of sample balanced test sets ranging from 0.76 to 0.97-accuracy in line and even exceeding the state of the art. We then use the classifier to discover hate and counter speech in more than 135,000 fully-resolved Twitter conversations occurring from 2013 to 2018 and study their frequency and interaction. Altogether, our results highlight the potential of automated methods to evaluate the impact of coordinated counter speech in stabilizing conversations on social media.  Denotes equal contribution.",https://aclanthology.org/2020.alw-1.13,Association for Computational Linguistics,2020,November,Proceedings of the Fourth Workshop on Online Abuse and Harms,"Garland, Joshua  and
Ghazi-Zahedi, Keyan  and
Young, Jean-Gabriel  and
H{\'e}bert-Dufresne, Laurent  and
Galesic, Mirta",Countering hate on social media: Large scale classification of hate and counter speech,10.18653/v1/2020.alw-1.13,alw,109
2021.nlp4posimpact-1.16,"['Domain-specific NLP', 'Dialogue Systems', 'Data Management and Generation', 'Question Answering (QA)', 'Classification Applications']","['Data Preparation', 'Chatbots', 'Medical and Clinical NLP']",,"Technologies for enhancing well-being, healthcare vigilance and monitoring are on the rise. However, despite patient interest, such technologies suffer from low adoption. One hypothesis for this limited adoption is loss of human interaction that is central to doctorpatient encounters. In this paper we seek to address this limitation via a conversational agent that adopts one aspect of in-person doctor-patient interactions: A human avatar to facilitate medical grounded question answering. This is akin to the in-person scenario where the doctor may point to the human body or the patient may point to their own body to express their conditions. Additionally, our agent has multiple interaction modes, that may give more options for the patient to use the agent, not just for medical question answering, but also to engage in conversations about general topics and current events. Both the avatar, and the multiple interaction modes could help improve adherence. We present a high level overview of the design of our agent, Marie Bot Wellbeing. We also report implementation details of our early prototype , and present preliminary results.",https://aclanthology.org/2021.nlp4posimpact-1.16,Association for Computational Linguistics,2021,August,Proceedings of the 1st Workshop on NLP for Positive Impact,"Yan, Xinxin  and
Nakashole, Ndapa",A Grounded Well-being Conversational Agent with Multiple Interaction Modes: Preliminary Results,10.18653/v1/2021.nlp4posimpact-1.16,nlp4posimpact,714
2020.findings-emnlp.232,['Model Architectures'],['Transformer Models'],,"We present BlockBERT, a lightweight and efficient BERT model for better modeling longdistance dependencies. Our model extends BERT by introducing sparse block structures into the attention matrix to reduce both memory consumption and training/inference time, which also enables attention heads to capture either short-or long-range contextual information. We conduct experiments on language model pre-training and several benchmark question answering datasets with various paragraph lengths. BlockBERT uses 18.7-36.1% less memory and 12.0-25.1% less time to learn the model. During testing, BlockBERT saves 27.8% inference time, while having comparable and sometimes better prediction accuracy, compared to an advanced BERT-based model, RoBERTa.",https://aclanthology.org/2020.findings-emnlp.232,Association for Computational Linguistics,2020,November,Findings of the Association for Computational Linguistics: EMNLP 2020,"Qiu, Jiezhong  and
Ma, Hao  and
Levy, Omer  and
Yih, Wen-tau  and
Wang, Sinong  and
Tang, Jie",Blockwise Self-Attention for Long Document Understanding,10.18653/v1/2020.findings-emnlp.232,findings,836
2021.ranlp-1.29,"['Data Management and Generation', 'Question Answering (QA)', 'Learning Paradigms', 'Low-resource Languages', 'Cross-lingual Application', 'Model Architectures']","['Transformer Models', 'Transfer Learning', 'Data Augmentation']",,"For many tasks, state-of-the-art results have been achieved with Transformer-based architectures, resulting in a paradigmatic shift in practices from the use of task-specific architectures to the fine-tuning of pre-trained language models. The ongoing trend consists in training models with an ever-increasing amount of data and parameters, which requires considerable resources. It leads to a strong search to improve resource efficiency based on algorithmic and hardware improvements evaluated only for English. This raises questions about their usability when applied to small-scale learning problems, for which a limited amount of training data is available, especially for underresourced languages tasks. The lack of appropriately sized corpora is a hindrance to applying data-driven and transfer learning-based approaches with strong instability cases. In this paper, we establish a state-of-the-art of the efforts dedicated to the usability of Transformerbased models and propose to evaluate these improvements on the question-answering performances of French language which have few resources. We address the instability relating to data scarcity by investigating various training strategies with data augmentation, hyperparameters optimization and cross-lingual transfer. We also introduce a new compact model for French FrALBERT which proves to be competitive in low-resource settings.",https://aclanthology.org/2021.ranlp-1.29,INCOMA Ltd.,2021,September,Proceedings of the International Conference on Recent Advances in Natural Language Processing (RANLP 2021),"Cattan, Oralie  and
Servan, Christophe  and
Rosset, Sophie",On the Usability of Transformers-based Models for a French Question-Answering Task,10.26615/978-954-452-072-4_029,ranlp,138
J16-2005,"['Domain-specific NLP', 'Data Management and Generation', 'Multilingual NLP', 'Information Extraction', 'Low-resource Languages']","['Data Preparation', 'NLP for News and Media']",['NLP for Social Media'],"in terms of utility as training data for a Chinese-English machine translation system. Relative to traditional parallel data resources, the automatically extracted parallel data yield substantial translation quality improvements in translating microblog text and modest improvements in translating edited news content.",https://aclanthology.org/J16-2005,MIT Press,2016,June,,"Ling, Wang  and
Marujo, Lu{\'\i}s  and
Dyer, Chris  and
Black, Alan W.  and
Trancoso, Isabel",Mining Parallel Corpora from Sina Weibo and Twitter,10.1162/COLI_a_00249,J16,334
2021.nllp-1.14,"['Parsing', 'Data Management and Generation', 'Domain-specific NLP']","['NLP for the Legal Domain', 'Data Preparation']",['Annotation Processes'],"Automated Compliance Checking ACC systems aim to semantically parse building regulations to a set of rules. However, semantic parsing is known to be hard and requires large amounts of training data. The complexity of creating such training data has led to research that focuses on small sub-tasks, such as shallow parsing or the extraction of a limited subset of rules. This study introduces a shallow parsing task for which training data is relatively cheap to create, with the aim of learning a lexicon for ACC. We annotate a small domain-specific dataset of 200 sentences, SPAR.txt 1 , and train a sequence tagger that achieves 79,93 F1-score on the test set. We then show through manual evaluation that the model identifies most 89,84% defined terms in a set of building regulation documents, and that both contiguous and discontiguous Multi-Word Expressions MWE are discovered with reasonable accuracy 70,3%.",https://aclanthology.org/2021.nllp-1.14,Association for Computational Linguistics,2021,November,Proceedings of the Natural Legal Language Processing Workshop 2021,"Kruiper, Ruben  and
Konstas, Ioannis  and
Gray, Alasdair J.G.  and
Sadeghineko, Farhad  and
Watson, Richard  and
Kumar, Bimal","SPaR.txt, a Cheap Shallow Parsing Approach for Regulatory Texts",10.18653/v1/2021.nllp-1.14,nllp,470
W19-6108,"['Embeddings', 'Data Management and Generation', 'Model Architectures']","['Data Preparation', 'Recurrent Neural Networks (RNNs)']",['Long Short-Term Memory (LSTM) Models'],"In this paper, we investigate the effect of enhancing lexical embeddings in LSTM language models LM with syntactic and semantic representations. We evaluate the language models using perplexity, and we evaluate the performance of the models on the task of predicting human sentence acceptability judgments. We train LSTM language models on sentences automatically annotated with universal syntactic dependency roles Nivre et al., 2016, dependency tree depth features, and universal semantic tags Abzianidze et al., 2017 to predict sentence acceptability judgments. Our experiments indicate that syntactic depth and tags lower the perplexity compared to a plain LSTM language model, while semantic tags increase the perplexity. Our experiments also show that neither syntactic nor semantic tags improve the performance of LSTM language models on the task of predicting sentence acceptability judgments.",https://aclanthology.org/W19-6108,"Link{\""o}ping University Electronic Press",2019,September{--}October,Proceedings of the 22nd Nordic Conference on Computational Linguistics,"Ek, Adam  and
Bernardy, Jean-Philippe  and
Lappin, Shalom",Language Modeling with Syntactic and Semantic Representation for Sentence Acceptability Predictions,,W19,503
2020.lrec-1.45,['Data Management and Generation'],['Data Preparation'],,"Revision plays a major role in writing and the analysis of writing processes. Revisions can be analyzed using a product-oriented approach focusing on a finished product, the text that has been produced or a process-oriented approach focusing on the process that the writer followed to generate this product. Although several language resources exist for the product-oriented approach to revisions, there are hardly any resources available yet for an in-depth analysis of the process of revisions. Therefore, we provide an extensive dataset on revisions made during writing accessible via hdl.handle.net/10411/VBDYGX. This dataset is based on keystroke data and eye tracking data of 65 students from a variety of backgrounds undergraduate and graduate English as a first language and English as a second language students and a variety of tasks argumentative text and academic abstract. In total, 7,120 revisions were identified in the dataset. For each revision, 18 features have been manually annotated and 31 features have been automatically extracted. As a case study, we show two potential use cases of the dataset. In addition, future uses of the dataset are described.",https://aclanthology.org/2020.lrec-1.45,European Language Resources Association,2020,May,Proceedings of the 12th Language Resources and Evaluation Conference,"Conijn, Rianne  and
Dux Speltz, Emily  and
van Zaanen, Menno  and
Van Waes, Luuk  and
Chukharev-Hudilainen, Evgeny",A Process-oriented Dataset of Revisions during Writing,10.34894/ny0dss,lrec,499
W19-2906,"['Learning Paradigms', 'Model Architectures', 'Finite State Machines']",['Reinforcement Learning'],,"Processing difficulty in online language comprehension has been explained in terms of surprisal and entropy reduction. Although both hypotheses have been supported by experimental data, we do not fully understand their relative contributions on processing difficulty. To develop a better understanding, we propose a mechanistic model of perceptual decision making that interacts with a simulated task environment with temporal dynamics. The proposed model collects noisy bottom-up evidence over multiple timesteps, integrates it with its top-down expectation, and makes perceptual decisions, producing processing time data directly without relying on any linking hypothesis. Temporal dynamics in the task environment was determined by a simple finitestate grammar, which was designed to create the situations where the surprisal and entropy reduction hypotheses predict different patterns. After the model was trained to maximize rewards, the model developed an adaptive policy and both surprisal and entropy effects were observed especially in a measure reflecting earlier processing.",https://aclanthology.org/W19-2906,Association for Computational Linguistics,2019,June,Proceedings of the Workshop on Cognitive Modeling and Computational Linguistics,"Cho, Pyeong Whan  and
Lewis, Richard",A Modeling Study of the Effects of Surprisal and Entropy in Perceptual Decision Making of an Adaptive Agent,10.18653/v1/W19-2906,W19,459
2020.computerm-1.2,"['Data Management and Generation', 'Domain-specific NLP', 'Information Extraction', 'Low-resource Languages']",['NLP for Finance'],,"Automatic term extraction ATE from texts is critical for effective terminology work in small speech communities. We present TermPortal, a workbench for terminology work in Iceland, featuring the first ATE system for Icelandic. The tool facilitates standardization in terminology work in Iceland, as it exports data in standard formats in order to streamline gathering and distribution of the material. In the project we focus on the domain of finance in order to do be able to fulfill the needs of an important and large field. We present a comprehensive survey amongst the most prominent organizations in that field, the results of which emphasize the need for a good, up-to-date and accessible termbank and the willingness to use terms in Icelandic. Furthermore we present the ATE tool for Icelandic, which uses a variety of methods and shows great potential with a recall rate of up to 95% and a high C-value, indicating that it competently finds term candidates that are important to the input text.",https://aclanthology.org/2020.computerm-1.2,European Language Resources Association,2020,May,Proceedings of the 6th International Workshop on Computational Terminology,"Steingr{\'\i}msson, Stein{\th}{\'o}r  and
{\TH}orbergsd{\'o}ttir, {\'A}g{\'u}sta  and
Danielsson, Hjalti  and
Ornolfsson, Gunnar Thor",TermPortal: A Workbench for Automatic Term Extraction from Icelandic Texts,,computerm,1159
2021.germeval-1.14,"['Domain-specific NLP', 'Low-resource Languages', 'Classification Applications', 'Model Architectures']","['Misinformation Detection', 'Hate and Offensive Speech Detection', 'NLP for News and Media']",['NLP for Social Media'],"In this paper, we report on our approach to addressing the GermEval 2021 Shared Task on the Identification of Toxic, Engaging, and Fact-Claiming Comments for the German language. We submitted three runs for each subtask based on ensembles of three models each using contextual embeddings from pre-trained language models using SVM and neural-network-based classifiers. We include language-specific as well as language-agnostic language models -both with and without finetuning. We observe that for the runs we submitted that the SVM models overfitted the training data and this affected the aggregation method simple majority voting of the ensembles. The model records a lower performance on the test set than on the training set. Exploring the issue of overfitting we uncovered that due to a bug in the pipeline the runs we submitted had not been trained on the full set but only on a small training set. Therefore in this paper we also include the results we get when trained on the full training set which demonstrate the power of ensembles.",https://aclanthology.org/2021.germeval-1.14,Association for Computational Linguistics,2021,September,"Proceedings of the GermEval 2021 Shared Task on the Identification of Toxic, Engaging, and Fact-Claiming Comments","Akomeah, Kwabena Odame  and
Kruschwitz, Udo  and
Ludwig, Bernd","UR@NLP\_A\_Team @ GermEval 2021: Ensemble-based Classification of Toxic, Engaging and Fact-Claiming Comments",,germeval,165
2019.nsurl-1.3,"['Model Architectures', 'Audio Generation and Processing', 'Low-resource Languages', 'Classification Applications']",['Recurrent Neural Networks (RNNs)'],['Long Short-Term Memory (LSTM) Models'],"Gender recognition in speech processing is one of the most challenging tasks. While many studies rely on extracting features and designing enhancement classifiers, classification accuracy is still not satisfactory. The remarkable improvement in performance achieved through the use of neural networks for automatic speech recognition has encouraged the use of deep neural networks in other voice techniques such as speech, emotion, language and gender recognition. An earlier study showed a significant improvement in the gender recognition of pictures and videos. In this paper, speech is used to create a gender recognition scheme based on neural networks. Attention-based BiLSTM architecture is proposed to discover the best approach for gender identification in Yorb. Acoustic features, including time, frequency, and cepstral features are extracted to train the model. The model obtained the state-of-the-art performance in speech-based gender recognition with 99% accuracy and F 1 score.",https://aclanthology.org/2019.nsurl-1.3,Association for Computational Linguistics,2019,11--12 September,Proceedings of The First International Workshop on NLP Solutions for Under Resourced Languages (NSURL 2019) co-located with ICNLSP 2019 - Short Papers,"Modupe, Ibukunola Abosede  and
Sefara, Tshephisho Joseph  and
Sunday, Ojo",Yor\`ub\'a Gender Recognition from Speech using Attention-based BiLSTM,,nsurl,1198
P19-1653,"['Machine Translation (MT)', 'Learning Paradigms', 'Model Architectures', 'Image and Video Processing']","['Multimodal Learning', 'Neural MT (NMT)']",,"Previous work on multimodal machine translation has shown that visual information is only needed in very specific cases, for example in the presence of ambiguous words where the textual context is not sufficient. As a consequence, models tend to learn to ignore this information. We propose a translate-and-refine approach to this problem where images are only used by a second stage decoder. This approach is trained jointly to generate a good first draft translation and to improve over this draft by i making better use of the target language textual context both left and right-side contexts and ii making use of visual context. This approach leads to the state of the art results. Additionally, we show that it has the ability to recover from erroneous or missing words in the source language.",https://aclanthology.org/P19-1653,Association for Computational Linguistics,2019,July,Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,"Ive, Julia  and
Madhyastha, Pranava  and
Specia, Lucia",Distilling Translations with Visual Awareness,10.18653/v1/P19-1653,P19,1240
2020.tlt-1.8,"['Data Management and Generation', 'Low-resource Languages']",['Data Analysis'],,"Using data from the World Atlas of Language Structures and the Universal Dependencies treebanks, we provide converging evidence from linguistic typology and comparative corpus linguistics for an efficiency-based trade-off in the encoding of referentially accessible subjects. Specifically, when familiar subjects are marked as bound elements attaching to the verb, the chances of having obligatory independent subject pronouns decrease significantly across the world's languages. At the same time, there is a trend against not encoding the subject at all, leading us to postulate an overall tendency to encode familiar subjects once and only once in a neutral topiccomment utterance. This tendency is mirrored in more fine-grained corpus data from Slavic: East Slavic languages, in contrast to the other members of the genus, have past forms without verbal subject encoding, and it is precisely with these former participle forms that the use of independent subject pronouns is significantly higher than with other, non-participial verb forms. By contrast, the occurrence of independent subject pronouns does not differ across various verb forms in other Slavic languages, as none of them has been affected by a loss of verbal subject encoding.",https://aclanthology.org/2020.tlt-1.8,Association for Computational Linguistics,2020,October,Proceedings of the 19th International Workshop on Treebanks and Linguistic Theories,"Berdicevskis, Aleksandrs  and
Schmidtke-Bode, Karsten  and
Ser{\v{z}}ant, Ilja",Subjects tend to be coded only once: Corpus-based and grammar-based evidence for an efficiency-driven trade-off,10.18653/v1/2020.tlt-1.8,tlt,1281
P19-1206,"['Learning Paradigms', 'Automatic Text Summarization', 'Model Architectures']","['Abstractive Text Summarization', 'Unsupervised Learning', 'Document Summarization', 'Recurrent Neural Networks (RNNs)']",,"This paper focuses on the end-to-end abstractive summarization of a single product review without supervision. We assume that a review can be described as a discourse tree, in which the summary is the root, and the child sentences explain their parent in detail. By recursively estimating a parent from its children, our model learns the latent discourse tree without an external parser and generates a concise summary. We also introduce an architecture that ranks the importance of each sentence on the tree to support summary generation focusing on the main review point. The experimental results demonstrate that our model is competitive with or outperforms other unsupervised approaches. In particular, for relatively long reviews, it achieves a competitive or better performance than supervised models. The induced tree shows that the child sentences provide additional information about their parent, and the generated summary abstracts the entire review.",https://aclanthology.org/P19-1206,Association for Computational Linguistics,2019,July,Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,"Isonuma, Masaru  and
Mori, Junichiro  and
Sakata, Ichiro",Unsupervised Neural Single-Document Summarization of Reviews via Learning Latent Discourse Structure and its Ranking,10.18653/v1/P19-1206,P19,1352
2020.argmining-1.7,"['Domain-specific NLP', 'Classification Applications', 'Topic Modeling', 'Text Clustering', 'Model Architectures']","['NLP for News and Media', 'Stance Detection', 'Transformer Models', 'Multilabel Text Classification']",,"Today's news volume makes it impractical for readers to get a diverse and comprehensive view of published articles written from opposing viewpoints. We introduce a transformer-based news aggregation system, composed of topic modeling, semantic clustering, claim extraction, and textual entailment that identifies viewpoints presented in articles within a semantic cluster and classifies them into positive, neutral and negative entailments. Our novel embedded topic model using BERT-based embeddings outperforms baseline topic modeling algorithms by an 11% relative improvement. We compare recent semantic similarity models in the context of news aggregation, evaluate transformer-based models for claim extraction on news data, and demonstrate the use of textual entailment models for diverse viewpoint identification.",https://aclanthology.org/2020.argmining-1.7,Association for Computational Linguistics,2020,December,Proceedings of the 7th Workshop on Argument Mining,"Carlebach, Mark  and
Cheruvu, Ria  and
Walker, Brandon  and
Ilharco Magalhaes, Cesar  and
Jaume, Sylvain",News Aggregation with Diverse Viewpoint Identification Using Neural Embeddings and Semantic Understanding Models,,argmining,1185
2020.aespen-1.2,"['Ethics', 'Data Management and Generation']",,,"Not all conflict datasets offer equal levels of coverage, depth, use-ability, and content. A review of the inclusion criteria, methodology, and sourcing of leading publicly available conflict datasets demonstrates that there are significant discrepancies in the output produced by ostensibly similar projects. This keynote will question the presumption of substantial overlap between datasets, and identify a number of important gaps left by deficiencies across core criteria for effective conflict data collection and analysis, including:",https://aclanthology.org/2020.aespen-1.2,European Language Resources Association (ELRA),2020,May,Proceedings of the Workshop on Automated Extraction of Socio-political Events from News 2020,"Raleigh, Clionadh",Keynote Abstract: Too soon? The limitations of AI for event data,,aespen,526
2021.codi-main.5,"['Information Extraction', 'Text Clustering', 'Low-resource Languages', 'Discourse Analysis']",['Coreference Resolution'],,"The diversity of coreference chains is usually tackled by means of global features length, types and number of referring expressions, distance between them, etc.. In this paper, we propose a novel approach that provides a description of their composition in terms of sequences of expressions. To this end, we apply sequence analysis techniques to bring out the various strategies for introducing a referent and keeping it active throughout discourse. We discuss a first application of this method to a French written corpus annotated with coreference chains. We obtain clusters that are linguistically coherent and interpretable in terms of reference strategies and we demonstrate the influence of text genre and semantic type of the referent on chain composition.",https://aclanthology.org/2021.codi-main.5,Association for Computational Linguistics,2021,November,Proceedings of the 2nd Workshop on Computational Approaches to Discourse,"Federzoni, Silvia  and
Ho-Dac, Lydia-Mai  and
Fabre, C{\'e}cile",Coreference Chains Categorization by Sequence Clustering,10.18653/v1/2021.codi-main.5,codi,243
2020.lrec-1.227,"['Data Management and Generation', 'Low-resource Languages']",['Data Preparation'],['Annotation Processes'],"This paper examines the procedure for lexico-semantic annotation of the Basic Corpus of Polish Metaphors that is the first step for annotating metaphoric expressions occurring in it. The procedure involves correcting the morphosyntactic annotation of part of the corpus that is automatically annotated on the morphosyntactic level. The main procedure concerns annotation of adjectives, adverbs, nouns and verbs including gerunds and participles, including abbreviations of the words that belong to the above classes. It is composed of three steps: deciding whether a particular occurrence of a word is asemantic e.g. anaphoric or strictly grammatical, whether we are dealing with a multi-word expression, reciprocal usages of the si marker and pluralia tantum, which may involve annotation with two lexical units having two different lemmas for a single token. We propose an interannotator agreement statistics adequate for this procedure. Finally, we discuss the preliminary results of annotation of a fragment of the corpus.",https://aclanthology.org/2020.lrec-1.227,European Language Resources Association,2020,May,Proceedings of the 12th Language Resources and Evaluation Conference,"Hajnicz, El{\.z}bieta",Interannotator Agreement for Lexico-Semantic Annotation of a Corpus,,lrec,436
2021.udw-1.15,"['Data Management and Generation', 'Multilingual NLP', 'Low-resource Languages']",['Data Preparation'],['Annotation Processes'],"We attempt to shed some light on the various ways how languages specify date and time, and on the options we have when trying to annotate them uniformly across Universal Dependencies. Examples from several language families are discussed, and their annotation is proposed. Our hope is to eventually make this or similar proposal an integral part of the UD annotation guidelines, which would help improve consistency of the UD treebanks. The current annotations are far from consistent, as can be seen from the survey we provide in appendices to this paper.",https://aclanthology.org/2021.udw-1.15,Association for Computational Linguistics,2021,December,"Proceedings of the Fifth Workshop on Universal Dependencies (UDW, SyntaxFest 2021)","Zeman, Daniel",Date and Time in Universal Dependencies,,udw,1083
2019.gwc-1.22,"['Data Management and Generation', 'Low-resource Languages']",['Data Analysis'],,"Within a larger frame of facilitating human-robot interaction, we present here the creation of a core vocabulary to be learned by a robot. It is extracted from two tokenised and lemmatized scenarios pertaining to two imagined microworlds in which the robot is supposed to play an assistive role. We also evaluate two resources for their utility for expanding this vocabulary so as to better cope with the robot's communication needs. The language under study is Romanian and the resources used are the Romanian wordnet and word embedding vectors extracted from the large representative corpus of contemporary Romanian, CoRoLa. The evaluation is made for two situations: one in which the words are not semantically disambiguated before expanding the lexicon, and another one in which they are disambiguated with senses from the Romanian wordnet. The appropriateness of each resource is discussed.",https://aclanthology.org/2019.gwc-1.22,Global Wordnet Association,2019,July,Proceedings of the 10th Global Wordnet Conference,"Irimia, Elena  and
Mitrofan, Maria  and
Mititelu, Verginica",Evaluating the Wordnet and CoRoLa-based Word Embedding Vectors for Romanian as Resources in the Task of Microworlds Lexicon Expansion,,gwc,553
2020.latechclfl-1.9,"['Ethics', 'Biases in NLP', 'Domain-specific NLP', 'Data Management and Generation', 'Classification Applications']","['Data Augmentation', 'Gender Bias']",,"Downstream effects of biased training data have become a major concern of the NLP community. How this may impact the automated curation and annotation of cultural heritage material is currently not well known. In this work, we create an experimental framework to measure the effects of different types of stylistic and social bias within training data for the purposes of literary classification, as one important subclass of cultural material. Because historical collections are often sparsely annotated, much like our knowledge of history is incomplete, researchers often cannot know the underlying distributions of different document types and their various sub-classes. This means that bias is likely to be an intrinsic feature of training data when it comes to cultural heritage material. Our aim in this study is to investigate which classification methods may help mitigate the effects of different types of bias within curated samples of training data. We find that machine learning techniques such as BERT or SVM are robust against reproducing certain kinds of social and stylistic bias within our test data, except in the most extreme cases. We hope that this work will spur further research into the potential effects of bias within training data for other cultural heritage material beyond the study of literature.",https://aclanthology.org/2020.latechclfl-1.9,International Committee on Computational Linguistics,2020,December,"Proceedings of the The 4th Joint SIGHUM Workshop on Computational Linguistics for Cultural Heritage, Social Sciences, Humanities and Literature","Bagga, Sunyam  and
Piper, Andrew",Measuring the Effects of Bias in Training Data for Literary Classification,,latechclfl,1340
W17-4306,"['Learning Paradigms', 'Information Extraction', 'Knowledge Representation and Reasoning']","['Multimodal Learning', 'Knowledge Graphs', 'Relation Extraction']",,"This work is on a previously formalized semantic evaluation task of spatial role labeling SpRL that aims at extraction of formal spatial meaning from text. Here, we report the results of initial efforts towards exploiting visual information in the form of images to help spatial language understanding. We discuss the way of designing new models in the framework of declarative learning-based programming DeLBP. The DeLBP framework facilitates combining modalities and representing various data in a unified graph. The learning and inference models exploit the structure of the unified graph as well as the global first order domain constraints beyond the data to predict the semantics which forms a structured meaning representation of the spatial context. Continuous representations are used to relate the various elements of the graph originating from different modalities. We improved over the state-of-the-art results on SpRL.",https://aclanthology.org/W17-4306,Association for Computational Linguistics,2017,September,Proceedings of the 2nd Workshop on Structured Prediction for Natural Language Processing,"Kordjamshidi, Parisa  and
Rahgooy, Taher  and
Manzoor, Umar",Spatial Language Understanding with Multimodal Graphs using Declarative Learning based Programming,10.18653/v1/W17-4306,W17,1150
2020.rdsm-1.4,"['Model Architectures', 'Classification Applications']","['Rumor Detection', 'Stance Detection']",,"Correctly classifying stances of replies can be significantly helpful for the automatic detection and classification of online rumours. One major challenge is that there are considerably more non-relevant replies comments than informative ones supports and denies, making the task highly imbalanced. In this paper we revisit the task of rumour stance classification, aiming to improve the performance over the informative minority classes. We experiment with traditional methods for imbalanced data treatment with feature-and BERT-based classifiers. Our models outperform all systems in RumourEval 2017 shared task and rank second in RumourEval 2019.",https://aclanthology.org/2020.rdsm-1.4,Association for Computational Linguistics,2020,December,Proceedings of the 3rd International Workshop on Rumours and Deception in Social Media (RDSM),"Li, Yue  and
Scarton, Carolina",Revisiting Rumour Stance Classification: Dealing with Imbalanced Data,,rdsm,352
2020.challengehml-1.5,['Learning Paradigms'],['Unsupervised Learning'],,"Allowing humans to communicate through natural language with robots requires connections between words and percepts. The process of creating these connections is called symbol grounding and has been studied for nearly three decades. Although many studies have been conducted, not many considered grounding of synonyms and the employed algorithms either work only offline or in a supervised manner. In this paper, a cross-situational learning based grounding framework is proposed that allows grounding of words and phrases through corresponding percepts without human supervision and online, i.e. it does not require any explicit training phase, but instead updates the obtained mappings for every new encountered situation. The proposed framework is evaluated through an interaction experiment between a human tutor and a robot, and compared to an existing unsupervised grounding framework. The results show that the proposed framework is able to ground words through their corresponding percepts online and in an unsupervised manner, while outperforming the baseline framework.",https://aclanthology.org/2020.challengehml-1.5,Association for Computational Linguistics,2020,July,Second Grand-Challenge and Workshop on Multimodal Language (Challenge-HML),"Roesler, Oliver",Unsupervised Online Grounding of Natural Language during Human-Robot Interactions,10.18653/v1/2020.challengehml-1.5,challengehml,282
N19-4010,['Embeddings'],['Word Embeddings'],,"We present FLAIR, an NLP framework designed to facilitate training and distribution of state-of-the-art sequence labeling, text classification and language models. The core idea of the framework is to present a simple, unified interface for conceptually very different types of word and document embeddings. This effectively hides all embedding-specific engineering complexity and allows researchers to ""mix and match"" various embeddings with little effort. The framework also implements standard model training and hyperparameter selection routines, as well as a data fetching module that can download publicly available NLP datasets and convert them into data structures for quick set up of experiments. Finally, FLAIR also ships with a ""model zoo"" of pre-trained models to allow researchers to use state-of-the-art NLP models in their applications. This paper gives an overview of the framework and its functionality.",https://aclanthology.org/N19-4010,Association for Computational Linguistics,2019,June,Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics (Demonstrations),"Akbik, Alan  and
Bergmann, Tanja  and
Blythe, Duncan  and
Rasul, Kashif  and
Schweter, Stefan  and
Vollgraf, Roland",FLAIR: An Easy-to-Use Framework for State-of-the-Art NLP,10.18653/v1/N19-4010,N19,988
2020.amta-research.12,"['Audio Generation and Processing', 'Machine Translation (MT)']",['Automatic Speech Recognition (ASR)'],,"For spoken language translation SLT in live scenarios such as conferences, lectures and meetings, it is desirable to show the translation to the user as quickly as possible, avoiding an annoying lag between speaker and translated captions. In other words, we would like low-latency, online SLT. If we assume a pipeline of automatic speech recognition ASR and machine translation MT then a simple but effective approach to online SLT is to pair an online ASR system, with a retranslation strategy, where the MT system retranslates every update received from ASR. However this can result in annoying ""flicker"" as the MT system updates its translation. A possible solution is to add a fixed delay, or ""mask"" to the the output of the MT system, but a fixed global mask re-introduces undesirable latency to the output. We introduce a method for dynamically determining the mask length, which provides a better latency-flicker trade-off curve than a fixed mask, without affecting translation quality.",https://aclanthology.org/2020.amta-research.12,Association for Machine Translation in the Americas,2020,October,Proceedings of the 14th Conference of the Association for Machine Translation in the Americas (Volume 1: Research Track),"Yao, Yuekun  and
Haddow, Barry",Dynamic Masking for Improved Stability in Online Spoken Language Translation,,amta,838
2021.iwcs-1.15,"['Parsing', 'Model Architectures']","['Transformer Models', 'Semantic Parsing']",['Semantic Role Labeling'],"Frame-semantic parsers traditionally predict predicates, frames, and semantic roles in a fixed order. This paper explores the 'chickenor-egg' problem of interdependencies between these components theoretically and practically. We introduce a flexible BERT-based sequence labeling architecture that allows for predicting frames and roles independently from each other or combining them in several ways. Our results show that our setups can approximate more complex traditional models' performance, while allowing for a clearer view of the interdependencies between the pipeline's components, and of how frame and role prediction models make different use of BERT's layers.",https://aclanthology.org/2021.iwcs-1.15,Association for Computational Linguistics,2021,June,Proceedings of the 14th International Conference on Computational Semantics (IWCS),"Minnema, Gosse  and
Nissim, Malvina",Breeding Fillmore's Chickens and Hatching the Eggs: Recombining Frames and Roles in Frame-Semantic Parsing,10.33612/diss.999190209,iwcs,488
2021.icnlsp-1.19,"['Audio Generation and Processing', 'Evaluation Techniques', 'Data Management and Generation', 'Learning Paradigms', 'Classification Applications']","['Multimodal Learning', 'Data Preparation']",,"In this work we propose a multimodal speech analytics framework for automatically assessing the quality of a public speaker's capabilities. For this purpose, we present the Public Speaking Quality PuSQ dataset, a new publicly available data collection that contains speeches from various speakers, along with respective annotations of how are these speeches perceived by the audience in terms of two labels namely: ""expressiveness"" and overall ""enjoyment"" i.e. if the listener enjoys the speech as a whole. Towards this end, several annotators have been asked to provide their input for each speech recording and interannotator agreement is taken into account in the final ground truth generation. In addition, we present a multimodal classifier that takes into account both audio and text information and predicts the overall recordings' label with regards to its speech quality in terms of the two aforementioned labels. To this end, we adopt a hierarchical approach according to which we first analyze the speech signal in a segment-basis 50ms of audio and sentences of text to extract emotions from both text and audio and then aggregate these decisions for the whole recording, while adding some highlevel speaking style characteristics to produce the overall representation that is used by the final classifier.",https://aclanthology.org/2021.icnlsp-1.19,Association for Computational Linguistics,2021,12--13 November,Proceedings of The Fourth International Conference on Natural Language and Speech Processing (ICNLSP 2021),"Eleftheriou, Sofia  and
Koromilas, Panagiotis  and
Giannakopoulos, Theodoros",Automatic Assessment of Speaking Skills Using Aural and Textual Information,,icnlsp,581
2020.intexsempar-1.6,"['Text Generation', 'Data Management and Generation']",['Data Augmentation'],,"Generation of natural language responses to the queries of structured language like SQL is very challenging as it requires generalization to new domains and the ability to answer ambiguous queries among other issues. We have participated in the CoSQL shared task organized in the IntEx-SemPar workshop at EMNLP 2020. We have trained a number of Neural Machine Translation NMT models to efficiently generate the natural language responses from SQL. Our shuffled backtranslation model has led to a BLEU score of 7.47 on the unknown test dataset. In this paper, we will discuss our methodologies to approach the problem and future directions to improve the quality of the generated natural language responses.",https://aclanthology.org/2020.intexsempar-1.6,Association for Computational Linguistics,2020,November,Proceedings of the First Workshop on Interactive and Executable Semantic Parsing,"Bandyopadhyay, Saptarashmi  and
Zhao, Tianyang",Natural Language Response Generation from SQL with Generalization and Back-translation,10.18653/v1/2020.intexsempar-1.6,intexsempar,1213
2022.eamt-1.54,"['Domain-specific NLP', 'Dialogue Systems', 'Data Management and Generation', 'Multilingual NLP', 'Low-resource Languages']","['Data Preparation', 'Chatbots']",,This paper describes a multilingual chatbot developed for public administration within the ENRICH4ALL project. We argue for multilingual chatbots powered through machine translation MT and discuss the integration of the eTranslation service in a chatbot solution. Related Work The benefits of having e-government chatbots are several: they can process service requests in huge,https://aclanthology.org/2022.eamt-1.54,European Association for Machine Translation,2022,June,Proceedings of the 23rd Annual Conference of the European Association for Machine Translation,"Anastasiou, Dimitra  and
Ruge, Anders  and
Ion, Radu  and
Seg{\u{a}}rceanu, Svetlana  and
Suciu, George  and
Pedretti, Olivier  and
Gratz, Patrick  and
Afkari, Hoorieh",A Machine Translation-Powered Chatbot for Public Administration,,eamt,1235
2022.slpat-1.7,"['Ethics', 'Text Generation']",['Text Simplification'],,"This paper outlines the ethical implications of text simplification within the framework of assistive systems. We argue that a distinction should be made between the technologies that perform text simplification and the realisation of these in assistive technologies. When using the latter as a motivation for research, it is important that the subsequent ethical implications be carefully considered. We provide guidelines for the framing of text simplification independently of assistive systems, as well as suggesting directions for future research and discussion based on the concerns raised.",https://aclanthology.org/2022.slpat-1.7,Association for Computational Linguistics,2022,May,Ninth Workshop on Speech and Language Processing for Assistive Technologies (SLPAT-2022),"Gooding, Sian",On the Ethical Considerations of Text Simplification,10.18653/v1/2022.slpat-1.7,slpat,614
2020.eamt-1.36,['Machine Translation (MT)'],,,This paper presents a case study of applying machine translation quality estimation QE for the purpose of machine translation MT engine selection. The goal is to understand how well the QE predictions correlate with several MT evaluation metrics automatic and human. Our findings show that our industry-level QE system is not reliable enough for MT selection when the MT systems have similar performance. We suggest that QE can be used with more success for other tasks relevant for translation industry such as risk prevention.,https://aclanthology.org/2020.eamt-1.36,European Association for Machine Translation,2020,November,Proceedings of the 22nd Annual Conference of the European Association for Machine Translation,"Zaretskaya, Anna  and
Concei{\c{c}}{\~a}o, Jos{\'e}  and
Bane, Frederick",Estimation vs Metrics: is QE Useful for MT Model Selection?,,eamt,1090
2021.sigmorphon-1.22,"['Machine Translation (MT)', 'Error Detection and Correction', 'Low-resource Languages', 'Model Architectures', 'Finite State Machines']",,,"Traditionally, character-level transduction problems have been solved with finite-state models designed to encode structural and linguistic knowledge of the underlying process, whereas recent approaches rely on the power and flexibility of sequence-to-sequence models with attention. Focusing on the less explored unsupervised learning scenario, we compare the two model classes side by side and find that they tend to make different types of errors even when achieving comparable performance. We analyze the distributions of different error classes using two unsupervised tasks as testbeds: converting informally romanized text into the native script of its language for Russian, Arabic, and Kannada and translating between a pair of closely related languages Serbian and Bosnian. Finally, we investigate how combining finite-state and sequence-to-sequence models at decoding time affects the output quantitatively and qualitatively. 1",https://aclanthology.org/2021.sigmorphon-1.22,Association for Computational Linguistics,2021,August,"Proceedings of the 18th SIGMORPHON Workshop on Computational Research in Phonetics, Phonology, and Morphology","Ryskina, Maria  and
Hovy, Eduard  and
Berg-Kirkpatrick, Taylor  and
Gormley, Matthew R.",Comparative Error Analysis in Neural and Finite-state Models for Unsupervised Character-level Transduction,10.18653/v1/2021.sigmorphon-1.22,sigmorphon,1384
D19-1221,"['Biases in NLP', 'Ethics', 'Learning Paradigms', 'Adversarial Attacks and Robustness']",['Adversarial Learning'],,"Adversarial examples highlight model vulnerabilities and are useful for evaluation and interpretation. We define universal adversarial triggers: input-agnostic sequences of tokens that trigger a model to produce a specific prediction when concatenated to any input from a dataset. We propose a gradientguided search over tokens which finds short trigger sequences e.g., one word for classification and four words for language modeling that successfully trigger the target prediction. For example, triggers cause SNLI entailment accuracy to drop from 89.94% to 0.55%, 72% of ""why"" questions in SQuAD to be answered ""to kill american people"", and the GPT-2 language model to spew racist output even when conditioned on non-racial contexts. Furthermore, although the triggers are optimized using white-box access to a specific model, they transfer to other models for all tasks we consider. Finally, since triggers are input-agnostic, they provide an analysis of global model behavior. For instance, they confirm that SNLI models exploit dataset biases and help to diagnose heuristics learned by reading comprehension models.",https://aclanthology.org/D19-1221,Association for Computational Linguistics,2019,November,Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"Wallace, Eric  and
Feng, Shi  and
Kandpal, Nikhil  and
Gardner, Matt  and
Singh, Sameer",Universal Adversarial Triggers for Attacking and Analyzing NLP,10.18653/v1/D19-1221,D19,414
2021.nodalida-main.6,"['Data Management and Generation', 'Low-resource Languages']",['Data Analysis'],,"The present paper deals with a computational analysis of translationese in professional and student English-to-German translations belonging to different registers. Building upon an information-theoretical approach, we test translation conformity to source and target language in terms of a neural language model's perplexity over Part of Speech PoS sequences. Our primary focus is on register diversification vs. convergence, reflected in the use of constructions with a higher vs. lower perplexity score. Our results show that, against our expectations, professional translations elicit higher perplexity scores from the target language model than students' translations. An analysis of the distribution of PoS patterns across registers shows that this apparent paradox is the effect of higher stylistic diversification and register sensitivity in professional translations. Our results contribute to the understanding of human translationese and shed light on the variation in texts generated by different translators, which is valuable for translation studies, multilingual language processing, and machine translation.",https://aclanthology.org/2021.nodalida-main.6,"Link{\""o}ping University Electronic Press, Sweden",2021,May 31--2 June,Proceedings of the 23rd Nordic Conference on Computational Linguistics (NoDaLiDa),"Bizzoni, Yuri  and
Lapshinova-Koltunski, Ekaterina",Measuring Translationese across Levels of Expertise: Are Professionals more Surprising than Students?,,nodalida,716
2020.udw-1.21,"['Parsing', 'Data Management and Generation', 'Low-resource Languages']","['Data Preparation', 'Syntactic Parsing']",['Dependency Parsing'],"This paper presents the first treebank for the Laz language, which is also the first Universal Dependencies Treebank for a South Caucasian language. This treebank aims to create a syntactically and morphologically annotated resource for further research. We also aim to document an endangered language in a systematic fashion within an inherently cross-linguistic framework: the Universal Dependencies Project UD. As of now, our treebank consists of 576 sentences and 2,306 tokens annotated in light with the UD guidelines. We evaluated the treebank on the dependency parsing task using a pretrained multilingual parsing model, and the results are comparable with other low-resourced treebanks with no training set. We aim to expand our treebank in the near future to include 1,500 sentences. The bigger goal for our project is to create a set of treebanks for minority languages in Anatolia.",https://aclanthology.org/2020.udw-1.21,Association for Computational Linguistics,2020,December,Proceedings of the Fourth Workshop on Universal Dependencies (UDW 2020),"T{\""u}rk, Utku  and
Bayar, Kaan  and
{\""O}zercan, Ay{\c{s}}eg{\""u}l Dilara  and
{\""O}zt{\""u}rk, G{\""o}rkem Yi{\u{g}}it  and
{\""O}zate{\c{s}}, {\c{S}}aziye Bet{\""u}l",First Steps towards Universal Dependencies for Laz,,udw,888
2020.clssts-1.2,"['Domain-specific NLP', 'Data Management and Generation', 'Information Retrieval', 'Low-resource Languages', 'Evaluation Techniques', 'Cross-lingual Application']",['Data Preparation'],['Annotation Processes'],"The Machine Translation for English Retrieval of Information in Any Language MATERIAL research program, sponsored by the Intelligence Advanced Research Projects Activity IARPA, focuses on rapid development of end-to-end systems capable of retrieving foreign language speech and text documents relevant to different types of English queries that may be further restricted by domain. Those systems also provide evidence of relevance of the retrieved content in the form of English summaries. The program focuses on Less-Resourced Languages and provides its performer teams very limited amounts of annotated training data. This paper describes the corpora that were created for system development and evaluation for the six languages released by the program to date: Tagalog, Swahili, Somali, Lithuanian, Bulgarian and Pashto. The corpora include build packs to train Machine Translation and Automatic Speech Recognition systems; document sets in three text and three speech genres annotated for domain and partitioned for analysis, development and evaluation; and queries of several types together with corresponding binary relevance judgments against the entire set of documents. The paper also describes a detection metric called Actual Query Weighted Value developed by the program to evaluate end-to-end system performance.",https://aclanthology.org/2020.clssts-1.2,European Language Resources Association,2020,May,Proceedings of the workshop on Cross-Language Search and Summarization of Text and Speech (CLSSTS2020),"Zavorin, Ilya  and
Bills, Aric  and
Corey, Cassian  and
Morrison, Michelle  and
Tong, Audrey  and
Tong, Richard",Corpora for Cross-Language Information Retrieval in Six Less-Resourced Languages,,clssts,947
2020.nlpcovid19-acl.14,"['Domain-specific NLP', 'Multilingual NLP', 'Classification Applications', 'Data Management and Generation']","['Data Preparation', 'Sentiment Analysis (SA)', 'NLP for News and Media']",['NLP for Social Media'],"Social media data can be a very salient source of information during crises. User-generated messages provide a window into people's minds during such times, allowing us insights about their moods and opinions. Due to the vast amounts of such messages, a large-scale analysis of population-wide developments becomes possible. In this paper, we analyze Twitter messages tweets collected during the first months of the COVID-19 pandemic in Europe with regard to their sentiment. This is implemented with a neural network for sentiment analysis using multilingual sentence embeddings. We separate the results by country of origin, and correlate their temporal development with events in those countries. This allows us to study the effect of the situation on people's moods. We see, for example, that lockdown announcements correlate with a deterioration of mood in almost all surveyed countries, which recovers within a short time span.",https://aclanthology.org/2020.nlpcovid19-acl.14,Association for Computational Linguistics,2020,July,Proceedings of the 1st Workshop on {NLP} for {COVID-19} at {ACL} 2020,"Kruspe, Anna  and
H{\""a}berle, Matthias  and
Kuhn, Iona  and
Zhu, Xiao Xiang",Cross-language sentiment analysis of European Twitter messages during the COVID-19 pandemic,10.1155/2022/8898100,nlpcovid19,844
2020.aacl-main.82,"['Information Extraction', 'Domain-specific NLP', 'Data Management and Generation', 'Model Architectures']","['Recurrent Neural Networks (RNNs)', 'Relation Extraction', 'Entity Linking', 'Named Entity Recognition (NER)', 'Data Preparation']","['Long Short-Term Memory (LSTM) Models', 'Annotation Processes']","We propose a newly annotated dataset for information extraction on recipes. Unlike previous approaches to machine comprehension of procedural texts, we avoid a priori pre-defining domain-specific predicates to recognize e.g., the primitive instructions in MILK and focus on basic understanding of the expressed semantics rather than directly reduce them to a simplified state representation e.g., ProPara. We thus frame the semantic comprehension of procedural text such as recipes, as fairly generic NLP subtasks, covering i entity recognition ingredients, tools and actions, ii relation extraction what ingredients and tools are involved in the actions, and iii zero anaphora resolution link actions to implicit arguments, e.g., results from previous recipe steps. Further, our Recipe Instruction Semantic Corpus RISeC dataset includes textual descriptions for the zero anaphora, to facilitate language generation thereof. Besides the dataset itself, we contribute a pipeline neural architecture that addresses entity and relation extraction as well as identification of zero anaphora. These basic building blocks can facilitate more advanced downstream applications e.g., question answering, conversational agents.",https://aclanthology.org/2020.aacl-main.82,Association for Computational Linguistics,2020,December,Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing,"Jiang, Yiwei  and
Zaporojets, Klim  and
Deleu, Johannes  and
Demeester, Thomas  and
Develder, Chris",Recipe Instruction Semantics Corpus RISeC: Resolving Semantic Structure and Zero Anaphora in Recipes,10.18653/v1/2020.aacl-main.82,aacl,229
W17-3535,"['Text Generation', 'Domain-specific NLP']",['NLP for News and Media'],,"Many data-to-text NLG systems work with data sets which are incomplete, ie some of the data is missing. We have worked with data journalists to understand how they describe incomplete data, and are building NLG algorithms based on these insights. A pilot evaluation showed mixed results, and highlighted several areas where we need to improve our system.",https://aclanthology.org/W17-3535,Association for Computational Linguistics,2017,September,Proceedings of the 10th International Conference on Natural Language Generation,"Inglis, Stephanie  and
Reiter, Ehud  and
Sripada, Somayajulu",Textually Summarising Incomplete Data,10.18653/v1/W17-3535,W17,1129
2022.wassa-1.5,"['Multilingual NLP', 'Data Management and Generation', 'Low-resource Languages', 'Classification Applications']","['Data Preparation', 'Sentiment Analysis (SA)', 'Emotion Detection']","['Aspect-Based SA (ABSA)', 'Annotation Processes']","In this paper, we present the SentEMO platform, a tool that provides aspect-based sentiment analysis and emotion detection of unstructured text data such as reviews, emails and customer care conversations. Currently, models have been trained for five domains and one general domain and are implemented in a pipeline approach, where the output of one model serves as the input for the next. The results are presented in three interactive dashboards, allowing companies to gain more insights into what stakeholders think of their products and services. The SentEMO platform is available at https://sentemo.ugent.be/ 1 .",https://aclanthology.org/2022.wassa-1.5,Association for Computational Linguistics,2022,May,"Proceedings of the 12th Workshop on Computational Approaches to Subjectivity, Sentiment {\&} Social Media Analysis","De Geyndt, Ellen  and
De Clercq, Orphee  and
Van Hee, Cynthia  and
Lefever, Els  and
Singh, Pranaydeep  and
Parent, Olivier  and
Hoste, Veronique",SentEMO: A Multilingual Adaptive Platform for Aspect-based Sentiment and Emotion Analysis,10.18653/v1/2022.wassa-1.5,wassa,195
W19-1007,"['Classification Applications', 'Question Answering (QA)', 'Knowledge Representation and Reasoning', 'Image and Video Processing']",['Visual QA (VQA)'],,"We present ImageTTR, an extension to the Python implementation of Type Theory with Records pyTTR which connects formal record type representation with image classifiers implemented as deep neural networks. The Type Theory with Records framework serves as a knowledge representation system for natural language the representations of which are grounded in perceptual information of neural networks. We demonstrate the benefits of this symbolic and data-driven hybrid approach on the task of visual question answering.",https://aclanthology.org/W19-1007,Association for Computational Linguistics,2019,June,"Proceedings of the {IWCS} 2019 Workshop on Computing Semantics with Types, Frames and Related Structures","Matsson, Arild  and
Dobnik, Simon  and
Larsson, Staffan",ImageTTR: Grounding Type Theory with Records in Image Classification for Visual Question Answering,10.18653/v1/W19-1007,W19,516
2020.clinicalnlp-1.25,"['Domain-specific NLP', 'Data Management and Generation']","['Data Preparation', 'Medical and Clinical NLP']",,"Domain pretraining followed by task finetuning has become the standard paradigm for NLP tasks, but requires in-domain labelled data for task fine-tuning. To overcome this, we propose to utilise unlabelled domain data by assigning pseudo-labels from a general model. We evaluate the approach on two clinical STS datasets, and achieve r = 0.80 on N2C2-STS. Further investigation reveals that if the data distribution of unlabelled sentence pairs is closer to the test data, we can obtain better performance. By leveraging a large general-purpose STS dataset and small-scale in-domain training data, we obtain further improvements to r = 0.90, a new SOTA.",https://aclanthology.org/2020.clinicalnlp-1.25,Association for Computational Linguistics,2020,November,Proceedings of the 3rd Clinical Natural Language Processing Workshop,"Wang, Yuxia  and
Verspoor, Karin  and
Baldwin, Timothy",Learning from Unlabelled Data for Clinical Semantic Textual Similarity,10.18653/v1/2020.clinicalnlp-1.25,clinicalnlp,573
2021.dravidianlangtech-1.3,"['Cross-lingual Application', 'Domain-specific NLP', 'Multilingual NLP', 'Learning Paradigms', 'Low-resource Languages', 'Classification Applications', 'Model Architectures']","['Transformer Models', 'Transfer Learning', 'Hate and Offensive Speech Detection', 'NLP for News and Media']",['NLP for Social Media'],"Offensive speech identification in countries like India poses several challenges due to the usage of code-mixed and romanized variants of multiple languages by the users in their posts on social media. The challenge of offensive language identification on social media for Dravidian languages is harder, considering the low resources available for the same. In this paper, we explored the zeroshot learning and few-shot learning paradigms based on multilingual language models for offensive speech detection in code-mixed and romanized variants of three Dravidian languages -Malayalam, Tamil, and Kannada. We propose a novel and flexible approach of selective translation and transliteration to reap better results from fine-tuning and ensembling multilingual transformer networks like XLM-RoBERTa and mBERT. We implemented pretrained, fine-tuned, and ensembled versions of XLM-RoBERTa for offensive speech classification. Further, we experimented with interlanguage, inter-task, and multi-task transfer learning techniques to leverage the rich resources available for offensive speech identification in the English language and to enrich the models with knowledge transfer from related tasks. The proposed models yielded good results and are promising for effective offensive speech identification in low resource settings. 1",https://aclanthology.org/2021.dravidianlangtech-1.3,Association for Computational Linguistics,2021,April,Proceedings of the First Workshop on Speech and Language Technologies for Dravidian Languages,"Sai, Siva  and
Sharma, Yashvardhan",Towards Offensive Language Identification for Dravidian Languages,,dravidianlangtech,1028
2016.lilt-14.6,"['Data Management and Generation', 'Classification Applications']",['Data Preparation'],['Annotation Processes'],"Modal auxiliaries have different readings, depending on the context in which they occur Kratzer, 1981 . Several projects have attempted to classify uses of modal auxiliaries in corpora according to their reading using supervised machine learning techniques e.g., Rubinstein et al., 2013 , Ruppenhofer & Rehbein, 2012 . In each study, traditional taxonomic labels, such as 'epistemic' and 'deontic' are used by human annotators to label instances of modal auxiliaries in a corpus. In order to achieve higher agreement among annotators, results in these previous studies are reported after collapsing some of the initial categories. The results show that human annotators have fairly good agreement on some of the categories, such as whether or not a use is epistemic, but poor agreement on others. They also show that annotators agree more on modals such as might than on modals such as could. In this study, we used traditional taxonomic categories on sentences containing modal auxiliary verbs that were randomly extracted from the English Gigaword 4 th edition corpus Parker et al., 2009 . The lowest inner-annotator agreement using traditional taxonomic labels occurred with uses of could, with raw agreements of 42%  48%  = 0.196  0.259, compared to might, for instance, with raw agreement of 98%. In response to the low numbers, rather than collapsing traditional categories, we tried a new method of classifying uses of could with respect to where the reading situates the eventuality being described relative to the speech time. For example, the sentence 'Jess could swim.'",https://aclanthology.org/2016.lilt-14.6,CSLI Publications,2016,sept,"Linguistic Issues in Language Technology, Volume 14, 2016 - Modality: Logic, Semantics, Annotation, and Machine Learning","Moon, Lori  and
Kirvaitis, Patricija  and
Madden, Noreen",Selective Annotation of Modal Readings: Delving into the Difficult Data,,lilt,401
2020.figlang-1.7,['Classification Applications'],['Sarcasm Detection'],,"Sarcasm is a form of communication in which the person states opposite of what he actually means. It is ambiguous in nature. In this paper, we propose using machine learning techniques with BERT and GloVe embeddings to detect sarcasm in tweets. The dataset is preprocessed before extracting the embeddings. The proposed model also uses the context in which the user is reacting to along with his actual response.",https://aclanthology.org/2020.figlang-1.7,Association for Computational Linguistics,2020,July,Proceedings of the Second Workshop on Figurative Language Processing,"Khatri, Akshay  and
P, Pranav",Sarcasm Detection in Tweets with BERT and GloVe Embeddings,10.18653/v1/2020.figlang-1.7,figlang,981
P17-1078,"['Text Preprocessing', 'Low-resource Languages', 'Model Architectures']",['Text Segmentation'],['Word Segmentation'],"Neural word segmentation research has benefited from large-scale raw texts by leveraging them for pretraining character and word embeddings. On the other hand, statistical segmentation research has exploited richer sources of external information, such as punctuation, automatic segmentation and POS. We investigate the effectiveness of a range of external training sources for neural word segmentation by building a modular segmentation model, pretraining the most important submodule using rich external sources. Results show that such pretraining significantly improves the model, leading to accuracies competitive to the best methods on six benchmarks.",https://aclanthology.org/P17-1078,Association for Computational Linguistics,2017,July,Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),"Yang, Jie  and
Zhang, Yue  and
Dong, Fei",Neural Word Segmentation with Rich Pretraining,10.18653/v1/P17-1078,P17,521
2021.bsnlp-1.10,"['Information Extraction', 'Multilingual NLP', 'Low-resource Languages', 'Cross-lingual Application']",['Named Entity Recognition (NER)'],,"This document describes our participation at the 3 rd Shared Task on SlavNER, part of the 8 th Balto-Slavic Natural Language Processing Workshop, where we focused exclusively in the Named Entity Recognition NER task. We addressed this task by combining multilingual contextual embedding models, such as XLM-R Conneau et al., 2020 , with characterlevel embeddings and a biaffine classifier Yu et al., 2020 . This allowed us to train downstream models for NER using all the available training data. We are able to show that this approach results in good performance when replicating the scenario of the 2 nd Shared Task.",https://aclanthology.org/2021.bsnlp-1.10,Association for Computational Linguistics,2021,April,Proceedings of the 8th Workshop on Balto-Slavic Natural Language Processing,"Ferreira, Pedro  and
Cardoso, Ruben  and
Mendes, Afonso",Priberam Labs at the 3rd Shared Task on SlavNER,,bsnlp,1199
2020.peoples-1.9,"['Domain-specific NLP', 'Low-resource Languages', 'Data Management and Generation']","['Data Preparation', 'Medical and Clinical NLP', 'Data Analysis', 'NLP for News and Media']",['NLP for Social Media'],"The paper focuses on a large collection of Dutch tweets to gain insight into the perception and reactions of users during the early months of the COVID-19 pandemic. We focused on five major communities of users: government and health organizations, news media, politicians, the general public and conspiracy theory supporters, investigating differences among them in topic dominance and the expressions of emotions. Through topic modeling we monitor the evolution of the conversation about COVID-19 among these communities. Our results indicate that the focus on COVID-19 shifted from the virus itself to its impact on the economy between February and April. Surprisingly, the overall emotional public response appears to be substantially positive and expressing trust, although differences can be observed in specific groups of users.",https://aclanthology.org/2020.peoples-1.9,Association for Computational Linguistics,2020,December,"Proceedings of the Third Workshop on Computational Modeling of People's Opinions, Personality, and Emotion's in Social Media","Marinov, Boris  and
Spenader, Jennifer  and
Caselli, Tommaso",Topic and Emotion Development among Dutch COVID-19 Twitter Communities in the early Pandemic,,peoples,817
W19-7001,['Evaluation Techniques'],,,"Translation process research TPR aims at describing what translators do, and one of the technical dimensions of translators' work is editing applying detailed changes to text. In this presentation, we will analyze how different methods for process data collection describe editing. We will review keyloggers used in typical TPR applications, track changes used by word processors, and edit rates based on estimation of edit distances. The purpose of this presentation is to discuss the limitations of these methods when describing editing behavior, and to incentivize researchers in looking for ways to present process data in simplified formats, closer to those that describe product data. Research background The technical dimension of translation, revision and post-editing is characterized by writing actions. Editing, part of this technical dimension, is a set of actions that is applied to pre-existing text. This implies that editing cannot be analyzed in the same way as translating or writing from scratch. We see editing as being composed of four actions: delete, insert, move and replace do Carmo 2017. This presentation discusses the implications of this definition of editing and of different methods to describe it. If we want to know which words were edited and how, we need data that accurately describes the actions performed. After we have that data, we may extract from it features that can be used to",https://aclanthology.org/W19-7001,European Association for Machine Translation,2019,August,Proceedings of the Second MEMENTO workshop on Modelling Parameters of Cognitive Effort in Translation Production,"do Carmo, F{\'e}lix","Edit distances do not describe editing, but they can be useful for translation process research",,W19,316
2021.law-1.12,"['Data Management and Generation', 'Low-resource Languages']",['Data Preparation'],['Annotation Processes'],"This paper presents several challenges faced when annotating Turkish treebanks in accordance with the Universal Dependencies UD guidelines and proposes solutions to address them. Most of these challenges stem from the lack of adequate support in the UD framework to accurately represent null morphemes and complex derivations, which results in significant loss of information for Turkish. This loss negatively impacts the tools that are developed based on these treebanks. We raised and discussed these issues within the community on the official UD portal. This paper presents these issues and our proposals to more accurately represent morphosyntactic information for Turkish while adhering to guidelines of UD. This work aims to contribute to the representation of Turkish and other agglutinative languages in UD-based treebanks, which in turn aids to develop more accurately annotated datasets for such languages.",https://aclanthology.org/2021.law-1.12,Association for Computational Linguistics,2021,November,Proceedings of The Joint 15th Linguistic Annotation Workshop (LAW) and 3rd Designing Meaning Representations (DMR) Workshop,"Bedir, Talha  and
{\c{S}}ahin, Karahan  and
Gungor, Onur  and
Uskudarli, Suzan  and
{\""O}zg{\""u}r, Arzucan  and
G{\""u}ng{\""o}r, Tunga  and
Ozturk Basaran, Balkiz",Overcoming the challenges in morphological annotation of Turkish in universal dependencies framework,10.18653/v1/2021.law-1.12,law,1436
2021.clpsych-1.24,"['Domain-specific NLP', 'Classification Applications', 'Model Architectures']","['Medical and Clinical NLP', 'Personality Trait Prediction', 'Emotion Detection', 'NLP for News and Media']","['NLP for Mental Health', 'NLP for Social Media']","Eating disorders are a growing problem especially among young people, yet they have been under-studied in computational research compared to other mental health disorders such as depression. Computational methods have a great potential to aid with the automatic detection of mental health problems, but stateof-the-art machine learning methods based on neural networks are notoriously difficult to interpret, which is a crucial problem for applications in the mental health domain. We propose leveraging the power of deep learning models for automatically detecting signs of anorexia based on social media data, while at the same time focusing on interpreting their behavior. We train a hierarchical attention network to detect people with anorexia, and use its internal encodings to discover different clusters of anorexia symptoms. We interpret the identified patterns from multiple perspectives, including emotion expression, psycho-linguistic features and personality traits, and we offer novel hypotheses to interpret our findings from a psycho-social perspective. Some interesting findings are patterns of word usage in some users with anorexia which show that they feel less as being part of a group compared to control cases, as well as that they have abandoned explanatory activity as a result of a greater feeling of helplessness and fear.",https://aclanthology.org/2021.clpsych-1.24,Association for Computational Linguistics,2021,June,Proceedings of the Seventh Workshop on Computational Linguistics and Clinical Psychology: Improving Access,"Uban, Ana Sabina  and
Chulvi, Berta  and
Rosso, Paolo",Understanding Patterns of Anorexia Manifestations in Social Media Data with Deep Learning,10.18653/v1/2021.clpsych-1.24,clpsych,1143
2020.challengehml-1.9,"['Question Answering (QA)', 'Domain-specific NLP', 'Adversarial Attacks and Robustness']","['Visual QA (VQA)', 'Medical and Clinical NLP']",,"Deep Neural Networks have been successfully used for the task of Visual Question Answering for the past few years owing to the availability of relevant large scale datasets. However these datasets are created in artificial settings and rarely reflect the real world scenario. Recent research effectively applies these VQA models for answering visual questions for the blind. Despite achieving high accuracy these models appear to be susceptible to variation in input questions.We analyze popular VQA models through the lens of attribution input's influence on predictions to gain valuable insights. Further, We use these insights to craft adversarial attacks which inflict significant damage to these systems with negligible change in meaning of the input questions. We believe this will enhance development of systems more robust to the possible variations in inputs when deployed to assist the visually impaired.",https://aclanthology.org/2020.challengehml-1.9,Association for Computational Linguistics,2020,July,Second Grand-Challenge and Workshop on Multimodal Language (Challenge-HML),"Halbe, Shaunak",Exploring Weaknesses of VQA Models through Attribution Driven Insights,10.18653/v1/2020.challengehml-1.9,challengehml,275
2021.dialdoc-1.12,"['Embeddings', 'Dialogue Systems', 'Model Architectures']",['Transformer Models'],,"Document-grounded goal-oriented dialog system understands users' utterances, and generates proper responses by using information obtained from documents. The Dialdoc21 shared task consists of two subtasks; subtask1, finding text spans associated with users' utterances from documents, and subtask2, generating responses based on information obtained from subtask1. In this paper, we propose two models i.e., a knowledge span prediction model and a response generation model for the sub-task1 and the subtask2. In the subtask1, dialogue act losses are used with RoBERTa, and title embeddings are added to input representation of RoBERTa. In the subtask2, various special tokens and embeddings are added to input representation of BART's encoder. Then, we propose a method to assign different difficulty scores to leverage curriculum learning. In the subtask1, our span prediction model achieved F1-scores of 74.81 ranked at top 7 and 73.41 ranked at top 5 in test-dev phase and test phase, respectively. In the subtask2, our response generation model achieved sacre-BLEUs of 37.50 ranked at top 3 and 41.06 ranked at top 1 in in test-dev phase and test phase, respectively.",https://aclanthology.org/2021.dialdoc-1.12,Association for Computational Linguistics,2021,August,Proceedings of the 1st Workshop on Document-grounded Dialogue and Conversational Question Answering (DialDoc 2021),"Kim, Boeun  and
Lee, Dohaeng  and
Kim, Sihyung  and
Lee, Yejin  and
Huang, Jin-Xia  and
Kwon, Oh-Woog  and
Kim, Harksoo",Document-Grounded Goal-Oriented Dialogue Systems on Pre-Trained Language Model with Diverse Input Representation,10.18653/v1/2021.dialdoc-1.12,dialdoc,960
2020.rocling-1.4,"['Text Preprocessing', 'Information Extraction', 'Model Architectures', 'Low-resource Languages']",['Text Segmentation'],['Word Segmentation'],"The prevalence of the web has brought about the construction of many large-scale, automatically segmented and tagged corpora, which inevitably introduces errors due to automation and are likely to have negative impacts on downstream tasks. Collocation extraction from Chinese corpora is one such task that is profoundly influenced by the quality of word segmentation. This paper explores methods to mitigate the negative impacts of word segmentation errors on collocation extraction in Chinese. In particular, we experimented with a simple model that aims to combine several association measures linearly to avoid retrieving false collocations resulting from word segmentation errors. The results of the experiment show that this simple model could not differentiate between true collocations and false collocations",https://aclanthology.org/2020.rocling-1.4,The Association for Computational Linguistics and Chinese Language Processing (ACLCLP),2020,September,Proceedings of the 32nd Conference on Computational Linguistics and Speech Processing (ROCLING 2020),"Liao, Yongfu  and
Hsieh, Shu-Kai",Mitigating Impacts of Word Segmentation Errors on Collocation Extraction in {C}hinese,,rocling,802
2021.case-1.6,"['Data Management and Generation', 'Information Extraction']","['Data Analysis', 'Event Extraction']",,"Language provides speakers with a rich system of modality for expressing thoughts about events, without being committed to their actual occurrence. Modality is commonly used in the political news domain, where both actual and possible courses of events are discussed. NLP systems struggle with these semantic phenomena, often incorrectly extracting events which did not happen, which can lead to issues in downstream applications. We present an opendomain, lexicon-based event extraction system that captures various types of modality. This information is valuable for Question Answering, Knowledge Graph construction and Factchecking tasks, and our evaluation shows that the system is sufficiently strong to be used in downstream applications.",https://aclanthology.org/2021.case-1.6,Association for Computational Linguistics,2021,August,Proceedings of the 4th Workshop on Challenges and Applications of Automated Extraction of Socio-political Events from Text (CASE 2021),"Bijl de Vroe, Sander  and
Guillou, Liane  and
Stanojevi{\'c}, Milo{\v{s}}  and
McKenna, Nick  and
Steedman, Mark",Modality and Negation in Event Extraction,10.18653/v1/2021.case-1.6,case,1337
W19-4726,"['Domain-specific NLP', 'Language Change Analysis', 'Data Management and Generation']","['Semantic Change Analysis', 'Data Preparation']",,"We investigate changes in the meanings of words used in the UK Parliament across two different decade-long epochs. We use word embeddings to explore changes in the distribution of words of interest and uncover words that appear to have undergone semantic transformation in the intervening period. We explore different ways of obtaining target words for this purpose. We find that semantic changes are generally in line with those found in other corpora, and little evidence that parliamentary language is more static than general English. It also seems that words with senses that have been recorded in the dictionary as having fallen into disuse do not undergo semantic changes in this domain.",https://aclanthology.org/W19-4726,Association for Computational Linguistics,2019,August,Proceedings of the 1st International Workshop on Computational Approaches to Historical Language Change,"Abercrombie, Gavin  and
Batista-Navarro, Riza",Semantic Change in the Language of UK Parliamentary Debates,10.18653/v1/W19-4726,W19,1468
2022.in2writing-1.1,['Text Generation'],['Data-to-Text Generation'],,"Today, data-to-text systems are used as commercial solutions for automated text production of large quantities of text. Therefore, they already represent a new technology of writing. This new technology requires the author, as an act of writing, both to configure a system that then takes over the transformation into a real text, but also to maintain strategies of traditional writing. What should an environment look like, where a human guides a machine to write texts? Based on a comparison of the NLG pipeline architecture with the results of the research on the human writing process, this paper attempts to take an overview of which tasks need to be solved and which strategies are necessary to produce good texts in this environment. From this synopsis, principles for the design of data-to-text systems as a functioning writing environment are then derived.",https://aclanthology.org/2022.in2writing-1.1,Association for Computational Linguistics,2022,May,Proceedings of the First Workshop on Intelligent and Interactive Writing Assistants (In2Writing 2022),"Schneider, Adela  and
Madsack, Andreas  and
Heininger, Johanna  and
Chen, Ching-Yi  and
Wei{\ss}graeber, Robert",Data-to-text systems as writing environment,10.18653/v1/2022.in2writing-1.1,in2writing,664
2021.deelio-1.13,"['Evaluation Techniques', 'Low-resource Languages', 'Model Architectures']",,,"The field of explainable AI has recently seen an explosion in the number of explanation methods for highly non-linear deep neural networks. The extent to which such methods -that are often proposed and tested in the domain of computer vision -are appropriate to address the explainability challenges in NLP is yet relatively unexplored. In this work, we consider Contextual Decomposition CD -a Shapley-based input feature attribution method that has been shown to work well for recurrent NLP models -and we test the extent to which it is useful for models that contain attention operations. To this end, we extend CD to cover the operations necessary for attention-based models. We then compare how long distance subject-verb relationships are processed by models with and without attention, considering a number of different syntactic structures in two different languages: English and Dutch. Our experiments confirm that CD can successfully be applied for attentionbased models as well, providing an alternative Shapley-based attribution method for modern neural networks. In particular, using CD, we show that the English and Dutch models demonstrate similar processing behaviour, but that under the hood there are consistent differences between our attention and non-attention models.",https://aclanthology.org/2021.deelio-1.13,Association for Computational Linguistics,2021,June,Proceedings of Deep Learning Inside Out (DeeLIO): The 2nd Workshop on Knowledge Extraction and Integration for Deep Learning Architectures,"Kersten, Tom  and
Wong, Hugh Mee  and
Jumelet, Jaap  and
Hupkes, Dieuwke",Attention vs non-attention for a Shapley-based explanation method,10.18653/v1/2021.deelio-1.13,deelio,1440
2020.multilingualbio-1.3,"['Domain-specific NLP', 'Data Management and Generation', 'Knowledge Representation and Reasoning', 'Low-resource Languages']",['Medical and Clinical NLP'],,"Medical language exhibits large variations regarding users, institutions, and language registers. With large parts of clinical information only documented in free text, NLP plays an important role in unlocking potentially re-usable and interoperable meaning from medical records in a multitude of natural languages. This study highlights the role of interface vocabularies. It describes the architectural principles and the evolution of a German interface vocabulary, which is under development by combining machine translation with human annotation and rule-based term generation, yielding a resource with 7.7 million raw entries, each of which linked to the reference terminology SNOMED CT, an international standard with about 350 thousand concepts. The purpose is to offer a high coverage of German medical jargon, in order to optimise terminology grounding of clinical texts by NLP systems. The core resource is a manually maintained table of English-to-German word and chunk translations, supported by a set of language generation rules. We describe a workflow consisting in the enrichment and modification of this table by human and machine efforts, together with top-down and bottomup methods for terminology population. A term generator generates the final vocabulary by creating one-to-many German variants per SNOMED CT English description. Filtering against a large collection of domain terminologies and corpora drastically reduces the size of the vocabulary in favour of terms that can reasonably be expected to match clinical text passages within a text-mining pipeline. An evaluation was performed by a comparison between the current version of the German interface vocabulary and the English description table of the SNOMED CT International release. An exact term matching was performed with a small parallel corpus constituted by text snippets from different clinical documents. With overall low retrieval parameters with F-values around 30%, the performance of the German language scenario reaches 80 -90% of the English one. Interestingly, annotations are slightly better with machine-translated German -English texts, using the International SNOMED CT resource only.",https://aclanthology.org/2020.multilingualbio-1.3,European Language Resources Association,2020,May,Proceedings of the LREC 2020 Workshop on Multilingual Biomedical Text Processing (MultilingualBIO 2020),"Schulz, Stefan  and
Hammer, Larissa  and
Hashemian-Nik, David  and
Kreuzthaler, Markus",Localising the Clinical Terminology SNOMED CT by Semi-automated Creation of a German Interface Vocabulary,,multilingualbio,1461
2020.sigdial-1.27,"['Audio Generation and Processing', 'Discourse Analysis']",,,"Acoustic/prosodic a/p entrainment has been associated with multiple positive social aspects of human-human conversations. However, research on its effects is still preliminary, first because how to model it is far from standardized, and second because most of the reported findings rely on small corpora or on corpora collected in experimental setups. The present article has a twofold purpose: 1 it proposes a unifying statistical framework for modeling a/p entrainment, and 2 it tests on two large corpora of spontaneous telephone interactions whether three metrics derived from this framework predict positive social aspects of the conversations. The corpora differ in their spoken language, domain, and positive social outcome attached. To our knowledge, this is the first article studying relations between a/p entrainment and positive social outcomes in such large corpora of spontaneous dialog. Our results suggest that our metrics effectively predict, up to some extent, positive social aspects of conversations, which not only validates the methodology, but also provides further insights into the elusive topic of entrainment in human-human conversation.",https://aclanthology.org/2020.sigdial-1.27,Association for Computational Linguistics,2020,July,Proceedings of the 21th Annual Meeting of the Special Interest Group on Discourse and Dialogue,"G{\'a}lvez, Ramiro H.  and
Gauder, Lara  and
Luque, Jordi  and
Gravano, Agust{\'\i}n",A unifying framework for modeling acoustic/prosodic entrainment: definition and evaluation on two large corpora,10.18653/v1/2020.sigdial-1.27,sigdial,638
N18-1039,"['Data Management and Generation', 'Image and Video Processing', 'Learning Paradigms', 'Classification Applications', 'Model Architectures']",['Data Preparation'],,"The present work investigates whether different quantification mechanisms set comparison, vague quantification, and proportional estimation can be jointly learned from visual scenes by a multi-task computational model. The motivation is that, in humans, these processes underlie the same cognitive, nonsymbolic ability, which allows an automatic estimation and comparison of set magnitudes. We show that when information about lowercomplexity tasks is available, the higher-level proportional task becomes more accurate than when performed in isolation. Moreover, the multi-task model is able to generalize to unseen combinations of target/non-target objects. Consistently with behavioral evidence showing the interference of absolute number in the proportional task, the multi-task model no longer works when asked to provide the number of target objects in the scene.",https://aclanthology.org/N18-1039,Association for Computational Linguistics,2018,June,"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)","Pezzelle, Sandro  and
Sorodoc, Ionut-Teodor  and
Bernardi, Raffaella","Comparatives, Quantifiers, Proportions: a Multi-Task Model for the Learning of Quantities from Vision",10.18653/v1/N18-1039,N18,244
L16-1178,"['Data Management and Generation', 'Low-resource Languages', 'Classification Applications']","['Data Preparation', 'Sentiment Analysis (SA)']",['Annotation Processes'],"The automatic analysis of texts containing opinions of users about, e.g., products or political views has gained attention within the last decades. However, previous work on the task of analyzing user reviews about mobile applications in app stores is limited. Publicly available corpora do not exist, such that a comparison of different methods and models is difficult. We fill this gap by contributing the Sentiment Corpus of App Reviews SCARE, which contains fine-grained annotations of application aspects, subjective evaluative phrases and relations between both. This corpus consists of 1,760 annotated application reviews from the Google Play Store with 2,487 aspects and 3,959 subjective phrases. We describe the process and methodology how the corpus was created. The Fleiss- between four annotators reveals an agreement of 0.72. We provide a strong baseline with a linear-chain conditional random field and word-embedding features with a performance of 0.62 for aspect detection and 0.63 for the extraction of subjective phrases. The corpus is available to the research community to support the development of sentiment analysis methods on mobile application reviews.",https://aclanthology.org/L16-1178,European Language Resources Association (ELRA),2016,May,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),"S{\""a}nger, Mario  and
Leser, Ulf  and
Kemmerer, Steffen  and
Adolphs, Peter  and
Klinger, Roman",SCARE  The Sentiment Corpus of App Reviews with Fine-grained Annotations in German,,L16,1157
2020.coling-main.251,"['Model Architectures', 'Text Generation']",['Large Language Models (LLMs)'],,"Human use language not just to convey information but also to express their inner feelings and mental states. In this work, we adapt the state-of-the-art language generation models to generate affective emotional text. We posit a model capable of generating affect-driven and topic focused sentences without losing grammatical correctness as the affect intensity increases. We propose to incorporate emotion as prior for the probabilistic state-of-the-art text generation model such as GPT-2. The model gives a user the flexibility to control the category and intensity of emotion as well as the topic of the generated text. Previous attempts at modelling fine-grained emotions fall out on grammatical correctness at extreme intensities, but our model is resilient to this and delivers robust results at all intensities. We conduct automated evaluations and human studies to test the performance of our model, and provide a detailed comparison of the results with other models. In all evaluations, our model outperforms existing affective text generation models.",https://aclanthology.org/2020.coling-main.251,International Committee on Computational Linguistics,2020,December,Proceedings of the 28th International Conference on Computational Linguistics,"Goswamy, Tushar  and
Singh, Ishika  and
Barkati, Ahsan  and
Modi, Ashutosh",Adapting a Language Model for Controlled Affective Text Generation,10.18653/v1/2020.coling-main.251,coling,177
2020.iwltp-1.7,"['Multilingual NLP', 'Audio Generation and Processing', 'Machine Translation (MT)']",['Automatic Speech Recognition (ASR)'],,"This paper presents our progress towards deploying a versatile communication platform in the task of highly multilingual live speech translation for conferences and remote meetings live subtitling. The platform has been designed with a focus on very low latency and high flexibility while allowing research prototypes of speech and text processing tools to be easily connected, regardless of where they physically run. We outline our architecture solution and also briefly compare it with the ELG platform. Technical details are provided on the most important components and we summarize the test deployment events we ran so far.",https://aclanthology.org/2020.iwltp-1.7,European Language Resources Association,2020,May,Proceedings of the 1st International Workshop on Language Technology Platforms,"Franceschini, Dario  and
Canton, Chiara  and
Simonini, Ivan  and
Schweinfurth, Armin  and
Glott, Adelheid  and
St{\""u}ker, Sebastian  and
Nguyen, Thai-Son  and
Schneider, Felix  and
Ha, Thanh-Le  and
Waibel, Alex  and
Haddow, Barry  and
Williams, Philip  and
Sennrich, Rico  and
Bojar, Ond{\v{r}}ej  and
Sagar, Sangeet  and
Mach{\'a}{\v{c}}ek, Dominik  and
Smr{\v{z}}, Otakar",Removing European Language Barriers with Innovative Machine Translation Technology,,iwltp,1106
S16-1128,"['Question Answering (QA)', 'Information Retrieval', 'Low-resource Languages', 'Model Architectures']",['Community QA'],,"Community question answering platforms need to automatically rank answers and questions with respect to a given question. In this paper, we present the approaches for the Answer Selection and Question Retrieval tasks of SemEval-2016 task 3. We develop a bag-of-vectors approach with various vectorand text-based features, and different neural network approaches including CNNs and LSTMs to capture the semantic similarity between questions and answers for ranking purpose. Our evaluation demonstrates that our approaches significantly outperform the baselines.",https://aclanthology.org/S16-1128,Association for Computational Linguistics,2016,June,Proceedings of the 10th International Workshop on Semantic Evaluation ({S}em{E}val-2016),"Mohtarami, Mitra  and
Belinkov, Yonatan  and
Hsu, Wei-Ning  and
Zhang, Yu  and
Lei, Tao  and
Bar, Kfir  and
Cyphers, Scott  and
Glass, Jim",SLS at SemEval-2016 Task 3: Neural-based Approaches for Ranking in Community Question Answering,10.18653/v1/S16-1128,S16,1400
2021.ranlp-1.58,"['Information Retrieval', 'Data Management and Generation', 'Classification Applications']","['Personality Trait Prediction', 'Data Analysis', 'Information Filtering']",,"In recent years, a number of studies have used linear models for personality prediction based on text. In this paper, we empirically analyze and compare the lexical signals captured in such models. We identify lexical cues for each dimension of the MBTI personality scheme in several different ways, considering different datasets, feature sets, and learning algorithms. We conduct a series of correlation analyses between the resulting MBTI data and explore their connection to other signals, such as for Big-5 traits, emotion, sentiment, age, and gender. The analysis shows intriguing correlation patterns between different personality dimensions and other traits, and also provides evidence for the robustness of the data.",https://aclanthology.org/2021.ranlp-1.58,INCOMA Ltd.,2021,September,Proceedings of the International Conference on Recent Advances in Natural Language Processing (RANLP 2021),"He, Xiaoli  and
de Melo, Gerard",Personality Predictive Lexical Cues and Their Correlations,10.26615/978-954-452-072-4_058,ranlp,1332
2021.emnlp-main.352,"['Text Generation', 'Model Architectures']","['Transformer Models', 'Data-to-Text Generation']",,"We propose to tackle data-to-text generation tasks by directly splicing together retrieved segments of text from ""neighbor"" sourcetarget pairs. Unlike recent work that conditions on retrieved neighbors but generates text token-by-token, left-to-right, we learn a policy that directly manipulates segments of neighbor text, by inserting or replacing them in partially constructed generations. Standard techniques for training such a policy require an oracle derivation for each generation, and we prove that finding the shortest such derivation can be reduced to parsing under a particular weighted context-free grammar. We find that policies learned in this way perform on par with strong baselines in terms of automatic and human evaluation, but allow for more interpretable and controllable generation.",https://aclanthology.org/2021.emnlp-main.352,Association for Computational Linguistics,2021,November,Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,"Wiseman, Sam  and
Backurs, Arturs  and
Stratos, Karl",Data-to-text Generation by Splicing Together Nearest Neighbors,10.18653/v1/2021.emnlp-main.352,emnlp,60
2020.nlpcovid19-acl.16,"['Domain-specific NLP', 'Topic Modeling', 'Data Management and Generation', 'Low-resource Languages', 'Classification Applications']","['Data Preparation', 'Medical and Clinical NLP', 'Rumor Detection', 'NLP for News and Media']",['NLP for Social Media'],"In March 2020, the World Health Organization announced the COVID-19 outbreak as a pandemic. Most previous social media related research has been on English tweets and COVID-19. In this study, we collect approximately 1 million Arabic tweets from the Twitter streaming API related to COVID-19. Focussing on outcomes that we believe will be useful for Public Health Organizations, we analyse them in three different ways: identifying the topics discussed during the period, detecting rumours, and predicting the source of the tweets. We use the k-means algorithm for the first goal with k=5. The topics discussed can be grouped as follows: COVID-19 statistics, prayers for God, COVID-19 locations, advise and education for prevention, and advertising. We sample 2000 tweets and label them manually for false information, correct information, and unrelated. Then, we apply three different machine learning algorithms, Logistic Regression, Support Vector Classification, and Nave Bayes with two sets of features, word frequency approach and word embeddings. We find that Machine Learning classifiers are able to correctly identify the rumour related tweets with 84% accuracy. We also try to predict the source of the rumour related tweets depending on our previous model which is about classifying tweets into five categories: academic, media, government, health professional, and public. Around 60% of the rumour related tweets are classified as written by health professionals and academics.",https://aclanthology.org/2020.nlpcovid19-acl.16,Association for Computational Linguistics,2020,July,Proceedings of the 1st Workshop on {NLP} for {COVID-19} at {ACL} 2020,"Alsudias, Lama  and
Rayson, Paul",COVID-19 and Arabic Twitter: How can Arab World Governments and Public Health Organizations Learn from Social Media?,10.12785/ijcds/140115,nlpcovid19,522
K17-2004,"['Multilingual NLP', 'Low-resource Languages', 'Model Architectures']",['Recurrent Neural Networks (RNNs)'],['Long Short-Term Memory (LSTM) Models'],"This paper presents the submissions by the University of Zurich to the SIGMOR-PHON 2017 shared task on morphological reinflection. The task is to predict the inflected form given a lemma and a set of morpho-syntactic features. We focus on neural network approaches that can tackle the task in a limited-resource setting. As the transduction of the lemma into the inflected form is dominated by copying over lemma characters, we propose two recurrent neural network architectures with hard monotonic attention that are strong at copying and, yet, substantially different in how they achieve this. The first approach is an encoderdecoder model with a copy mechanism. The second approach is a neural statetransition system over a set of explicit edit actions, including a designated COPY action. We experiment with character alignment and find that naive, greedy alignment consistently produces strong results for some languages. Our best system combination is the overall winner of the SIG-MORPHON 2017 Shared Task 1 without external resources. At a setting with 100 training samples, both our approaches, as ensembles of models, outperform the next best competitor.",https://aclanthology.org/K17-2004,Association for Computational Linguistics,2017,August,Proceedings of the {C}o{NLL} {SIGMORPHON} 2017 Shared Task: Universal Morphological Reinflection,"Makarov, Peter  and
Ruzsics, Tatiana  and
Clematide, Simon",Align and Copy: UZH at SIGMORPHON 2017 Shared Task for Morphological Reinflection,10.18653/v1/K17-2004,K17,970
2020.law-1.17,"['Data Management and Generation', 'Discourse Analysis']",['Data Preparation'],['Annotation Processes'],"This study develops the strand of research on topic transitions in social talk which aims to gain a better understanding of interlocutors' conversational goals. Lu and Malamud 2020 proposed that one way to identify such transitions is to annotate coherence relations, and then to identify utterances potentially expressing new topics as those that fail to participate in these relations. This work validates and refines their suggested annotation methodology, focusing on annotating most prominent coherence relations in face-to-face social dialogue. The result is a publicly accessible gold standard corpus with efficient and reliable annotation, whose broad coverage provides a foundation for future steps of identifying and classifying new topic utterances. 1",https://aclanthology.org/2020.law-1.17,Association for Computational Linguistics,2020,December,Proceedings of the 14th Linguistic Annotation Workshop,"Luu, Alex  and
Malamud, Sophia A.",Annotating Coherence Relations for Studying Topic Transitions in Social Talk,,law,794
P19-1087,"['Domain-specific NLP', 'Evaluation Techniques', 'Data Management and Generation', 'Information Extraction', 'Model Architectures']","['Data Preparation', 'Medical and Clinical NLP']",['Annotation Processes'],"This paper describes novel models tailored for a new application, that of extracting the symptoms mentioned in clinical conversations along with their status. Lack of any publicly available corpus in this privacy-sensitive domain led us to develop our own corpus, consisting of about 3K conversations annotated by professional medical scribes. We propose two novel deep learning approaches to infer the symptom names and their status: 1 a new hierarchical span-attribute tagging SA-T model, trained using curriculum learning, and 2 a variant of sequence-to-sequence model which decodes the symptoms and their status from a few speaker turns within a sliding window over the conversation. This task stems from a realistic application of assisting medical providers in capturing symptoms mentioned by patients from their clinical conversations. To reflect this application, we define multiple metrics. From inter-rater agreement, we find that the task is inherently difficult. We conduct comprehensive evaluations on several contrasting conditions and observe that the performance of the models range from an F-score of 0.5 to 0.8 depending on the condition. Our analysis not only reveals the inherent challenges of the task, but also provides useful directions to improve the models.",https://aclanthology.org/P19-1087,Association for Computational Linguistics,2019,July,Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,"Du, Nan  and
Chen, Kai  and
Kannan, Anjuli  and
Tran, Linh  and
Chen, Yuhui  and
Shafran, Izhak",Extracting Symptoms and their Status from Clinical Conversations,10.18653/v1/P19-1087,P19,1169
2020.aespen-1.11,"['Text Clustering', 'Domain-specific NLP', 'Information Extraction', 'Model Architectures']","['NLP for News and Media', 'Coreference Resolution']",,"This paper summarizes our group's efforts in the event sentence coreference identification shared task, which is organized as part of the Automated Extraction of Socio-Political Events from News AESPEN Workshop. Our main approach consists of three steps. We initially use a transformer based model to predict whether a pair of sentences refer to the same event or not. Later, we use these predictions as the initial scores and recalculate the pair scores by considering the relation of sentences in a pair with respect to other sentences. As the last step, final scores between these sentences are used to construct the clusters, starting with the pairs with the highest scores. Our proposed approach outperforms the baseline approach across all evaluation metrics.",https://aclanthology.org/2020.aespen-1.11,European Language Resources Association (ELRA),2020,May,Proceedings of the Workshop on Automated Extraction of Socio-political Events from News 2020,"{\""O}rs, Faik Kerem  and
Yeniterzi, S{\""u}veyda  and
Yeniterzi, Reyyan",Event Clustering within News Articles,,aespen,1064
K18-2015,"['Text Preprocessing', 'Parsing', 'Multilingual NLP', 'Learning Paradigms', 'Low-resource Languages', 'Model Architectures']","['Syntactic Parsing', 'Part-of-Speech (POS) Tagging', 'Transfer Learning', 'Text Segmentation', 'Recurrent Neural Networks (RNNs)']","['Word Segmentation', 'Dependency Parsing', 'Long Short-Term Memory (LSTM) Models']","This paper describes our system SLT-Interactions for the CoNLL 2018 shared task: Multilingual Parsing from Raw Text to Universal Dependencies. Our system performs three main tasks: word segmentation only for few treebanks, POS tagging and parsing. While segmentation is learned separately, we use neural stacking for joint learning of POS tagging and parsing tasks. For all the tasks, we employ simple neural network architectures that rely on long short-term memory LSTM networks for learning task-dependent features. At the basis of our parser, we use an arc-standard algorithm with Swap action for general non-projective parsing. Additionally, we use neural stacking as a knowledge transfer mechanism for cross-domain parsing of low resource domains. Our system shows substantial gains against the UDPipe baseline, with an average improvement of 4.18% in LAS across all languages. Overall, we are placed at the 12 th position on the official test sets.",https://aclanthology.org/K18-2015,Association for Computational Linguistics,2018,October,Proceedings of the {C}o{NLL} 2018 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies,"Bhat, Riyaz A.  and
Bhat, Irshad  and
Bangalore, Srinivas",The SLT-Interactions Parsing System at the CoNLL 2018 Shared Task,10.18653/v1/K18-2015,K18,389
2020.wmt-1.127,"['Learning Paradigms', 'Low-resource Languages', 'Machine Translation (MT)']","['Neural MT (NMT)', 'Supervised Learning', 'Unsupervised Learning']",,"We present our submission to the very low resource supervised machine translation task at the Fifth Conference on Machine Translation. The goal of this task is to create a system which translates between German and the lowresource language Upper Sorbian. We use a decoder-only transformer architecture and formulate the translation task as language modeling. To address the low-resource aspect of the problem, we pretrain over a similar language parallel corpus. Then, we employ an intermediate back-translation step before fine-tuning. Finally, we present an analysis of the system's performance.",https://aclanthology.org/2020.wmt-1.127,Association for Computational Linguistics,2020,November,Proceedings of the Fifth Conference on Machine Translation,"Berckmann, Tucker  and
Hiziroglu, Berkan",Low-Resource Translation as Language Modeling,,wmt,1161
2020.inlg-1.22,"['Evaluation Techniques', 'Data Management and Generation', 'Text Generation']","['Data Preparation', 'Data-to-Text Generation']",['Annotation Processes'],"Most Natural Language Generation systems need to produce accurate texts. We propose a methodology for high-quality human evaluation of the accuracy of generated texts, which is intended to serve as a gold-standard for accuracy evaluations of data-to-text systems. We use our methodology to evaluate the accuracy of computer generated basketball summaries. We then show how our gold standard evaluation can be used to validate automated metrics.",https://aclanthology.org/2020.inlg-1.22,Association for Computational Linguistics,2020,December,Proceedings of the 13th International Conference on Natural Language Generation,"Thomson, Craig  and
Reiter, Ehud",A Gold Standard Methodology for Evaluating Accuracy in Data-To-Text Systems,10.18653/v1/2020.inlg-1.22,inlg,350
2021.teachingnlp-1.24,['Domain-specific NLP'],,,"In this paper we provide an account of how we ported a text and data mining course online in summer 2020 as a result of the COVID-19 pandemic and how we improved it in a second pilot run. We describe the course, how we adapted it over the two pilot runs and what teaching techniques we used to improve students' learning and community building online. We also provide information on the relentless feedback collected during the course which helped us to adapt our teaching from one session to the next and one pilot to the next. We discuss the lessons learned and promote the use of innovative teaching techniques applied to the digital such as digital badges and pair programming in break-out rooms for teaching Natural Language Processing courses to beginners and students with different backgrounds.",https://aclanthology.org/2021.teachingnlp-1.24,Association for Computational Linguistics,2021,June,Proceedings of the Fifth Workshop on Teaching NLP,"Alex, Beatrice  and
Llewellyn, Clare  and
Orzechowski, Pawel  and
Boutchkova, Maria","The Online Pivot: Lessons Learned from Teaching a Text and Data Mining Course in Lockdown, Enhancing online Teaching with Pair Programming and Digital Badges",10.18653/v1/2021.teachingnlp-1.24,teachingnlp,755
C16-1274,"['Learning Paradigms', 'Classification Applications', 'Model Architectures']","['Supervised Learning', 'Recurrent Neural Networks (RNNs)']",,"We describe an attentive encoder that combines tree-structured recursive neural networks and sequential recurrent neural networks for modelling sentence pairs. Since existing attentive models exert attention on the sequential structure, we propose a way to incorporate attention into the tree topology. Specially, given a pair of sentences, our attentive encoder uses the representation of one sentence, which generated via an RNN, to guide the structural encoding of the other sentence on the dependency parse tree. We evaluate the proposed attentive encoder on three tasks: semantic similarity, paraphrase identification and true-false question selection. Experimental results show that our encoder outperforms all baselines and achieves state-of-the-art results on two tasks.",https://aclanthology.org/C16-1274,The COLING 2016 Organizing Committee,2016,December,"Proceedings of {COLING} 2016, the 26th International Conference on Computational Linguistics: Technical Papers","Zhou, Yao  and
Liu, Cong  and
Pan, Yan",Modelling Sentence Pairs with Tree-structured Attentive Encoder,10.48550/arxiv.1610.02806,C16,65
2021.semeval-1.122,"['Domain-specific NLP', 'Classification Applications', 'Model Architectures']","['Hate and Offensive Speech Detection', 'NLP for News and Media']",['NLP for Social Media'],"This article introduces the system description of the hub team, which explains the related work and experimental results of our team's participation in SemEval 2021 Task 5: Toxic Spans Detection. The data for this shared task comes from some posts on the Internet. The task goal is to identify the toxic content contained in these text data. We need to find the span of the toxic text in the text data as accurately as possible. In the same post, the toxic text may be one paragraph or multiple paragraphs. Our team uses a classification scheme based on word-level to accomplish this task. The system we used to submit the results is ALBERT+BILSTM+CRF. The result evaluation index of the task submission is the F1 score, and the final score of the prediction result of the test set submitted by our team is 0.6640226029.",https://aclanthology.org/2021.semeval-1.122,Association for Computational Linguistics,2021,August,Proceedings of the 15th International Workshop on Semantic Evaluation (SemEval-2021),"Huang, Bo  and
Bai, Yang  and
Zhou, Xiaobing",hub at SemEval-2021 Task 5: Toxic Span Detection Based on Word-Level Classification,10.18653/v1/2021.semeval-1.122,semeval,48
2021.gem-1.10,"['Multilingual NLP', 'Text Generation', 'Evaluation Techniques', 'Low-resource Languages']",,,"We introduce GEM, a living benchmark for natural language Generation NLG, its Evaluation, and Metrics. Measuring progress in NLG relies on a constantly evolving ecosystem of automated metrics, datasets, and human evaluation standards. Due to this moving target, new models often still evaluate on divergent anglo-centric corpora with wellestablished, but flawed, metrics. This disconnect makes it challenging to identify the limitations of current models and opportunities for progress. Addressing this limitation, GEM provides an environment in which models can easily be applied to a wide set of tasks and in which evaluation strategies can be tested. Regular updates to the benchmark will help NLG research become more multilingual and evolve the challenge alongside models. This paper serves as the description of the data for which we are organizing a shared task at our ACL 2021 Workshop and to which we invite the entire NLG community to participate.",https://aclanthology.org/2021.gem-1.10,Association for Computational Linguistics,2021,August,"Proceedings of the 1st Workshop on Natural Language Generation, Evaluation, and Metrics (GEM 2021)","Gehrmann, Sebastian  and
Adewumi, Tosin  and
Aggarwal, Karmanya  and
Ammanamanchi, Pawan Sasanka  and
Aremu, Anuoluwapo  and
Bosselut, Antoine  and
Chandu, Khyathi Raghavi  and
Clinciu, Miruna-Adriana  and
Das, Dipanjan  and
Dhole, Kaustubh  and
Du, Wanyu  and
Durmus, Esin  and
Du{\v{s}}ek, Ond{\v{r}}ej  and
Emezue, Chris Chinenye  and
Gangal, Varun  and
Garbacea, Cristina  and
Hashimoto, Tatsunori  and
Hou, Yufang  and
Jernite, Yacine  and
Jhamtani, Harsh  and
Ji, Yangfeng  and
Jolly, Shailza  and
Kale, Mihir  and
Kumar, Dhruv  and
Ladhak, Faisal  and
Madaan, Aman  and
Maddela, Mounica  and
Mahajan, Khyati  and
Mahamood, Saad  and
Majumder, Bodhisattwa Prasad  and
Martins, Pedro Henrique  and
McMillan-Major, Angelina  and
Mille, Simon  and
van Miltenburg, Emiel  and
Nadeem, Moin  and
Narayan, Shashi  and
Nikolaev, Vitaly  and
Niyongabo Rubungo, Andre  and
Osei, Salomey  and
Parikh, Ankur  and
Perez-Beltrachini, Laura  and
Rao, Niranjan Ramesh  and
Raunak, Vikas  and
Rodriguez, Juan Diego  and
Santhanam, Sashank  and
Sedoc, Jo{\~a}o  and
Sellam, Thibault  and
Shaikh, Samira  and
Shimorina, Anastasia  and
Sobrevilla Cabezudo, Marco Antonio  and
Strobelt, Hendrik  and
Subramani, Nishant  and
Xu, Wei  and
Yang, Diyi  and
Yerukola, Akhila  and
Zhou, Jiawei","The GEM Benchmark: Natural Language Generation, its Evaluation and Metrics",10.18653/v1/2021.gem-1.10,gem,245
2021.alta-1.16,"['Question Answering (QA)', 'Classification Applications', 'Learning Paradigms']",['Transfer Learning'],,"Fine-tuning pre-trained language models for downstream tasks has become a norm for NLP. Recently it is found that intermediate training based on high-level inference tasks such as Question Answering QA can improve the performance of some language models for target tasks. However it is not clear if intermediate training generally benefits various language models. In this paper, using the SQuAD-2.0 QA task for intermediate training for target text classification tasks, we experimented on eight tasks for single-sequence classification and eight tasks for sequence-pair classification using two base and two compact language models. Our experiments show that QA-based intermediate training generates varying transfer performance across different language models, except for similar QA tasks.",https://aclanthology.org/2021.alta-1.16,Australasian Language Technology Association,2021,December,Proceedings of the The 19th Annual Workshop of the Australasian Language Technology Association,"Zhang, Shiwei  and
Zhang, Xiuzhen",Does QA-based intermediate training help fine-tuning language models for text classification?,10.48550/arxiv.2112.15051,alta,1458
K16-1007,"['Parsing', 'Data Management and Generation', 'Information Extraction']",['Data Preparation'],['Annotation Processes'],"Domain-independent meaning representation of text has received a renewed interest in the NLP community. Comparison plays a crucial role in shaping objective and subjective opinion and measurement in natural language, and is often expressed in complex constructions including ellipsis. In this paper, we introduce a novel framework for jointly capturing the semantic structure of comparison and ellipsis constructions. Our framework models ellipsis and comparison as interconnected predicate-argument structures, which enables automatic ellipsis resolution. We show that a structured prediction model trained on our dataset of 2,800 gold annotated review sentences yields promising results. Together with this paper we release the dataset and an annotation tool which enables two-stage expert annotation on top of tree structures.",https://aclanthology.org/K16-1007,Association for Computational Linguistics,2016,August,Proceedings of The 20th {SIGNLL} Conference on Computational Natural Language Learning,"Bakhshandeh, Omid  and
Cornelia Wellwood, Alexis  and
Allen, James",Learning to Jointly Predict Ellipsis and Comparison Structures,10.18653/v1/K16-1007,K16,144
2022.in2writing-1.8,"['Evaluation Techniques', 'Text Generation', 'Domain-specific NLP']","['Story Generation', 'Poetry Generation']",,"The application of artificial intelligence AI for text generation in creative domains raises questions regarding the credibility of AI-generated content. In two studies, we explored if readers can differentiate between AI-based and human-written texts generated based on the first line of texts and poems of classic authors and how the stylistic qualities of these texts are rated. Participants read 9 AI-based continuations and either 9 human-written continuations Study 1, N=120 or 9 original continuations Study 2, N=302. Participants' task was to decide whether a continuation was written with an AI-tool or not, to indicate their confidence in each decision, and to assess the stylistic text quality. Results showed that participants generally had low accuracy for differentiating between text types but were overconfident in their decisions. Regarding the assessment of stylistic quality, AIcontinuations were perceived as less wellwritten, inspiring, fascinating, interesting, and aesthetic than both human-written and original continuations.",https://aclanthology.org/2022.in2writing-1.8,Association for Computational Linguistics,2022,May,Proceedings of the First Workshop on Intelligent and Interactive Writing Assistants (In2Writing 2022),"Gunser, Vivian Emily  and
Gottschling, Steffen  and
Brucker, Birgit  and
Richter, Sandra  and
{\c{C}}akir, D{\^\i}lan Canan  and
Gerjets, Peter",The Pure Poet: How Good is the Subjective Credibility and Stylistic Quality of Literary Short Texts Written with an Artificial Intelligence Tool as Compared to Texts Written by Human Authors?,10.18653/v1/2022.in2writing-1.8,in2writing,673
D16-1005,"['Domain-specific NLP', 'Data Management and Generation', 'Information Extraction', 'Low-resource Languages', 'Classification Applications', 'Model Architectures']","['Data Preparation', 'Temporal Event Understanding', 'NLP for News and Media']",['Annotation Processes'],"Determining whether a major societal event has already happened, is still on-going, or may occur in the future is crucial for event prediction, timeline generation, and news summarization. We introduce a new task and a new corpus, EventStatus, which has 4500 English and Spanish articles about civil unrest events labeled as PAST, ON-GOING, or FU-TURE. We show that the temporal status of these events is difficult to classify because local tense and aspect cues are often lacking, time expressions are insufficient, and the linguistic contexts have rich semantic compositionality. We explore two approaches for event status classification: 1 a feature-based SVM classifier augmented with a novel induced lexicon of future-oriented verbs, such as ""threatened"" and ""planned"", and 2 a convolutional neural net. Both types of classifiers improve event status recognition over a state-of-the-art TempEval model, and our analysis offers linguistic insights into the semantic compositionality challenges for this new task.",https://aclanthology.org/D16-1005,Association for Computational Linguistics,2016,November,Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing,"Huang, Ruihong  and
Cases, Ignacio  and
Jurafsky, Dan  and
Condoravdi, Cleo  and
Riloff, Ellen","Distinguishing Past, On-going, and Future Events: The EventStatus Corpus",10.18653/v1/D16-1005,D16,55
W19-3105,"['Low-resource Languages', 'Knowledge Representation and Reasoning', 'Finite State Machines']",,,"In finite-state language processing pipelines, a lexicon is often a key component. It needs to be comprehensive to ensure accuracy, reducing out-of-vocabulary misses. However, in memory-constrained environments e.g., mobile phones, the size of the component automata must be kept small. Indeed, a delicate balance between comprehensiveness, speed, and memory must be struck to conform to device requirements while providing a good user experience. In this paper, we describe a compression scheme for lexicons when represented as finite-state transducers. We efficiently encode the graph of the transducer while storing transition labels separately. The graph encoding scheme is based on the LOUDS Level Order Unary Degree Sequence tree representation, which has constant time tree traversal for queries while being information-theoretically optimal in space. We find that our encoding is near the theoretical lower bound for such graphs and substantially outperforms more traditional representations in space while remaining competitive in latency benchmarks.",https://aclanthology.org/W19-3105,Association for Computational Linguistics,2019,September,Proceedings of the 14th International Conference on Finite-State Methods and Natural Language Processing,"Cognetta, Marco  and
Allauzen, Cyril  and
Riley, Michael",On the Compression of Lexicon Transducers,10.18653/v1/W19-3105,W19,1221
2020.sustainlp-1.21,"['Finite State Machines', 'Model Architectures', 'Classification Applications']",['Sentiment Analysis (SA)'],,"We compare a classical CNN architecture for sequence classification involving several convolutional and max-pooling layers against a simple model based on weighted finite state automata WFA. Each model has its advantages and disadvantages and it is possible that they could be combined. However, we believe that the first research goal should be to investigate and understand how do these two apparently dissimilar models compare in the context of specific natural language processing tasks. This paper is the first step towards that goal. Our experiments with five sequence classification datasets suggest that, despite the apparent simplicity of WFA models and training algorithms, the performance of WFAs is comparable to that of the CNNs.",https://aclanthology.org/2020.sustainlp-1.21,Association for Computational Linguistics,2020,November,Proceedings of SustaiNLP: Workshop on Simple and Efficient Natural Language Processing,"Quattoni, Ariadna  and
Carreras, Xavier",A comparison between CNNs and WFAs for Sequence Classification,10.18653/v1/2020.sustainlp-1.21,sustainlp,1007
2020.wmt-1.132,"['Machine Translation (MT)', 'Data Management and Generation', 'Learning Paradigms', 'Low-resource Languages', 'Model Architectures']","['Transfer Learning', 'Neural MT (NMT)', 'Data Augmentation']",,"We describe the National Research Council of Canada NRC neural machine translation systems for the German-Upper Sorbian supervised track of the 2020 shared task on Unsupervised MT and Very Low Resource Supervised MT. Our models are ensembles of Transformer models, built using combinations of BPE-dropout, lexical modifications, and backtranslation.",https://aclanthology.org/2020.wmt-1.132,Association for Computational Linguistics,2020,November,Proceedings of the Fifth Conference on Machine Translation,"Knowles, Rebecca  and
Larkin, Samuel  and
Stewart, Darlene  and
Littell, Patrick",NRC Systems for Low Resource German-Upper Sorbian Machine Translation 2020: Transfer Learning with Lexical Modifications,,wmt,274
C16-1135,"['Question Answering (QA)', 'Domain-specific NLP', 'Classification Applications', 'Model Architectures']","['Community QA', 'NLP for News and Media']",,"Community Question Answering cQA forums have become a popular medium for soliciting answers to specific user questions from experts and experienced users in a given topic. However, for a given question, users sometimes have to sift through a large number of low-quality or irrelevant answers to find out the answer which satisfies their information need. To alleviate this, the problem of Answer Quality Prediction AQP aims to predict the quality of an answer posted in response to a forum question. Current AQP systems either learn models using -a various hand-crafted features HCF or b Deep Learning DL techniques which automatically learn the feature representations. In this paper, we propose a novel approach for AQP known as -""Deep Feature Fusion Network DFFN"" which combines the advantages of both hand-crafted features and deep learning based systems. Given a question-answer pair along with its metadata, a DFFN architecture independently -a learns features using the Deep Neural Network DNN and b computes hand-crafted features leveraging various external resources and then combines them using a fully connected neural network trained to predict the quality of the given answer. DFFN is an end-end differentiable model and trained as a single system. We propose two different DFFN architectures which vary mainly in the way they model the input question/answer pair -a DFFN-CNN which uses a Convolutional Neural Network CNN and b DFFN-BLNA which uses a Bi-directional LSTM with Neural Attention BLNA. Both these proposed variants of DFFN DFFN-CNN and DFFN-BLNA achieve state-of-the-art performance on the standard SemEval-2015 and SemEval-2016 benchmark datasets and outperforms baseline approaches which individually employ either HCF or DL based techniques alone.",https://aclanthology.org/C16-1135,The COLING 2016 Organizing Committee,2016,December,"Proceedings of {COLING} 2016, the 26th International Conference on Computational Linguistics: Technical Papers","Suggu, Sai Praneeth  and
Naga Goutham, Kushwanth  and
Chinnakotla, Manoj K.  and
Shrivastava, Manish",Hand in Glove: Deep Feature Fusion Network Architectures for Answer Quality Prediction in Community Question Answering,,C16,504
2020.nlptea-1.10,"['Data Management and Generation', 'Error Detection and Correction', 'Model Architectures', 'Low-resource Languages']","['Grammatical Error Correction (GEC)', 'Data Augmentation']",,"A better Chinese Grammatical Error Diagnosis CGED system for automatic Grammatical Error Correction GEC can benefit foreign Chinese learners and lower Chinese learning barriers. In this paper, we introduce our solution to the CGED2020 Shared Task Grammatical Error Correction in detail. The task aims to detect and correct grammatical errors that occur in essays written by foreign Chinese learners. Our solution combined data augmentation methods, spelling check methods, and generative grammatical correction methods, and achieved the best recall score in the Top 1 Correction track. Our final result ranked fourth among the participants.",https://aclanthology.org/2020.nlptea-1.10,Association for Computational Linguistics,2020,December,Proceedings of the 6th Workshop on Natural Language Processing Techniques for Educational Applications,"Wang, Yi  and
Yuan, Ruibin  and
Luo, Yan{`}gen  and
Qin, Yufang  and
Zhu, NianYong  and
Cheng, Peng  and
Wang, Lihuan",Chinese Grammatical Error Correction Based on Hybrid Models with Data Augmentation,10.18653/v1/2020.nlptea-1.10,nlptea,457
2022.constraint-1.6,"['Domain-specific NLP', 'Classification Applications', 'Data Management and Generation']","['Data Augmentation', 'Medical and Clinical NLP', 'Hate and Offensive Speech Detection', 'NLP for News and Media']",,"Harmful or abusive online content has been increasing over time, raising concerns for social media platforms, government agencies, and policymakers. Such harmful or abusive content can have major negative impact on society, e.g., cyberbullying can lead to suicides, rumors about COVID-19 can cause vaccine hesitance, promotion of fake cures for COVID-19 can cause health harms and deaths. The content that is posted and shared online can be textual, visual, or a combination of both, e.g., in a meme. Here, we describe our experiments in detecting the roles of the entities hero, villain, victim in harmful memes, which is part of the CONSTRAINT-2022 shared task, as well as our system for the task. We further provide a comparative analysis of different experimental settings i.e., unimodal, multimodal, attention, and augmentation. For reproducibility, we make our experimental code publicly available. 1",https://aclanthology.org/2022.constraint-1.6,Association for Computational Linguistics,2022,May,Proceedings of the Workshop on Combating Online Hostile Posts in Regional Languages during Emergency Situations,"Nandi, Rabindra Nath  and
Alam, Firoj  and
Nakov, Preslav",Detecting the Role of an Entity in Harmful Memes: Techniques and their Limitations,10.18653/v1/2022.constraint-1.6,constraint,990
2021.spnlp-1.4,['Learning Paradigms'],['Reinforcement Learning'],,"Large volumes of interaction logs can be collected from NLP systems that are deployed in the real world. How can this wealth of information be leveraged? Using such interaction logs in an offline reinforcement learning RL setting is a promising approach. However, due to the nature of NLP tasks and the constraints of production systems, a series of challenges arise. We present a concise overview of these challenges and discuss possible solutions. * All authors contributed equally, order has been randomized see https://bit.ly/38PgRjm.",https://aclanthology.org/2021.spnlp-1.4,Association for Computational Linguistics,2021,August,Proceedings of the 5th Workshop on Structured Prediction for NLP (SPNLP 2021),"Kreutzer, Julia  and
Riezler, Stefan  and
Lawrence, Carolin",Offline Reinforcement Learning from Human Feedback in Real-World Sequence-to-Sequence Tasks,10.18653/v1/2021.spnlp-1.4,spnlp,305
2020.onion-1.3,"['Classification Applications', 'Audio Generation and Processing', 'Learning Paradigms', 'Low-resource Languages', 'Image and Video Processing']",['Multimodal Learning'],,"This paper presents an approach to automatic head movement detection and classification in data from a corpus of video-recorded face-toface conversations in Danish involving 12 different speakers. A number of classifiers were trained with different combinations of visual, acoustic and word features and tested in a leave-one-out cross validation scenario. The visual movement features were extracted from the raw video data using OpenPose, the acoustic ones from the sound files using Praat, and the word features from the transcriptions. The best results were obtained by a Multilayer Perceptron classifier, which reached an average 0.68 F1 score across the 12 speakers for head movement detection, and 0.40 for head movement classification given four different classes. In both cases, the classifier outperformed a simple most frequent class baseline, a more advanced baseline only relying on velocity features, and linear classifiers using different combinations of features.",https://aclanthology.org/2020.onion-1.3,European Language Resources Association (ELRA),2020,May,"Proceedings of LREC2020 Workshop ``People in language, vision and the mind'' (ONION2020)","Paggio, Patrizia  and
Agirrezabal, Manex  and
Jongejan, Bart  and
Navarretta, Costanza",Automatic Detection and Classification of Head Movements in Face-to-Face Conversations,,onion,839
2021.nlp4posimpact-1.13,"['Ethics', 'Biases in NLP', 'Domain-specific NLP', 'Text Clustering', 'Information Extraction', 'Classification Applications']","['NLP for the Legal Domain', 'Named Entity Recognition (NER)']",,"This article explores the potential for Natural Language Processing NLP to enable a more effective, prevention focused and less confrontational policing model that has hitherto been too resource consuming to implement at scale. Problem-Oriented Policing POP is a potential replacement, at least in part, for traditional policing which adopts a reactive approach, relying heavily on the criminal justice system. By contrast, POP seeks to prevent crime by manipulating the underlying conditions that allow crimes to be committed. Identifying these underlying conditions requires a detailed understanding of crime events -tacit knowledge that is often held by police officers but which can be challenging to derive from structured police data. One potential source of insight exists in unstructured free text data commonly collected by police for the purposes of investigation or administration. Yet police agencies do not typically have the skills or resources to analyse these data at scale. In this article we argue that NLP offers the potential to unlock these unstructured data and by doing so allow police to implement more POP initiatives. However we caution that using NLP models without adequate knowledge runs the risk of perpetuating existing, or introducing new, biases that have the potential to produce unfavourable outcomes.",https://aclanthology.org/2021.nlp4posimpact-1.13,Association for Computational Linguistics,2021,August,Proceedings of the 1st Workshop on NLP for Positive Impact,"Dixon, Anthony  and
Birks, Daniel",Improving Policing with Natural Language Processing,10.18653/v1/2021.nlp4posimpact-1.13,nlp4posimpact,978
R19-1011,"['Data Management and Generation', 'Information Extraction', 'Language Change Analysis']",['Data Preparation'],,"In the last few years, the increasing availability of large corpora spanning several time periods has opened new opportunities for the diachronic analysis of language. This type of analysis can bring to the light not only linguistic phenomena related to the shift of word meanings over time, but it can also be used to study the impact that societal and cultural trends have on this language change. This paper introduces a new resource for performing the diachronic analysis of named entities built upon Wikipedia page revisions. This resource enables the analysis over time of changes in the relations between entities concepts, surface forms words, and the contexts surrounding entities and surface forms, by analysing the whole history of Wikipedia internal links. We provide some useful use cases that prove the impact of this resource on diachronic studies and delineate some possible future usage.",https://aclanthology.org/R19-1011,INCOMA Ltd.,2019,September,Proceedings of the International Conference on Recent Advances in Natural Language Processing (RANLP 2019),"Basile, Pierpaolo  and
Caputo, Annalina  and
Lawless, Seamus  and
Semeraro, Giovanni",Diachronic Analysis of Entities by Exploiting Wikipedia Page revisions,10.26615/978-954-452-056-4_011,R19,30
2020.crac-1.2,"['Data Management and Generation', 'Information Extraction', 'Classification Applications', 'Low-resource Languages']","['Sentiment Analysis (SA)', 'Data Preparation', 'Coreference Resolution']",['Aspect-Based SA (ABSA)'],"While it has been claimed that anaphora or coreference resolution plays an important role in opinion mining, it is not clear to what extent coreference resolution actually boosts performance, if at all. In this paper, we investigate the potential added value of coreference resolution for the aspect-based sentiment analysis of restaurant reviews in two languages, English and Dutch. We focus on the task of aspect category classification and investigate whether including coreference information prior to classification to resolve implicit aspect mentions is beneficial. Because coreference resolution is not a solved task in NLP, we rely on both automatically-derived and gold-standard coreference relations, allowing us to investigate the true upper bound. By training a classifier on a combination of lexical and semantic features, we show that resolving the coreferential relations prior to classification is beneficial in a joint optimization setup. However, this is only the case when relying on gold-standard relations and the result is more outspoken for English than for Dutch. When validating the optimal models, however, we found that only the Dutch pipeline is able to achieve a satisfying performance on a held-out test set and does so regardless of whether coreference information was included.",https://aclanthology.org/2020.crac-1.2,Association for Computational Linguistics,2020,December,"Proceedings of the Third Workshop on Computational Models of Reference, Anaphora and Coreference","De Clercq, Orphee  and
Hoste, Veronique",It's absolutely divine! Can fine-grained sentiment analysis benefit from coreference resolution?,,crac,999
2021.dialdoc-1.7,"['Question Answering (QA)', 'Data Management and Generation', 'Dialogue Systems', 'Model Architectures']",['Data Augmentation'],,"We participate in the DialDoc Shared Task subtask 1 Knowledge Identification. The task requires identifying the grounding knowledge in form of a document span for the next dialogue turn. We employ two well-known pre-trained language models RoBERTa and ELECTRA to identify candidate document spans and propose a metric-based ensemble method for span selection. Our methods include data augmentation, model pre-training/fine-tuning, postprocessing, and ensemble. On the submission page, we rank 2nd based on the average of normalized F1 and EM scores used for the final evaluation. Specifically, we rank 2nd on EM and 3rd on F1.",https://aclanthology.org/2021.dialdoc-1.7,Association for Computational Linguistics,2021,August,Proceedings of the 1st Workshop on Document-grounded Dialogue and Conversational Question Answering (DialDoc 2021),"Li, Jiapeng  and
Li, Mingda  and
Ma, Longxuan  and
Zhang, Wei-Nan  and
Liu, Ting",Technical Report on Shared Task in DialDoc21,10.18653/v1/2021.dialdoc-1.7,dialdoc,698
K19-1096,"['Learning Paradigms', 'Domain-specific NLP', 'Classification Applications']","['Unsupervised Learning', 'NLP for News and Media']",['NLP for Social Media'],"We investigate the political roles of ""Internet trolls"" in social media. Political trolls, such as the ones linked to the Russian Internet Research Agency IRA, have recently gained enormous attention for their ability to sway public opinion and even influence elections. Analysis of the online traces of trolls has shown different behavioral patterns, which target different slices of the population. However, this analysis is manual and laborintensive, thus making it impractical as a firstresponse tool for newly-discovered troll farms. In this paper, we show how to automate this analysis by using machine learning in a realistic setting. In particular, we show how to classify trolls according to their political role -left, news feed, right-by using features extracted from social media, i.e., Twitter, in two scenarios: i in a traditional supervised learning scenario, where labels for trolls are available, and ii in a distant supervision scenario, where labels for trolls are not available, and we rely on more-commonly-available labels for news outlets mentioned by the trolls. Technically, we leverage the community structure and the text of the messages in the online social network of trolls represented as a graph, from which we extract several types of learned representations, i.e., embeddings, for the trolls. Experiments on the ""IRA Russian Troll"" dataset show that our methodology improves over the state-of-the-art in the first scenario, while providing a compelling case for the second scenario, which has not been explored in the literature thus far.",https://aclanthology.org/K19-1096,Association for Computational Linguistics,2019,November,Proceedings of the 23rd Conference on Computational Natural Language Learning (CoNLL),"Atanasov, Atanas  and
De Francisci Morales, Gianmarco  and
Nakov, Preslav",Predicting the Role of Political Trolls in Social Media,10.18653/v1/K19-1096,K19,326
W18-3307,"['Audio Generation and Processing', 'Learning Paradigms', 'Low-resource Languages', 'Classification Applications']","['Transfer Learning', 'Automatic Speech Recognition (ASR)', 'Emotion Detection']",,"During the last decade, the applications of signal processing have drastically improved with deep learning. However areas of affecting computing such as emotional speech synthesis or emotion recognition from spoken language remains challenging. In this paper, we investigate the use of a neural Automatic Speech Recognition ASR as a feature extractor for emotion recognition. We show that these features outperform the eGeMAPS feature set to predict the valence and arousal emotional dimensions, which means that the audio-to-text mapping learned by the ASR system contains information related to the emotional dimensions in spontaneous speech. We also examine the relationship between first layers closer to speech and last layers closer to text of the ASR and valence/arousal.",https://aclanthology.org/W18-3307,Association for Computational Linguistics,2018,July,Proceedings of Grand Challenge and Workshop on Human Multimodal Language (Challenge-{HML}),"Tits, No{\'e}  and
El Haddad, Kevin  and
Dutoit, Thierry",ASR-based Features for Emotion Recognition: A Transfer Learning Approach,10.18653/v1/W18-3307,W18,263
2020.smm4h-1.17,"['Learning Paradigms', 'Domain-specific NLP', 'Low-resource Languages', 'Model Architectures']","['Transformer Models', 'Transfer Learning', 'Medical and Clinical NLP', 'NLP for News and Media']",['NLP for Social Media'],"In this paper we present the drug adverse effects detection system developed during our participation in the Social Media Mining for Health Applications Shared Task 2020. We experimented with transfer learning approach for English and Russian, BERT and RoBERTa architectures and several strategies for regression head composition. Our final submissions in both languages overcome average F 1 by several percents margin.",https://aclanthology.org/2020.smm4h-1.17,Association for Computational Linguistics,2020,December,Proceedings of the Fifth Social Media Mining for Health Applications Workshop {\&} Shared Task,"Blinov, Pavel  and
Avetisian, Manvel",Transformer Models for Drug Adverse Effects Detection from Tweets,,smm4h,283
2022.hcinlp-1.7,['Data Management and Generation'],"['Data Preparation', 'Data Analysis']",,"In this short paper, we compare existing value systems and approaches in NLP and HCI for collecting narrative data. Building on these parallel discussions, we shed light on the challenges facing some popular NLP dataset types, which we discuss these in relation to widelyused narrative-based HCI research methods; and we highlight points where NLP methods can broaden qualitative narrative studies. In particular, we point towards contextuality, positionality, dataset size, and open research design as central points of difference and windows for collaboration when studying narratives. Through the use case of narratives, this work contributes to a larger conversation regarding the possibilities for bridging NLP and HCI through speculative mixed-methods.",https://aclanthology.org/2022.hcinlp-1.7,Association for Computational Linguistics,2022,July,Proceedings of the Second Workshop on Bridging Human--Computer Interaction and Natural Language Processing,"Sultana, Sharifa  and
Zhang, Renwen  and
Lim, Hajin  and
Antoniak, Maria",Narrative Datasets through the Lenses of NLP and HCI,10.18653/v1/2022.hcinlp-1.7,hcinlp,733
2020.nlptea-1.11,"['Error Detection and Correction', 'Low-resource Languages']",['Grammatical Error Correction (GEC)'],,"In this paper, we introduce our system for NLPTEA 2020 shared task of Chinese Grammatical Error Diagnosis CGED. In recent years, pre-trained models have been extensively studied, and several downstream tasks have benefited from their utilization. In this study, we treat the grammar error diagnosis GED task as a grammatical error correction GEC problem and use a method that incorporates a pre-trained model into an encoderdecoder model to solve this problem.",https://aclanthology.org/2020.nlptea-1.11,Association for Computational Linguistics,2020,December,Proceedings of the 6th Workshop on Natural Language Processing Techniques for Educational Applications,"Wang, Hongfei  and
Komachi, Mamoru",TMU-NLP System Using BERT-based Pre-trained Model to the NLP-TEA CGED Shared Task 2020,10.18653/v1/2020.nlptea-1.11,nlptea,649
2020.osact-1.9,"['Domain-specific NLP', 'Model Architectures', 'Classification Applications', 'Low-resource Languages']","['NLP for News and Media', 'Hate and Offensive Speech Detection']",['NLP for Social Media'],"In this paper, we describe our efforts at OSACT Shared Task on Offensive Language Detection. The shared task consists of two subtasks: offensive language detection Subtask A and hate speech detection Subtask B. For offensive language detection, a system combination of Support Vector Machines SVMs and Deep Neural Networks DNNs achieved the best results on development set, which ranked 1st in the official results for Subtask A with F1-score of 90.51% on the test set. For hate speech detection, DNNs were less effective and a system combination of multiple SVMs with different parameters achieved the best results on development set, which ranked 4th in official results for Subtask B with F1-macro score of 80.63% on the test set.",https://aclanthology.org/2020.osact-1.9,European Language Resource Association,2020,May,"Proceedings of the 4th Workshop on Open-Source Arabic Corpora and Processing Tools, with a Shared Task on Offensive Language Detection","Hassan, Sabit  and
Samih, Younes  and
Mubarak, Hamdy  and
Abdelali, Ahmed  and
Rashed, Ammar  and
Chowdhury, Shammur Absar",ALT Submission for OSACT Shared Task on Offensive Language Detection,,osact,642
2019.icon-1.24,"['Domain-specific NLP', 'Data Management and Generation', 'Classification Applications', 'Learning Paradigms', 'Model Architectures']","['NLP for News and Media', 'Recurrent Neural Networks (RNNs)', 'Supervised Learning', 'Sarcasm Detection', 'Data Preparation']","['Long Short-Term Memory (LSTM) Models', 'NLP for Social Media']","Self-deprecating sarcasm is a special category of sarcasm, which is nowadays popular and useful for many real-life applications, such as brand endorsement, product campaign, digital marketing, and advertisement. The selfdeprecating style of campaign and marketing strategy is mainly adopted to excel brand endorsement and product sales value. In this paper, we propose an LSTM-based deep learning approach for detecting self-deprecating sarcasm in textual data. To the best of our knowledge, there is no prior work related to self-deprecating sarcasm detection using deep learning techniques. Starting with a filtering step to identify self-referential tweets, the proposed approach adopts a deep learning model using LSTM for detecting self-deprecating sarcasm. The proposed approach is evaluated over three Twitter datasets and performs significantly better in terms of precision, recall, and f-score.",https://aclanthology.org/2019.icon-1.24,NLP Association of India,2019,December,Proceedings of the 16th International Conference on Natural Language Processing,"Kamal, Ashraf  and
Abulaish, Muhammad",An LSTM-Based Deep Learning Approach for Detecting Self-Deprecating Sarcasm in Textual Data,,icon,1001
2021.cl-2.9,['Finite State Machines'],,,"Weighted finite automata WFAs are often used to represent probabilistic models, such as n-gram language models, because among other things, they are efficient for recognition tasks in time and space. The probabilistic source to be represented as a WFA, however, may come in many forms. Given a generic probabilistic model over sequences, we propose an algorithm to approximate it as a WFA such that the Kullback-Leibler divergence between the source model and the WFA target model is minimized. The proposed algorithm involves a counting step and a difference of convex optimization step, both of which can be performed efficiently. We demonstrate the usefulness of our approach on various tasks, including distilling n-gram models from neural models, building compact language models, and building open-vocabulary character models. The algorithms used for these experiments are available in an open-source software library.",https://aclanthology.org/2021.cl-2.9,MIT Press,2021,June,,"Suresh, Ananda Theertha  and
Roark, Brian  and
Riley, Michael  and
Schogol, Vlad",Approximating Probabilistic Models as Weighted Finite Automata,10.1162/coli_a_00401,cl,1397
2020.emnlp-main.370,"['Data Management and Generation', 'Commonsense Reasoning']",['Data Preparation'],['Annotation Processes'],"When humans read or listen, they make implicit commonsense inferences that frame their understanding of what happened and why. As a step toward AI systems that can build similar mental models, we introduce GLUCOSE, a large-scale dataset of implicit commonsense causal knowledge, encoded as causal minitheories about the world, each grounded in a narrative context. To construct GLUCOSE, we drew on cognitive psychology to identify ten dimensions of causal explanation, focusing on events, states, motivations, and emotions. Each GLUCOSE entry includes a storyspecific causal statement paired with an inference rule generalized from the statement. This paper details two concrete contributions. First, we present our platform for effectively crowdsourcing GLUCOSE data at scale, which uses semi-structured templates to elicit causal explanations. Using this platform, we collected a total of 670K specific statements and general rules that capture implicit commonsense knowledge about everyday situations. Second, we show that existing knowledge resources and pretrained language models do not include or readily predict GLUCOSE's rich inferential content. However, when state-of-the-art neural models are trained on this knowledge, they can start to make commonsense inferences on unseen stories that match humans' mental models.",https://aclanthology.org/2020.emnlp-main.370,Association for Computational Linguistics,2020,November,Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),"Mostafazadeh, Nasrin  and
Kalyanpur, Aditya  and
Moon, Lori  and
Buchanan, David  and
Berkowitz, Lauren  and
Biran, Or  and
Chu-Carroll, Jennifer",GLUCOSE: GeneraLized and COntextualized Story Explanations,10.18653/v1/2020.emnlp-main.370,emnlp,925
2021.americasnlp-1.28,"['Machine Translation (MT)', 'Data Management and Generation', 'Low-resource Languages']","['Data Preparation', 'Neural MT (NMT)']",,"We evaluated a range of neural machine translation techniques developed specifically for low-resource scenarios. Unsuccessfully. In the end, we submitted two runs: i a standard phrase-based model, and ii a random babbling baseline using character trigrams. We found that it was surprisingly hard to beat i, in spite of this model being, in theory, a bad fit for polysynthetic languages; and more interestingly, that ii was better than several of the submitted systems, highlighting how difficult low-resource machine translation for polysynthetic languages is.",https://aclanthology.org/2021.americasnlp-1.28,Association for Computational Linguistics,2021,June,Proceedings of the First Workshop on Natural Language Processing for Indigenous Languages of the Americas,"Bollmann, Marcel  and
Aralikatte, Rahul  and
Murrieta Bello, H{\'e}ctor  and
Hershcovich, Daniel  and
de Lhoneux, Miryam  and
S{\o}gaard, Anders",Moses and the Character-Based Random Babbling Baseline: CoAStaL at AmericasNLP 2021 Shared Task,10.18653/v1/2021.americasnlp-1.28,americasnlp,103
2022.suki-1.6,"['Question Answering (QA)', 'Learning Paradigms', 'Dialogue Systems', 'Data Management and Generation']","['Knowledge Base QA', 'Transfer Learning', 'Response Generation']",,"Structured Knowledge has recently emerged as an essential component to support fine-grained Question Answering QA. In general, QA systems query a Knowledge Base KB to detect and extract the raw answers as final prediction. However, as lacking of context, language generation can offer a much informative and complete response. In this paper, we propose to combine the power of transfer learning and the advantage of entity placeholders to produce high-quality verbalization of extracted answers from a structured KB. We claim that such approach is especially well-suited for answer generation. Our experiments show 44.25%, 3.26% and 29.10% relative gain in BLEU over the state-of-the-art on the VQuAnDA, ParaQA and VANiLLa datasets, respectively. We additionally provide minor hallucinations corrections in VANiLLa standing for 5% of each of the training and testing set. We witness a median absolute gain of 0.81 SacreBLEU. This strengthens the importance of data quality when using automated evaluation.",https://aclanthology.org/2022.suki-1.6,Association for Computational Linguistics,2022,July,Proceedings of the Workshop on Structured and Unstructured Knowledge Integration (SUKI),"Montella, Sebastien  and
Rojas-Barahona, Lina  and
Bechet, Frederic  and
Heinecke, Johannes  and
Nasr, Alexis",Transfer Learning and Masked Generation for Answer Verbalization,10.18653/v1/2022.suki-1.6,suki,406
2022.mia-1.4,"['Data Management and Generation', 'Multilingual NLP', 'Question Answering (QA)', 'Low-resource Languages', 'Cross-lingual Application']","['Data Preparation', 'Open-Domain QA']",,"It can be challenging to build effective open question answering open QA systems for languages other than English, mainly due to a lack of labeled data for training. We present a data efficient method to bootstrap such a system for languages other than English. Our approach requires only limited QA resources in the given language, along with machine-translated data, and at least a bilingual language model. To evaluate our approach, we build such a system for the Icelandic language and evaluate performance over trivia style datasets. The corpora used for training are English in origin but machine translated into Icelandic. We train a bilingual Icelandic/English language model to embed English context and Icelandic questions following methodology introduced with DensePhrases Lee et al., 2021. The resulting system is an open domain cross-lingual QA system between Icelandic and English. Finally, the system is adapted for Icelandic only open QA, demonstrating how it is possible to efficiently create an open QA system with limited access to curated datasets in the language of interest.",https://aclanthology.org/2022.mia-1.4,Association for Computational Linguistics,2022,July,Proceedings of the Workshop on Multilingual Information Access (MIA),"Sn{\ae}bjarnarson, V{\'e}steinn  and
Einarsson, Hafsteinn",Cross-Lingual QA as a Stepping Stone for Monolingual Open QA in Icelandic,10.18653/v1/2022.mia-1.4,mia,424
2021.naacl-main.443,"['Knowledge Representation and Reasoning', 'Model Architectures']","['Transformer Models', 'Abstract Meaning Representation (AMR)']",,"Meaning Representation parsing is a sentence-to-graph prediction task where target nodes are not explicitly aligned to sentence tokens. However, since graph nodes are semantically based on one or more sentence tokens, implicit alignments can be derived. Transition-based parsers operate over the sentence from left to right, capturing this inductive bias via alignments at the cost of limited expressiveness. In this work, we propose a transition-based system that combines hard-attention over sentences with a targetside action pointer mechanism to decouple source tokens from node representations and address alignments. We model the transitions as well as the pointer mechanism through straightforward modifications within a single Transformer architecture. Parser state and graph structure information are efficiently encoded using attention heads. We show that our action-pointer approach leads to increased expressiveness and attains large gains +1.6 points against the best transition-based AMR parser in very similar conditions. While using no graph re-categorization, our single model yields the second best SMATCH score on AMR 2.0 81.8, which is further improved to 83.4 with silver data and ensemble decoding.",https://aclanthology.org/2021.naacl-main.443,Association for Computational Linguistics,2021,June,Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,"Zhou, Jiawei  and
Naseem, Tahira  and
Fernandez Astudillo, Ram{\'o}n  and
Florian, Radu",AMR Parsing with Action-Pointer Transformer,10.18653/v1/2021.naacl-main.443,naacl,643
I17-1004,"['Machine Translation (MT)', 'Model Architectures']",['Neural MT (NMT)'],,"Attention in neural machine translation provides the possibility to encode relevant parts of the source sentence at each translation step. As a result, attention is considered to be an alignment model as well. However, there is no work that specifically studies attention and provides analysis of what is being learned by attention models. Thus, the question still remains that how attention is similar or different from the traditional alignment. In this paper, we provide detailed analysis of attention and compare it to traditional alignment. We answer the question of whether attention is only capable of modelling translational equivalent or it captures more information. We show that attention is different from alignment in some cases and is capturing useful information other than alignments.",https://aclanthology.org/I17-1004,Asian Federation of Natural Language Processing,2017,November,Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 1: Long Papers),"Ghader, Hamidreza  and
Monz, Christof",What does Attention in Neural Machine Translation Pay Attention to?,10.48550/arxiv.1710.03348,I17,312
2022.findings-acl.178,"['Topic Modeling', 'Domain-specific NLP', 'Classification Applications', 'Model Architectures']","['Multilabel Text Classification', 'NLP for News and Media']",,"Many recent deep learning-based solutions have adopted the attention mechanism in various tasks in the field of NLP. However, the inherent characteristics of deep learning models and the flexibility of the attention mechanism increase the models' complexity, thus leading to challenges in model explainability. To address this challenge, we propose a novel practical framework by utilizing a two-tier attention architecture to decouple the complexity of explanation and the decision-making process. We apply it in the context of a news article classification task. The experiments on two largescaled news corpora demonstrate that the proposed model can achieve competitive performance with many state-of-the-art alternatives and illustrate its appropriateness from an explainability perspective. We release the source code here 1 .",https://aclanthology.org/2022.findings-acl.178,Association for Computational Linguistics,2022,May,Findings of the Association for Computational Linguistics: ACL 2022,"Liu, Dairui  and
Greene, Derek  and
Dong, Ruihai",A Novel Perspective to Look At Attention: Bi-level Attention-based Explainable Topic Modeling for News Classification,10.18653/v1/2022.findings-acl.178,findings,179
2020.fever-1.1,"['Information Retrieval', 'Information Extraction', 'Classification Applications']",['Misinformation Detection'],,"Automatic fact checking is an important task motivated by the need for detecting and preventing the spread of misinformation across the web. The recently released FEVER challenge provides a benchmark task that assesses systems' capability for both the retrieval of required evidence and the identification of authentic claims. Previous approaches share model parameters, facilitating future works on the automatic fact checking task and its practical usage.",https://aclanthology.org/2020.fever-1.1,Association for Computational Linguistics,2020,July,Proceedings of the Third Workshop on Fact Extraction and VERification (FEVER),"Nie, Yixin  and
Bauer, Lisa  and
Bansal, Mohit",Simple Compounded-Label Training for Fact Extraction and Verification,10.18653/v1/2020.fever-1.1,fever,246
2020.peoples-1.1,"['Domain-specific NLP', 'Data Management and Generation']","['Data Preparation', 'Medical and Clinical NLP', 'NLP for News and Media']","['NLP for Mental Health', 'NLP for Social Media']","Twitter is a medium where, when used adequately, users' interests can be derived from what he follows. This characteristic can make it attractive for a source of personality derivation. We set out to test the hypothesis that, analogous to the Lexical hypothesis, which posits that word use should reveal personality, following behavior on social media should reveal personality aspects. We used a two-step approach, wherein the first stage, we selected accounts for whom it was possible to infer personality profiles to some extent using available literature on personality and interests. On these accounts, we trained a regression model and segmented the derived features using hierarchical cluster analysis. In the second stage, we obtained a small sample of users' personalities via a questionnaire and tested whether the model from stage 1 correlated with the users from step 2. The the explained variance for the neurotic and neutral neuroticism groups indicated significant results R 2 = .131, p = .0205; R 2 = .22, p = .0044. Confirming the hypothesis that following behavior should be correlated with one's interests and that interests are correlated with the neuroticism personality dimension.",https://aclanthology.org/2020.peoples-1.1,Association for Computational Linguistics,2020,December,"Proceedings of the Third Workshop on Computational Modeling of People's Opinions, Personality, and Emotion's in Social Media","Cornelisse, Joran",Inferring Neuroticism of Twitter Users by Utilizing their Following Interests,,peoples,80
2020.onion-1.1,['Domain-specific NLP'],,,"We present a study on prototype effects. We designed an experiment investigating the effect of adapting a prototypical image towards more human, male or female, prototypes, and additionally investigating the effect of self-recognition in a manipulated image. Results show that decisions are affected by prototypicality, but we find less evidence that selfrecognition further enhances perceptions of attractiveness. This study has implications for the psychological perception of faces, and may contribute to the study of Christian imagery.",https://aclanthology.org/2020.onion-1.1,European Language Resources Association (ELRA),2020,May,"Proceedings of LREC2020 Workshop ``People in language, vision and the mind'' (ONION2020)","Lembke, Carla Sophie  and
Folger{\o}, Per Olav  and
Andresen, Alf Edgar  and
Johansson, Christer",Prototypes and Recognition of Self in Depictions of Christ,,onion,810
O17-1014,"['Question Answering (QA)', 'Information Retrieval']",['Community QA'],,"In recent years, community-based question and answer CQA sites have grown rapidly in number and size. These sites represent a valuable source of online knowledge; however, they often suffer from the problem of duplicate questions. The task of question retrieval QR aims to find previously answered semantically similar questions in CQA archives. Nevertheless, synonymous lexical variations pose a big challenge for question retrieval. Some QR approaches address this issue by calculating the probability of correlation between new questions and archived questions. Much recent research has also focused on surface string similarity among questions. In this paper, we propose a method that first builds a continuous bag-of-words CBoW model with data from Asus's Republic of Gamers ROG forum and then determines the similarity between a given new question and the Q&As in our database. Unlike most other methods, we calculate the similarity between the given question and the archived questions and descriptions separately with two different features. In addition, we factor user reputation into our ranking model. Our experimental results on the ROG forum dataset show that our CBoW model with reputation features outperforms other top methods.",https://aclanthology.org/O17-1014,The Association for Computational Linguistics and Chinese Language Processing (ACLCLP),2017,November,Proceedings of the 29th Conference on Computational Linguistics and Speech Processing ({ROCLING} 2017),"Weng, Sam  and
Wu, Kevin Chun-Kai  and
Wang, Yu-Chun  and
Tsai, Richard Tzong-Han",Question Retrieval with Distributed Representations and Participant Reputation in Community Question Answering,,O17,1478
P18-1066,"['Domain-specific NLP', 'Low-resource Languages', 'Cross-lingual Application', 'Data Management and Generation']","['Data Preparation', 'Data Analysis', 'NLP for News and Media']","['Annotation Processes', 'NLP for Social Media']","Cross-cultural differences and similarities are common in cross-lingual natural language understanding, especially for research in social media. For instance, people of distinct cultures often hold different opinions on a single named entity. Also, understanding slang terms across languages requires knowledge of cross-cultural similarities. In this paper, we study the problem of computing such cross-cultural differences and similarities. We present a lightweight yet effective approach, and evaluate it on two novel tasks: 1 mining cross-cultural differences of named entities and 2 finding similar terms for slang across languages. Experimental results show that our framework substantially outperforms a number of baseline methods on both tasks. The framework could be useful for machine translation applications and research in computational social science.",https://aclanthology.org/P18-1066,Association for Computational Linguistics,2018,July,Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),"Lin, Bill Yuchen  and
Xu, Frank F.  and
Zhu, Kenny  and
Hwang, Seung-won",Mining Cross-Cultural Differences and Similarities in Social Media,10.18653/v1/P18-1066,P18,995
O16-1015,"['Audio Generation and Processing', 'Learning Paradigms', 'Low-resource Languages', 'Classification Applications']","['Emotion Detection', 'Supervised Learning']",,"In this paper, we use super-vectors in support vector machines for automatic speech emotion recognition. In our implementation, an utterance is converted to a super-vector formed by the mean vectors of a Gaussian mixture model adapted from a universal background model. The proposed method is evaluated on FAU-Aibo database which is wellknown to be used in INTERSPEECH 2009 Emotion Challenge. In the case of HMMbased dynamic modeling classifier, we achieve an unweighted average UA recall rate of 40.0%, over a baseline of 35.5%, by using the delta features and increasing the number of mixture components. In the case of SVM-based static modeling classifier, we achieve an unweighted average UA recall rate of 38.9%, over a baseline of 38.2%, by using the proposed super-vectors.",https://aclanthology.org/O16-1015,The Association for Computational Linguistics and Chinese Language Processing (ACLCLP),2016,October,Proceedings of the 28th Conference on Computational Linguistics and Speech Processing ({ROCLING} 2016),"Chen, Chia-Ying  and
Chen, Chia-Ping",Support Super-Vector Machines in Automatic Speech Emotion Recognition,,O16,727
2021.iwcs-1.12,"['Parsing', 'Data Management and Generation']","['Syntactic Parsing', 'Data Preparation']","['Dependency Parsing', 'Annotation Processes']","Dependency parsing is a tool widely used in the field of Natural Language Processing and computational linguistics. However, there is hardly any work that connects dependency parsing to monotonicity, which is an essential part of logic and linguistic semantics. In this paper, we present a system that automatically annotates monotonicity information based on Universal Dependency parse trees. Our system utilizes surface-level monotonicity facts about quantifiers, lexical items, and token-level polarity information. We compared our system's performance with existing systems in the literature, including NatLog and ccg2mono, on a small evaluation dataset. Results show that our system outperforms NatLog and ccg2mono.",https://aclanthology.org/2021.iwcs-1.12,Association for Computational Linguistics,2021,June,Proceedings of the 14th International Conference on Computational Semantics (IWCS),"Chen, Zeming  and
Gao, Qiyue",Monotonicity Marking from Universal Dependency Trees,10.48550/arxiv.2104.08659,iwcs,1354
2021.computel-1.8,"['Audio Generation and Processing', 'Multilingual NLP', 'Learning Paradigms', 'Low-resource Languages', 'Cross-lingual Application']","['Transfer Learning', 'Automatic Speech Recognition (ASR)']",,"This study presents new experiments on Zyrian Komi speech recognition. We use Deep-Speech to train ASR models from a language documentation corpus that contains both contemporary and archival recordings. Earlier studies have shown that transfer learning from English and using a domain matching Komi language model both improve the CER and WER. In this study we experiment with transfer learning from a more relevant source language, Russian, and including Russian text in the language model construction. The motivation for this is that Russian and Komi are contemporary contact languages, and Russian is regularly present in the corpus. We found that despite the close contact of Russian and Komi, the size of the English speech corpus yielded greater performance when used as the source language. Additionally, we can report that already an update in DeepSpeech version improved the CER by 3.9% against the earlier studies, which is an important step in the development of Komi ASR.",https://aclanthology.org/2021.computel-1.8,Association for Computational Linguistics,2021,March,Proceedings of the 4th Workshop on the Use of Computational Methods in the Study of Endangered Languages Volume 1 (Papers),"Hjortnaes, Nils  and
Partanen, Niko  and
Rie{\ss}ler, Michael  and
Tyers, Francis M.",The Relevance of the Source Language in Transfer Learning for ASR,10.33011/computel.v1i.959,computel,576
E17-2067,"['Evaluation Techniques', 'Embeddings', 'Low-resource Languages']",['Word Embeddings'],,"We explore the ability of word embeddings to capture both semantic and morphological similarity, as affected by the different types of linguistic properties surface form, lemma, morphological tag used to compose the representation of each word. We train several models, where each uses a different subset of these properties to compose its representations. By evaluating the models on semantic and morphological measures, we reveal some useful insights on the relationship between semantics and morphology.",https://aclanthology.org/E17-2067,Association for Computational Linguistics,2017,April,"Proceedings of the 15th Conference of the {E}uropean Chapter of the Association for Computational Linguistics: Volume 2, Short Papers","Avraham, Oded  and
Goldberg, Yoav",The Interplay of Semantics and Morphology in Word Embeddings,10.18653/v1/e17-2067,E17,324
2021.fnp-1.19,"['Learning Paradigms', 'Domain-specific NLP', 'Automatic Text Summarization', 'Embeddings']","['Extractive Text Summarization', 'Abstractive Text Summarization', 'Word Embeddings', 'Document Summarization', 'NLP for Finance', 'Reinforcement Learning']",,"In this paper we show the results of our participation in the FNS 2021 shared task. In our work we propose an end to end financial narrative summarization system that first selects salient sentences from the document and then paraphrases extracted sentences. This method generates an overall concise summary that maximises the ROU GE metric with the gold standard summary. The end to end system is developed using a hybrid extractive and abstractive architecture followed by joint training using policy-based reinforcement learning to bridge together the two networks. Empirically, we achieve better scores than the proposed baselines and toplines of FNS 2021 LexRank, TextRank, MUSE topline and POLY baseline and we were ranked 2nd in the shared task competition.",https://aclanthology.org/2021.fnp-1.19,Association for Computational Linguistics,2021,15-16 September,Proceedings of the 3rd Financial Narrative Processing Workshop,"Zmandar, Nadhem  and
Singh, Abhishek  and
El-Haj, Mahmoud  and
Rayson, Paul",Joint abstractive and extractive method for long financial document summarization,,fnp,397
2021.hackashop-1.1,"['Learning Paradigms', 'Classification Applications', 'Model Architectures', 'Domain-specific NLP']","['NLP for Finance', 'NLP for News and Media', 'Stance Detection', 'Adversarial Learning']",,"Cross-target generalization constitutes an important issue for news Stance Detection SD. In this short paper, we investigate adversarial cross-genre SD, where knowledge from annotated user-generated data is leveraged to improve news SD on targets unseen during training. We implement a BERT-based adversarial network and show experimental performance improvements over a set of strong baselines. Given the abundance of user-generated data, which are considerably less expensive to retrieve and annotate than news articles, this constitutes a promising research direction.",https://aclanthology.org/2021.hackashop-1.1,Association for Computational Linguistics,2021,April,Proceedings of the EACL Hackashop on News Media Content Analysis and Automated Report Generation,"Conforti, Costanza  and
Berndt, Jakob  and
Basaldella, Marco  and
Pilehvar, Mohammad Taher  and
Giannitsarou, Chryssi  and
Toxvaerd, Flavio  and
Collier, Nigel",Adversarial Training for News Stance Detection: Leveraging Signals from a Multi-Genre Corpus.,,hackashop,909
2022.ltedi-1.15,"['Learning Paradigms', 'Domain-specific NLP', 'Classification Applications', 'Model Architectures']","['Medical and Clinical NLP', 'NLP for News and Media']","['NLP for Mental Health', 'NLP for Social Media']","This paper presents a state-of-the-art solution to the LT-EDI-ACL 2022 Task 4: Detecting Signs of Depression from Social Media Text. The goal of this task is to detect the severity levels of depression of people from social media posts, where people often share their feelings on a daily basis. To detect the signs of depression, we propose a framework with pre-trained language models using rich information instead of training from scratch, gradient boosting and deep learning models for modeling various aspects, and supervised contrastive learning for the generalization ability. Moreover, ensemble techniques are also employed in consideration of the different advantages of each method. Experiments show that our framework achieves a 2nd prize ranking with a macro F1-score of 0.552, showing the effectiveness and robustness of our approach.",https://aclanthology.org/2022.ltedi-1.15,Association for Computational Linguistics,2022,May,"Proceedings of the Second Workshop on Language Technology for Equality, Diversity and Inclusion","Wang, Wei-Yao  and
Tang, Yu-Chien  and
Du, Wei-Wei  and
Peng, Wen-Chih",NYCU\_TWD@LT-EDI-ACL2022: Ensemble Models with VADER and Contrastive Learning for Detecting Signs of Depression from Social Media,10.18653/v1/2022.ltedi-1.15,ltedi,222
2020.nlpcss-1.17,"['Information Extraction', 'Domain-specific NLP', 'Data Management and Generation', 'Model Architectures']","['NLP for News and Media', 'Data Preparation']",,"Mapping local news coverage from textual content is a challenging problem that requires extracting precise location mentions from news articles. While traditional named entity taggers are able to extract geo-political entities and certain non geo-political entities, they cannot recognize precise location mentions such as addresses, streets and intersections that are required to accurately map the news article. We fine-tune a BERT-based language model for achieving high level of granularity in location extraction. We incorporate the model into an end-to-end tool that further geocodes the extracted locations for the broader objective of mapping news coverage.",https://aclanthology.org/2020.nlpcss-1.17,Association for Computational Linguistics,2020,November,Proceedings of the Fourth Workshop on Natural Language Processing and Computational Social Science,"Gupta, Sarang  and
Nishu, Kumari",Mapping Local News Coverage: Precise location extraction in textual news content using fine-tuned BERT based language model,10.18653/v1/2020.nlpcss-1.17,nlpcss,905
2020.insights-1.8,"['Learning Paradigms', 'Domain-specific NLP', 'Classification Applications']","['Hate and Offensive Speech Detection', 'NLP for News and Media']",['NLP for Social Media'],"Research on hate speech classification has received increased attention. In real-life scenarios, a small amount of labeled hate speech data is available to train a reliable classifier. Semi-supervised learning takes advantage of a small amount of labeled data and a large amount of unlabeled data. In this paper, label propagation-based semi-supervised learning is explored for the task of hate speech classification. The quality of labeling the unlabeled set depends on the input representations. In this work, we show that pre-trained representations are label agnostic, and when used with label propagation yield poor results. Neural network-based fine-tuning can be adopted to learn task-specific representations using a small amount of labeled data. We show that fully fine-tuned representations may not always be the best representations for the label propagation and intermediate representations may perform better in a semi-supervised setup.",https://aclanthology.org/2020.insights-1.8,Association for Computational Linguistics,2020,November,Proceedings of the First Workshop on Insights from Negative Results in NLP,"D{'}Sa, Ashwin Geet  and
Illina, Irina  and
Fohr, Dominique  and
Klakow, Dietrich  and
Ruiter, Dana",Label Propagation-Based Semi-Supervised Learning for Hate Speech Classification,10.18653/v1/2020.insights-1.8,insights,1073
2021.nllp-1.7,"['Domain-specific NLP', 'Information Extraction', 'Classification Applications', 'Data Management and Generation']","['NLP for the Legal Domain', 'Data Preparation']",,"Using a corpus of compiled codes from U.S. states containing labeled tax law sections, we train text classifiers to automatically tag taxlaw documents and, further, to identify the associated revenue source e.g. income, property, or sales. After evaluating classifier performance in held-out test data, we apply them to an historical corpus of U.S. state legislation to extract the flow of relevant laws over the years 1910 through 2010. We document that the classifiers are effective in the historical corpus, for example by automatically detecting establishments of state personal income taxes. The trained models with replication code are published at https://github.com/ luyang521/tax-classification.",https://aclanthology.org/2021.nllp-1.7,Association for Computational Linguistics,2021,November,Proceedings of the Natural Legal Language Processing Workshop 2021,"Ash, Elliott  and
Guillot, Malka  and
Han, Luyang",Machine Extraction of Tax Laws from Legislative Texts,10.18653/v1/2021.nllp-1.7,nllp,51
2022.in2writing-1.11,['Text Generation'],['Story Generation'],['Narrative Plot in Storytelling'],"Large language models LLMs enabled by the datasets and computing power of the last decade have recently gained popularity for their capacity to generate plausible natural language text from human-provided prompts. This ability makes them appealing to fiction writers as prospective co-creative agents, addressing the common challenge of writer's block, or getting unstuck. However, creative writers face additional challenges, including maintaining narrative consistency, developing plot structure, architecting reader experience, and refining their expressive intent, which are not well-addressed by current LLM-backed tools. In this paper, we define these needs by grounding them in cognitive and theoretical literature, then survey previous computational narrative research that holds promise for supporting each of them in a co-creative setting.",https://aclanthology.org/2022.in2writing-1.11,Association for Computational Linguistics,2022,May,Proceedings of the First Workshop on Intelligent and Interactive Writing Assistants (In2Writing 2022),"Kreminski, Max  and
Martens, Chris",Unmet Creativity Support Needs in Computationally Supported Creative Writing,10.18653/v1/2022.in2writing-1.11,in2writing,418
D17-1175,"['Parsing', 'Model Architectures']",['Syntactic Parsing'],['Dependency Parsing'],"Although sequence-to-sequence seq2seq network has achieved significant success in many NLP tasks such as machine translation and text summarization, simply applying this approach to transition-based dependency parsing cannot yield a comparable performance gain as in other stateof-the-art methods, such as stack-LSTM and head selection. In this paper, we propose a stack-based multi-layer attention model for seq2seq learning to better leverage structural linguistics information. In our method, two binary vectors are used to track the decoding stack in transition-based parsing, and multi-layer attention is introduced to capture multiple word dependencies in partial trees. We conduct experiments on PTB and CTB datasets, and the results show that our proposed model achieves state-of-the-art accuracy and significant improvement in labeled precision with respect to the baseline seq2seq model.",https://aclanthology.org/D17-1175,Association for Computational Linguistics,2017,September,Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing,"Zhang, Zhirui  and
Liu, Shujie  and
Li, Mu  and
Zhou, Ming  and
Chen, Enhong",Stack-based Multi-layer Attention for Transition-based Dependency Parsing,10.18653/v1/D17-1175,D17,348
Y18-1044,"['Data Management and Generation', 'Low-resource Languages']",['Data Analysis'],,"One interesting linguistic question has been whether and how the characteristics of sandhied Tone 3 are similar to or different from an underlying Tone 2 in Mandarin. In this study, we took a micro view to investigate the vowel height, duration, and rime structure in syllables, and their influence on the degree of Tone 3 sandhi through native speakers' speech production and perception. It was found that vowel height and duration played important roles in shaping the F0 contour of the sandhied Tone 3 in production. As to perception, sandhied Tone 3 syllables with low vowels and longer duration were more likely to be perceived as Tone 3.",https://aclanthology.org/Y18-1044,Association for Computational Linguistics,2018,1{--}3 December,"Proceedings of the 32nd Pacific Asia Conference on Language, Information and Computation","Lin, Yu-Jung  and
Hsu, Yu-Yin",Whether and How Mandarin Sandhied Tone 3 and Underlying Tone 2 differ in Terms of Vowel Quality?,,Y18,997
2021.hcinlp-1.10,"['Ethics', 'Data Management and Generation', 'Classification Applications', 'Domain-specific NLP']","['Data Preparation', 'Hate and Offensive Speech Detection']",['Annotation Processes'],"In this paper we discuss several challenges related to the development of a 3D game, whose goal is to raise awareness on cyberbullying while collecting linguistic annotation on offensive language. The game is meant to be used by teenagers, thus raising a number of issues that need to be tackled during development. For example, the game aesthetics should be appealing for players belonging to this age group, but at the same time all possible solutions should be implemented to meet privacy requirements. Also, the task of linguistic annotation should be possibly hidden, adopting so-called orthogonal game mechanics, without affecting the quality of collected data. While some of these challenges are being tackled in the game development, some others are discussed in this paper but still lack an ultimate solution.",https://aclanthology.org/2021.hcinlp-1.10,Association for Computational Linguistics,2021,April,Proceedings of the First Workshop on Bridging Human{--}Computer Interaction and Natural Language Processing,"Bonetti, Federico  and
Tonelli, Sara",Challenges in Designing Games with a Purpose for Abusive Language Annotation,,hcinlp,417
2022.clinicalnlp-1.6,"['Domain-specific NLP', 'Machine Translation (MT)', 'Information Extraction', 'Learning Paradigms', 'Low-resource Languages', 'Cross-lingual Application']","['Transfer Learning', 'Neural MT (NMT)', 'Named Entity Recognition (NER)', 'Medical and Clinical NLP']",,"In this work, cross-linguistic span prediction based on contextualized word embedding models is used together with neural machine translation NMT to transfer and apply the stateof-the-art models in natural language processing NLP to a low-resource language clinical corpus. Two directions are evaluated: a English models can be applied to translated texts to subsequently transfer the predicted annotations to the source language and b existing high-quality annotations can be transferred beyond translation and then used to train NLP models in the target language. Effectiveness and loss of transmission is evaluated using the German Berlin-Tbingen-Oncology Corpus BRONCO dataset with transferred external data from NCBI disease, SemEval-2013 drug-drug interaction DDI and i2b2/VA 2010 data. The use of English models for translated clinical texts has always involved attempts to take full advantage of the benefits associated with them large pre-trained biomedical word embeddings. To improve advances in this area, we provide a general-purpose pipeline to transfer any annotated BRAT or CoNLL format to various target languages. For the entity class medication, good results were obtained with 0.806 F 1-score after re-alignment. Limited success occurred in the diagnosis and treatment class with results just below 0.5 F 1-score due to differences in annotation guidelines.",https://aclanthology.org/2022.clinicalnlp-1.6,Association for Computational Linguistics,2022,July,Proceedings of the 4th Clinical Natural Language Processing Workshop,"Sch{\""a}fer, Henning  and
Idrissi-Yaghir, Ahmad  and
Horn, Peter  and
Friedrich, Christoph",Cross-Language Transfer of High-Quality Annotations: Combining Neural Machine Translation with Cross-Linguistic Span Alignment to Apply NER to Clinical Texts in a Low-Resource Language,10.18653/v1/2022.clinicalnlp-1.6,clinicalnlp,1026
2020.dmr-1.8,"['Parsing', 'Knowledge Representation and Reasoning']",['Semantic Parsing'],,"We propose an approach and a software framework for semantic parsing of natural language sentences to discourse representation structures with use of fuzzy meaning representations such as fuzzy sets and compatibility intervals. We explain the motivation for using fuzzy meaning representations in semantic parsing and describe the design of the proposed approach and the software framework, discussing various examples. We argue that the use of fuzzy meaning representations have potential to improve understanding and reasoning capabilities of systems working with natural language.",https://aclanthology.org/2020.dmr-1.8,Association for Computational Linguistics,2020,December,Proceedings of the Second International Workshop on Designing Meaning Representations,"Kapustin, Pavlo  and
Kapustin, Michael",Semantic parsing with fuzzy meaning representations,,dmr,873
2022.udfestbr-1.2,"['Parsing', 'Domain-specific NLP', 'Low-resource Languages', 'Data Management and Generation']","['Data Preparation', 'Syntactic Parsing']","['Dependency Parsing', 'Annotation Processes']","We present the second version of PetroGold, a gold-standard treebank for the oil & gas domain in the Portuguese language. The corpus went through a series of revisions guided by three methods tested in the literature: inter-annotator disagreement, inconsistent n-grams and verification rules. We perform an intrinsic evaluation and the model scores 90.92%, 89.09% and 84.07% in the UAS unlabeled attachment score, LAS labeled attachment score and CLAS content-word labeled attachment score metrics respectively, CLAS being 1.11% higher than in the first version. We perform an experiment where we verify a negative impact in the intrinsic evaluation when simplifying the annotation related to prepositional verbal arguments and we conclude by discussing the results and future work.",https://aclanthology.org/2022.udfestbr-1.2,Association for Computational Linguistics,2022,March,Proceedings of the {U}niversal {D}ependencies {B}razilian Festival,"Souza, Elvis  and
Freitas, Claudia",Polishing the gold -- how much revision do we need in treebanks?,,udfestbr,1313
2020.aacl-srw.14,"['Learning Paradigms', 'Low-resource Languages', 'Evaluation Techniques', 'Multilingual NLP', 'Model Architectures', 'Machine Translation (MT)']","['Neural MT (NMT)', 'Adversarial Learning']",,Can we trust that the attention heatmaps produced by a neural machine translation NMT model reflect its true internal reasoning? We isolate and examine in detail the notion of faithfulness in NMT models. We provide a measure of faithfulness for NMT based on a variety of stress tests where model parameters are perturbed and measuring faithfulness based on how often the model output changes. We show that our proposed faithfulness measure for NMT models can be improved using a novel differentiable objective that rewards faithful behaviour by the model through probability divergence. Our experimental results on multiple language pairs show that our objective function is effective in increasing faithfulness and can lead to a useful analysis of NMT model behaviour and more trustworthy attention heatmaps. Our proposed objective improves faithfulness without reducing the translation quality and it also seems to have a useful regularization effect on the NMT model and can even improve translation quality in some cases.,https://aclanthology.org/2020.aacl-srw.14,Association for Computational Linguistics,2020,December,Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing: Student Research Workshop,"Moradi, Pooya  and
Kambhatla, Nishant  and
Sarkar, Anoop",Training with Adversaries to Improve Faithfulness of Attention in Neural Machine Translation,10.18653/v1/2020.aacl-srw.14,aacl,763
U18-1008,['Text Clustering'],,,"Cluster labeling is the assignment of representative labels to clusters of documents or words. Once assigned, the labels can play an important role in applications such as navigation, search and document classification. However, finding appropriately descriptive labels is still a challenging task. In this paper, we propose various approaches for assigning labels to word clusters by leveraging word embeddings and the synonymy and hypernymy relations in the WordNet lexical ontology. Experiments carried out using the WebAP document dataset have shown that one of the approaches stand out in the comparison and is capable of selecting labels that are reasonably aligned with those chosen by a pool of four human annotators.",https://aclanthology.org/U18-1008,,2018,December,Proceedings of the Australasian Language Technology Association Workshop 2018,"Poostchi, Hanieh  and
Piccardi, Massimo",Cluster Labeling by Word Embeddings and WordNet's Hypernymy,,U18,588
2016.amta-researchers.6,"['Model Architectures', 'Low-resource Languages', 'Machine Translation (MT)']",,,"The objective of interactive translation prediction ITP, a paradigm of computer-aided translation, is to assist professional translators by offering context-based computer-generated suggestions as they type. While most state-of-the-art ITP systems are tightly coupled to a machine translation MT system often created ad-hoc for this purpose, our proposal follows a resourceagnostic approach, one that does not need access to the inner workings of the bilingual resources MT systems or any other bilingual resources used to generate the suggestions, thus allowing to include new resources almost seamlessly. As we do not expect the user to tolerate more than a few proposals each time, the set of potential suggestions need to be filtered and ranked; the resource-agnostic approach has been evaluated before using a set of intuitive length-based and position-based heuristics designed to determine which suggestions to show, achieving promising results. In this paper, we propose a more principled suggestion ranking approach using a regressor a multilayer perceptron that achieves significantly better results.",https://aclanthology.org/2016.amta-researchers.6,The Association for Machine Translation in the Americas,2016,October 28 - November 1,Conferences of the Association for Machine Translation in the Americas: MT Researchers' Track,"Torregrosa, Daniel  and
P{\'e}rez-Ortiz, Juan Antonio  and
Forcada, Mikel",Ranking suggestions for black-box interactive translation prediction systems with multilayer perceptrons,,amta,519
L18-1138,"['Multilingual NLP', 'Low-resource Languages']",,,"Building language resources for endangered languages, especially in the case of dictionaries, requires a substantial amount of manual work. This, however, is a time-consuming undertaking, and it is also why we propose an automated method for expanding the knowledge in the existing dictionaries. In this paper, we present an approach to automatically combine conceptually divided translations from multilingual dictionaries for small Uralic languages. This is done for the noun dictionaries of Skolt Sami, Erzya, Moksha and Komi-Zyrian in such a way that the combined translations are included in the dictionaries of each language and then evaluated by professional linguists fluent in these languages. Inclusion of the method as a part of the new crowdsourced MediaWiki based pipeline for editing the dictionaries is discussed. The method can be used there not only to expand the existing dictionaries but also to provide the editors with translations when they are adding a new lexical entry to the system.",https://aclanthology.org/L18-1138,European Language Resources Association (ELRA),2018,May,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),"H{\""a}m{\""a}l{\""a}inen, Mika  and
Tarvainen, Liisa Lotta  and
Rueter, Jack",Combining Concepts and Their Translations from Structured Dictionaries of Uralic Minority Languages,,L18,1202
2020.lrec-1.758,"['Domain-specific NLP', 'Data Management and Generation', 'Classification Applications', 'Low-resource Languages']","['NLP for News and Media', 'Data Preparation', 'Hate and Offensive Speech Detection']","['NLP for Social Media', 'Annotation Processes']","This paper introduces a corpus of Turkish offensive language. To our knowledge, this is the first corpus of offensive language for Turkish. The corpus consists of randomly sampled micro-blog posts from Twitter. The annotation guidelines are based on a careful review of the annotation practices of recent efforts for other languages. The corpus contains 36 232 tweets sampled randomly from the Twitter stream during a period of 18 months between Apr 2018 to Sept 2019. We found approximately 19 % of the tweets in the data contain some type of offensive language, which is further subcategorized based on the target of the offense. We describe the annotation process, discuss some interesting aspects of the data, and present results of automatically classifying the corpus using state-of-the-art text classification methods. The classifiers achieve 77.3 % F1 score on identifying offensive tweets, 77.9 % F1 score on determining whether a given offensive document is targeted or not, and 53.0 % F1 score on classifying the targeted offensive documents into three subcategories.",https://aclanthology.org/2020.lrec-1.758,European Language Resources Association,2020,May,Proceedings of the 12th Language Resources and Evaluation Conference,"{\c{C}}{\""o}ltekin, {\c{C}}a{\u{g}}r{\i}",A Corpus of Turkish Offensive Language on Social Media,,lrec,637
2021.metanlp-1.9,"['Learning Paradigms', 'Classification Applications', 'Model Architectures']","['Few-shot Learning', 'Emotion Detection', 'Transformer Models']",,"In this paper, we place ourselves in a classification scenario in which the target classes and data type are not accessible during training. We use a meta-learning approach to determine whether or not meta-trained information from common social network data with fine-grained emotion labels can achieve competitive performance on messages labeled with different emotion categories. We leverage fewshot learning to match with the classification scenario and consider metric learning based meta-learning by setting up Prototypical Networks with a Transformer encoder, trained in an episodic fashion. This approach proves to be effective for capturing meta-information from a source emotional tag set to predict previously unseen emotional tags. Even though shifting the data type triggers an expected performance drop, our meta-learning approach achieves decent results when compared to the fully supervised one.",https://aclanthology.org/2021.metanlp-1.9,Association for Computational Linguistics,2021,August,Proceedings of the 1st Workshop on Meta Learning and Its Applications to Natural Language Processing,"Guibon, Ga{\""e}l  and
Labeau, Matthieu  and
Flamein, H{\'e}l{\`e}ne  and
Lefeuvre, Luce  and
Clavel, Chlo{\'e}",Meta-learning for Classifying Previously Unseen Data Source into Previously Unseen Emotional Categories,10.18653/v1/2021.metanlp-1.9,metanlp,1401
P19-1252,"['Domain-specific NLP', 'Classification Applications', 'Data Management and Generation']","['Data Preparation', 'NLP for News and Media']",['NLP for Social Media'],"In this paper, we investigate the importance of social network information compared to content information in the prediction of a Twitter user's occupational class. We show that the content information of a user's tweets, the profile descriptions of a user's follower/following community, and the user's social network provide useful information for classifying a user's occupational group. In our study, we extend an existing dataset for this problem, and we achieve significantly better performance by using social network homophily that has not been fully exploited in previous work. In our analysis, we found that by using the graph convolutional network to exploit social homophily, we can achieve competitive performance on this dataset with just a small fraction of the training data.",https://aclanthology.org/P19-1252,Association for Computational Linguistics,2019,July,Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,"Pan, Jiaqi  and
Bhardwaj, Rishabh  and
Lu, Wei  and
Chieu, Hai Leong  and
Pan, Xinghao  and
Puay, Ni Yi",Twitter Homophily: Network Based Prediction of User's Occupation,10.18653/v1/P19-1252,P19,56
D19-1167,"['Machine Translation (MT)', 'Multilingual NLP', 'Learning Paradigms', 'Low-resource Languages', 'Cross-lingual Application']","['Transfer Learning', 'Neural MT (NMT)']",,"Multilingual Neural Machine Translation NMT models have yielded large empirical success in transfer learning settings. However, these black-box representations are poorly understood, and their mode of transfer remains elusive. In this work, we attempt to understand massively multilingual NMT representations with 103 languages using Singular Value Canonical Correlation Analysis SVCCA, a representation similarity framework that allows us to compare representations across different languages, layers and models. Our analysis validates several empirical results and long-standing intuitions, and unveils new observations regarding how representations evolve in a multilingual translation model. We draw three major conclusions from our analysis, with implications on cross-lingual transfer learning: i Encoder representations of different languages cluster based on linguistic similarity, ii Representations of a source language learned by the encoder are dependent on the target language, and vice-versa, and iii Representations of high resource and/or linguistically similar languages are more robust when fine-tuning on an arbitrary language pair, which is critical to determining how much cross-lingual transfer can be expected in a zero or few-shot setting. We further connect our findings with existing empirical observations in multilingual NMT and transfer learning.",https://aclanthology.org/D19-1167,Association for Computational Linguistics,2019,November,Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),"Kudugunta, Sneha  and
Bapna, Ankur  and
Caswell, Isaac  and
Firat, Orhan",Investigating Multilingual NMT Representations at Scale,10.18653/v1/D19-1167,D19,872
K17-1011,"['Machine Translation (MT)', 'Low-resource Languages']",['Statistical MT (SMT)'],,"Pairwise ranking methods are the basis of many widely used discriminative training approaches for structure prediction problems in natural language processing NLP. Decomposing the problem of ranking hypotheses into pairwise comparisons enables simple and efficient solutions. However, neglecting the global ordering of the hypothesis list may hinder learning. We propose a listwise learning framework for structure prediction problems such as machine translation. Our framework directly models the entire translation list's ordering to learn parameters which may better fit the given listwise samples. Furthermore, we propose top-rank enhanced loss functions, which are more sensitive to ranking errors at higher positions. Experiments on a large-scale Chinese-English translation task show that both our listwise learning framework and top-rank enhanced listwise losses lead to significant improvements in translation quality.",https://aclanthology.org/K17-1011,Association for Computational Linguistics,2017,August,Proceedings of the 21st Conference on Computational Natural Language Learning ({C}o{NLL} 2017),"Chen, Huadong  and
Huang, Shujian  and
Chiang, David  and
Dai, Xinyu  and
Chen, Jiajun",Top-Rank Enhanced Listwise Optimization for Statistical Machine Translation,10.18653/v1/K17-1011,K17,1082
W18-5116,"['Data Management and Generation', 'Low-resource Languages', 'Classification Applications', 'Domain-specific NLP']","['Data Preparation', 'NLP for News and Media']",,"This paper presents two large newly constructed datasets of moderated news comments from two highly popular online news portals in the respective countries: the Slovene RTV MCC and the Croatian 24sata. The datasets are analyzed by performing manual annotation of the types of the content which have been deleted by moderators and by investigating deletion trends among users and threads. Next, initial experiments on automatically detecting the deleted content in the datasets are presented. Both datasets are published in encrypted form, to enable others to perform experiments on detecting content to be deleted without revealing potentially inappropriate content. Finally, the baseline classification models trained on the non-encrypted datasets are disseminated as well to enable real-world use.",https://aclanthology.org/W18-5116,Association for Computational Linguistics,2018,October,Proceedings of the 2nd Workshop on Abusive Language Online ({ALW}2),"Ljube{\v{s}}i{\'c}, Nikola  and
Erjavec, Toma{\v{z}}  and
Fi{\v{s}}er, Darja",Datasets of Slovene and Croatian Moderated News Comments,10.18653/v1/W18-5116,W18,666
U16-1006,"['Learning Paradigms', 'Information Extraction', 'Model Architectures']","['Unsupervised Learning', 'Recurrent Neural Networks (RNNs)', 'Relation Extraction']",['Long Short-Term Memory (LSTM) Models'],"Relation extraction models based on deep learning have been attracting a lot of attention recently. Little research is carried out to reduce their need of labeled training data. In this work, we propose an unsupervised pre-training method based on the sequence-to-sequence model for deep relation extraction models. The pre-trained models need only half or even less training data to achieve equivalent performance as the same models without pre-training.",https://aclanthology.org/U16-1006,,2016,December,Proceedings of the Australasian Language Technology Association Workshop 2016,"Li, Zhuang  and
Qu, Lizhen  and
Xu, Qiongkai  and
Johnson, Mark",Unsupervised Pre-training With Seq2Seq Reconstruction Loss for Deep Relation Extraction Models,,U16,3
Y17-1039,"['Machine Translation (MT)', 'Data Management and Generation', 'Low-resource Languages']","['Data Preparation', 'Statistical MT (SMT)']",,"This paper presents several experiments on constructing Indonesian-Korean Statistical Machine Translation SMT system. A parallel corpus containing around 40,000 segments on each side has been developed for training the baseline SMT system that is built based on n-gram language model and the phrase-based translation table model. This system still has several problems, including non-translated phrases, mistranslation, incorrect phrase orders, and remaining Korean particles in the target language. To overcome these problems, some techniques are employed i.e. POS part-of-speech tag model, POS-based reordering rules, multiple steps translation, additional post-process, and their combinations. We then test the SMT system by randomly extracting segments from the parallel corpus. In general, the additional techniques lead to better performance in terms of BLEU score compared to the baseline system",https://aclanthology.org/Y17-1039,The National University (Phillippines),2017,November,"Proceedings of the 31st Pacific Asia Conference on Language, Information and Computation","Mawalim, Candy Olivia  and
Lestari, Dessi Puji  and
Purwarianti, Ayu",Rule-based Reordering and Post-Processing for Indonesian-Korean Statistical Machine Translation,,Y17,1431
W17-2340,"['Domain-specific NLP', 'Information Extraction']","['Event Extraction', 'Medical and Clinical NLP']",['Biomedical NLP'],Biomedical events describe complex interactions between various biomedical entities. Event trigger is a word or a phrase which typically signifies the occurrence of an event. Event trigger identification is an important first step in all event extraction methods. However many of the current approaches either rely on complex handcrafted features or consider features only within a window. In this paper we propose a method that takes the advantage of recurrent neural network RNN to extract higher level features present across the sentence. Thus hidden state representation of RNN along with word and entity type embedding as features avoid relying on the complex hand-crafted features generated using various NLP toolkits. Our experiments have shown to achieve state-ofart F1-score on Multi Level Event Extraction MLEE corpus. We have also performed category-wise analysis of the result and discussed the importance of various features in trigger identification task.,https://aclanthology.org/W17-2340,Association for Computational Linguistics,2017,August,{B}io{NLP} 2017,"V S S Patchigolla, Rahul  and
Sahu, Sunil  and
Anand, Ashish",Biomedical Event Trigger Identification Using Bidirectional Recurrent Neural Network Based Models,10.18653/v1/W17-2340,W17,69
2019.gwc-1.8,"['Data Management and Generation', 'Knowledge Representation and Reasoning']",['Data Analysis'],,"The paper presents an effort on transferability of noun -verb and nounadjective derivative and semantic relations to noun -noun relations. The approach relies on information from semantic classes and existing inter-POS derivative and morphosemantic relations between noun and verb, and noun and adjective synsets. We have added semantic relations between nouns in WordNet that are indirectly linked via verbs and adjectives. Observations on the combination between the relations and semantic classes of nouns they link, may facilitate further efforts in assigning semantic properties to nouns pointing to their abilities to participate in predicate-argument structures.",https://aclanthology.org/2019.gwc-1.8,Global Wordnet Association,2019,July,Proceedings of the 10th Global Wordnet Conference,"Dimitrova, Tsvetana  and
Stefanova, Valentina",On Hidden Semantic Relations between Nouns in WordNet,,gwc,1330
D16-1240,['Data Management and Generation'],['Data Analysis'],,"Implicative verbs e.g. manage entail their complement clauses, while non-implicative verbs e.g. want do not. For example, while managing to solve the problem entails solving the problem, no such inference follows from wanting to solve the problem. Differentiating between implicative and non-implicative verbs is therefore an essential component of natural language understanding, relevant to applications such as textual entailment and summarization. We present a simple method for predicting implicativeness which exploits known constraints on the tense of implicative verbs and their complements. We show that this yields an effective, data-driven way of capturing this nuanced property in verbs.",https://aclanthology.org/D16-1240,Association for Computational Linguistics,2016,November,Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing,"Pavlick, Ellie  and
Callison-Burch, Chris",Tense Manages to Predict Implicative Behavior in Verbs,10.18653/v1/D16-1240,D16,1420
2022.suki-1.8,['Knowledge Representation and Reasoning'],,,"When humans perform a particular task, they do so hierarchically: splitting higher-level tasks into smaller sub-tasks. However, most works on natural language NL command of situated agents have treated the procedures to be executed as flat sequences of simple actions, or any hierarchies of procedures have been shallow at best. In this paper, we propose a formalism of procedures as programs, a method for representing hierarchical procedural knowledge for agent command and control aimed at enabling easy application to various scenarios. We further propose a modeling paradigm of hierarchical modular networks, which consist of a planner and reactors that convert NL intents to predictions of executable programs and probe the environment for information necessary to complete the program execution. We instantiate this framework on the IQA and ALFRED datasets for NL instruction following. Our model outperforms reactive baselines by a large margin on both datasets. We also demonstrate that our framework is more data-efficient, and that it allows for fast iterative development. 1",https://aclanthology.org/2022.suki-1.8,Association for Computational Linguistics,2022,July,Proceedings of the Workshop on Structured and Unstructured Knowledge Integration (SUKI),"Zhou, Shuyan  and
Yin, Pengcheng  and
Neubig, Graham",Hierarchical Control of Situated Agents through Natural Language,10.18653/v1/2022.suki-1.8,suki,1250
2020.nli-1.2,"['Data Management and Generation', 'Classification Applications']","['Data Augmentation', 'Data Preparation']",,"Data augmentation methods are commonly used in computer vision and speech. However, in domains dealing with textual data, such techniques are not that common. Most of the existing methods rely on rephrasing, i.e. new sentences are generated by changing a source sentence, preserving its meaning. We argue that in tasks with opposable classes such as ""Positive"" and ""Negative"" in sentiment analysis, it might be beneficial to also ""invert"" the source sentence, reversing its meaning, to generate examples of the opposing class. Methods that use somewhat similar intuition exist in the space of adversarial learning, but are not always applicable to text classification in our experiments, some of them were even detrimental to the resulting classifier accuracy. We propose and evaluate two reversal-based methods on an NLI task of recognising a type of a simple logical expression from its description in plain-text form. After gathering a dataset on MTurk, we show that a simple heuristic using notion of negating the main verb has potential not only on its own, but that it can also boost existing state-of-the-art rephrasingbased approaches.",https://aclanthology.org/2020.nli-1.2,Association for Computational Linguistics,2020,July,Proceedings of the First Workshop on Natural Language Interfaces,"Tarasov, Alexey",Towards Reversal-Based Textual Data Augmentation for NLI Problems with Opposable Classes,10.18653/v1/2020.nli-1.2,nli,132
2021.adaptnlp-1.8,"['Learning Paradigms', 'Text Clustering', 'Cross-lingual Application', 'Low-resource Languages']",['Few-shot Learning'],,"In meta-learning, the knowledge learned from previous tasks is transferred to new ones, but this transfer only works if tasks are related. Sharing information between unrelated tasks might hurt performance, and it is unclear how to transfer knowledge across tasks that have a hierarchical structure. Our research extends a meta-learning model, MAML, by exploiting hierarchical task relationships. Our algorithm, TreeMAML, adapts the model to each task with a few gradient steps, but the adaptation follows the hierarchical tree structure: in each step, gradients are pooled across tasks clusters and subsequent steps follow down the tree. We also implement a clustering algorithm that generates the tasks tree without previous knowledge of the task structure, allowing us to make use of implicit relationships between the tasks. We show that TreeMAML successfully trains natural language processing models for crosslingual Natural Language Inference by taking advantage of the language phylogenetic tree. This result is useful, since most languages in the world are under-resourced and the improvement on cross-lingual transfer allows the internationalization of NLP models.",https://aclanthology.org/2021.adaptnlp-1.8,Association for Computational Linguistics,2021,April,Proceedings of the Second Workshop on Domain Adaptation for NLP,"Garcia, Jezabel  and
Freddi, Federica  and
McGowan, Jamie  and
Nieradzik, Tim  and
Liao, Feng-Ting  and
Tian, Ye  and
Shiu, Da-shan  and
Bernacchia, Alberto",Cross-Lingual Transfer with MAML on Trees,,adaptnlp,899
2022.mia-1.2,"['Data Management and Generation', 'Low-resource Languages', 'Classification Applications', 'Model Architectures']","['Data Preparation', 'Data Analysis']",['Annotation Processes'],"The modes of discourse aid in comprehending the convention and purpose of various forms of languages used during communication. In this study, we introduce a discourse mode annotated corpus for the low-resource Bengali also referred to as Bengali language. The corpus consists of sentence-level annotation of three discourse modes, narrative, descriptive, and informative of the text excerpted from a number of Bengali novels. We analyze the annotated corpus to expose various linguistic aspects of discourse modes, such as class distributions and average sentence lengths. To automatically determine the mode of discourse, we apply CML classical machine learning classifiers with ngram based statistical features and a finetuned BERT Bidirectional Encoder Representations from Transformers based language model. We observe that fine-tuned BERT-based model yields better results than CML classifiers. Our created discourse mode annotated dataset, the first of its kind in Bengali, and the evaluation, provide baselines for the automatic discourse mode identification in Bengali and can assist various downstream natural language processing tasks.",https://aclanthology.org/2022.mia-1.2,Association for Computational Linguistics,2022,July,Proceedings of the Workshop on Multilingual Information Access (MIA),"Sazzed, Salim",An Annotated Dataset and Automatic Approaches for Discourse Mode Identification in Low-resource Bengali Language,10.18653/v1/2022.mia-1.2,mia,90
2021.wmt-1.117,"['Machine Translation (MT)', 'Learning Paradigms', 'Low-resource Languages']","['Neural MT (NMT)', 'Active Learning']",,"Neural machine translation NMT is sensitive to domain shift. In this paper, we address this problem in an active learning setting where we can spend a given budget on translating in-domain data, and gradually finetune a pre-trained out-of-domain NMT model on the newly translated data. Existing active learning methods for NMT usually select sentences based on uncertainty scores, but these methods require costly translation of full sentences even when only one or two key phrases within the sentence are informative. To address this limitation, we re-examine previous work from the phrase-based machine translation PBMT era that selected not full sentences, but rather individual phrases. However, while incorporating these phrases into PBMT systems was relatively simple, it is less trivial for NMT systems, which need to be trained on full sequences to capture larger structural properties of sentences unique to the new domain. To overcome these hurdles, we propose to select both full sentences and individual phrases from unlabelled data in the new domain for routing to human translators. In a German-English translation task, our active learning approach achieves consistent improvements over uncertainty-based sentence selection methods, improving up to 1.2 BLEU score over strong active learning baselines. 1",https://aclanthology.org/2021.wmt-1.117,Association for Computational Linguistics,2021,November,Proceedings of the Sixth Conference on Machine Translation,"Hu, Junjie  and
Neubig, Graham",Phrase-level Active Learning for Neural Machine Translation,10.48550/arxiv.2106.11375,wmt,1358
2020.nlpcss-1.18,"['Data Management and Generation', 'Low-resource Languages', 'Domain-specific NLP']","['Data Preparation', 'Data Analysis', 'NLP for News and Media']",['NLP for Social Media'],"I test two hypotheses that play an important role in modern sociolinguistics and language evolution studies: first, that non-native production is simpler than native; second, that production addressed to non-native speakers is simpler than that addressed to natives. The second hypothesis is particularly important for theories about contact-induced simplification, since the accommodation to non-natives may explain how the simplification can spread from adult learners to the whole community. To test the hypotheses, I create a very large corpus of native and non-native written speech in four languages English, French, Italian, Spanish, extracting data from an internet forum where native languages of the participants are known and the structure of the interactions can be inferred. The corpus data yield inconsistent evidence with respect to the first hypothesis, but largely support the second one, suggesting that foreigner-directed speech is indeed simpler than native-directed. Importantly, when testing the first hypothesis, I contrast production of different speakers, which can introduce confounds and is a likely reason for the inconsistencies. When testing the second hypothesis, the comparison is always within the production of the same speaker but with different addressees, which makes it more reliable.",https://aclanthology.org/2020.nlpcss-1.18,Association for Computational Linguistics,2020,November,Proceedings of the Fourth Workshop on Natural Language Processing and Computational Social Science,"Berdicevskis, Aleksandrs",Foreigner-directed speech is simpler than native-directed: Evidence from social media,10.18653/v1/2020.nlpcss-1.18,nlpcss,477
2022.deeplo-1.22,"['Machine Translation (MT)', 'Data Management and Generation', 'Low-resource Languages', 'Cross-lingual Application', 'Model Architectures']","['Transformer Models', 'Data Preparation']",,"The lack of resources for languages in the Americas has proven to be a problem for the creation of digital systems such as machine translation, search engines, chat bots, and more. The scarceness of digital resources for a language causes a higher impact on populations where the language is spoken by millions of people. We introduce the first official large combined corpus for deep learning of an indigenous South American low-resource language spoken by millions called Quechua. Specifically, our curated corpus is created from text gathered from the southern region of Peru where a dialect of Quechua is spoken that has not traditionally been used for digital systems as a target dialect in the past. In order to make our work repeatable by others, we also offer a public, pre-trained, BERT model called Qu-BERT which is the largest linguistic model ever trained for any Quechua type, not just the southern region dialect. We furthermore test our corpus and its corresponding BERT model on two major tasks: 1 named-entity recognition NER and 2 part-of-speech POS tagging by using state-of-the-art techniques where we achieve results comparable to other work on higher-resource languages. In this article, we describe the methodology, challenges, and results from the creation of QuBERT which is on on par with other state-of-the-art multilingual models for natural language processing achieving between 71 and 74% F1 score on NER and 84-87% on POS tasks.",https://aclanthology.org/2022.deeplo-1.22,Association for Computational Linguistics,2022,July,Proceedings of the Third Workshop on Deep Learning for Low-Resource Natural Language Processing,"Jundi, Iman  and
Lapesa, Gabriella",How to Translate Your Samples and Choose Your Shots? Analyzing Translate-train \& Few-shot Cross-lingual Transfer,10.18653/v1/2022.deeplo-1.22,deeplo,107
2020.lt4hala-1.4,"['Text Preprocessing', 'Low-resource Languages']",,,"Classical Armenian, Old Georgian and Syriac are under-resourced digital languages. Even though a lot of printed critical editions or dictionaries are available, there is currently a lack of fully tagged corpora that could be reused for automatic text analysis. In this paper, we introduce an ongoing project of lemmatization and POS-tagging for these languages, relying on a recurrent neural network RNN, specific morphological tags and dedicated datasets. For this paper, we have combine different corpora previously processed by automatic out-of-context lemmatization and POS-tagging, and manual proofreading by the collaborators of the GREgORI Project UCLouvain, Louvain-la-Neuve, Belgium. We intend to compare a rule based approach and a RNN approach by using PIE specialized by Calfa Paris, France. We introduce here first results. We reach a mean accuracy of 91,63% in lemmatization and of 92,56% in POS-tagging. The datasets, which were constituted and used for this project, are not yet representative of the different variations of these languages through centuries, but they are homogenous and allow reaching tangible results, paving the way for further analysis of wider corpora.",https://aclanthology.org/2020.lt4hala-1.4,European Language Resources Association (ELRA),2020,May,Proceedings of LT4HALA 2020 - 1st Workshop on Language Technologies for Historical and Ancient Languages,"Vidal-Gor{\`e}ne, Chahan  and
Kindt, Bastien","Lemmatization and POS-tagging process by using joint learning approach. Experimental results on Classical Armenian, Old Georgian, and Syriac",,lt4hala,821
2022.naacl-main.181,"['Question Answering (QA)', 'Classification Applications', 'Model Architectures']","['Transformer Models', 'Claim Verification']",,"Inference tasks such as answer sentence selection AS2 or fact verification are typically solved by fine-tuning transformer-based models as individual sentence-pair classifiers. Recent studies show that these tasks benefit from modeling dependencies across multiple candidate sentences jointly. In this paper, we first show that popular pre-trained transformers perform poorly when used for fine-tuning on multi-candidate inference tasks. We then propose a new pre-training objective that models the paragraph-level semantics across multiple input sentences. Our evaluation on three AS2 and one fact verification datasets demonstrates the superiority of our pre-training technique over the traditional ones for transformers used as joint models for multi-candidate inference tasks, as well as when used as cross-encoders for sentence-pair formulations of these tasks.",https://aclanthology.org/2022.naacl-main.181,Association for Computational Linguistics,2022,July,Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,"Di Liello, Luca  and
Garg, Siddhant  and
Soldaini, Luca  and
Moschitti, Alessandro",Paragraph-based Transformer Pre-training for Multi-Sentence Inference,10.18653/v1/2022.naacl-main.181,naacl,863
2021.rocling-1.43,"['Information Extraction', 'Machine Translation (MT)', 'Low-resource Languages']",,,"This paper presents a method for automatically identifying bilingual grammar patterns and extracting bilingual phrase instances from a given English-Chinese sentence pair. In our approach, the English-Chinese sentence pair is parsed to identify English grammar patterns and Chinese counterparts. The method involves generating translations of each English grammar pattern and calculating translation probability of words from a word-aligned parallel corpora. The results allow us to extract the most probable English-Chinese phrase pairs in the sentence pair. We present a prototype system that applies the method to extract grammar patterns and phrases in parallel sentences. An evaluation on randomly selected examples from a dictionary shows that our approach has reasonably good performance. We use human judge to assess the bilingual phrases generated by our approach. The results have potential to assist language learning and machine translation research.",https://aclanthology.org/2021.rocling-1.43,The Association for Computational Linguistics and Chinese Language Processing (ACLCLP),2021,October,Proceedings of the 33rd Conference on Computational Linguistics and Speech Processing (ROCLING 2021),"Chen, Yi-Jyun  and
Chung, Hsin-Yun  and
Chang, Jason S.",Identify Bilingual Patterns and Phrases from a Bilingual Sentence Pair,,rocling,636
2022.socialnlp-1.3,"['Learning Paradigms', 'Domain-specific NLP', 'Classification Applications', 'Model Architectures']","['Transformer Models', 'Rumor Detection', 'Supervised Learning', 'NLP for News and Media']",['NLP for Social Media'],"Kyle 1985 proposes two types of rumors: informed rumors that are based on some private information and uninformed rumors that are not based on any information i.e. bluffing. Also, prior studies find that when people have credible source of information, they are likely to use a more confident textual tone in their spreading of rumors. Motivated by these theoretical findings, we propose a double-channel structure to determine the ex-ante veracity of rumors on social media. Our ultimate goal is to classify each rumor into true, false, or unverifiable category. We first assign each text into either certain informed rumor or uncertain uninformed rumor category. Then, we apply lie detection algorithm to informed rumors and thread-reply agreement detection algorithm to uninformed rumors. Using the dataset of Se-mEval 2019 Task 7, which requires ex-ante threefold classification true, false, or unverifiable of social media rumors, our model yields a macro-F1 score of 0.4027, outperforming all the baseline models and the second-place winner Gorrell et al., 2019 . Furthermore, we empirically validate that the double-channel structure outperforms single-channel structures which use either lie detection or agreement detection algorithm to all posts. 1",https://aclanthology.org/2022.socialnlp-1.3,Association for Computational Linguistics,2022,July,Proceedings of the Tenth International Workshop on Natural Language Processing for Social Media,"Kim, Alex Gunwoo  and
Yoon, Sangwon",Detecting Rumor Veracity with Only Textual Information by Double-Channel Structure,10.18653/v1/2022.socialnlp-1.3,socialnlp,441
N19-1285,"['Audio Generation and Processing', 'Machine Translation (MT)', 'Low-resource Languages', 'Model Architectures']",,,"Spoken language translation applications for speech suffer due to conversational speech phenomena, particularly the presence of disfluencies. With the rise of end-to-end speech translation models, processing steps such as disfluency removal that were previously an intermediate step between speech recognition and machine translation need to be incorporated into model architectures. We use a sequence-to-sequence model to translate from noisy, disfluent speech to fluent text with disfluencies removed using the recently collected 'copy-edited' references for the Fisher Spanish-English dataset. We are able to directly generate fluent translations and introduce considerations about how to evaluate success on this task. This work provides a baseline for a new task, the translation of conversational speech with joint removal of disfluencies.",https://aclanthology.org/N19-1285,Association for Computational Linguistics,2019,June,"Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)","Salesky, Elizabeth  and
Sperber, Matthias  and
Waibel, Alexander",Fluent Translations from Disfluent Speech in End-to-End Speech Translation,10.18653/v1/N19-1285,N19,1356
2020.acl-main.296,"['Information Extraction', 'Classification Applications']",['Sentiment Analysis (SA)'],['Aspect-Based SA (ABSA)'],"Aspect terms extraction and opinion terms extraction are two key problems of fine-grained Aspect Based Sentiment Analysis ABSA. The aspect-opinion pairs can provide a global profile about a product or service for consumers and opinion mining systems. However, traditional methods can not directly output aspect-opinion pairs without given aspect terms or opinion terms. Although some recent co-extraction methods have been proposed to extract both terms jointly, they fail to extract them as pairs. To this end, this paper proposes an end-to-end method to solve the task of Pair-wise Aspect and Opinion Terms Extraction PAOTE. Furthermore, this paper treats the problem from a perspective of joint term and relation extraction rather than under the sequence tagging formulation performed in most prior works. We propose a multi-task learning framework based on shared spans, where the terms are extracted under the supervision of span boundaries. Meanwhile, the pair-wise relations are jointly identified using the span representations. Extensive experiments show that our model consistently outperforms stateof-the-art methods.",https://aclanthology.org/2020.acl-main.296,Association for Computational Linguistics,2020,July,Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,"Zhao, He  and
Huang, Longtao  and
Zhang, Rong  and
Lu, Quan  and
Xue, Hui",SpanMlt: A Span-based Multi-Task Learning Framework for Pair-wise Aspect and Opinion Terms Extraction,10.18653/v1/2020.acl-main.296,acl,1399
2021.nlp4prog-1.2,"['Text Generation', 'Data Management and Generation', 'Evaluation Techniques']",['Data Preparation'],,"We introduce CONTEST, a benchmark for NLP-based unit test completion, the task of predicting a test's assert statements given its setup and focal method, i.e. the method to be tested. CONTEST is large-scale with 365k datapoints. Besides the test code and tested code, it also features context code called by either. We found context to be crucial for accurately predicting assertions. We also introduce baselines based on transformer encoderdecoders, and study the effects of including syntactic information and context. Overall, our models achieve a BLEU score of 38.2, while only generating unparsable code in 1.92% of cases.",https://aclanthology.org/2021.nlp4prog-1.2,Association for Computational Linguistics,2021,August,Proceedings of the 1st Workshop on Natural Language Processing for Programming (NLP4Prog 2021),"Villmow, Johannes  and
Depoix, Jonas  and
Ulges, Adrian",ConTest: A Unit Test Completion Benchmark featuring Context,10.18653/v1/2021.nlp4prog-1.2,nlp4prog,462
K18-2024,"['Embeddings', 'Parsing', 'Multilingual NLP', 'Low-resource Languages', 'Model Architectures']","['Syntactic Parsing', 'Word Embeddings', 'Recurrent Neural Networks (RNNs)']","['Dependency Parsing', 'Long Short-Term Memory (LSTM) Models']",We propose two word representation models for agglutinative languages that better capture the similarities between words which have similar tasks in sentences. Our models highlight the morphological features in words and embed morphological information into their dense representations. We have tested our models on an LSTM-based dependency parser with character-based word embeddings proposed by Ballesteros et al. 2015. We participated in the CoNLL 2018 Shared Task on multilingual parsing from raw text to universal dependencies as the BOUN team. We show that our morphologybased embedding models improve the parsing performance for most of the agglutinative languages.,https://aclanthology.org/K18-2024,Association for Computational Linguistics,2018,October,Proceedings of the {C}o{NLL} 2018 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies,"{\""O}zate{\c{s}}, {\c{S}}aziye Bet{\""u}l  and
{\""O}zg{\""u}r, Arzucan  and
G{\""u}ng{\""o}r, Tunga  and
{\""O}zt{\""u}rk, Balk{\i}z",A Morphology-Based Representation Model for LSTM-Based Dependency Parsing of Agglutinative Languages,10.18653/v1/K18-2024,K18,797
2020.nlptea-1.18,"['Domain-specific NLP', 'Data Management and Generation', 'Low-resource Languages']","['Data Preparation', 'Data Analysis']",,"Language and music are closely related. Regarding the linguistic feature richness, pop songs are probably suitable to be used as extracurricular materials in language teaching. In order to prove this point, this paper presents the Contemporary Chinese Pop Lyrics CCPL corpus. Based on that, we investigated and evaluated the appropriateness of pop songs for Teaching Chinese as a Second Language TCSL with the assistance of Natural Language Processing methods from the perspective of Chinese character coverage, lexical coverage and the addressed topic similarity. Some suggestions in Chinese teaching with the aid of pop lyrics are provided.",https://aclanthology.org/2020.nlptea-1.18,Association for Computational Linguistics,2020,December,Proceedings of the 6th Workshop on Natural Language Processing Techniques for Educational Applications,"Chi, Xiangyu  and
Rao, Gaoqi",A Corpus Linguistic Perspective on the Appropriateness of Pop Songs for Teaching Chinese as a Second Language,10.18653/v1/2020.nlptea-1.18,nlptea,495
2020.nlposs-1.9,"['Audio Generation and Processing', 'Data Management and Generation', 'Learning Paradigms', 'Classification Applications', 'Model Architectures']","['Data Augmentation', 'Automatic Speech Recognition (ASR)', 'Supervised Learning']",,"We describe Howl, an open-source wake word detection toolkit with native support for open speech datasets such as Mozilla Common Voice MCV and Google Speech Commands GSC. We report benchmark results of various models supported by our toolkit on GSC and our own freely available wake word detection dataset, built from MCV. One of our models is deployed in Firefox Voice, a plugin enabling speech interactivity for the Firefox web browser. Howl represents, to the best of our knowledge, the first fully productionized, open-source wake word detection toolkit with a web browser deployment target. Our codebase is at howl.ai.",https://aclanthology.org/2020.nlposs-1.9,Association for Computational Linguistics,2020,November,Proceedings of Second Workshop for NLP Open Source Software (NLP-OSS),"Tang, Raphael  and
Lee, Jaejun  and
Razi, Afsaneh  and
Cambre, Julia  and
Bicking, Ian  and
Kaye, Jofish  and
Lin, Jimmy","Howl: A Deployed, Open-Source Wake Word Detection System",10.18653/v1/2020.nlposs-1.9,nlposs,1231
2020.amta-research.6,"['Low-resource Languages', 'Model Architectures', 'Learning Paradigms']",['Unsupervised Learning'],,"Word alignments identify translational correspondences between words in a parallel sentence pair and are used, for instance, to learn bilingual dictionaries, to train statistical machine translation systems or to perform quality estimation. Variational autoencoders have been recently used in various of natural language processing to learn in an unsupervised way latent representations that are useful for language generation tasks. In this paper, we study these models for the task of word alignment and propose and assess several evolutions of a vanilla variational autoencoders. We demonstrate that these techniques can yield competitive results as compared to Giza++ and to a strong neural network alignment system for two language pairs.",https://aclanthology.org/2020.amta-research.6,Association for Machine Translation in the Americas,2020,October,Proceedings of the 14th Conference of the Association for Machine Translation in the Americas (Volume 1: Research Track),"Ngo Ho, Anh Khoa  and
Yvon, Fran{\c{c}}ois",Generative latent neural models for automatic word alignment,10.48550/arxiv.2009.13117,amta,608
2020.intexsempar-1.2,"['Parsing', 'Data Management and Generation', 'Learning Paradigms']","['Active Learning', 'Data Preparation', 'Semantic Parsing']",['Annotation Processes'],"Collecting training data for semantic parsing is a time-consuming and expensive task. As a result, there is growing interest in industry to reduce the number of annotations required to train a semantic parser, both to cut down on costs and to limit customer data handled by annotators. In this paper, we propose uncertainty and traffic-aware active learning, a novel active learning method that uses model confidence and utterance frequencies from customer traffic to select utterances for annotation. We show that our method significantly outperforms baselines on an internal customer dataset and the Facebook Task Oriented Parsing TOP dataset. On our internal dataset, our method achieves the same accuracy as random sampling with 2,000 fewer annotations.",https://aclanthology.org/2020.intexsempar-1.2,Association for Computational Linguistics,2020,November,Proceedings of the First Workshop on Interactive and Executable Semantic Parsing,"Sen, Priyanka  and
Yilmaz, Emine",Uncertainty and Traffic-Aware Active Learning for Semantic Parsing,10.18653/v1/2020.intexsempar-1.2,intexsempar,1058
2021.nllp-1.4,"['Domain-specific NLP', 'Data Management and Generation', 'Information Extraction', 'Learning Paradigms', 'Low-resource Languages']","['NLP for the Legal Domain', 'Data Preparation', 'Supervised Learning']",['Annotation Processes'],"We present the task of Automated Punishment Extraction APE in sentencing decisions from criminal court cases in Hebrew. Addressing APE will enable the identification of sentenc ing patterns and constitute an important step ping stone for many follow up legal NLP ap plications in Hebrew, including the prediction of sentencing decisions. We curate a dataset of sexual assault sentencing decisions and a manuallyannotated evaluation dataset, and implement rulebased and supervised models. We find that while supervised models can iden tify the sentence containing the punishment with good accuracy, rulebased approaches outperform them on the full APE task. We con clude by presenting a first analysis of sentenc ing patterns in our dataset and analyze com mon models' errors, indicating avenues for fu ture work, such as distinguishing between pro bation and actual imprisonment punishment. We will make all our resources available upon request, including data, annotation, and first benchmark models.",https://aclanthology.org/2021.nllp-1.4,Association for Computational Linguistics,2021,November,Proceedings of the Natural Legal Language Processing Workshop 2021,"Wenger, Mohr  and
Kalir, Tom  and
Berger, Noga  and
Chalamish, Carmit Klar  and
Keydar, Renana  and
Stanovsky, Gabriel",Automated Extraction of Sentencing Decisions from Court Cases in the Hebrew Language,10.18653/v1/2021.nllp-1.4,nllp,971
2022.wnu-1.2,"['Domain-specific NLP', 'Data Management and Generation', 'Knowledge Representation and Reasoning', 'Classification Applications', 'Model Architectures']","['Transformer Models', 'Data Augmentation']",,"Transformer-based models have shown promising performance in numerous NLP tasks. However, recent work has shown the limitation of such models in showing compositional generalization, which requires models to generalize to novel compositions of known concepts. In this work, we explore two strategies for compositional generalization on the task of kinship prediction from stories: 1 data augmentation and 2 predicting and using intermediate structured representation in form of kinship graphs. Our experiments show that data augmentation boosts generalization performance by around 20% on average relative to a baseline model from prior work not using these strategies. However, predicting and using intermediate kinship graphs leads to a deterioration in the generalization of kinship prediction by around 50% on average relative to models that only leverage data augmentation.",https://aclanthology.org/2022.wnu-1.2,Association for Computational Linguistics,2022,July,Proceedings of the 4th Workshop of Narrative Understanding (WNU2022),"Wei, Kangda  and
Ghosh, Sayan  and
Srivastava, Shashank",Compositional Generalization for Kinship Prediction through Data Augmentation,10.18653/v1/2022.wnu-1.2,wnu,1183
O18-1002,"['Audio Generation and Processing', 'Classification Applications', 'Adversarial Attacks and Robustness']",['Automatic Speech Recognition (ASR)'],,"An adversarial attack is an exploitative process in which minute alterations are made to natural inputs, causing the inputs to be misclassified by neural models. In the field of speech recognition, this has become an issue of increasing significance. Although adversarial attacks were originally introduced in computer vision, they have since infiltrated the realm of speech recognition. In 2017, a genetic attack was shown to be quite potent against the Speech Commands Model. Limited-vocabulary speech classifiers, such as the Speech Commands Model, are used in a variety of applications, particularly in telephony; as such, adversarial examples produced by this attack pose as a major security threat. This paper explores various methods of detecting these adversarial examples with combinations of audio preprocessing. One particular combined defense incorporating compressions, speech coding, filtering, and audio panning was shown to be quite effective against the attack on the Speech Commands Model, detecting audio adversarial examples with 93.5% precision and 91.2% recall.",https://aclanthology.org/O18-1002,The Association for Computational Linguistics and Chinese Language Processing (ACLCLP),2018,October,Proceedings of the 30th Conference on Computational Linguistics and Speech Processing ({ROCLING} 2018),"Rajaratnam, Krishan  and
Shah, Kunal  and
Kalita, Jugal",Isolated and Ensemble Audio Preprocessing Methods for Detecting Adversarial Examples against Automatic Speech Recognition,10.48550/arxiv.1809.04397,O18,1152
2020.coling-main.464,"['Question Answering (QA)', 'Data Management and Generation', 'Model Architectures', 'Low-resource Languages']","['Data Preparation', 'Community QA', 'Transformer Models']",['Annotation Processes'],"In community-based question answering CQA platforms, it takes time for a user to get useful information from among many answers. Although one solution is an answer ranking method, the user still needs to read through the top-ranked answers carefully. This paper proposes a new task of selecting a diverse and non-redundant answer set rather than ranking the answers. Our method is based on determinantal point processes DPPs, and it calculates the answer importance and similarity between answers by using BERT. We built a dataset focusing on a Japanese CQA site, and the experiments on this dataset demonstrated that the proposed method outperformed several baseline methods.",https://aclanthology.org/2020.coling-main.464,International Committee on Computational Linguistics,2020,December,Proceedings of the 28th International Conference on Computational Linguistics,"Fujita, Shogo  and
Shibata, Tomohide  and
Okumura, Manabu",Diverse and Non-redundant Answer Set Extraction on Community QA based on DPPs,10.18653/v1/2020.coling-main.464,coling,967
2021.nlp4call-1.3,"['Error Detection and Correction', 'Data Management and Generation', 'Low-resource Languages', 'Classification Applications']",['Data Preparation'],,"We present DaLAJ 1.0, a Dataset for Linguistic Acceptability Judgments for Swedish, comprising 9 596 sentences in its first version. DaLAJ is based on the SweLL second language learner data Volodina et al., 2019 , consisting of essays at different levels of proficiency. To make sure the dataset can be freely available despite the GDPR regulations, we have sentence-scrambled learner essays and removed part of the metadata about learners, keeping for each sentence only information about the mother tongue and the level of the course where the essay has been written. We use the normalized version of learner language as the basis for DaLAJ sentences, and keep only one error per sentence. We repeat the same sentence for each individual correction tag used in the sentence. For DaLAJ 1.0 four error categories of 35 available in SweLL are used, all connected to lexical or wordbuilding choices. The dataset is included in the SwedishGlue benchmark. 1 Below, we describe the format of the dataset, our insights and motivation for the chosen approach to data sharing.",https://aclanthology.org/2021.nlp4call-1.3,LiU Electronic Press,2021,May,Proceedings of the 10th Workshop on NLP for Computer Assisted Language Learning,"Volodina, Elena  and
Mohammed, Yousuf Ali  and
Klezl, Julia",DaLAJ -- a dataset for linguistic acceptability judgments for Swedish,10.48550/arxiv.2105.06681,nlp4call,1416
E17-3024,"['Language Change Analysis', 'Domain-specific NLP', 'Data Management and Generation', 'Multilingual NLP', 'Low-resource Languages']","['Data Analysis', 'NLP for News and Media']",,"This paper details a software designed to track neologisms in seven languages through newspapers monitor corpora. The platform combines state-of-the-art processes to track linguistic changes and a web platform for linguists to create and manage their corpora, accept or reject automatically identified neologisms, describe linguistically the accepted neologisms and follow their lifecycle on the monitor corpora. In the following, after a short state-of-the-art in Neologism Retrieval, Analysis and Life-tracking, we describe the overall architecture of the system. The platform can be freely browsed at www.neoveille.org where detailed presentation is given. Access to the editing modules is available upon request. Credits Neoveille is an international Project funded by the ANR IDEX specific funding scheme. It gathers seven Linguistics and Research Centers. See website for details.",https://aclanthology.org/E17-3024,Association for Computational Linguistics,2017,April,Proceedings of the Software Demonstrations of the 15th Conference of the {E}uropean Chapter of the Association for Computational Linguistics,"Cartier, Emmanuel","Neoveille, a Web Platform for Neologism Tracking",10.18653/v1/e17-3024,E17,866
2020.autosimtrans-1.2,"['Model Architectures', 'Low-resource Languages', 'Audio Generation and Processing', 'Learning Paradigms']",['Adversarial Learning'],,"End-to-end speech translation usually leverages audio-to-text parallel data to train an available speech translation model which has shown impressive results on various speech translation tasks. Due to the artificial cost of collecting audio-to-text parallel data, the speech translation is a natural low-resource translation scenario, which greatly hinders its improvement. In this paper, we proposed a new adversarial training method to leverage target monolingual data to relieve the lowresource shortcoming of speech translation. In our method, the existing speech translation model is considered as a Generator to gain a target language output, and another neural Discriminator is used to guide the distinction between outputs of speech translation model and true target monolingual sentences. Experimental results on the CCMT 2019-BSTC dataset speech translation task demonstrate that the proposed methods can significantly improve the performance of the end-to-end speech translation.",https://aclanthology.org/2020.autosimtrans-1.2,Association for Computational Linguistics,2020,July,Proceedings of the First Workshop on Automatic Simultaneous Translation,"Li, Xuancai  and
Kehai, Chen  and
Zhao, Tiejun  and
Yang, Muyun",End-to-End Speech Translation with Adversarial Training,10.18653/v1/2020.autosimtrans-1.2,autosimtrans,1043
Q19-1010,"['Data Management and Generation', 'Low-resource Languages', 'Finite State Machines']",['Data Analysis'],,"Autosegmental representations ARs; Goldsmith, 1976 are claimed to enable local analyses of otherwise non-local phenomena Odden, 1994 . Focusing on the domain of tone, we investigate this ability of ARs using a computationally well-defined notion of locality extended from Chandlee 2014. The result is a more nuanced understanding of the way in which ARs interact with phonological locality.",https://aclanthology.org/Q19-1010,MIT Press,2019,,,"Chandlee, Jane  and
Jardine, Adam",Autosegmental Input Strictly Local Functions,10.1162/tacl_a_00260,Q19,9
2018.gwc-1.38,"['Knowledge Representation and Reasoning', 'Low-resource Languages', 'Machine Translation (MT)']",['Taxonomy Construction'],,"One of the fundamental building blocks of a wordnet is synonym sets or synsets, which group together similar word meanings or synonyms. These synsets can consist either one or more synonyms. This paper describes an automatic method for composing synsets with multiple synonyms by using Google Translate and Semantic Mirrors' method. Also, we will give an overview of the results and discuss the advantages of the proposed method from wordnet's point of view.",https://aclanthology.org/2018.gwc-1.38,Global Wordnet Association,2018,January,Proceedings of the 9th Global Wordnet Conference,"Lohk, Ahti  and
Tombak, Mati  and
Vare, Kadri",An Experiment: Using Google Translate and Semantic Mirrors to Create Synsets with Many Lexical Units,,gwc,260
2020.iwslt-1.32,"['Data Management and Generation', 'Machine Translation (MT)']","['Neural MT (NMT)', 'Data Augmentation']",,"A variety of natural language tasks require processing of textual data which contains a mix of natural language and formal languages such as mathematical expressions. In this paper, we take unit conversions as an example and propose a data augmentation technique which lead to models learning both translation and conversion tasks as well as how to adequately switch between them for end-to-end localization.",https://aclanthology.org/2020.iwslt-1.32,Association for Computational Linguistics,2020,July,Proceedings of the 17th International Conference on Spoken Language Translation,"Dinu, Georgiana  and
Mathur, Prashant  and
Federico, Marcello  and
Lauly, Stanislas  and
Al-Onaizan, Yaser",Joint Translation and Unit Conversion for End-to-end Localization,10.18653/v1/2020.iwslt-1.32,iwslt,1315
2020.parlaclarin-1.13,"['Domain-specific NLP', 'Data Management and Generation', 'Low-resource Languages']","['NLP for the Legal Domain', 'Data Analysis']",,"The TAPS corpus makes it possible to share a large volume of French parliamentary data. The TEI-compliant approach behind its design choices facilitates the publishing and the interoperability of data, but also the implementation of exploratory data analysis techniques in order to process institutional or political discourse. We demonstrate its application to the debates occurred in the context of a specific legislative process, which generated a strong opposition.",https://aclanthology.org/2020.parlaclarin-1.13,European Language Resources Association,2020,May,Proceedings of the Second ParlaCLARIN Workshop,"Diwersy, Sascha  and
Luxardo, Giancarlo",Querying a large annotated corpus of parliamentary debates,,parlaclarin,104
2020.nlp4convai-1.11,"['Classification Applications', 'Model Architectures']","['Recurrent Neural Networks (RNNs)', 'Multilabel Text Classification']",['Long Short-Term Memory (LSTM) Models'],"Slot Filling SF is one of the sub-tasks of Spoken Language Understanding SLU which aims to extract semantic constituents from a given natural language utterance. It is formulated as a sequence labeling task. Recently, it has been shown that contextual information is vital for this task. However, existing models employ contextual information in a restricted manner, e.g., using self-attention. Such methods fail to distinguish the effects of the context on the word representation and the word label. To address this issue, in this paper, we propose a novel method to incorporate the contextual information in two different levels, i.e., representation level and task-specific i.e., label level. Our extensive experiments on three benchmark datasets on SF show the effectiveness of our model leading to new state-of-theart results on all three benchmark datasets for the task of SF.",https://aclanthology.org/2020.nlp4convai-1.11,Association for Computational Linguistics,2020,July,Proceedings of the 2nd Workshop on Natural Language Processing for Conversational AI,"Pouran Ben Veyseh, Amir  and
Dernoncourt, Franck  and
Nguyen, Thien Huu",Improving Slot Filling by Utilizing Contextual Information,10.18653/v1/2020.nlp4convai-1.11,nlp4convai,201
Y16-2009,"['Information Extraction', 'Embeddings', 'Learning Paradigms', 'Low-resource Languages']","['Word Embeddings', 'Supervised Learning', 'Word Sense Disambiguation (WSD)']",,"In this paper, we propose a method that employs sentences similarities from context word embeddings for supervised word sense disambiguation. In particular, if N example sentences exist in training data, an N-dimensional vector with N similarities between each pair of example sentences is added to a basic feature vector. This new feature vector is used to train a classifier and identification. We evaluated the proposed method using the feature vectors based on Bag-of-Words, SemEval-2 baseline as basic feature vectors and SemEval-2 Japanese task. The experimental results suggest that the method is more effective than the method with only basic vectors.",https://aclanthology.org/Y16-2009,,2016,October,"Proceedings of the 30th Pacific Asia Conference on Language, Information and Computation: Oral Papers","Yamaki, Shoma  and
Shinnou, Hiroyuki  and
Komiya, Kanako  and
Sasaki, Minoru",Supervised Word Sense Disambiguation with Sentences Similarities from Context Word Embeddings,,Y16,1476
D18-1134,"['Question Answering (QA)', 'Data Management and Generation', 'Model Architectures']","['Data Preparation', 'Open-Domain QA', 'Recurrent Neural Networks (RNNs)']",['Long Short-Term Memory (LSTM) Models'],"Previous work on question-answering systems mainly focuses on answering individual questions, assuming they are independent and devoid of context. Instead, we investigate sequential question answering, asking multiple related questions. We present QBLink, a new dataset of fully human-authored questions. We extend existing strong question answering frameworks to include previous questions to improve the overall question-answering accuracy in open-domain question answering.",https://aclanthology.org/D18-1134,Association for Computational Linguistics,2018,October-November,Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing,"Elgohary, Ahmed  and
Zhao, Chen  and
Boyd-Graber, Jordan",A dataset and baselines for sequential open-domain question answering,10.18653/v1/D18-1134,D18,1170
2021.iwcs-1.6,"['Learning Paradigms', 'Parsing', 'Information Extraction', 'Model Architectures']","['Semantic Parsing', 'Supervised Learning', 'Word Sense Disambiguation (WSD)']",['Semantic Role Labeling'],"Despite recent advances in semantic role labeling propelled by pre-trained text encoders like BERT, performance lags behind when applied to predicates observed infrequently during training or to sentences in new domains. In this work, we investigate how semantic role labeling performance on low-frequency predicates and out-of-domain data can be improved by using VerbNet, a verb lexicon that groups verbs into hierarchical classes based on shared syntactic and semantic behavior and defines semantic representations describing relations between arguments. We find that Verb-Net classes provide an effective level of abstraction, improving generalization on lowfrequency predicates by allowing them to learn from the training examples of other predicates belonging to the same class. We also find that joint training of VerbNet role labeling and predicate disambiguation of VerbNet classes for polysemous verbs leads to improvements in both tasks, naturally supporting the extraction of VerbNet's semantic representations.",https://aclanthology.org/2021.iwcs-1.6,Association for Computational Linguistics,2021,June,Proceedings of the 14th International Conference on Computational Semantics (IWCS),"Gung, James  and
Palmer, Martha",Predicate Representations and Polysemy in VerbNet Semantic Parsing,,iwcs,1105
2022.dadc-1.7,"['Multilingual NLP', 'Low-resource Languages']",,,"Logical approaches to representing language have developed and evaluated computational models of quantifier words since the 19th century, but today's NLU models still struggle to capture their semantics. We rely on Generalized Quantifier Theory for language-independent representations of the semantics of quantifier words, to quantify their contribution to the errors of NLU models. We find that quantifiers are pervasive in NLU benchmarks, and their occurrence at test time is associated with performance drops. Multilingual models also exhibit unsatisfying quantifier reasoning abilities, but not necessarily worse for non-English languages. To facilitate directly-targeted probing, we present an adversarial generalized quantifier NLI task GQNLI and show that pre-trained language models have a clear lack of robustness in generalized quantifier reasoning.",https://aclanthology.org/2022.dadc-1.7,Association for Computational Linguistics,2022,July,Proceedings of the First Workshop on Dynamic Adversarial Data Collection,"Cui, Ruixiang  and
Hershcovich, Daniel  and
S{\o}gaard, Anders",Generalized Quantifiers as a Source of Error in Multilingual NLU Benchmarks,10.18653/v1/2022.dadc-1.7,dadc,451
2020.aespen-1.9,"['Domain-specific NLP', 'Information Extraction', 'Data Management and Generation', 'Learning Paradigms', 'Low-resource Languages', 'Model Architectures']","['NLP for News and Media', 'Supervised Learning', 'Data Preparation', 'Event Extraction']",,"This article introduces Hadath, a supervised protocol for coding event data from text written in Arabic. Hadath contributes to recent efforts in advancing multi-language event coding using computer-based solutions. In this application, we focus on extracting event data about the conflict in Afghanistan from 2008 to 2018 using Arabic information sources. The implementation relies first on a Machine Learning algorithm to classify news stories relevant to the Afghan conflict. Then, using Hadath, we implement the Natural Language Processing component for event coding from Arabic script. The output database contains daily geo-referenced information at the district level on who did what to whom, when and where in the Afghan conflict. The data helps to identify trends in the dynamics of violence, the provision of governance, and traditional conflict resolution in Afghanistan for different actors over time and across space.",https://aclanthology.org/2020.aespen-1.9,European Language Resources Association (ELRA),2020,May,Proceedings of the Workshop on Automated Extraction of Socio-political Events from News 2020,"Osorio, Javier  and
Reyes, Alejandro  and
Beltr{\'a}n, Alejandro  and
Ahmadzai, Atal",Supervised Event Coding from Text Written in Arabic: Introducing Hadath,,aespen,605
2020.osact-1.12,"['Domain-specific NLP', 'Model Architectures', 'Classification Applications', 'Low-resource Languages']","['NLP for News and Media', 'Hate and Offensive Speech Detection']",['NLP for Social Media'],"The abusive content on Arabic social media such as hate speech, sexism, racism has become pervasive, and it has a lot of negative psychological effects on users. In this paper, we introduce our work aiming to detect Arabic offensive language and hate speech. We present our two deep neural networks Convolutional Neural Network CNN and Bidirectional Gated Recurrent Unit Bi-GRU used to tackle this problem. These models have been further augmented with attention layers. In addition, we have tested various pre-processing and oversampling techniques to increase the performance of our models. Several machine learning algorithms with different features have been also tested. Our bidirectional GRU model augmented with attention layer has achieved the highest results among our proposed models on a labeled dataset of Arabic tweets, where we achieved 0.859 F1 score for the task of offensive language detection, and 0.75 F1 score for the task of hate speech detection.",https://aclanthology.org/2020.osact-1.12,European Language Resource Association,2020,May,"Proceedings of the 4th Workshop on Open-Source Arabic Corpora and Processing Tools, with a Shared Task on Offensive Language Detection","Haddad, Bushr  and
Orabe, Zoher  and
Al-Abood, Anas  and
Ghneim, Nada",Arabic Offensive Language Detection with Attention-based Deep Neural Networks,,osact,53
2021.crac-1.12,"['Data Management and Generation', 'Information Extraction', 'Learning Paradigms', 'Adversarial Attacks and Robustness']","['Data Augmentation', 'Coreference Resolution']",,"While coreference resolution is defined independently of dataset domain, most models for performing coreference resolution do not transfer well to unseen domains. We consolidate a set of 8 coreference resolution datasets targeting different domains to evaluate the offthe-shelf performance of models. We then mix three datasets for training; even though their domain, annotation guidelines, and metadata differ, we propose a method for jointly training a single model on this heterogeneous data mixture by using data augmentation to account for annotation differences and sampling to balance the data quantities. We find that in a zeroshot setting, models trained on a single dataset transfer poorly while joint training yields improved overall performance, leading to better generalization in coreference resolution models. This work contributes a new benchmark for robust coreference resolution and multiple new state-of-the-art results. 1",https://aclanthology.org/2021.crac-1.12,Association for Computational Linguistics,2021,November,"Proceedings of the Fourth Workshop on Computational Models of Reference, Anaphora and Coreference","Toshniwal, Shubham  and
Xia, Patrick  and
Wiseman, Sam  and
Livescu, Karen  and
Gimpel, Kevin",On Generalization in Coreference Resolution,10.18653/v1/2021.crac-1.12,crac,45
2020.ngt-1.3,"['Learning Paradigms', 'Model Architectures', 'Machine Translation (MT)']","['Neural MT (NMT)', 'Supervised Learning', 'Transformer Models']",,"We propose a novel procedure for training multiple Transformers with tied parameters which compresses multiple models into one enabling the dynamic choice of the number of encoder and decoder layers during decoding. In training an encoder-decoder model, typically, the output of the last layer of the N -layer encoder is fed to the M -layer decoder, and the output of the last decoder layer is used to compute loss. Instead, our method computes a single loss consisting of N  M losses, where each loss is computed from the output of one of the M decoder layers connected to one of the N encoder layers. Such a model subsumes N  M models with different number of encoder and decoder layers, and can be used for decoding with fewer than the maximum number of encoder and decoder layers. Given our flexible tied model, we also address to a-priori selection of the number of encoder and decoder layers for faster decoding, and explore recurrent stacking of layers and knowledge distillation for model compression. We present a cost-benefit analysis of applying the proposed approaches for neural machine translation and show that they reduce decoding costs while preserving translation quality.",https://aclanthology.org/2020.ngt-1.3,Association for Computational Linguistics,2020,July,Proceedings of the Fourth Workshop on Neural Generation and Translation,"Dabre, Raj  and
Rubino, Raphael  and
Fujita, Atsushi",Balancing Cost and Benefit with Tied-Multi Transformers,10.18653/v1/2020.ngt-1.3,ngt,1370
2021.acl-long.407,"['Parsing', 'Knowledge Representation and Reasoning', 'Model Architectures']","['Semantic Parsing', 'Graph Neural Networks (GNNs)']",['Semantic Role Labeling'],"Frame Identification FI is a fundamental and challenging task in frame semantic parsing. The task aims to find the exact frame evoked by a target word in a given sentence. It is generally regarded as a classification task in existing work, where frames are treated as discrete labels or represented using one-hot embeddings. However, the valuable knowledge about frames is neglected. In this paper, we propose a Knowledge-Guided Frame Identification framework KGFI that integrates three types frame knowledge, including frame definitions, frame elements and frameto frame relations, to learn better frame representation, which guides the KGFI to jointly map target words and frames into the same embedding space and subsequently identify the best frame by calculating the dot-product similarity scores between the target word embedding and all of the frame embeddings. The extensive experimental results demonstrate KG-FI significantly outperforms the state-of-theart methods on two benchmark datasets.",https://aclanthology.org/2021.acl-long.407,Association for Computational Linguistics,2021,August,Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),"Su, Xuefeng  and
Li, Ru  and
Li, Xiaoli  and
Pan, Jeff Z.  and
Zhang, Hu  and
Chai, Qinghua  and
Han, Xiaoqi",A Knowledge-Guided Framework for Frame Identification,10.18653/v1/2021.acl-long.407,acl,367
2020.sustainlp-1.3,"['Domain-specific NLP', 'Information Extraction', 'Model Architectures', 'Learning Paradigms']","['Relation Extraction', 'Supervised Learning', 'Medical and Clinical NLP', 'Unsupervised Learning']",['Biomedical NLP'],"Extracting biomedical relations from large corpora of scientific documents is a challenging natural language processing task. Existing approaches usually focus on identifying a relation either in a single sentence mention-level or across an entire corpus pair-level. In both cases, recent methods have achieved strong results by learning a point estimate to represent the relation; this is then used as the input to a relation classifier. However, the relation expressed in text between a pair of biomedical entities is often more complex than can be captured by a point estimate. To address this issue, we propose a latent variable model with an arbitrarily flexible distribution to represent the relation between an entity pair. Additionally, our model provides a unified architecture for both mention-level and pair-level relation extraction. We demonstrate that our model achieves results competitive with strong baselines for both tasks while having fewer parameters and being significantly faster to train. We make our code publicly available.",https://aclanthology.org/2020.sustainlp-1.3,Association for Computational Linguistics,2020,November,Proceedings of SustaiNLP: Workshop on Simple and Efficient Natural Language Processing,"Shah, Harshil  and
Fauqueur, Julien",Learning Informative Representations of Biomedical Relations with Latent Variable Models,10.18653/v1/2020.sustainlp-1.3,sustainlp,450
S18-1028,"['Biases in NLP', 'Domain-specific NLP', 'Classification Applications', 'Model Architectures']","['Sentiment Analysis (SA)', 'NLP for News and Media']",['NLP for Social Media'],"Traditional sentiment analysis approaches mainly focus on classifying the sentiment polarities or emotion categories of texts. However, they can't exploit the sentiment intensity information. Therefore, the SemEval-2018 Task 1 is aimed to automatically determine the intensity of emotions or sentiment of tweets to mine fine-grained sentiment information. In order to address this task, we propose a system based on an attention CNN-LSTM model. In our model, LSTM is used to extract the long-term contextual information from texts. We apply attention techniques to selecting this information. A CNN layer with different kernel sizes is used to extract local features. The dense layers take the pooled CNN feature maps and predict the intensity scores. Our system achieves an average Pearson correlation score of 0.722 ranked 12/48 in the emotion intensity regression task, and 0.810 in the valence regression task ranked 15/38. It indicates that our system can be further extended.",https://aclanthology.org/S18-1028,Association for Computational Linguistics,2018,June,Proceedings of The 12th International Workshop on Semantic Evaluation,"Wu, Chuhan  and
Wu, Fangzhao  and
Liu, Junxin  and
Yuan, Zhigang  and
Wu, Sixing  and
Huang, Yongfeng",THU\_NGN at SemEval-2018 Task 1: Fine-grained Tweet Sentiment Intensity Analysis with Attention CNN-LSTM,10.18653/v1/S18-1028,S18,639
D18-1487,"['Biases in NLP', 'Classification Applications', 'Model Architectures']",,,"Prevalence estimation is the task of inferring the relative frequency of classes of unlabeled examples in a group-for example, the proportion of a document collection with positive sentiment. Previous work has focused on aggregating and adjusting discriminative individual classifiers to obtain prevalence point estimates. But imperfect classifier accuracy ought to be reflected in uncertainty over the predicted prevalence for scientifically valid inference. In this work, we present 1 a generative probabilistic modeling approach to prevalence estimation, and 2 the construction and evaluation of prevalence confidence intervals; in particular, we demonstrate that an off-theshelf discriminative classifier can be given a generative re-interpretation, by backing out an implicit individual-level likelihood function, which can be used to conduct fast and simple group-level Bayesian inference. Empirically, we demonstrate our approach provides better confidence interval coverage than an alternative, and is dramatically more robust to shifts in the class prior between training and testing. 1",https://aclanthology.org/D18-1487,Association for Computational Linguistics,2018,October-November,Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing,"Keith, Katherine  and
O{'}Connor, Brendan",Uncertainty-aware generative models for inferring document class prevalence,10.18653/v1/D18-1487,D18,1158
2020.lrec-1.647,"['Parsing', 'Low-resource Languages', 'Data Management and Generation']","['Syntactic Parsing', 'Data Preparation']",['Dependency Parsing'],"We present AETHEL, a semantic compositionality dataset for written Dutch. AETHEL consists of two parts. First, it contains a lexicon of supertags for about 900 000 words in context. The supertags correspond to types of the simply typed linear lambda-calculus, enhanced with dependency decorations that capture grammatical roles supplementary to function-argument structures. On the basis of these types, AETHEL further provides 72 192 validated derivations, presented in four equivalent formats: natural-deduction and sequent-style proofs, linear logic proofnets and the associated programs lambda terms for meaning composition. AETHEL's types and derivations are obtained by means of an extraction algorithm applied to the syntactic analyses of Lassy Small, the gold standard corpus of written Dutch. We discuss the extraction algorithm and show how 'virtual elements' in the original Lassy annotation of unbounded dependencies and coordination phenomena give rise to higher-order types. We suggest some example usecases highlighting the benefits of a type-driven approach at the syntax semantics interface. The following resources are open-sourced with AETHEL: the lexical mappings between words and types, a subset of the dataset consisting of 7 924 semantic parses, and the Python code that implements the extraction algorithm.",https://aclanthology.org/2020.lrec-1.647,European Language Resources Association,2020,May,Proceedings of the 12th Language Resources and Evaluation Conference,"Kogkalidis, Konstantinos  and
Moortgat, Michael  and
Moot, Richard",\AETHEL: Automatically Extracted Typelogical Derivations for Dutch,,lrec,382
2021.hcinlp-1.5,['Evaluation Techniques'],,,"HCI and NLP traditionally focus on different evaluation methods. While HCI involves a small number of people directly and deeply, NLP traditionally relies on standardized benchmark evaluations that involve a larger number of people indirectly. We present five methodological proposals at the intersection of HCI and NLP and situate them in the context of ML-based NLP models. Our goal is to foster interdisciplinary collaboration and progress in both fields by emphasizing what the fields can learn from each other.",https://aclanthology.org/2021.hcinlp-1.5,Association for Computational Linguistics,2021,April,Proceedings of the First Workshop on Bridging Human{--}Computer Interaction and Natural Language Processing,"Heuer, Hendrik  and
Buschek, Daniel",Methods for the Design and Evaluation of HCI+NLP Systems,10.48550/arxiv.2102.13461,hcinlp,721
2020.eval4nlp-1.13,"['Evaluation Techniques', 'Model Architectures']",['Transformer Models'],,"Current evaluation metrics for language modeling and generation rely heavily on the accuracy of predicted or generated words as compared to a reference ground truth. While important, token-level accuracy only captures one aspect of a language model's behavior, and ignores linguistic properties of words that may allow some mis-predicted tokens to be useful in practice. Furthermore, statistics directly tied to prediction accuracy including perplexity may be confounded by the Zipfian nature of written language, as the majority of the prediction attempts will occur with frequently-occurring types. A model's performance may vary greatly between high-and low-frequency words, which in practice could lead to failure modes such as repetitive and dull generated text being produced by a downstream consumer of a language model. To address this, we propose two new intrinsic evaluation measures within the framework of a simple word prediction task that are designed to give a more holistic picture of a language model's performance. We evaluate several commonly-used large English language models using our proposed metrics, and demonstrate that our approach reveals functional differences in performance between the models that are obscured by more traditional metrics.",https://aclanthology.org/2020.eval4nlp-1.13,Association for Computational Linguistics,2020,November,Proceedings of the First Workshop on Evaluation and Comparison of NLP Systems,"Dudy, Shiran  and
Bedrick, Steven",Are Some Words Worth More than Others?,10.18653/v1/2020.eval4nlp-1.13,eval4nlp,61
C16-1120,"['Parsing', 'Learning Paradigms', 'Low-resource Languages', 'Classification Applications', 'Model Architectures']","['Semantic Parsing', 'Recurrent Neural Networks (RNNs)']","['Semantic Role Labeling', 'Long Short-Term Memory (LSTM) Models']","This paper describes a unified neural architecture for identifying and classifying multi-typed semantic relations between words in a sentence. We investigate two typical and well-studied tasks: semantic role labeling SRL which identifies the relations between predicates and arguments, and relation classification RC which focuses on the relation between two entities or nominals. While mostly studied separately in prior work, we show that the two tasks can be effectively connected and modeled using a general architecture. Experiments on CoNLL-2009 benchmark datasets show that our SRL models significantly outperform state-of-the-art approaches. Our RC models also yield competitive performance with the best published records. Furthermore, we show that the two tasks can be trained jointly with multi-task learning, resulting in additive significant improvements for SRL.",https://aclanthology.org/C16-1120,The COLING 2016 Organizing Committee,2016,December,"Proceedings of {COLING} 2016, the 26th International Conference on Computational Linguistics: Technical Papers","Guo, Jiang  and
Che, Wanxiang  and
Wang, Haifeng  and
Liu, Ting  and
Xu, Jun",A Unified Architecture for Semantic Role Labeling and Relation Classification,,C16,365
2022.insights-1.9,"['Multilingual NLP', 'Learning Paradigms', 'Low-resource Languages', 'Cross-lingual Application']","['Few-shot Learning', 'Transfer Learning']",,"We aim to learn language models for Creole languages for which large volumes of data are not readily available, and therefore explore the potential transfer from ancestor languages the 'Ancestry Transfer Hypothesis'. We find that standard transfer methods do not facilitate ancestry transfer. Surprisingly, different from other non-Creole languages, a very distinct twophase pattern emerges for Creoles: As our training losses plateau, and language models begin to overfit on their source languages, perplexity on the Creoles drop. We explore if this compression phase can lead to practically useful language models the 'Ancestry Bottleneck Hypothesis', but also falsify this. Moreover, we show that Creoles even exhibit this two-phase pattern even when training on random, unrelated languages. Thus Creoles seem to be typological outliers and we speculate whether there is a link between the two observations.",https://aclanthology.org/2022.insights-1.9,Association for Computational Linguistics,2022,May,Proceedings of the Third Workshop on Insights from Negative Results in NLP,"Lent, Heather  and
Bugliarello, Emanuele  and
S{\o}gaard, Anders",Ancestor-to-Creole Transfer is Not a Walk in the Park,10.18653/v1/2022.insights-1.9,insights,1283
2020.acl-main.385,"['Model Architectures', 'Embeddings']",['Transformer Models'],,"In the Transformer model, ""self-attention"" combines information from attended embeddings into the representation of the focal embedding in the next layer. Thus, across layers of the Transformer, information originating from different tokens gets increasingly mixed. This makes attention weights unreliable as explanations probes. In this paper, we consider the problem of quantifying this flow of information through self-attention. We propose two methods for approximating the attention to input tokens given attention weights, attention rollout and attention flow, as post hoc methods when we use attention weights as the relative relevance of the input tokens. We show that these methods give complementary views on the flow of information, and compared to raw attention, both yield higher correlations with importance scores of input tokens obtained using an ablation method and input gradients.",https://aclanthology.org/2020.acl-main.385,Association for Computational Linguistics,2020,July,Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,"Abnar, Samira  and
Zuidema, Willem",Quantifying Attention Flow in Transformers,10.18653/v1/2020.acl-main.385,acl,286
Q18-1006,"['Information Extraction', 'Classification Applications', 'Model Architectures']",['Temporal Event Understanding'],,"Extracting the information from text when an event happened is challenging. Documents do not only report on current events, but also on past events as well as on future events. Often, the relevant time information for an event is scattered across the document. In this paper we present a novel method to automatically anchor events in time. To our knowledge it is the first approach that takes temporal information from the complete document into account. We created a decision tree that applies neural network based classifiers at its nodes. We use this tree to incrementally infer, in a stepwise manner, at which time frame an event happened. We evaluate the approach on the TimeBank-EventTime Corpus Reimers et al., 2016 achieving an accuracy of 42.0% compared to an inter-annotator agreement IAA of 56.7%. For events that span over a single day we observe an accuracy improvement of 33.1 points compared to the state-of-the-art CAEVO system Chambers et al., 2014. Without retraining, we apply this model to the SemEval-2015 Task 4 on automatic timeline generation and achieve an improvement of 4.01 points F 1 -score compared to the state-of-the-art. Our code is publically available. 1 * During author's internship in the research training group AIPHES at UKP Lab, TU Darmstadt. 1 https://github.com/ukplab/ tacl2017-event-time-extraction",https://aclanthology.org/Q18-1006,MIT Press,2018,,,"Reimers, Nils  and
Dehghani, Nazanin  and
Gurevych, Iryna",Event Time Extraction with a Decision Tree of Neural Classifiers,10.1162/tacl_a_00006,Q18,463
2022.acl-short.28,"['Text Generation', 'Data Management and Generation', 'Model Architectures', 'Domain-specific NLP']","['Data Preparation', 'Text Simplification', 'Recurrent Neural Networks (RNNs)', 'NLP for News and Media']",['Long Short-Term Memory (LSTM) Models'],"Document-level text simplification often deletes some sentences besides performing lexical, grammatical or structural simplification to reduce text complexity. In this work, we focus on sentence deletions for text simplification and use a news genre-specific functional discourse structure, which categorizes sentences based on their contents and their function roles in telling a news story, for predicting sentence deletion. We incorporate sentence categories into a neural net model in two ways for predicting sentence deletions, either as additional features or by jointly predicting sentence deletions and sentence categories. Experimental results using human-annotated data show that incorporating the functional structure improves the recall of sentence deletion prediction by 6.5% and 10.7% respectively using the two methods, and improves the overall F1-score by 3.6% and 4.3% respectively.",https://aclanthology.org/2022.acl-short.28,Association for Computational Linguistics,2022,May,Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),"Zhang, Bohan  and
Choubey, Prafulla Kumar  and
Huang, Ruihong",Predicting Sentence Deletions for Text Simplification Using a Functional Discourse Structure,10.18653/v1/2022.acl-short.28,acl,911
2020.scil-1.39,"['Evaluation Techniques', 'Model Architectures']",,,"An emerging line of work uses psycholinguistic methods to evaluate the syntactic generalizations acquired by neural language models NLMs. While this approach has shown NLMs to be capable of learning a wide range of linguistic knowledge, confounds in the design of previous experiments may have obscured the potential of NLMs to learn certain grammatical phenomena. Here we re-evaluate the performance of a range of NLMs on reflexive anaphor licensing. Under our paradigm, the models consistently show stronger evidence of learning than reported in previous work. Our approach demonstrates the value of well-controlled psycholinguistic methods in gaining a fine-grained understanding of NLM learning potential. 1",https://aclanthology.org/2020.scil-1.39,Association for Computational Linguistics,2020,January,Proceedings of the Society for Computation in Linguistics 2020,"Hu, Jennifer  and
Chen, Sherry Yong  and
Levy, Roger",A closer look at the performance of neural language models on reflexive anaphor licensing,10.7275/67qw-mf84,scil,1265
P18-1062,"['Learning Paradigms', 'Automatic Text Summarization']","['Abstractive Text Summarization', 'Unsupervised Learning']",,"We introduce a novel graph-based framework for abstractive meeting speech summarization that is fully unsupervised and does not rely on any annotations. Our work combines the strengths of multiple recent approaches while addressing their weaknesses. Moreover, we leverage recent advances in word embeddings and graph degeneracy applied to NLP to take exterior semantic knowledge into account, and to design custom diversity and informativeness measures. Experiments on the AMI and ICSI corpus show that our system improves on the state-of-the-art. Code and data are publicly available 1 , and our system can be interactively tested 2 . * Work done as part of 3 rd year project, with equal contribution.",https://aclanthology.org/P18-1062,Association for Computational Linguistics,2018,July,Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),"Shang, Guokan  and
Ding, Wensi  and
Zhang, Zekun  and
Tixier, Antoine  and
Meladianos, Polykarpos  and
Vazirgiannis, Michalis  and
Lorr{\'e}, Jean-Pierre",Unsupervised Abstractive Meeting Summarization with Multi-Sentence Compression and Budgeted Submodular Maximization,10.18653/v1/P18-1062,P18,175
W18-5302,"['Question Answering (QA)', 'Parsing', 'Evaluation Techniques', 'Domain-specific NLP']","['Semantic Parsing', 'Medical and Clinical NLP']","['Semantic Role Labeling', 'Biomedical NLP']","Question answering QA systems usually rely on advanced natural language processing components to precisely understand the questions and extract the answers. Semantic role labeling SRL is known to boost performance for QA, but its use for biomedical texts has not yet been fully studied. We analyzed the performance of three SRL tools BioKIT, BIOSMILE and PathLSTM on 1776 questions from the BioASQ challenge. We compared the systems regarding the coverage of the questions and snippets, as well as based on pre-defined criteria, such as easiness of installation, supported formats and usability. Finally, we integrated two of the tools in a simple QA system to further evaluate their performance over the official BioASQ test sets.",https://aclanthology.org/W18-5302,Association for Computational Linguistics,2018,November,Proceedings of the 6th {B}io{ASQ} Workshop A challenge on large-scale biomedical semantic indexing and question answering,"Eckert, Fabian  and
Neves, Mariana",Semantic role labeling tools for biomedical question answering: a study of selected tools on the BioASQ datasets,10.18653/v1/W18-5302,W18,585
2020.emnlp-main.165,"['Domain-specific NLP', 'Information Extraction', 'Learning Paradigms', 'Ethics', 'Model Architectures']","['Relation Extraction', 'Supervised Learning', 'Medical and Clinical NLP', 'Transformer Models']",['Biomedical NLP'],"Unlike other domains, medical texts are inevitably accompanied by private information, so sharing or copying these texts is strictly restricted. However, training a medical relation extraction model requires collecting these privacy-sensitive texts and storing them on one machine, which comes in conflict with privacy protection. In this paper, we propose a privacypreserving medical relation extraction model based on federated learning, which enables training a central model with no single piece of private local data being shared or exchanged. Though federated learning has distinct advantages in privacy protection, it suffers from the communication bottleneck, which is mainly caused by the need to upload cumbersome local parameters. To overcome this bottleneck, we leverage a strategy based on knowledge distillation. Such a strategy uses the uploaded predictions of ensemble local models to train the central model without requiring uploading local parameters. Experiments on three publicly available medical relation extraction datasets demonstrate the effectiveness of our method.",https://aclanthology.org/2020.emnlp-main.165,Association for Computational Linguistics,2020,November,Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),"Sui, Dianbo  and
Chen, Yubo  and
Zhao, Jun  and
Jia, Yantao  and
Xie, Yuantao  and
Sun, Weijian",FedED: Federated Learning via Ensemble Distillation for Medical Relation Extraction,10.18653/v1/2020.emnlp-main.165,emnlp,5
2021.latechclfl-1.3,"['Domain-specific NLP', 'Data Management and Generation', 'Multilingual NLP', 'Information Extraction', 'Learning Paradigms', 'Low-resource Languages', 'Model Architectures']","['Data Preparation', 'Named Entity Recognition (NER)', 'Supervised Learning']",['Annotation Processes'],"Pretrained language models like BERT have advanced the state of the art for many NLP tasks. For resource-rich languages, one has the choice between a number of language-specific models, while multilingual models are also worth considering. These models are well known for their crosslingual performance, but have also shown competitive in-language performance on some tasks. We consider monolingual and multilingual models from the perspective of historical texts, and in particular for texts enriched with editorial notes: how do language models deal with the historical and editorial content in these texts? We present a new Named Entity Recognition dataset for Dutch based on 17th and 18th century United East India Company VOC reports extended with modern editorial notes. Our experiments with multilingual and Dutch pretrained language models confirm the crosslingual abilities of multilingual models while showing that all language models can leverage mixed-variant data. In particular, language models successfully incorporate notes for the prediction of entities in historical texts. We also find that multilingual models outperform monolingual models on our data, but that this superiority is linked to the task at hand: multilingual models lose their advantage when confronted with more semantical tasks.",https://aclanthology.org/2021.latechclfl-1.3,Association for Computational Linguistics,2021,November,"Proceedings of the 5th Joint SIGHUM Workshop on Computational Linguistics for Cultural Heritage, Social Sciences, Humanities and Literature","Arnoult, Sophie I.  and
Petram, Lodewijk  and
Vossen, Piek",Batavia asked for advice. Pretrained language models for Named Entity Recognition in historical texts.,10.18653/v1/2021.latechclfl-1.3,latechclfl,1205
2020.challengehml-1.1,"['Model Architectures', 'Classification Applications', 'Learning Paradigms']","['Transformer Models', 'Sentiment Analysis (SA)', 'Multimodal Learning', 'Multilabel Text Classification', 'Emotion Detection']",,"Understanding expressed sentiment and emotions are two crucial factors in human multimodal language. This paper describes a Transformer-based joint-encoding TBJE for the task of Emotion Recognition and Sentiment Analysis. In addition to use the Transformer architecture, our approach relies on a modular co-attention and a glimpse layer to jointly encode one or more modalities. The proposed solution has also been submitted to the ACL20: Second Grand-Challenge on Multimodal Language to be evaluated on the CMU-MOSEI dataset. The code to replicate the presented experiments is open-source 1 .",https://aclanthology.org/2020.challengehml-1.1,Association for Computational Linguistics,2020,July,Second Grand-Challenge and Workshop on Multimodal Language (Challenge-HML),"Delbrouck, Jean-Benoit  and
Tits, No{\'e}  and
Brousmiche, Mathilde  and
Dupont, St{\'e}phane",A Transformer-based joint-encoding for Emotion Recognition and Sentiment Analysis,10.18653/v1/2020.challengehml-1.1,challengehml,594
2021.privatenlp-1.5,"['Ethics', 'Data Management and Generation', 'Information Extraction', 'Learning Paradigms', 'Model Architectures']","['Transformer Models', 'Data Preparation', 'Named Entity Recognition (NER)']",['Annotation Processes'],"We curated WikiPII, an automatically labeled dataset composed of Wikipedia biography pages, annotated for personal information extraction. Although automatic annotation can lead to a high degree of label noise, it is an inexpensive process and can generate large volumes of annotated documents. We trained a BERT-based NER model with WikiPII and showed that with an adequately large training dataset, the model can significantly decrease the cost of manual information extraction, despite the high level of label noise. In a similar approach, organizations can leverage text mining techniques to create customized annotated datasets from their historical data without sharing the raw data for human annotation. Also, we explore collaborative training of NER models through federated learning when the annotation is noisy. Our results suggest that depending on the level of trust to the ML operator and the volume of the available data, distributed training can be an effective way of training a personal information identifier in a privacy-preserved manner. Research material is available at https://github.com/ ratmcu/wikipiifed.",https://aclanthology.org/2021.privatenlp-1.5,Association for Computational Linguistics,2021,June,Proceedings of the Third Workshop on Privacy in Natural Language Processing,"Hathurusinghe, Rajitha  and
Nejadgholi, Isar  and
Bolic, Miodrag",A Privacy-Preserving Approach to Extraction of Personal Information through Automatic Annotation and Federated Learning,10.18653/v1/2021.privatenlp-1.5,privatenlp,466
2020.sltu-1.16,"['Data Management and Generation', 'Audio Generation and Processing', 'Low-resource Languages']",,,"This paper focuses on the technical improvement of Elpis, a language technology which assists people in the process of transcription, particularly for low-resource language documentation situations. To provide better support for the diversity of file formats encountered by people working to document the world's languages, a Data Transformer interface has been developed to abstract the complexities of designing individual data import scripts. This work took place as part of a larger project of code quality improvement and the publication of template code that can be used for development of other language technologies.",https://aclanthology.org/2020.sltu-1.16,European Language Resources association,2020,May,Proceedings of the 1st Joint Workshop on Spoken Language Technologies for Under-resourced languages (SLTU) and Collaboration and Computing for Under-Resourced Languages (CCURL),"Buckeridge, Nicholas  and
Foley, Ben",Scaling Language Data Import/Export with a Data Transformer Interface,,sltu,984
2020.lrec-1.34,"['Data Management and Generation', 'Knowledge Representation and Reasoning']",['Data Preparation'],,"We introduce in this paper a generic approach to combine implicit crowdsourcing and language learning in order to mass-produce language resources LRs for any language for which a crowd of language learners can be involved. We present the approach by explaining its core paradigm that consists in pairing specific types of LRs with specific exercises, by detailing both its strengths and challenges, and by discussing how much these challenges have been addressed at present. Accordingly, we also report on on-going proof-of-concept efforts aiming at developing the first prototypical implementation of the approach in order to correct and extend an LR called ConceptNet based on the input crowdsourced from language learners. We then present an international network called the European Network for Combining Language Learning with Crowdsourcing Techniques enetCollect that provides the context to accelerate the implementation of the generic approach. Finally, we exemplify how it can be used in several language learning scenarios to produce a multitude of NLP resources and how it can therefore alleviate the long-standing NLP issue of the lack of LRs.",https://aclanthology.org/2020.lrec-1.34,European Language Resources Association,2020,May,Proceedings of the 12th Language Resources and Evaluation Conference,"Nicolas, Lionel  and
Lyding, Verena  and
Borg, Claudia  and
Forascu, Corina  and
Fort, Kar{\""e}n  and
Zdravkova, Katerina  and
Kosem, Iztok  and
{\v{C}}ibej, Jaka  and
Arhar Holdt, {\v{S}}pela  and
Millour, Alice  and
K{\""o}nig, Alexander  and
Rodosthenous, Christos  and
Sangati, Federico  and
ul Hassan, Umair  and
Katinskaia, Anisia  and
Barreiro, Anabela  and
Aparaschivei, Lavinia  and
HaCohen-Kerner, Yaakov",Creating Expert Knowledge by Relying on Language Learners: a Generic Approach for Mass-Producing Language Resources by Combining Implicit Crowdsourcing and Language Learning,,lrec,1369
2021.mtsummit-at4ssl.2,"['Ethics', 'Low-resource Languages']",,,"This paper identifies some common and specific pitfalls in the development of sign language technologies targeted at deaf communities, with a specific focus on signing avatars. It makes the call to urgently interrogate some of the ideologies behind those technologies, including issues of ethical and responsible development. The paper addresses four separate and interlinked issues: ideologies about deaf people and mediated communication, bias in data sets and learning, user feedback, and applications of the technologies. The paper ends with several take away points for both technology developers and deaf NGOs. Technology developers should give more consideration to diversifying their team and working interdisciplinary, and be mindful of the biases that inevitably creep into data sets. There should also be a consideration of the technologies' end users. Sign language interpreters are not the end users nor should they be seen as the benchmark for language use. Technology developers and deaf NGOs can engage in a dialogue about how to prioritize application domains and prioritize within application domains. Finally, deaf NGOs policy statements will need to take a longer view, and use avatars to think of a significantly better system compared to what sign language interpreting services can provide. 1 The writing of this paper has benefited from ongoing discussions on this theme as part of the EU COST Action network 'Language in the Human Machine Era' https://lithme.eu/",https://aclanthology.org/2021.mtsummit-at4ssl.2,Association for Machine Translation in the Americas,2021,August,Proceedings of the 1st International Workshop on Automatic Translation for Signed and Spoken Languages (AT4SSL),"De Meulder, Maartje",Is ``good enough'' good enough? Ethical and responsible development of sign language technologies,,mtsummit,360
W18-5501,"['Data Management and Generation', 'Classification Applications', 'Model Architectures']","['Claim Verification', 'Data Preparation']",,"We present the results of the first Fact Extraction and VERification FEVER Shared Task. The task challenged participants to classify whether human-written factoid claims could be SUPPORTED or REFUTED using evidence retrieved from Wikipedia. We received entries from 23 competing teams, 19 of which scored higher than the previously published baseline. The best performing system achieved a FEVER score of 64.21%. In this paper, we present the results of the shared task and a summary of the systems, highlighting commonalities and innovations among participating systems.",https://aclanthology.org/W18-5501,Association for Computational Linguistics,2018,November,Proceedings of the First Workshop on Fact Extraction and {VER}ification ({FEVER}),"Thorne, James  and
Vlachos, Andreas  and
Cocarascu, Oana  and
Christodoulopoulos, Christos  and
Mittal, Arpit",The Fact Extraction and VERification FEVER Shared Task,10.18653/v1/W18-5501,W18,1484
W17-2208,"['Information Extraction', 'Data Management and Generation', 'Low-resource Languages', 'Text Preprocessing']","['Text Segmentation', 'Data Preparation', 'Named Entity Recognition (NER)']","['Sentence Segmentation', 'Annotation Processes']","This paper presents an approach to extract co-occurrence networks from literary texts. It is a deliberate decision not to aim for a fully automatic pipeline, as the literary research questions need to guide both the definition of the nature of the things that co-occur as well as how to decide cooccurrence. We showcase the approach on a Middle High German romance, Parzival. Manual inspection and discussion shows the huge impact various choices have.",https://aclanthology.org/W17-2208,Association for Computational Linguistics,2017,August,"Proceedings of the Joint {SIGHUM} Workshop on Computational Linguistics for Cultural Heritage, Social Sciences, Humanities and Literature","Blessing, Andre  and
Echelmeyer, Nora  and
John, Markus  and
Reiter, Nils",An End-to-end Environment for Research Question-Driven Entity Extraction and Network Analysis,10.18653/v1/W17-2208,W17,1248
2022.semeval-1.229,"['Embeddings', 'Information Extraction', 'Learning Paradigms', 'Classification Applications', 'Model Architectures']","['Transformer Models', 'Named Entity Recognition (NER)']",,"This paper describes our system used in the SemEval-2022 Task 11 Multilingual Complex Named Entity Recognition, achieving 3rd for track 1 on the leaderboard. We propose Dictionary-fused BERT, a flexible approach for entity dictionaries integration. The main ideas of our systems are: 1 integrating external knowledge an entity dictionary into pretrained models to obtain contextualized word and entity representations 2 designing a robust loss function leveraging a logit matrix 3 adding an auxiliary task, which is an on-top binary classification to decide whether the token is a mention word or not, makes the main task easier to learn. It is worth noting that our system achieves an F1 of 0.914 in the postevaluation stage by updating the entity dictionary to the one of Meng et al. 2021 , which is higher than the score of 1st on the leaderboard of the evaluation stage.",https://aclanthology.org/2022.semeval-1.229,Association for Computational Linguistics,2022,July,Proceedings of the 16th International Workshop on Semantic Evaluation (SemEval-2022),"Ma, Long  and
Jian, Xiaorong  and
Li, Xuan",PAI at SemEval-2022 Task 11: Name Entity Recognition with Contextualized Entity Representations and Robust Loss Functions,10.18653/v1/2022.semeval-1.229,semeval,1117
K17-1010,"['Domain-specific NLP', 'Data Management and Generation', 'Question Answering (QA)', 'Learning Paradigms', 'Classification Applications']","['Data Preparation', 'Multiple Choice QA (MCQA)', 'Supervised Learning']",['Annotation Processes'],"Question answering QA systems are easily distracted by irrelevant or redundant words in questions, especially when faced with long or multi-sentence questions in difficult domains. This paper introduces and studies the notion of essential question terms with the goal of improving such QA solvers. We illustrate the importance of essential question terms by showing that humans' ability to answer questions drops significantly when essential terms are eliminated from questions. We then develop a classifier that reliably 90% mean average precision identifies and ranks essential terms in questions. Finally, we use the classifier to demonstrate that the notion of question term essentiality allows state-of-the-art QA solvers for elementary-level science questions to make better and more informed decisions, improving performance by up to 5%. We also introduce a new dataset of over 2,200 crowd-sourced essential terms annotated science questions.",https://aclanthology.org/K17-1010,Association for Computational Linguistics,2017,August,Proceedings of the 21st Conference on Computational Natural Language Learning ({C}o{NLL} 2017),"Khashabi, Daniel  and
Khot, Tushar  and
Sabharwal, Ashish  and
Roth, Dan",Learning What is Essential in Questions,10.18653/v1/K17-1010,K17,226
2021.bucc-1.1,"['Machine Translation (MT)', 'Data Management and Generation', 'Learning Paradigms', 'Low-resource Languages', 'Cross-lingual Application']","['Multimodal Learning', 'Data Augmentation']",,"AI now and in future will have to grapple continuously with the problem of low resource. AI will increasingly be ML intensive. But ML needs data often with annotation. However, annotation is costly. Over the years, through work on multiple problems, we have developed insight into how to do language processing in low resource setting. Following 6 methods-individually and in combination-seem to be the way forward: 1. Artificially augment resource e.g. subwords 2. Cooperative NLP e.g., pivot in MT 3. Linguistic embellishment e.g. factor based MT, source reordering 4. Joint Modeling e.g., Coref and NER, Sentiment and Emotion: each task helping the other to either boost accuracy or reduce resource requirement 5. Multimodality e.g., eye tracking based NLP, also picture+text+speech based Sentiment Analysis 6. Cross Lingual Embedding e.g., embedding from multiple languages helping MT, close to 2 above The present talk will focus on low resource machine translation. We describe the use of techniques from the above list and bring home the seriousness and methodology of doing Machine Translation in low resource settings.",https://aclanthology.org/2021.bucc-1.1,INCOMA Ltd.,2021,September,Proceedings of the 14th Workshop on Building and Using Comparable Corpora (BUCC 2021),"Bhattacharyya, Pushpak",Invited Presentation,,bucc,289
2021.crac-1.10,"['Learning Paradigms', 'Information Extraction', 'Cross-lingual Application', 'Low-resource Languages']","['Transfer Learning', 'Coreference Resolution']",,"In this paper, we develop bilingual transfer learning approaches to improve Arabic coreference resolution by leveraging additional English annotation via bilingual or multilingual pre-trained transformers. We show that bilingual transfer learning improves the strong transformer-based neural coreference models by 2-4 F1. We also systemically investigate the effectiveness of several pre-trained transformer models that differ in training corpora, languages covered, and model capacity. Our best model achieves a new stateof-the-art performance of 64.55 F1 on the Arabic OntoNotes dataset. Our code is publicly available at https://github.com/ bnmin/arabic_coref.",https://aclanthology.org/2021.crac-1.10,Association for Computational Linguistics,2021,November,"Proceedings of the Fourth Workshop on Computational Models of Reference, Anaphora and Coreference","Min, Bonan",Exploring Pre-Trained Transformers and Bilingual Transfer Learning for Arabic Coreference Resolution,10.18653/v1/2021.crac-1.10,crac,796
U19-1009,"['Data Management and Generation', 'Classification Applications', 'Domain-specific NLP']","['Data Preparation', 'NLP for News and Media']",,"Politically-contested issues are often discussed with different emphases by different people. This emphasis is called a frame. In this paper, we examine the performance of classifiers trained using the media frames Corpus MFC Card et al., 2015 ; a collection of US news labelled with fifteen different frame categories. Specifically, we compare pre-trained language models XLNet, Bert, and Roberta, fine-tuned using MFC, against results from the literature and simpler models in their ability to predict frames from text. We also test these models on a new corpus that we have derived from Australian parliamentary speeches. Our experimental results first show that the fine-tuned models significantly outperform the current best methods on MFC. We also show that the model fine-tuned on US news articles can be convincingly applied to predict policy frames in Australian parliamentary speeches, though the accuracy is significantly reduced, suggesting potential discrepancy in framing strategies and/or text usage between US News and Australian Parliamentary Speeches.",https://aclanthology.org/U19-1009,Australasian Language Technology Association,2019,4--6 December,Proceedings of the The 17th Annual Workshop of the Australasian Language Technology Association,"Khanehzar, Shima  and
Turpin, Andrew  and
Mikolajczak, Gosia",Modeling Political Framing Across Policy Issues and Contexts,,U19,295
2020.sdp-1.23,"['Domain-specific NLP', 'Information Retrieval', 'Knowledge Representation and Reasoning']","['NLP for Bibliometrics and Scientometrics', 'Knowledge Graphs']",,"To provide AI researchers with modern tools for dealing with the explosive growth of the research literature in their field, we introduce a new platform, AI Research Navigator, that combines classical keyword search with neural retrieval to discover and organize relevant literature. The system provides search at multiple levels of textual granularity, from sentences to aggregations across documents, both in natural language and through navigation in a domain specific Knowledge Graph. We give an overview of the overall architecture of the system and of the components for document analysis, question answering, search, analytics, expert search, and recommendations.",https://aclanthology.org/2020.sdp-1.23,Association for Computational Linguistics,2020,November,Proceedings of the First Workshop on Scholarly Document Processing,"Fadaee, Marzieh  and
Gureenkova, Olga  and
Rejon Barrera, Fernando  and
Schnober, Carsten  and
Weerkamp, Wouter  and
Zavrel, Jakub",A New Neural Search and Insights Platform for Navigating and Organizing AI Research,10.18653/v1/2020.sdp-1.23,sdp,1374
W19-4401,"['Biases in NLP', 'Ethics', 'Domain-specific NLP', 'Data Management and Generation']","['Data Preparation', 'Data Analysis']",,The issues of algorithmic fairness and bias have recently featured prominently in many publications highlighting the fact that training the algorithms for maximum performance may often result in predictions that are biased against various groups. Educational applications based on NLP and speech processing technologies often combine multiple complex machine learning algorithms and are thus vulnerable to the same sources of bias as other machine learning systems. Yet such systems can have high impact on people's lives especially when deployed as part of highstakes tests. In this paper we discuss different definitions of fairness and possible ways to apply them to educational applications. We then use simulated and real data to consider how test-takers' native language backgrounds can affect their automated scores on an English language proficiency assessment. We illustrate that total fairness may not be achievable and that different definitions of fairness may require different solutions.,https://aclanthology.org/W19-4401,Association for Computational Linguistics,2019,August,Proceedings of the Fourteenth Workshop on Innovative Use of NLP for Building Educational Applications,"Loukina, Anastassia  and
Madnani, Nitin  and
Zechner, Klaus",The many dimensions of algorithmic fairness in educational applications,10.18653/v1/W19-4401,W19,88
2020.iwslt-1.34,"['Data Management and Generation', 'Low-resource Languages', 'Machine Translation (MT)']","['Neural MT (NMT)', 'Statistical MT (SMT)', 'Data Preparation', 'Data Analysis']",,"Translationese is a phenomenon present in human translations, simultaneous interpreting, and even machine translations. Some translationese features tend to appear in simultaneous interpreting with higher frequency than in human text translation, but the reasons for this are unclear. This study analyzes translationese patterns in translation, interpreting, and machine translation outputs in order to explore possible reasons. In our analysis we i detail two non-invasive ways of detecting translationese and ii compare translationese across human and machine translations from text and speech. We find that machine translation shows traces of translationese, but does not reproduce the patterns found in human translation, offering support to the hypothesis that such patterns are due to the model human vs. machine rather than to the data written vs. spoken.",https://aclanthology.org/2020.iwslt-1.34,Association for Computational Linguistics,2020,July,Proceedings of the 17th International Conference on Spoken Language Translation,"Bizzoni, Yuri  and
Juzek, Tom S  and
Espa{\~n}a-Bonet, Cristina  and
Dutta Chowdhury, Koel  and
van Genabith, Josef  and
Teich, Elke",How Human is Machine Translationese? Comparing Human and Machine Translations of Text and Speech,10.18653/v1/2020.iwslt-1.34,iwslt,328
2021.naloma-1.7,"['Classification Applications', 'Model Architectures']",,,"We propose a probabilistic account of semantic inference and classification formulated in terms of probabilistic type theory with records, building on Cooper et al. 2014 Cooper et al.  , 2015. We suggest probabilistic type theoretic formulations of Naive Bayes Classifiers and Bayesian Networks. A central element of these constructions is a type-theoretic version of a random variable. We illustrate this account with a simple language game combining probabilistic classification of perceptual input with probabilistic semantic inference.",https://aclanthology.org/2021.naloma-1.7,Association for Computational Linguistics,2021,June,Proceedings of the 1st and 2nd Workshops on Natural Logic Meets Machine Learning (NALOMA),"Larsson, Staffan  and
Cooper, Robin",Bayesian Classification and Inference in a Probabilistic Type Theory with Records,,naloma,166
S18-1176,"['Question Answering (QA)', 'Knowledge Representation and Reasoning', 'Model Architectures']","['Multiple Choice QA (MCQA)', 'Recurrent Neural Networks (RNNs)']",,"In this document we present an end-to-end machine reading comprehension system that solves multiple choice questions with a textual entailment perspective. Since some of the knowledge required is not explicitly mentioned in the text, we try to exploit common sense knowledge by using pretrained word embeddings during contextual embeddings and by dynamically generating a weighted representation of related script knowledge. In the model two kinds of prediction structure are ensembled, and the final accuracy of our system is 10 percent higher than the naiive baseline.",https://aclanthology.org/S18-1176,Association for Computational Linguistics,2018,June,Proceedings of The 12th International Workshop on Semantic Evaluation,"Jiang, Zhengping  and
Sun, Qi",CSReader at SemEval-2018 Task 11: Multiple Choice Question Answering as Textual Entailment,10.18653/v1/S18-1176,S18,783
2020.cmcl-1.8,"['Parsing', 'Data Management and Generation', 'Low-resource Languages']","['Morphological Parsing', 'Data Preparation']",,"Grammatical gender is a consistent and informative cue to the plural class of German nouns. We find that neural encoder-decoder models learn to rely on this cue to predict plural class, but adult speakers are relatively insensitive to it. This suggests that the neural models are not an effective cognitive model of German plural formation.",https://aclanthology.org/2020.cmcl-1.8,Association for Computational Linguistics,2020,November,Proceedings of the Workshop on Cognitive Modeling and Computational Linguistics,"McCurdy, Kate  and
Lopez, Adam  and
Goldwater, Sharon","Conditioning, but on Which Distribution? Grammatical Gender in German Plural Inflection",10.18653/v1/2020.cmcl-1.8,cmcl,938
S17-2164,"['Domain-specific NLP', 'Information Extraction', 'Classification Applications', 'Model Architectures']",['NLP for Bibliometrics and Scientometrics'],,"This paper describes the system presented by the LABDA group at SemEval 2017 Task 10 ScienceIE, specifically for the subtasks of identification and classification of keyphrases from scientific articles. For the task of identification, we use the BANNER tool, a named entity recognition system, which is based on conditional random fields CRF and has obtained successful results in the biomedical domain. To classify keyphrases, we study the UMLS semantic network and propose a possible linking between the keyphrase types and the UMLS semantic groups. Based on this semantic linking, we create a dictionary for each keyphrase type. Then, a feature indicating if a token is found in one of these dictionaries is incorporated to feature set used by the BANNER tool. The final results on the test dataset show that our system still needs to be improved, but the conditional random fields and, consequently, the BAN-NER system can be used as a first approximation to identify and classify keyphrases.",https://aclanthology.org/S17-2164,Association for Computational Linguistics,2017,August,Proceedings of the 11th International Workshop on Semantic Evaluation ({S}em{E}val-2017),"Segura-Bedmar, Isabel  and
Col{\'o}n-Ruiz, Crist{\'o}bal  and
Mart{\'\i}nez, Paloma",LABDA at SemEval-2017 Task 10: Extracting Keyphrases from Scientific Publications by combining the BANNER tool and the UMLS Semantic Network,10.18653/v1/S17-2164,S17,204
Q16-1020,['Embeddings'],['Word Embeddings'],,"Continuous word representations have been remarkably useful across NLP tasks but remain poorly understood. We ground word embeddings in semantic spaces studied in the cognitive-psychometric literature, taking these spaces as the primary objects to recover. To this end, we relate log co-occurrences of words in large corpora to semantic similarity assessments and show that co-occurrences are indeed consistent with an Euclidean semantic space hypothesis. Framing word embedding as metric recovery of a semantic space unifies existing word embedding algorithms, ties them to manifold learning, and demonstrates that existing algorithms are consistent metric recovery methods given co-occurrence counts from random walks. Furthermore, we propose a simple, principled, direct metric recovery algorithm that performs on par with the state-ofthe-art word embedding and manifold learning methods. Finally, we complement recent focus on analogies by constructing two new inductive reasoning datasets-series completion and classification-and demonstrate that word embeddings can be used to solve them as well.",https://aclanthology.org/Q16-1020,MIT Press,2016,,,"Hashimoto, Tatsunori B.  and
Alvarez-Melis, David  and
Jaakkola, Tommi S.",Word Embeddings as Metric Recovery in Semantic Spaces,10.1162/tacl_a_00098,Q16,38
2016.tc-1.12,"['Information Extraction', 'Multilingual NLP', 'Low-resource Languages']",['Named Entity Recognition (NER)'],,"This paper describes the development of a language independent process for identifying proper-names in a text. The process is derived from a machine originally designed to analyse non-concatenative morphologies in natural languages. The particular context for this work is the task of managing the 5,000 or so proper-names found in a Bible, including the identification of close cognates and reporting instances where a related form does not appear to be present. The need for such a system is explained and the process by which the machine is able to identify names in the target text is described. The problems posed by disparate orthographies are noted. Results obtained from Eurasian, South American and African languages are discussed, common problems for the process identified and its possible use in the context of technical vocabulary suggested. Commonalities between the task of identifying morphology templates, ordered phoneme sets and syntax patterns are noted.",https://aclanthology.org/2016.tc-1.12,AsLing,2016,November 17-18,Proceedings of Translating and the Computer 38,"Riding, Jon D.  and
Boulton, Neil J.",What's in a name?,10.1145/2433396.2433457,tc,455
2021.emnlp-main.228,"['Learning Paradigms', 'Information Extraction', 'Model Architectures']","['Unsupervised Learning', 'Graph Neural Networks (GNNs)', 'Relation Extraction']",,"Most recent studies for relation extraction RE leverage the dependency tree of the input sentence to incorporate syntax-driven contextual information to improve model performance, with little attention paid to the limitation where high-quality dependency parsers in most cases unavailable, especially for indomain scenarios. To address this limitation, in this paper, we propose attentive graph convolutional networks A-GCN to improve neural RE methods with an unsupervised manner to build the context graph, without relying on the existence of a dependency parser. Specifically, we construct the graph from n-grams extracted from a lexicon built from pointwise mutual information PMI and apply attention over the graph. Therefore, different word pairs from the contexts within and across n-grams are weighted in the model and facilitate RE accordingly. Experimental results with further analyses on two English benchmark datasets for RE demonstrate the effectiveness of our approach, where state-of-the-art performance is observed on both datasets. 1",https://aclanthology.org/2021.emnlp-main.228,Association for Computational Linguistics,2021,November,Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,"Qin, Han  and
Tian, Yuanhe  and
Song, Yan",Relation Extraction with Word Graphs from N-grams,10.18653/v1/2021.emnlp-main.228,emnlp,920
2020.globalex-1.16,['Knowledge Representation and Reasoning'],,,"This paper describes our contribution to the Third Shared Task on Translation Inference across Dictionaries TIAD-2020. We describe an approach on translation inference based on symbolic methods, the propagation of concepts over a graph of interconnected dictionaries: Given a mapping from source language words to lexical concepts e.g., synsets as a seed, we use bilingual dictionaries to extrapolate a mapping of pivot and target language words to these lexical concepts. Translation inference is then performed by looking up the lexical concepts of a source language word and returning the target language words for which these lexical concepts have the respective highest score. We present two instantiations of this system: One using WordNet synsets as concepts, and one using lexical entries translations as concepts. With a threshold of 0, the latter configuration is the second among participant systems in terms of F1 score. We also describe additional evaluation experiments on Apertium data, a comparison with an earlier approach based on embedding projection, and an approach for constrained projection that outperforms the TIAD-2020 vanilla system by a large margin.",https://aclanthology.org/2020.globalex-1.16,European Language Resources Association,2020,May,Proceedings of the 2020 Globalex Workshop on Linked Lexicography,"Chiarcos, Christian  and
Schenk, Niko  and
F{\""a}th, Christian",Translation Inference by Concept Propagation,,globalex,1380
W19-3220,"['Domain-specific NLP', 'Classification Applications']","['Medical and Clinical NLP', 'NLP for News and Media']",['NLP for Social Media'],"In this study, we describe our methods to automatically classify Twitter posts conveying events of adverse drug reaction ADR. Based on our previous experience in tackling the ADR classification task, we empirically applied the vote-based undersampling ensemble approach along with linear support vector machine SVM to develop our classifiers as part of our participation in ACL 2019 Social Media Mining for Health Applications SMM4H shared task 1. The best-performed model on the test sets were trained on a merged corpus consisting of the datasets released by SMM4H 2017 and 2019. By using VUE, the corpus was randomly under-sampled with 2:1 ratio between the negative and positive classes to create an ensemble using the linear kernel trained with features including bag-of-word, domain knowledge, negation and word embedding. The best performing model achieved an F-measure of 0.551 which is about 5% higher than the average F-scores of 16 teams.",https://aclanthology.org/W19-3220,Association for Computational Linguistics,2019,August,Proceedings of the Fourth Social Media Mining for Health Applications ({\#}SMM4H) Workshop {\&} Shared Task,"Wang, Chen-Kai  and
Dai, Hong-Jie  and
Wang, Bo-Hung",BIGODM System in the Social Media Mining for Health Applications Shared Task 2019,10.18653/v1/W19-3220,W19,565
2021.splurobonlp-1.1,"['Domain-specific NLP', 'Model Architectures']",,,"This paper describes a method for learning from a teacher's potentially unreliable corrective feedback in an interactive task learning setting. The graphical model uses discourse coherence to jointly learn symbol grounding, domain concepts and valid plans. Our experiments show that the agent learns its domainlevel task in spite of the teacher's mistakes.",https://aclanthology.org/2021.splurobonlp-1.1,Association for Computational Linguistics,2021,August,Proceedings of Second International Combined Workshop on Spatial Language Understanding and Grounded Communication for Robotics,"Appelgren, Mattias  and
Lascarides, Alex",Symbol Grounding and Task Learning from Imperfect Corrections,10.18653/v1/2021.splurobonlp-1.1,splurobonlp,1102
2020.globalex-1.13,"['Learning Paradigms', 'Information Extraction', 'Model Architectures', 'Low-resource Languages']","['Recurrent Neural Networks (RNNs)', 'Supervised Learning', 'Word Sense Disambiguation (WSD)']",['Long Short-Term Memory (LSTM) Models'],"In this paper we describe the system submitted to the ELEXIS Monolingual Word Sense Alignment Task. We test different systems, which are two types of LSTMs and a system based on a pretrained Bidirectional Encoder Representations from Transformers BERT model, to solve the task. LSTM models use fastText pre-trained word vectors features with different settings. For training the models, we did not combine external data with the dataset provided for the task. We select a sub-set of languages among the proposed ones, namely a set of Romance languages, i.e., Italian, Spanish, Portuguese, together with English and Dutch. The Siamese LSTM with attention and PoS tagging LSTM-A performed better than the other two systems, achieving a 5-Class Accuracy score of 0.844 in the Overall Results, ranking the first position among five teams.",https://aclanthology.org/2020.globalex-1.13,European Language Resources Association,2020,May,Proceedings of the 2020 Globalex Workshop on Linked Lexicography,"Manna, Raffaele  and
Speranza, Giulia  and
di Buono, Maria Pia  and
Monti, Johanna",UNIOR NLP at MWSA Task - GlobaLex 2020: Siamese LSTM with Attention for Word Sense Alignment,,globalex,912
2021.alta-1.23,"['Domain-specific NLP', 'Classification Applications', 'Model Architectures']",['Medical and Clinical NLP'],,"The 2021 ALTA shared task is the 12th instance of a series of shared tasks organised by ALTA since 2010. Motivated by the advances in machine learning in the last 10 years, this year's task is a re-visit of the 2011 ALTA shared task. Set within the framework of Evidence Based Medicine EBM, the goal is to predict the quality of the clinical evidence present in a set of documents. This year's participant results did not improve over those of participants from 2011.",https://aclanthology.org/2021.alta-1.23,Australasian Language Technology Association,2021,December,Proceedings of the The 19th Annual Workshop of the Australasian Language Technology Association,"Moll{\'a}, Diego","Overview of the 2021 ALTA Shared Task: Automatic Grading of Evidence, 10 years later",,alta,125
2020.rocling-1.11,"['Audio Generation and Processing', 'Data Management and Generation', 'Low-resource Languages', 'Model Architectures']","['Data Augmentation', 'Automatic Speech Recognition (ASR)']",,"In this research, we developed the Taiwanese speech recognition system which used the Kaldi toolkit to implement. The Taiwanese corpus was collected by Taiwan Taiwanese National Reading Competition and Classmate Recording, and a total of about 11 hours of audio files were collected. Because the training data is small dataset, two audio augmentation methods are used to increase the training data, so that the acoustic model can be more robust and more effective training. One method is speed perturbation, which speeds up the original data by 1.1 times and slows it down by 0.9 times. Another method is to use multi-condition training data to simulate reverberation of the original speech and add background noise. The background noise includes music, speech, and noise. The acoustic model is trained for different hybrid deep neural network architectures which can use the advantages of each neural network by hybrid different neural networks, including TDNN, CNN-TDNN and CNN-LSTM-TDNN. In the experimental results, the CER in the domain of language modeling reaches 3.95%, and the CER of online decoding test is 3.06%. Compared with other researches on Taiwanese speech recognition of similar dataset size, the recognition results are better than other studies.",https://aclanthology.org/2020.rocling-1.11,The Association for Computational Linguistics and Chinese Language Processing (ACLCLP),2020,September,Proceedings of the 32nd Conference on Computational Linguistics and Speech Processing (ROCLING 2020),"Yeh, Yu-Fu  and
Su, Bo-Hao  and
Ou, Yang-Yen  and
Wang, Jhing-Fa  and
Tsai, An-Chao",{T}aiwanese Speech Recognition Based on Hybrid Deep Neural Network Architecture,,rocling,744
W17-1903,"['Figurative Language', 'Learning Paradigms', 'Classification Applications']","['Supervised Learning', 'Metaphors']",,"words refer to things that can not be seen, heard, felt, smelled, or tasted as opposed to concrete words. Among other applications, the degree of abstractness has been shown to be a useful information for metaphor detection. Our contribution to this topic are as follows: i we compare supervised techniques to learn and extend abstractness ratings for huge vocabularies ii we learn and investigate norms for multi-word units by propagating abstractness to verb-noun pairs which lead to better metaphor detection, iii we overcome the limitation of learning a single rating per word and show that multisense abstractness ratings are potentially useful for metaphor detection. Finally, with this paper we publish automatically created abstractness norms for 3 million English words and multi-words as well as automatically created sense-specific abstractness ratings.",https://aclanthology.org/W17-1903,Association for Computational Linguistics,2017,April,"Proceedings of the 1st Workshop on Sense, Concept and Entity Representations and their Applications","K{\""o}per, Maximilian  and
Schulte im Walde, Sabine","Improving Verb Metaphor Detection by Propagating Abstractness to Words, Phrases and Individual Senses",10.18653/v1/W17-1903,W17,122
2020.acl-main.330,"['Data Management and Generation', 'Classification Applications', 'Question Answering (QA)', 'Low-resource Languages', 'Automatic Text Summarization']",['Data Preparation'],,"Recently, large-scale datasets have vastly facilitated the development in nearly all domains of Natural Language Processing. However, there is currently no cross-task dataset in NLP, which hinders the development of multi-task learning. We propose MATINF, the first jointly labeled large-scale dataset for classification, question answering and summarization. MAT-INF contains 1.07 million question-answer pairs with human-labeled categories and usergenerated question descriptions. Based on such rich information, MATINF is applicable for three major NLP tasks, including classification, question answering, and summarization. We benchmark existing methods and a novel multi-task baseline over MATINF to inspire further research. Our comprehensive comparison and experiments over MATINF and other datasets demonstrate the merits held by MAT-INF. 1",https://aclanthology.org/2020.acl-main.330,Association for Computational Linguistics,2020,July,Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,"Xu, Canwen  and
Pei, Jiaxin  and
Wu, Hongtao  and
Liu, Yiyu  and
Li, Chenliang","MATINF: A Jointly Labeled Large-Scale Dataset for Classification, Question Answering and Summarization",10.18653/v1/2020.acl-main.330,acl,460
2021.insights-1.7,"['Learning Paradigms', 'Data Management and Generation', 'Low-resource Languages', 'Cross-lingual Application']","['Few-shot Learning', 'Transfer Learning', 'Data Preparation']",,"The training of NLP models often requires large amounts of labelled training data, which makes it difficult to expand existing models to new languages. While zero-shot cross-lingual transfer relies on multilingual word embeddings to apply a model trained on one language to another, Yarowsky and Ngai 2001 propose the method of annotation projection to generate training data without manual annotation. This method was successfully used for the tasks of named entity recognition and coarse-grained entity typing, but we show that it is outperformed by zero-shot cross-lingual transfer when applied to the similar task of fine-grained entity typing. In our study of fine-grained entity typing with the FIGER type ontology for German, we show that annotation projection amplifies the English model's tendency to underpredict level 2 labels and is beaten by zero-shot cross-lingual transfer on three novel test sets.",https://aclanthology.org/2021.insights-1.7,Association for Computational Linguistics,2021,November,Proceedings of the Second Workshop on Insights from Negative Results in NLP,"Weber, Sabine  and
Steedman, Mark",Zero-Shot Cross-Lingual Transfer is a Hard Baseline to Beat in German Fine-Grained Entity Typing,10.18653/v1/2021.insights-1.7,insights,959
2022.computel-1.13,['Low-resource Languages'],,,"Innu-Aimun is an Algonquian language spoken in Eastern Canada. It is the language of the Innu, an indigenous people that now lives for the most part in a dozen communities across Quebec and Labrador. Although it is alive, Innu-Aimun sees important preservation and revitalization challenges and issues. The state of its technology is still nascent, with very few existing applications. This paper proposes a first survey of the available linguistic resources and existing technology for Innu-Aimun. Considering the existing linguistic and textual resources, we argue that developing language technology is feasible and propose first steps towards NLP applications like machine translation. The goal of developing such technologies is first and foremost to help efforts in improving language transmission and cultural safety and preservation for Innu-Aimun speakers, as those are considered urgent and vital issues. Finally, we discuss the importance of close collaboration and consultation with the Innu community in order to ensure that language technologies are developed respectfully and in accordance with that goal.",https://aclanthology.org/2022.computel-1.13,Association for Computational Linguistics,2022,May,Proceedings of the Fifth Workshop on the Use of Computational Methods in the Study of Endangered Languages,"Cadotte, Antoine  and
Le Ngoc, Tan  and
Boivin, Mathieu  and
Sadat, Fatiha",Challenges and Perspectives for Innu-Aimun within Indigenous Language Technologies,10.18653/v1/2022.computel-1.13,computel,1142
2021.computel-1.11,"['Text Preprocessing', 'Low-resource Languages']",['Text Segmentation'],,"Any attempt to integrate NLP systems to the study of endangered languages must take into consideration traditional approaches by both NLP and linguistics. This paper tests different strategies and workflows for morpheme segmentation and glossing that may affect the potential to integrate machine learning. Two experiments train Transformer models on documentary corpora from five under-documented languages. In one experiment, a model learns segmentation and glossing as a joint step and another model learns the tasks into two sequential steps. We find the sequential approach yields somewhat better results. In a second experiment, one model is trained on surface segmented data, where strings of texts have been simply divided at morpheme boundaries. Another model is trained on canonically segmented data, the approach preferred by linguists, where abstract, underlying forms are represented. We find no clear advantage to either segmentation strategy and note that the difference between them disappears as training data increases. On average the models achieve more than a 0.5 F 1 -score, with the best models scoring 0.6 or above. An analysis of errors leads us to conclude consistency during manual segmentation and glossing may facilitate higher scores from automatic evaluation but in reality the scores may be lowered when evaluated against original data because instances of annotator error in the original data are ""corrected"" by the model.",https://aclanthology.org/2021.computel-1.11,Association for Computational Linguistics,2021,March,Proceedings of the 4th Workshop on the Use of Computational Methods in the Study of Endangered Languages Volume 1 (Papers),"Moeller, Sarah  and
Hulden, Mans",Integrating Automated Segmentation and Glossing into Documentary and Descriptive Linguistics,10.33011/computel.v1i.965,computel,89
2020.nlp4musa-1.3,"['Domain-specific NLP', 'Data Management and Generation', 'Model Architectures', 'Classification Applications']","['NLP for News and Media', 'Data Analysis']",,"Podcasts are an easily accessible medium of entertainment and information, often covering content from a variety of domains. However, only a few of them garner enough attention to be deemed 'popular'. In this work, we investigate the textual cues that assist in differing popular podcasts from unpopular ones. Despite having very similar polarity and subjectivity, the lexical cues contained in the podcasts are significantly different. Thus, we employ a triplet-based training method, to learn a text-based representation of a podcast, which is then used for a downstream task of ""popularity prediction"". Our best model received an F1 score of 0.82, achieving a relative improvement over the best baseline by 12.3%. * *Equal contribution. Ordered randomly.",https://aclanthology.org/2020.nlp4musa-1.3,Association for Computational Linguistics,2020,16-Oct,Proceedings of the 1st Workshop on NLP for Music and Audio (NLP4MusA),"Joshi, Brihi  and
Mittal, Shravika  and
Chetan, Aditya",Did You ``Read'' the Next Episode? Using Textual Cues for Predicting Podcast Popularity,,nlp4musa,240
2021.mtsummit-asltrw.1,"['Audio Generation and Processing', 'Domain-specific NLP', 'Low-resource Languages', 'Model Architectures']",['Automatic Speech Recognition (ASR)'],,"We address the problem of language model customization in applications where the ASR component needs to manage domain-specific terminology; although current state-of-the-art speech recognition technology provides excellent results for generic domains, the adaptation to specialized dictionaries or glossaries is still an open issue. In this work we present an approach for automatically selecting sentences, from a text corpus, that match, both semantically and morphologically, a glossary of terms words or composite words furnished by the user. The final goal is to rapidly adapt the language model of an hybrid ASR system with a limited amount of in-domain text data in order to successfully cope with the linguistic domain at hand; the vocabulary of the baseline model is expanded and tailored, reducing the resulting OOV rate. Data selection strategies based on shallow morphological seeds and semantic similarity via word2vec are introduced and discussed; the experimental setting consists in a simultaneous interpreting scenario, where ASRs in three languages are designed to recognize the domainspecific terms i.e. dentistry. Results using different metrics OOV rate, WER, precision and recall show the effectiveness of the proposed techniques.",https://aclanthology.org/2021.mtsummit-asltrw.1,Association for Machine Translation in the Americas,2021,August,Proceedings of the 1st Workshop on Automatic Spoken Language Translation in Real-World Settings (ASLTRW),"Gretter, Roberto  and
Matassoni, Marco  and
Falavigna, Daniele",Seed Words Based Data Selection for Language Model Adaptation,10.48550/arxiv.2107.09433,mtsummit,93
P19-1203,"['Question Answering (QA)', 'Learning Paradigms', 'Text Generation']",['Reinforcement Learning'],,"This paper investigates a new task named Conversational Question Generation CQG which is to generate a question based on a passage and a conversation history i.e., previous turns of question-answer pairs. CQG is a crucial task for developing intelligent agents that can drive question-answering style conversations or test user understanding of a given passage. Towards that end, we propose a new approach named Reinforced Dynamic Reasoning ReDR network, which is based on the general encoder-decoder framework but incorporates a reasoning procedure in a dynamic manner to better understand what has been asked and what to ask next about the passage. To encourage producing meaningful questions, we leverage a popular question answering QA model to provide feedback and fine-tune the question generator using a reinforcement learning mechanism. Empirical results on the recently released CoQA dataset demonstrate the effectiveness of our method in comparison with various baselines and model variants. Moreover, to show the applicability of our method, we also apply it to create multiturn question-answering conversations for passages in SQuAD. * Work done while visiting the Ohio State University. Shelly is in second grade. She is a new student at her school. Shelly's family has lived in many different places. Shelly was born in Florida. Her family moved to Tennessee when she was two years old. When she was four years old, they moved to Texas. They moved from there to Arizona, where they now live.",https://aclanthology.org/P19-1203,Association for Computational Linguistics,2019,July,Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,"Pan, Boyuan  and
Li, Hao  and
Yao, Ziyu  and
Cai, Deng  and
Sun, Huan",Reinforced Dynamic Reasoning for Conversational Question Generation,10.18653/v1/P19-1203,P19,155
2020.fever-1.2,"['Domain-specific NLP', 'Low-resource Languages', 'Classification Applications', 'Data Management and Generation']","['Claim Verification', 'Data Preparation', 'Stance Detection', 'NLP for News and Media']",['Annotation Processes'],"This work explores the application of textual entailment in news claim verification and stance prediction using a new corpus in Arabic. The publicly available corpus comes in two perspectives: a version consisting of 4,547 true and false claims and a version consisting of 3,786 pairs claim, evidence. We describe the methodology for creating the corpus and the annotation process. Using the introduced corpus, we also develop two machine learning baselines for two proposed tasks: claim verification and stance prediction. Our best model utilizes pretraining BERT and achieves 76.7 F1 on the stance prediction task and 64.3 F1 on the claim verification task. Our preliminary experiments shed some light on the limits of automatic claim verification that relies on claims text only. Results hint that while the linguistic features and world knowledge learned during pretraining are useful for stance prediction, such learned representations from pretraining are insufficient for verifying claims without access to context or evidence.",https://aclanthology.org/2020.fever-1.2,Association for Computational Linguistics,2020,July,Proceedings of the Third Workshop on Fact Extraction and VERification (FEVER),"Khouja, Jude",Stance Prediction and Claim Verification: An Arabic Perspective,10.18653/v1/2020.fever-1.2,fever,137
2022.wnu-1.3,"['Domain-specific NLP', 'Classification Applications', 'Data Management and Generation']","['Data Preparation', 'NLP for News and Media']",['NLP for Social Media'],"Internet forums such as Reddit offer people a platform to ask for advice when they encounter various issues at work, school or in relationships. Telling helpful comments apart from unhelpful comments to these advice-seeking posts can help people and dialogue agents to become more helpful in offering advice. We propose a dataset that contains both helpful and unhelpful comments in response to such requests. We then relate helpfulness to the closely related construct of empathy. Finally, we analyze the language features that are associated with helpful and unhelpful comments.",https://aclanthology.org/2022.wnu-1.3,Association for Computational Linguistics,2022,July,Proceedings of the 4th Workshop of Narrative Understanding (WNU2022),"Wang, Zhilin  and
Torres, Pablo E.",How to be Helpful on Online Support Forums?,10.18653/v1/2022.wnu-1.3,wnu,112
2022.clpsych-1.21,"['Domain-specific NLP', 'Classification Applications']","['NLP for News and Media', 'Medical and Clinical NLP']","['NLP for Mental Health', 'NLP for Social Media']","This paper presents transformer-based models created for the CLPsych 2022 shared task. Using posts from Reddit users over a period of time, we aim to predict changes in mood from post to post. We test models that preserve timeline information through explicit ordering of posts as well as those that do not order posts but preserve features on the length of time between a user's posts. We find that a model with temporal information may provide slight benefits over the same model without such information, although a RoBERTa transformer model provides enough information to make similar predictions without custom-encoded time information.",https://aclanthology.org/2022.clpsych-1.21,Association for Computational Linguistics,2022,July,Proceedings of the Eighth Workshop on Computational Linguistics and Clinical Psychology,"Culnan, John  and
Romero Diaz, Damian  and
Bethard, Steven",Exploring transformers and time lag features for predicting changes in mood over time,10.18653/v1/2022.clpsych-1.21,clpsych,82
2020.nuse-1.8,"['Data Management and Generation', 'Information Extraction', 'Classification Applications']","['Data Preparation', 'Event Extraction']",['Annotation Processes'],"In this paper we introduce the problem of extracting events from dialogue. Previous work on event extraction focused on newswire, however we are interested in extracting events from spoken dialogue. To ground this study, we annotated dialogue transcripts from fourteen episodes of the podcast This American Life. This corpus contains 1,038 utterances, made up of 16,962 tokens, of which 3,664 represent events. The agreement for this corpus has a Cohen's  of 0.83. We have open sourced this corpus for the NLP community. With this corpus in hand, we trained support vector machines SVM to correctly classify these phenomena with 0.68 F1, when using episodefold cross-validation. This is nearly 100% higher F1 than the baseline classifier. The SVM models achieved performance of over 0.75 F1 on some testing folds. We report the results for SVM classifiers trained with four different types of features verb classes, part of speech tags, named entities, and semantic role labels, and different machine learning protocols under-sampling and trigram context. This work is grounded in narratology and computational models of narrative. It is useful for extracting events, plot, and story content from spoken dialogue.",https://aclanthology.org/2020.nuse-1.8,Association for Computational Linguistics,2020,July,"Proceedings of the First Joint Workshop on Narrative Understanding, Storylines, and Events","Eisenberg, Joshua  and
Sheriff, Michael",Automatic extraction of personal events from dialogue,10.18653/v1/2020.nuse-1.8,nuse,238
W17-4113,"['Embeddings', 'Low-resource Languages', 'Model Architectures']",['Recurrent Neural Networks (RNNs)'],['Long Short-Term Memory (LSTM) Models'],Language models for agglutinative languages have always been hindered in past due to myriad of agglutinations possible to any given word through various affixes. We propose a method to diminish the problem of out-of-vocabulary words by introducing an embedding derived from syllables and morphemes which leverages the agglutinative property. Our model outperforms character-level embedding in perplexity by 16.87 with 9.50M parameters. Proposed method achieves state of the art performance over existing input prediction methods in terms of Key Stroke Saving and has been commercialized.,https://aclanthology.org/W17-4113,Association for Computational Linguistics,2017,September,Proceedings of the First Workshop on Subword and Character Level Models in {NLP},"Yu, Seunghak  and
Kulkarni, Nilesh  and
Lee, Haejun  and
Kim, Jihie",Syllable-level Neural Language Model for Agglutinative Language,10.18653/v1/W17-4113,W17,318
2020.lincr-1.4,"['Evaluation Techniques', 'Low-resource Languages', 'Text Generation']",,,"Linguistics predictability is the degree of confidence in which language unit word, part of speech, etc. will be the next in the sequence. Experiments have shown that the correct prediction simplifies the perception of a language unit and its integration into the context. As a result of an incorrect prediction, language processing slows down. Currently, to get a measure of the language unit predictability, a neurolinguistic experiment known as a cloze task has to be conducted on a large number of participants. Cloze tasks are resource-consuming and are criticized by some researchers as an insufficiently valid measure of predictability. In this paper, we compare different language models that attempt to simulate human respondents' performance on the cloze task. Using a language model to create cloze task simulations would require significantly less time and conduct studies related to linguistic predictability.",https://aclanthology.org/2020.lincr-1.4,European Language Resources Association,2020,May,Proceedings of the Second Workshop on Linguistic and Neurocognitive Resources,"Nikiforova, Anastasia  and
Pletenev, Sergey  and
Sinitsyna, Daria  and
Sorokin, Semen  and
Lopukhina, Anastasia  and
Howell, Nick",Language Models for Cloze Task Answer Generation in Russian,,lincr,1285
2020.signlang-1.23,"['Machine Translation (MT)', 'Data Management and Generation', 'Learning Paradigms', 'Low-resource Languages', 'Model Architectures']","['Transformer Models', 'Data Preparation', 'Neural MT (NMT)', 'Supervised Learning']",,"Sign language is the first language for those who were born deaf or lost their hearing in early childhood, so such individuals require services provided with sign language. To achieve flexible open-domain services with sign language, machine translations into sign language are needed. Machine translations generally require large-scale training corpora, but there are only small corpora for sign language. To overcome this data-shortage scenario, we developed a method that involves using a pre-trained language model of spoken language as the initial model of the encoder of the machine translation model. We evaluated our method by comparing it to baseline methods, including phrase-based machine translation, using only 130,000 phrase pairs of training data. Our method outperformed the baseline method, and we found that one of the reasons of translation error is from Pointing, which is a special feature used in sign language. We also conducted trials to improve the translation quality for Pointing. The results are somewhat disappointing, so we believe that there is still room for improving translation quality, especially for Pointing.",https://aclanthology.org/2020.signlang-1.23,European Language Resources Association (ELRA),2020,May,"Proceedings of the LREC2020 9th Workshop on the Representation and Processing of Sign Languages: Sign Language Resources in the Service of the Language Community, Technological Challenges and Application Perspectives","Miyazaki, Taro  and
Morita, Yusuke  and
Sano, Masanori",Machine Translation from Spoken Language to Sign Language using Pre-trained Language Model as Encoder,,signlang,363
2020.wosp-1.6,"['Text Clustering', 'Data Management and Generation', 'Topic Modeling']",['Data Preparation'],,"Mainly due to the open access movement, the number of scholarly papers we can freely access is drastically increasing. A huge amount of papers is a promising resource for text mining and machine learning. Given a set of papers, for example, we can grasp past or current trends in a research community. Compared to the trend detection, it is more difficult to forecast trends in the near future, since the number of occurrences of some features, which are major cues for automatic detection, such as the word frequency, is quite small before such a trend will emerge. As a first step toward trend forecasting, this paper is devoted to finding subtle trends. To do this, the authors propose an index for keywords, called normalized impact index, and visualize keywords and their indices as a heat map. The authors have conducted case studies using some keywords already known as popular, and we found some keywords whose frequencies are not so large but whose indices are large.",https://aclanthology.org/2020.wosp-1.6,Association for Computational Linguistics,2020,05-Aug,Proceedings of the 8th International Workshop on Mining Scientific Publications,"Ikeda, Daisuke  and
Taniguchi, Yuta  and
Koga, Kazunori",The Normalized Impact Index for Keywords in Scholarly Papers to Detect Subtle Research Topics,,wosp,700
2020.figlang-1.38,"['Domain-specific NLP', 'Model Architectures', 'Classification Applications']","['Sarcasm Detection', 'NLP for News and Media', 'Transformer Models']",['NLP for Social Media'],"We present a transformer-based sarcasm detection model that accounts for the context from the entire conversation thread for more robust predictions. Our model uses deep transformer layers to perform multi-head attentions among the target utterance and the relevant context in the thread. The context-aware models are evaluated on two datasets from social media, Twitter and Reddit, and show 3.1% and 7.0% improvements over their baselines. Our best models give the F1-scores of 79.0% and 75.0% for the Twitter and Reddit datasets respectively, becoming one of the highest performing systems among 36 participants in this shared task.",https://aclanthology.org/2020.figlang-1.38,Association for Computational Linguistics,2020,July,Proceedings of the Second Workshop on Figurative Language Processing,"Dong, Xiangjue  and
Li, Changmao  and
Choi, Jinho D.",Transformer-based Context-aware Sarcasm Detection in Conversation Threads from Social Media,10.18653/v1/2020.figlang-1.38,figlang,788
D17-1161,"['Domain-specific NLP', 'Parsing', 'Data Management and Generation', 'Learning Paradigms', 'Classification Applications']","['Data Preparation', 'Semantic Parsing', 'Supervised Learning']",,"Natural language constitutes a predominant medium for much of human learning and pedagogy. We consider the problem of concept learning from natural language explanations, and a small number of labeled examples of the concept. For example, in learning the concept of a phishing email, one might say 'this is a phishing email because it asks for your bank account number'. Solving this problem involves both learning to interpret open-ended natural language statements, as well as learning the concept itself. We present a joint model for 1 language interpretation semantic parsing and 2 concept learning classification that does not require labeling statements with logical forms. Instead, the model prefers discriminative interpretations of statements in context of observable features of the data as a weak signal for parsing. On a dataset of email-related concepts, this approach yields across-theboard improvements in classification performance, with a 30% relative improvement in F1 score over competitive classification methods in the low data regime.",https://aclanthology.org/D17-1161,Association for Computational Linguistics,2017,September,Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing,"Srivastava, Shashank  and
Labutov, Igor  and
Mitchell, Tom",Joint Concept Learning and Semantic Parsing from Natural Language Explanations,10.18653/v1/D17-1161,D17,428
K16-1005,"['Knowledge Representation and Reasoning', 'Model Architectures']",,,"Knowledge bases are useful resources for many natural language processing tasks, however, they are far from complete. In this paper, we define a novel entity representation as a mixture of its neighborhood in the knowledge base and apply this technique on TransE-a well-known embedding model for knowledge base completion. Experimental results show that the neighborhood information significantly helps to improve the results of the TransE, leading to better performance than obtained by other state-of-the-art embedding models on three benchmark datasets for triple classification, entity prediction and relation prediction tasks.",https://aclanthology.org/K16-1005,Association for Computational Linguistics,2016,August,Proceedings of The 20th {SIGNLL} Conference on Computational Natural Language Learning,"Nguyen, Dat Quoc  and
Sirts, Kairit  and
Qu, Lizhen  and
Johnson, Mark",Neighborhood Mixture Model for Knowledge Base Completion,10.18653/v1/K16-1005,K16,722
2020.ecomnlp-1.6,"['Dialogue Systems', 'Information Retrieval', 'Knowledge Representation and Reasoning']",['Chatbots'],,"Information retrieval chatbots are widely used as assistants, to help users formulate their requirements about the products they want to purchase, and navigate to the set of items that satisfies their requirements in the best way. The work of the modern chatbots is based mostly on the deep learning theory behind the knowledge model that can improve the performance of the system. In our work, we are developing a concept-based knowledge model that encapsulates objects and their common descriptions. The leveraging of the concept-based knowledge model allows the system to refine the initial users' requests and lead them to the set of objects with the maximal variability of parameters that matters less to them. Introducing the additional textual characteristics allows users to formulate their initial query as a phrase in natural language, rather than as some standard request in the form of, ""Attribute -value"".",https://aclanthology.org/2020.ecomnlp-1.6,Association for Computational Linguistics,2020,December,Proceedings of Workshop on Natural Language Processing in E-Commerce,"Galitsky, Boris  and
Ilvovsky, Dmitry  and
Goncharova, Elizaveta",On a Chatbot Navigating a User through a Concept-Based Knowledge Model,,ecomnlp,14
2022.eamt-1.8,"['Machine Translation (MT)', 'Evaluation Techniques']",,,"Although more and more professionals are using real-time machine translation during dialogues with interlocutors who speak a different language, the performance of real-time MT apps has received only limited attention in the academic literature. This study summarizes the findings of prior studies N = 34 reporting an evaluation of one or more real-time MT apps in a professional setting. Our findings show that real-time MT apps are often tested in realistic circumstances and that users are more frequently employed as judges of performance than professional translators. Furthermore, most studies report overall positive results with regard to performance, particularly when apps are tested in real-life situations.",https://aclanthology.org/2022.eamt-1.8,European Association for Machine Translation,2022,June,Proceedings of the 23rd Annual Conference of the European Association for Machine Translation,"Pluymaekers, Mark",How well do real-time machine translation apps perform in practice? Insights from a literature review,,eamt,42
2022.sigtyp-1.3,"['Low-resource Languages', 'Language Change Analysis']",,,"In this study we address the question to what extent syntactic word-order traits of different languages have evolved under correlation and whether such dependencies can be found universally across all languages or restricted to specific language families. To do so, we use logistic Brownian Motion under a Bayesian framework to model the trait evolution for 768 languages from 34 language families. We test for trait correlations both in single families and universally over all families. Separate models reveal no universal correlation patterns and Bayes Factor analysis of models over all covered families also strongly indicate lineage specific correlation patters instead of universal dependencies.",https://aclanthology.org/2022.sigtyp-1.3,Association for Computational Linguistics,2022,July,Proceedings of the 4th Workshop on Research in Computational Linguistic Typology and Multilingual NLP,"Hartung, Kai  and
J{\""a}ger, Gerhard  and
Gr{\""o}ttrup, S{\""o}ren  and
Georges, Munir",Typological Word Order Correlations with Logistic Brownian Motion,10.18653/v1/2022.sigtyp-1.3,sigtyp,1224
R19-1037,"['Evaluation Techniques', 'Learning Paradigms', 'Data Management and Generation']","['Unsupervised Learning', 'Data Analysis']",,"This paper presents a pilot study of entropy as a measure of gap complexity in open cloze tests aimed at learners of English. Entropy is used to quantify the information content in each gap, which can be used to estimate complexity. Our study shows that average gap entropy correlates positively with proficiency levels while individual gap entropy can capture contextual complexity. To the best of our knowledge, this is the first unsupervised information-theoretical approach to evaluating the quality of cloze tests.",https://aclanthology.org/R19-1037,INCOMA Ltd.,2019,September,Proceedings of the International Conference on Recent Advances in Natural Language Processing (RANLP 2019),"Felice, Mariano  and
Buttery, Paula",Entropy as a Proxy for Gap Complexity in Open Cloze Tests,10.26615/978-954-452-056-4_037,R19,184
2021.emnlp-main.229,"['Text Preprocessing', 'Parsing', 'Low-resource Languages']","['Syntactic Parsing', 'Part-of-Speech (POS) Tagging']",['Dependency Parsing'],"Pimentel et al. 2020b recently analysed probing from an information-theoretic perspective. They argue that probing should be seen as approximating a mutual information. This led to the rather unintuitive conclusion that representations encode exactly the same information about a target task as the original sentences. The mutual information, however, assumes the true probability distribution of a pair of random variables is known, leading to unintuitive results in settings where it is not. This paper proposes a new framework to measure what we term Bayesian mutual information, which analyses information from the perspective of Bayesian agents-allowing for more intuitive findings in scenarios with finite data. For instance, under Bayesian MI we have that data can add information, processing can help, and information can hurt, which makes it more intuitive for machine learning applications. Finally, we apply our framework to probing where we believe Bayesian mutual information naturally operationalises ease of extraction by explicitly limiting the available background knowledge to solve a task.",https://aclanthology.org/2021.emnlp-main.229,Association for Computational Linguistics,2021,November,Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,"Pimentel, Tiago  and
Cotterell, Ryan",A Bayesian Framework for Information-Theoretic Probing,10.18653/v1/2021.emnlp-main.229,emnlp,32
D18-1266,"['Learning Paradigms', 'Parsing']","['Reinforcement Learning', 'Semantic Parsing']",,"Semantic parsing from denotations faces two key challenges in model training: 1 given only the denotations e.g., answers, search for good candidate semantic parses, and 2 choose the best model update algorithm. We propose effective and general solutions to each of them. Using policy shaping, we bias the search procedure towards semantic parses that are more compatible to the text, which provide better supervision signals for training. In addition, we propose an update equation that generalizes three different families of learning algorithms, which enables fast model exploration. When experimented on a recently proposed sequential question answering dataset, our framework leads to a new state-of-theart model that outperforms previous work by 5.0% absolute on exact match accuracy.",https://aclanthology.org/D18-1266,Association for Computational Linguistics,2018,October-November,Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing,"Misra, Dipendra  and
Chang, Ming-Wei  and
He, Xiaodong  and
Yih, Wen-tau",Policy Shaping and Generalized Update Equations for Semantic Parsing from Denotations,10.18653/v1/D18-1266,D18,1191
2021.triton-1.23,"['Audio Generation and Processing', 'Evaluation Techniques', 'Data Management and Generation', 'Error Detection and Correction']","['Data Preparation', 'Automatic Speech Recognition (ASR)']",['Annotation Processes'],"This paper presents a comparative evaluation of four commercial ASR systems which are evaluated according to the post-editing effort required to reach ""publishable"" quality and according to the number of errors they produce. For the error annotation task, an original error typology for transcription errors is proposed. This study also seeks to examine whether there is a difference in the performance of these systems between native and non-native English speakers. The experimental results suggest that among the four systems, Trint and Microsoft obtain the best scores. It is also observed that most systems perform noticeably better with native speakers and that all systems are most prone to fluency errors.",https://aclanthology.org/2021.triton-1.23,INCOMA Ltd.,2021,July,Proceedings of the Translation and Interpreting Technology Online Conference,"Papadopoulou, Martha Maria  and
Zaretskaya, Anna  and
Mitkov, Ruslan",Benchmarking ASR Systems Based on Post-Editing Effort and Error Analysis,10.26615/978-954-452-071-7_023,triton,62
S16-1190,"['Domain-specific NLP', 'Information Extraction', 'Model Architectures']","['Medical and Clinical NLP', 'Temporal Event Understanding', 'Event Extraction']",,"Our experiments rely on a combination of machine-learning CRF and rule-based Hei-delTime systems. First, a CRF system identifies both EVENTS and TIMEX3, along with polarity values for EVENT and types of TIMEX. Second, the HeidelTime tool identifies DOCTIME and TIMEX3 elements, and computes DocTimeRel for each EVENT identified by the CRF. Third, another CRF system computes DocTimeRel for each previously identified EVENT, based on Doc-TimeRel computed by HeidelTime. In the first submission, all EVENTS and TIMEX3 are identified through one general CRF model while in the second submission, we combined two CRF models one for both EVENT and TIMEX3, and one only for TIMEX3 and we applied post-processing rules on the outputs.",https://aclanthology.org/S16-1190,Association for Computational Linguistics,2016,June,Proceedings of the 10th International Workshop on Semantic Evaluation ({S}em{E}val-2016),"Grouin, Cyril  and
Moriceau, V{\'e}ronique",LIMSI at SemEval-2016 Task 12: machine-learning and temporal information to identify clinical events and time expressions,10.18653/v1/S16-1190,S16,525
N19-1340,"['Embeddings', 'Parsing', 'Low-resource Languages', 'Model Architectures']","['Word Embeddings', 'Semantic Parsing', 'Recurrent Neural Networks (RNNs)']","['Semantic Role Labeling', 'Long Short-Term Memory (LSTM) Models']","Semantic role labeling SRL is a task to recognize all the predicate-argument pairs of a sentence, which has been in a performance improvement bottleneck after a series of latest works were presented. This paper proposes a novel syntax-agnostic SRL model enhanced by the proposed associated memory network AMN, which makes use of inter-sentence attention of label-known associated sentences as a kind of memory to further enhance dependency-based SRL. In detail, we use sentences and their labels from train dataset as an associated memory cue to help label the target sentence. Furthermore, we compare several associated sentences selecting strategies and label merging methods in AMN to find and utilize the label of associated sentences while attending them. By leveraging the attentive memory from known training data, Our full model reaches state-of-theart on CoNLL-2009 benchmark datasets for syntax-agnostic setting, showing a new effective research line of SRL enhancement other than exploiting external resources such as well pre-trained language models.",https://aclanthology.org/N19-1340,Association for Computational Linguistics,2019,June,"Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)","Guan, Chaoyu  and
Cheng, Yuhao  and
Zhao, Hai",Semantic Role Labeling with Associated Memory Network,10.18653/v1/N19-1340,N19,120
2021.socialnlp-1.1,"['Ethics', 'Domain-specific NLP', 'Data Management and Generation', 'Classification Applications', 'Model Architectures']","['Data Preparation', 'Stance Detection', 'NLP for News and Media']",['NLP for Social Media'],"The Moral Foundation Theory suggests five moral foundations that can capture the view of a user on a particular issue. It is widely used to identify sentence-level sentiment. In this paper, we study the nuanced stances and partisan sentiment towards entities of US politicians using Moral Foundation Theory, on two politically divisive issues -Gun Control and Immigration. We define the nuanced stances of the US politicians on these two topics by the grades given by related organizations to the politicians. To conduct this study, we first filter out 74k and 87k tweets on the topics Gun Control and Immigration, respectively, from an existing tweet corpus authored by US parliament members. Then, we identify moral foundations in these tweets using deep relational learning. Finally, doing qualitative and quantitative evaluations on this dataset, we found out that there is a strong correlation between moral foundation usage and politicians' nuanced stances on a particular topic. We also found notable differences in moral foundation usage by different political parties when they address different entities.",https://aclanthology.org/2021.socialnlp-1.1,Association for Computational Linguistics,2021,June,Proceedings of the Ninth International Workshop on Natural Language Processing for Social Media,"Roy, Shamik  and
Goldwasser, Dan",Analysis of Nuanced Stances and Sentiment Towards Entities of US Politicians through the Lens of Moral Foundation Theory,10.18653/v1/2021.socialnlp-1.1,socialnlp,1056
2021.konvens-1.23,"['Text Generation', 'Data Management and Generation', 'Low-resource Languages', 'Knowledge Representation and Reasoning']","['Data Preparation', 'Knowledge Graphs', 'Text Simplification']",,"Conceptual complexity is concerned with the background knowledge needed to understand concepts within a text and their implicit connections Hulpus , et al., 2019. In the present study, a recently proposed framework from Hulpus , et al.  2019 , which assesses the conceptual complexity of English newspaper articles, is replicated and adapted to German lexica entries aimed at three different age groups. The final results on the corpus of 885 German texts improve upon the original study in both a pairwise classification task and a ranking task, showing that the framework transfers well to a different language and a different genre. We release the dataset used, as well as an extended version with a total of ca. 3000 texts.",https://aclanthology.org/2021.konvens-1.23,KONVENS 2021 Organizers,2021,6--9 September,Proceedings of the 17th Conference on Natural Language Processing (KONVENS 2021),"Hewett, Freya  and
Stede, Manfred",Automatically evaluating the conceptual complexity of German texts,,konvens,667
2020.vardial-1.19,"['Domain-specific NLP', 'Multilingual NLP', 'Learning Paradigms', 'Low-resource Languages', 'Classification Applications']","['Multilabel Text Classification', 'Supervised Learning', 'NLP for News and Media']",['NLP for Social Media'],"This paper describes the Helsinki-Ljubljana contribution to the VarDial shared task on social media variety geolocation. Our solutions are based on the BERT Transformer models, the constrained versions of our models reaching 1st place in two subtasks and 3rd place in one subtask, while our unconstrained models outperform all the constrained systems by a large margin. We show in our analyses that Transformer-based models outperform traditional models by far, and that improvements obtained by pre-training models on large quantities of mostly standard text are significant, but not drastic, with single-language models also outperforming multilingual models. Our manual analysis shows that two types of signals are the most crucial for a misprediction: named entities and dialectal features, both of which are handled well by our models.",https://aclanthology.org/2020.vardial-1.19,International Committee on Computational Linguistics (ICCL),2020,December,"Proceedings of the 7th Workshop on NLP for Similar Languages, Varieties and Dialects","Scherrer, Yves  and
Ljube{\v{s}}i{\'c}, Nikola",HeLju@VarDial 2020: Social Media Variety Geolocation with BERT Models,,vardial,409
2021.teachingnlp-1.15,['Domain-specific NLP'],,,"We provide an overview of a new Computational Text Analysis course that will be taught at Barnard College over a six week period in May and June 2021. The course is targeted to non Computer Science at a U.S. Liberal Arts college that wish to incorporate fundamental Natural Language Processing tools in their research and studies. During the course, students will complete daily programming tutorials, read and review contemporary research papers, and propose and develop independent research projects.",https://aclanthology.org/2021.teachingnlp-1.15,Association for Computational Linguistics,2021,June,Proceedings of the Fifth Workshop on Teaching NLP,"Poliak, Adam  and
Jenifer, Jalisha",An Immersive Computational Text Analysis Course for Non-Computer Science Students at Barnard College,10.18653/v1/2021.teachingnlp-1.15,teachingnlp,1072
2021.eacl-main.239,"['Biases in NLP', 'Ethics', 'Learning Paradigms']",['Adversarial Learning'],,"Adversarial learning can learn fairer and less biased models of language than standard methods. However, current adversarial techniques only partially mitigate model bias, added to which their training procedures are often unstable. In this paper, we propose a novel approach to adversarial learning based on the use of multiple diverse discriminators, whereby discriminators are encouraged to learn orthogonal hidden representations from one another. Experimental results show that our method substantially improves over standard adversarial removal methods, in terms of reducing bias and the stability of training.",https://aclanthology.org/2021.eacl-main.239,Association for Computational Linguistics,2021,April,Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume,"Han, Xudong  and
Baldwin, Timothy  and
Cohn, Trevor",Diverse Adversaries for Mitigating Bias in Training,10.18653/v1/2021.eacl-main.239,eacl,757
W17-7410,['Data Management and Generation'],['Data Preparation'],['Annotation Processes'],"In this article, we provide an overview of a web-based text annotation platform, called PACTE. We highlight the various features contributing to making PACTE an ideal platform for research projects involving textual annotation of large corpora performed by geographically distributed teams. Introduction With the availability of large amount of textual data on the web, or from legacy documents, various text analysis projects emerge to study, analyze and understand the content of these texts. Projects arise from various disciplines, such as psychological studies e.g. detecting language patterns related to particular mental states or literary studies e.g. studying patterns used by particular authors, or criminology studies e.g. analyzing crime-related locations. Text analysis projects of large scale often involve multiple actors, in a distributed spatial setting, with collaborators all over the world. While their perspectives are different and their goals are varied, most text analysis projects require some common functionalities: document selection to gather a proper corpus for pattern analysis, text annotation to mark actual metadata about documents, paragraphs, sentences, words or word segments and annotation search to search the annotated segments for the ones of interest. Furthermore, many projects would benefit from basic automatic annotation of textual components sentences, nominal compounds, named entities, etc. Yet, each project would likely also have its particularities as to what are the important text patterns to study, and perhaps such patterns are best annotated by human experts. We are in the process of developing a text project management and annotation platform, called PACTE http://pacte.crim.ca, to support such large-scale distributed text analysis. A key component of PACTE is to not only allow for easy annotation whether manual or automatic, but to also provide the very essential search component, to retrieve through the mass of texts, segments of information containing specific annotations e.g. retrieving all documents mentioning a particular city. In its final state, PACTE will contain the common required project management functionalities, as well as common annotation services, but also allow for particularities e.g. specialized schema definition. The platform also aims at encouraging interdisciplinary collaborations, as much automatic textual analysis in the recent years is data-driven, using machine learning models which require a lot of annotated data. A known bottleneck to these supervised models is the lack of availability of annotated data. By providing a platform which makes it easy to annotate using user-defined schemas, we hope to encourage various users from various disciplines to perform annotation. In the remaining of this demonstration note, we will show Section 2 an example of an annotation project with definitions of the various terms used when discussing annotation projects e.g. types, schemas, features, groups, etc. We will then highlight section 3 the distinctive features of PACTE, mainly focusing on eight important aspects of PACTE, that it 1 is web-based, 2 handles large volumes of text for both annotation and search, 3 allows easy project management, 4 allows collaborative annotation, 5 provides some automatic annotation services, 6 allows users to define specific schemas for targeted manual annotation,  7  provides text search capabilities, 8 offers management of custom lexicon. Then, we compare PACTE to other platforms Section 4 and we give the current state and future development of PACTE Section 5.",https://aclanthology.org/W17-7410,,2017,,Proceedings of the 13th Joint {ISO}-{ACL} Workshop on Interoperable Semantic Annotation ({ISA}-13),"M{\'e}nard, Pierre Andr{\'e}  and
Barri{\`e}re, Caroline",PACTE: A colloaborative platform for textual annotation,,W17,1193
2020.lrec-1.229,"['Learning Paradigms', 'Data Management and Generation', 'Low-resource Languages', 'Model Architectures']","['Supervised Learning', 'Data Preparation', 'Recurrent Neural Networks (RNNs)']",['Long Short-Term Memory (LSTM) Models'],"This paper explores the use of Deep Learning methods for automatic estimation of quality of human translations. Automatic estimation can provide useful feedback for translation teaching, examination and quality control. Conventional methods for solving this task rely on manually engineered features and external knowledge. This paper presents an end-to-end neural model without feature engineering, incorporating a cross attention mechanism to detect which parts in sentence pairs are most relevant for assessing quality. Another contribution concerns oprediction of fine-grained scores for measuring different aspects of translation quality, such as terminological accuracy or idiomatic writing. Empirical results on a large human annotated dataset show that the neural model outperforms feature-based methods significantly. The dataset and the tools are available.",https://aclanthology.org/2020.lrec-1.229,European Language Resources Association,2020,May,Proceedings of the 12th Language Resources and Evaluation Conference,"Yuan, Yu  and
Sharoff, Serge",Sentence Level Human Translation Quality Estimation with Attention-based Neural Networks,10.48550/arxiv.2003.06381,lrec,1239
W19-7904,['Low-resource Languages'],,,"Language unit is a fundamental conception in modern linguistics, but the boundaries are not clear between language levels both in the past and present. As language is a multi-level system, quantification rather than microscopic grammatical analysis should be used to investigate into this question. In this paper, Menzerath-Altmann law is used to make out the basic language units in written Chinese. The results show that ""stroke > component > word > clause > sentence"" is the hierarchical structure of written Chinese.",https://aclanthology.org/W19-7904,Association for Computational Linguistics,2019,August,"Proceedings of the First Workshop on Quantitative Syntax (Quasy, SyntaxFest 2019)","Chen, Heng  and
Liu, Haitao",A quantitative probe into the hierarchical structure of written Chinese,10.18653/v1/W19-7904,W19,993
2022.spanlp-1.4,"['Image and Video Processing', 'Data Management and Generation', 'Information Extraction', 'Model Architectures']","['Transformer Models', 'Data Preparation', 'Document Layout Analysis (DLA)']",,"We propose a novel framework to conduct field extraction from forms with unlabeled data. To bootstrap the training process, we develop a rule-based method for mining noisy pseudo-labels from unlabeled forms. Using the supervisory signal from the pseudo-labels, we extract a discriminative token representation from a transformer-based model by modeling the interaction between text in the form. To prevent the model from overfitting to label noise, we introduce a refinement module based on a progressive pseudo-label ensemble. Experimental results demonstrate the effectiveness of our framework.",https://aclanthology.org/2022.spanlp-1.4,Association for Computational Linguistics,2022,May,Proceedings of the 1st Workshop on Semiparametric Methods in NLP: Decoupling Logic from Knowledge,"Gao, Mingfei  and
Chen, Zeyuan  and
Naik, Nikhil  and
Hashimoto, Kazuma  and
Xiong, Caiming  and
Xu, Ran",Field Extraction from Forms with Unlabeled Data,10.18653/v1/2022.spanlp-1.4,spanlp,1175
I17-1048,"['Embeddings', 'Data Management and Generation', 'Model Architectures']","['Data Preparation', 'Word Embeddings', 'Recurrent Neural Networks (RNNs)']",,"This study addresses the problem of identifying the meaning of unknown words or entities in a discourse with respect to the word embedding approaches used in neural language models. We proposed a method for on-the-fly construction and exploitation of word embeddings in both the input and output layers of a neural model by tracking contexts. This extends the dynamic entity representation used in Kobayashi et al.  2016  and incorporates a copy mechanism proposed independently by Gu et al.  2016  and Gulcehre et al. 2016. In addition, we construct a new task and dataset called Anonymized Language Modeling for evaluating the ability to capture word meanings while reading. Experiments conducted using our novel dataset show that the proposed variant of RNN language model outperformed the baseline model. Furthermore, the experiments also demonstrate that dynamic updates of an output layer help a model predict reappearing entities, whereas those of an input layer are effective to predict words following reappearing entities.",https://aclanthology.org/I17-1048,Asian Federation of Natural Language Processing,2017,November,Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 1: Long Papers),"Kobayashi, Sosuke  and
Okazaki, Naoaki  and
Inui, Kentaro",A Neural Language Model for Dynamically Representing the Meanings of Unknown Words and Entities in a Discourse,,I17,217
2021.dash-1.2,"['Learning Paradigms', 'Data Management and Generation', 'Model Architectures']","['Data Preparation', 'Active Learning']",['Annotation Processes'],"When a NLU model is updated, new utterances must be annotated to be included for training. However, manual annotation is very costly. We evaluate a semi-supervised learning workflow with a human in the loop in a production environment. The previous NLU model predicts the annotation of the new utterances, a human then reviews the predicted annotation. Only when the NLU prediction is assessed as incorrect the utterance is sent for human annotation. Experimental results show that the proposed workflow boosts the performance of the NLU model while significantly reducing the annotation volume. Specifically, in our setup, we see improvements of up to 14.16% for a recall-based metric and up to 9.57% for a F1score based metric, while reducing the annotation volume by 97% and overall cost by 60% for each iteration.",https://aclanthology.org/2021.dash-1.2,Association for Computational Linguistics,2021,June,Proceedings of the Second Workshop on Data Science with Human in the Loop: Language Advances,"Weber, Verena  and
Piovano, Enrico  and
Bradford, Melanie",It is better to Verify: Semi-Supervised Learning with a human in the loop for large-scale NLU models,10.18653/v1/2021.dash-1.2,dash,515
2021.socialnlp-1.3,"['Data Management and Generation', 'Low-resource Languages', 'Model Architectures', 'Domain-specific NLP']","['Data Preparation', 'NLP for News and Media']",,"Ranking the user comments posted on a news article is important for online news services because comment visibility directly affects the user experience. Research on ranking comments with different metrics to measure the comment quality has shown ""constructiveness"" used in argument analysis is promising from a practical standpoint. In this paper, we report a case study in which this constructiveness is examined in the real world. Specifically, we examine an in-house competition to improve the performance of ranking constructive comments and demonstrate the effectiveness of the best obtained model for a commercial service. * Equal contribution. 1 https://news.yahoo.co.jp/",https://aclanthology.org/2021.socialnlp-1.3,Association for Computational Linguistics,2021,June,Proceedings of the Ninth International Workshop on Natural Language Processing for Social Media,"Kobayashi, Hayato  and
Taguchi, Hiroaki  and
Tabuchi, Yoshimune  and
Koleejan, Chahine  and
Kobayashi, Ken  and
Fujita, Soichiro  and
Murao, Kazuma  and
Masuyama, Takeshi  and
Yatsuka, Taichi  and
Okumura, Manabu  and
Sekine, Satoshi",A Case Study of In-House Competition for Ranking Constructive Comments in a News Service,10.18653/v1/2021.socialnlp-1.3,socialnlp,40
2020.eamt-1.45,"['Machine Translation (MT)', 'Data Management and Generation', 'Low-resource Languages']",['Data Analysis'],,"This article describes the results of a workshop in which 50 translators tested two experimental translation interfaces, as part of a project which aimed at studying the details of editing work. In this work, editing is defined as a selection of four actions: deleting, inserting, moving and replacing words. Four texts, machine-translated from English into European Portuguese, were post-edited in four different sessions in which each translator swapped between texts and two work modes. One of the work modes involved a typical auto-complete feature, and the other was based on the four actions. The participants answered surveys before, during and after the workshop. A descriptive analysis of the answers to the surveys and of the logs recorded during the experiments was performed. The four editing actions mode is shown to be more intrusive, but to allow for more planned decisions: although they take more time in this mode, translators hesitate less and make fewer edits. The article shows the usefulness of the approach for research on the editing task.",https://aclanthology.org/2020.eamt-1.45,European Association for Machine Translation,2020,November,Proceedings of the 22nd Annual Conference of the European Association for Machine Translation,"Carmo, F{\'e}lix Do",Comparing Post-editing based on Four Editing Actions against Translating with an Auto-Complete Feature,,eamt,309
2021.eacl-main.73,"['Text Generation', 'Model Architectures']",['Text Style Transfer'],,"Author stylized rewriting is the task of rewriting an input text in a particular author's style. Recent works in this area have leveraged Transformer-based language models in a denoising autoencoder setup to generate author stylized text without relying on a parallel corpus of data. However, these approaches are limited by the lack of explicit control of target attributes and being entirely data-driven. In this paper, we propose a Director-Generator framework to rewrite content in the target author's style, specifically focusing on certain target attributes. We show that our proposed framework works well even with a limitedsized target author corpus. Our experiments on corpora consisting of relatively small-sized text authored by three distinct authors show significant improvements upon existing works to rewrite input texts in target author's style. Our quantitative and qualitative analyses further show that our model has better meaning retention and results in more fluent generations.",https://aclanthology.org/2021.eacl-main.73,Association for Computational Linguistics,2021,April,Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume,"Singh, Hrituraj  and
Verma, Gaurav  and
Garimella, Aparna  and
Srinivasan, Balaji Vasan",DRAG: Director-Generator Language Modelling Framework for Non-Parallel Author Stylized Rewriting,10.18653/v1/2021.eacl-main.73,eacl,779
O17-1028,"['Domain-specific NLP', 'Information Extraction', 'Automatic Text Summarization', 'Data Management and Generation']",['Data Preparation'],['Annotation Processes'],"Student feedback is an essential part of the instructor -student relationship. Traditionally student feedback is manually summarized by instructors, which is time consuming. Automatic student feedback summarization provides a potential solution to this. For summarizing student feedback, first, the opinion targets should be identified and extracted. In this context, opinion targets such as ""lecture slides"", ""teaching style"" are the important key points in the feedback that the students have shown their sentiment towards. In this paper, we focus on the opinion target extraction task of general student feedback. We model this problem as an information extraction task and extract opinion targets using a Conditional Random Fields CRF classifier. Our results show that this classifier outperforms the state-of-the-art techniques for student feedback summarization.",https://aclanthology.org/O17-1028,The Association for Computational Linguistics and Chinese Language Processing (ACLCLP),2017,November,Proceedings of the 29th Conference on Computational Linguistics and Speech Processing ({ROCLING} 2017),"Chathuranga, Janaka  and
Ediriweera, Shanika  and
Munasinghe, Pranidhith  and
Hasantha, Ravindu  and
Ranathunga, Surangika",Opinion Target Extraction for Student Course Feedback,,O17,36
2020.blackboxnlp-1.31,"['Evaluation Techniques', 'Multilingual NLP', 'Low-resource Languages']",,,"Recently, large-scale pre-trained neural network models such as BERT have achieved many state-of-the-art results in natural language processing. Recent work has explored the linguistic capacities of these models. However, no work has focused on the ability of these models to generalize these capacities to novel words. This type of generalization is exhibited by humans Berko, 1958 , and is intimately related to morphology-humans are in many cases able to identify inflections of novel words in the appropriate context. This type of morphological capacity has not been previously tested in BERT models, and is important for morphologically-rich languages, which are under-studied in the literature regarding BERT's linguistic capacities. In this work, we investigate this by considering monolingual and multilingual BERT models' abilities to agree in number with novel plural words in English, French, German, Spanish, and Dutch. We find that many models are not able to reliably determine plurality of novel words, suggesting potential deficiencies in the morphological capacities of BERT models.",https://aclanthology.org/2020.blackboxnlp-1.31,Association for Computational Linguistics,2020,November,Proceedings of the Third BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP,"Haley, Coleman",This is a BERT. Now there are several of them. Can they generalize to novel words?,10.18653/v1/2020.blackboxnlp-1.31,blackboxnlp,603
2022.ecnlp-1.7,"['Automatic Text Summarization', 'Domain-specific NLP', 'Dialogue Systems', 'Data Management and Generation', 'Learning Paradigms', 'Model Architectures']","['Extractive Text Summarization', 'Data Preparation', 'Transformer Models', 'Unsupervised Learning', 'Response Generation']",,"We model product reviews to generate comparative responses consisting of positive and negative experiences regarding the product. Specifically, we generate a single-sentence, comparative response from a given positive and a negative opinion. We contribute the first dataset for this task of Comparative Snippet Generation from contrasting opinions regarding a product, and a performance analysis of a pre-trained BERT model to generate such snippets.",https://aclanthology.org/2022.ecnlp-1.7,Association for Computational Linguistics,2022,May,Proceedings of The Fifth Workshop on e-Commerce and NLP (ECNLP 5),"Jain, Saurabh  and
Miao, Yisong  and
Kan, Min-Yen",Comparative Snippet Generation,10.18653/v1/2022.ecnlp-1.7,ecnlp,775
2020.trac-1.1,"['Domain-specific NLP', 'Low-resource Languages', 'Classification Applications', 'Model Architectures']","['Hate and Offensive Speech Detection', 'NLP for News and Media']",['NLP for Social Media'],"In this paper, we present the report and findings of the Shared Task on Aggression and Gendered Aggression Identification organised as part of the Second Workshop on Trolling, Aggression and Cyberbullying TRAC -2 at LREC 2020. The task consisted of two sub-tasks -aggression identification sub-task A and gendered aggression identification sub-task B -in three languages -Bengali, Hindi and English. For this task, the participants were provided with a dataset of approximately 5,000 instances from YouTube comments in each language. For testing, approximately 1,000 instances were provided in each language for each sub-task. A total of 70 teams registered to participate in the task and 19 teams submitted their test runs. The best system obtained a weighted F-score of approximately 0.80 in sub-task A for all the three languages. While approximately 0.87 in sub-task B for all the three languages.",https://aclanthology.org/2020.trac-1.1,European Language Resources Association (ELRA),2020,May,"Proceedings of the Second Workshop on Trolling, Aggression and Cyberbullying","Kumar, Ritesh  and
Ojha, Atul Kr.  and
Malmasi, Shervin  and
Zampieri, Marcos",Evaluating Aggression Identification in Social Media,,trac,790
2022.mia-1.9,"['Multilingual NLP', 'Question Answering (QA)', 'Learning Paradigms', 'Low-resource Languages', 'Cross-lingual Application', 'Model Architectures']",['Open-Domain QA'],,"People speaking different kinds of languages search for information in a cross-lingual manner. They tend to ask questions in their language and expect the answer to be in the same language, despite the evidence lying in another language. In this paper, we present our approach for this task of cross-lingual opendomain question-answering. Our proposed method employs a passage reranker, the fusionin-decoder technique for generation, and a wiki data entity-based post-processing system to tackle the inability to generate entities across all languages. Our end-2-end pipeline shows an improvement of 3 and 4.6 points on F1 and EM metrics respectively, when compared with the baseline CORA model on the XOR-TyDi dataset. We also evaluate the effectiveness of our proposed techniques in the zero-shot setting using the MKQA dataset and show an improvement of 5 points in F1 for high-resource and 3 points improvement for low-resource zero-shot languages. Our team, CMUmQA's submission in the MIA-Shared task ranked 1st in the constrained setup for the dev and 2nd in the test setting.",https://aclanthology.org/2022.mia-1.9,Association for Computational Linguistics,2022,July,Proceedings of the Workshop on Multilingual Information Access (MIA),"Agarwal, Sumit  and
Tripathi, Suraj  and
Mitamura, Teruko  and
Rose, Carolyn Penstein",Zero-shot cross-lingual open domain question answering,10.18653/v1/2022.mia-1.9,mia,1010
2020.nlpmc-1.5,"['Domain-specific NLP', 'Model Architectures', 'Dialogue Systems', 'Knowledge Representation and Reasoning']","['Ontologies', 'Medical and Clinical NLP']",,"HIV human immunodeficiency virus can damage a human's immune system and cause Acquired Immunodeficiency Syndrome AIDS which could lead to severe outcomes, including death. While HIV infections have decreased over the last decade, there is still a significant population where the infection permeates. PrEP and PEP are two proven preventive measures introduced that involve periodic dosage to stop the onset of HIV infection. However, the adherence rates for this medication is low in part due to the lack of information about the medication. There exist several communication barriers that prevent patient-provider communication from happening. In this work, we present our ontologybased method for automating the communication of this medication that can be deployed for live conversational agents for PrEP and PEP. This method facilitates a model of automated conversation between the machine and user can also answer relevant questions.",https://aclanthology.org/2020.nlpmc-1.5,Association for Computational Linguistics,2020,July,Proceedings of the First Workshop on Natural Language Processing for Medical Conversations,"Amith, Muhammad  and
Cui, Licong  and
Roberts, Kirk  and
Tao, Cui",Towards an Ontology-based Medication Conversational Agent for PrEP and PEP,10.18653/v1/2020.nlpmc-1.5,nlpmc,278
2022.dravidianlangtech-1.38,"['Ethics', 'Low-resource Languages', 'Classification Applications', 'Model Architectures']","['Hate and Offensive Speech Detection', 'Recurrent Neural Networks (RNNs)']",['Long Short-Term Memory (LSTM) Models'],"Identifying offensive speech is an exciting and essential area of research, with ample traction in recent times. This paper presents our system submission to the subtask 1, focusing on using supervised approaches for extracting Offensive spans from code-mixed Tamil-English comments. To identify offensive spans, we developed the Bidirectional Long Short-Term Memory BiLSTM model with Glove Embedding. With this method, the developed system achieved an overall F1 of 0.1728. Additionally, for comments with less than 30 characters, the developed system shows an F1 of 0.3890, competitive with other submissions.",https://aclanthology.org/2022.dravidianlangtech-1.38,Association for Computational Linguistics,2022,May,Proceedings of the Second Workshop on Speech and Language Technologies for Dravidian Languages,"Rajalakshmi, Ratnavel  and
More, Mohit  and
Shrikriti, Bhamatipati  and
Saharan, Gitansh  and
Samyuktha, Hanchate  and
Nandy, Sayantan",DLRG@TamilNLP-ACL2022: Offensive Span Identification in Tamil usingBiLSTM-CRF approach,10.18653/v1/2022.dravidianlangtech-1.38,dravidianlangtech,1408
2020.eamt-1.55,['Machine Translation (MT)'],"['Neural MT (NMT)', 'Statistical MT (SMT)']",,"In this paper the MTUOC project, aiming to provide an easy integration of neural and statistical machine translation systems, is presented. Almost all the required software to train and use neural and statistical MT systems is released under free licences. However, their use is not always easy and intuitive and medium-high specialized skills are required. MTUOC project provides simplified scripts for preprocessing and training MT systems, and a server and client for easy use of the trained systems. The server is compatible with popular CAT tools for a seamless integration. The project also distributes some free engines.",https://aclanthology.org/2020.eamt-1.55,European Association for Machine Translation,2020,November,Proceedings of the 22nd Annual Conference of the European Association for Machine Translation,"Oliver, Antoni",MTUOC: easy and free integration of NMT systems in professional translation environments,,eamt,1418
2020.semeval-1.92,"['Information Extraction', 'Model Architectures']",,,"Definition Extraction systems are a valuable knowledge source for both humans and algorithms. In this paper we describe our submissions to the DeftEval shared task SemEval-2020 Task 6, which is evaluated on an English textbook corpus. We provide a detailed explanation of our system for the joint extraction of definition concepts and the relations among them. Furthermore we provide an ablation study of our model variations and describe the results of an error analysis.",https://aclanthology.org/2020.semeval-1.92,International Committee for Computational Linguistics,2020,December,Proceedings of the Fourteenth Workshop on Semantic Evaluation,"H{\""u}bner, Marc  and
Alt, Christoph  and
Schwarzenberg, Robert  and
Hennig, Leonhard",Defx at SemEval-2020 Task 6: Joint Extraction of Concepts and Relations for Definition Extraction,10.18653/v1/2020.semeval-1.92,semeval,750
2022.slpat-1.6,"['Machine Translation (MT)', 'Domain-specific NLP', 'Low-resource Languages']",['Medical and Clinical NLP'],,"Communication between physician and patients can lead to misunderstandings, especially for disabled people. An automatic system that translates natural language into a pictographic language is one of the solutions that could help to overcome this issue. In this preliminary study, we present the French version of a translation system using the Arasaac pictographs and we investigate the strategies used by speech therapists to translate into pictographs. We also evaluate the medical coverage of this tool for translating physician questions and patient instructions.",https://aclanthology.org/2022.slpat-1.6,Association for Computational Linguistics,2022,May,Ninth Workshop on Speech and Language Processing for Assistive Technologies (SLPAT-2022),"Norr{\'e}, Magali  and
Vandeghinste, Vincent  and
Fran{\c{c}}ois, Thomas  and
Pierrette, Bouillon",Investigating the Medical Coverage of a Translation System into Pictographs for Patients with an Intellectual Disability,10.18653/v1/2022.slpat-1.6,slpat,635
2021.semeval-1.156,['Classification Applications'],"['Humor Detection', 'Hate and Offensive Speech Detection']",,"Humor recognition is a challenging task in natural language processing. This document presents my approaches to detect and rate humor and offense from the given English text. This task includes 2 tasks: task 1 which contains 3 subtasks 1a, 1b, and 1c, and task 2. Subtask 1a and 1c can be regarded as classification problems and take ALBERT as the basic model. Subtask 1b and 2 can be viewed as regression issues and take RoBERTa as the basic model. And finally, team-Gulu scores in subtask 1a with a weighted average F1 score of 0.9190, in subtask 1b with an RMSE score of 0.7405, in subtask 1c with a weighted average F1 score of 0.5561, and in subtask 2 with an RMSE score of 0.5807 on the private leader board.",https://aclanthology.org/2021.semeval-1.156,Association for Computational Linguistics,2021,August,Proceedings of the 15th International Workshop on Semantic Evaluation (SemEval-2021),"Yang, Maoqin",Gulu at SemEval-2021 Task 7: Detecting and Rating Humor and Offense,10.18653/v1/2021.semeval-1.156,semeval,1429
2022.acl-short.44,"['Embeddings', 'Data Management and Generation', 'Learning Paradigms']","['Data Augmentation', 'Sentence Embeddings', 'Unsupervised Learning']",,"In this paper, we propose Self-Contrastive Decorrelation SCD, a self-supervised approach. Given an input sentence, it optimizes a joint self-contrastive and decorrelation objective. Learning a representation is facilitated by leveraging the contrast arising from the instantiation of standard dropout at different rates. The proposed method is conceptually simple yet empirically powerful. It achieves comparable results with state-of-the-art methods on multiple benchmarks without using contrastive pairs. This study opens up avenues for efficient self-supervised learning methods that are more robust than current contrastive methods. 1",https://aclanthology.org/2022.acl-short.44,Association for Computational Linguistics,2022,May,Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),"Klein, Tassilo  and
Nabi, Moin",SCD: Self-Contrastive Decorrelation of Sentence Embeddings,10.18653/v1/2022.acl-short.44,acl,242
J18-3006,"['Machine Translation (MT)', 'Learning Paradigms', 'Model Architectures']",['Unsupervised Learning'],,"Orthographic similarities across languages provide a strong signal for unsupervised probabilistic transduction decipherment for closely related language pairs. The existing decipherment models, however, are not well suited for exploiting these orthographic similarities. We propose a loglinear model with latent variables that incorporates orthographic similarity features. Maximum likelihood training is computationally expensive for the proposed log-linear model. To address this challenge, we perform approximate inference via Markov chain Monte Carlo sampling and contrastive divergence. Our results show that the proposed log-linear model with contrastive divergence outperforms the existing generative decipherment models by exploiting the orthographic features. The model both scales to large vocabularies and preserves accuracy in low-and no-resource contexts.",https://aclanthology.org/J18-3006,MIT Press,2018,September,,"Naim, Iftekhar  and
Riley, Parker  and
Gildea, Daniel",Feature-Based Decipherment for Machine Translation,10.1162/coli_a_00326,J18,1492
2021.dash-1.9,['Data Management and Generation'],['Data Analysis'],,"From tweets to product reviews, text is ubiquitous on the web and often contains valuable information for both enterprises and consumers. However, the online text is generally noisy and incomplete, requiring users to process and analyze the data to extract insights. While there are systems effective for different stages of text analysis, users lack extensible platforms to support interactive text analysis workflows end-to-end. To facilitate integrated text analytics, we introduce LEAM, which aims at combining the strengths of spreadsheets, computational notebooks, and interactive visualizations. LEAM supports interactive analysis via GUI-based interactions and provides a declarative specification language, implemented based on a visual text algebra, to enable user-guided analysis. We evaluate LEAM through two case studies using two popular Kaggle text analytics workflows to understand the strengths and weaknesses of the system.",https://aclanthology.org/2021.dash-1.9,Association for Computational Linguistics,2021,June,Proceedings of the Second Workshop on Data Science with Human in the Loop: Language Advances,"Griggs, Peter  and
Demiralp, Cagatay  and
Rahman, Sajjadur","Towards integrated, interactive, and extensible text data analytics with Leam",10.18653/v1/2021.dash-1.9,dash,228
P18-2052,"['Machine Translation (MT)', 'Low-resource Languages']",['Neural MT (NMT)'],,"We empirically investigate learning from partial feedback in neural machine translation NMT, when partial feedback is collected by asking users to highlight a correct chunk of a translation. We propose a simple and effective way of utilizing such feedback in NMT training. We demonstrate how the common machine translation problem of domain mismatch between training and deployment can be reduced solely based on chunk-level user feedback. We conduct a series of simulation experiments to test the effectiveness of the proposed method. Our results show that chunk-level feedback outperforms sentence based feedback by up to 2.61% BLEU absolute.",https://aclanthology.org/P18-2052,Association for Computational Linguistics,2018,July,Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),"Petrushkov, Pavel  and
Khadivi, Shahram  and
Matusov, Evgeny",Learning from Chunk-based Feedback in Neural Machine Translation,10.18653/v1/P18-2052,P18,804
2021.acl-long.466,"['Commonsense Reasoning', 'Model Architectures', 'Text Generation']",['Data-to-Text Generation'],['Table-to-Text Generation'],"Table - to-text generation aims at automatically generating natural text to help people conveniently obtain salient information in tables. Although neural models for table-to-text have achieved remarkable progress, some problems are still overlooked. Previous methods cannot deduce the factual results from the entity's player or team performance and the relations between entities. To solve this issue, we first build an entity graph from the input tables and introduce a reasoning module to perform reasoning on the graph. Moreover, there are different relations e.g., the numeric size relation and the importance relation between records in different dimensions. And these relations may contribute to the data-to-text generation. However, it is hard for a vanilla encoder to capture these. Consequently, we propose to utilize two auxiliary tasks, Number Ranking NR and Importance Ranking IR, to supervise the encoder to capture the different relations. Experimental results on ROTOWIRE and RW-FG show that our method not only has a good generalization but also outperforms previous methods on several metrics: BLEU, Content Selection, Content Ordering.",https://aclanthology.org/2021.acl-long.466,Association for Computational Linguistics,2021,August,Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),"Li, Liang  and
Ma, Can  and
Yue, Yinliang  and
Hu, Dayong",Improving Encoder by Auxiliary Supervision Tasks for Table-to-Text Generation,10.18653/v1/2021.acl-long.466,acl,1389
2020.nlposs-1.4,['Model Architectures'],,,"The recent progress in natural language processing research has been supported by the development of a rich open source ecosystem in Python. Libraries allowing NLP practitioners but also non-specialists to leverage stateof-the-art models have been instrumental in the democratization of this technology. The maturity of the open-source NLP ecosystem however varies between languages. This work proposes a new open-source library aimed at bringing state-of-the-art NLP to Rust. Rust is a systems programming language for which the foundations required to build machine learning applications are available but still lacks readyto-use, end-to-end NLP libraries. The proposed library, rust-bert, implements modern language models and ready-to-use pipelines for example translation or summarization. This allows further development by the Rust community from both NLP experts and nonspecialists. It is hoped that this library will accelerate the development of the NLP ecosystem in Rust. The library is under active development and available at https://github. com/guillaume-be/rust-bert.",https://aclanthology.org/2020.nlposs-1.4,Association for Computational Linguistics,2020,November,Proceedings of Second Workshop for NLP Open Source Software (NLP-OSS),"Becquin, Guillaume",End-to-end NLP Pipelines in Rust,10.18653/v1/2020.nlposs-1.4,nlposs,579
D16-1034,"['Text Generation', 'Data Management and Generation', 'Low-resource Languages', 'Classification Applications']","['Data Preparation', 'Text Simplification']",,"In this paper we present PaCCSS-IT, a Parallel Corpus of Complex-Simple Sentences for ITalian. To build the resource we develop a new method for automatically acquiring a corpus of complex-simple paired sentences able to intercept structural transformations and particularly suitable for text simplification. The method requires a wide amount of texts that can be easily extracted from the web making it suitable also for less-resourced languages. We test it on the Italian language making available the biggest Italian corpus for automatic text simplification.",https://aclanthology.org/D16-1034,Association for Computational Linguistics,2016,November,Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing,"Brunato, Dominique  and
Cimino, Andrea  and
Dell{'}Orletta, Felice  and
Venturi, Giulia",PaCCSS-IT: A Parallel Corpus of Complex-Simple Sentences for Automatic Text Simplification,10.18653/v1/D16-1034,D16,1232
2021.eacl-demos.1,['Parsing'],['Discourse Parsing'],,"I present rst-workbench, a software package that simplifies the installation and usage of numerous end-to-end Rhetorical Structure Theory RST parsers. 1 The tool offers a webbased interface that allows users to enter text and let multiple RST parsers generate analyses concurrently. The resulting RST trees can be compared visually, manually post-edited in the browser and stored for later usage.",https://aclanthology.org/2021.eacl-demos.1,Association for Computational Linguistics,2021,April,Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: System Demonstrations,"Neumann, Arne",Using and comparing Rhetorical Structure Theory parsers with rst-workbench,10.18653/v1/2021.eacl-demos.1,eacl,172
2020.law-1.2,"['Domain-specific NLP', 'Knowledge Representation and Reasoning', 'Data Management and Generation']","['Data Preparation', 'Link Prediction']",['Annotation Processes'],"Research in Computational Linguistics is dependent on text corpora for training and testing new tools and methodologies. While there exists a plethora of annotated linguistic information, these corpora are often not interoperable without significant manual work. Moreover, these annotations might have evolved into different versions, making it challenging for researchers to know the data's provenance. This paper addresses this issue with a case study on event annotated corpora and by creating a new, more interoperable representation of this data in the form of nanopublications. We demonstrate how linguistic annotations from separate corpora can be reliably linked from the start, and thereby be accessed and queried as if they were a single dataset. We describe how such nanopublications can be created and demonstrate how SPARQL queries can be performed to extract interesting content from the new representations. The queries show that information of multiple corpora can be retrieved more easily and effectively because the information of different corpora is represented in a uniform data format.",https://aclanthology.org/2020.law-1.2,Association for Computational Linguistics,2020,December,Proceedings of the 14th Linguistic Annotation Workshop,"Lek, Timo  and
de Groot, Anna  and
Kuhn, Tobias  and
Morante, Roser",Provenance for Linguistic Corpora through Nanopublications,,law,837
2021.icnlsp-1.17,"['Information Extraction', 'Data Management and Generation', 'Low-resource Languages']","['Data Preparation', 'Anaphora Resolution']",['Annotation Processes'],"In this paper, we describe the different steps taken to build our annotated corpus which aims to treat a known linguistic phenomenon in Arabic texts called Anaphora. The objective behind the creation of this corpus 1 is to fill the lack of resources concerning the resolution anaphora especially pronominal and verbal in the Modern Standard Arabic language and this is by creating a newly annotated corpus that we have called A 3 C which contains the anaphoric relations. To satisfy this objective, we created A 3 T, an anaphoric annotating tool that uses linguistic and statistical rules to automatically detect anaphors and their referents. After that, we resort to human specialists to verify and correct our A 3 T annotation's errors for the corpus's credibility. This study discusses novel features that can aid in determining the best reference, as well as the problem of the lack of resources for verbal anaphora. Varieties of Anaphora in Arabic text What makes the anaphora resolution mechanism complex in natural language processing in general and in Arabic, in particular, is the fact that it can",https://aclanthology.org/2021.icnlsp-1.17,Association for Computational Linguistics,2021,12--13 November,Proceedings of The Fourth International Conference on Natural Language and Speech Processing (ICNLSP 2021),"Cheragui, Mohamed Amine  and
Dahou, Abdelhalim Hafedh  and
Abdelmoazz, Mohamed",A3C: Arabic Anaphora Annotated Corpus,,icnlsp,111
2020.clinicalnlp-1.27,"['Domain-specific NLP', 'Data Management and Generation', 'Classification Applications', 'Learning Paradigms']","['Data Preparation', 'Data Augmentation', 'Medical and Clinical NLP']",,"Eligibility criteria in the clinical trials specify the characteristics that a patient must or must not possess in order to be treated according to a standard clinical care guideline. As the process of manual eligibility determination is time-consuming, automatic structuring of the eligibility criteria into various semantic categories or aspects is the need of the hour. Existing methods use hand-crafted rules and feature-based statistical machine learning methods to dynamically induce semantic aspects. However, in order to deal with paucity of aspect-annotated clinical trials data, we propose a novel weakly-supervised co-training based method which can exploit a large pool of unlabeled criteria sentences to augment the limited supervised training data, and consequently enhance the performance. Experiments with 0.2M criteria sentences show that the proposed approach outperforms the competitive supervised baselines by 12% in terms of micro-averaged F1 score for all the aspects. Probing deeper into analysis, we observe domain-specific information boosts up the performance by a significant margin.",https://aclanthology.org/2020.clinicalnlp-1.27,Association for Computational Linguistics,2020,November,Proceedings of the 3rd Clinical Natural Language Processing Workshop,"Dasgupta, Tirthankar  and
Mondal, Ishani  and
Naskar, Abir  and
Dey, Lipika",Extracting Semantic Aspects for Structured Representation of Clinical Trial Eligibility Criteria,10.18653/v1/2020.clinicalnlp-1.27,clinicalnlp,1299
